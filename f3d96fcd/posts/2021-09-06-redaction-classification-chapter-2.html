<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Strick van Linschoten">
<meta name="dcterms.date" content="2021-09-06">
<meta name="description" content="How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training">

<title>Alex Strick van Linschoten - Training a classifier to detect redacted documents with fastai</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script defer="" data-domain="mlops.systems" src="https://plausible.io/js/script.js"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Alex Strick van Linschoten - Training a classifier to detect redacted documents with fastai">
<meta property="og:description" content="How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training">
<meta property="og:image" content="https://mlops.systems/posts/redacted_section.png">
<meta property="og:site-name" content="Alex Strick van Linschoten">
<meta property="og:image:height" content="161">
<meta property="og:image:width" content="283">
<meta name="twitter:title" content="Alex Strick van Linschoten - Training a classifier to detect redacted documents with fastai">
<meta name="twitter:description" content="How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training">
<meta name="twitter:image" content="https://mlops.systems/posts/redacted_section.png">
<meta name="twitter:creator" content="@strickvl">
<meta name="twitter:image-height" content="161">
<meta name="twitter:image-width" content="283">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Alex Strick van Linschoten</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../til.html">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/strickvl"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://sigmoid.social/web/@alexstrick"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/strickvl"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mlops.systems/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Training a classifier to detect redacted documents with fastai</h1>
                  <div>
        <div class="description">
          How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">fastai</div>
                <div class="quarto-category">redactionmodel</div>
                <div class="quarto-category">computervision</div>
                <div class="quarto-category">datalabelling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Strick van Linschoten </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 6, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-problem-domain-image-redaction" id="toc-the-problem-domain-image-redaction" class="nav-link active" data-scroll-target="#the-problem-domain-image-redaction">The Problem Domain: Image Redaction</a></li>
  <li><a href="#getting-the-data" id="toc-getting-the-data" class="nav-link" data-scroll-target="#getting-the-data">Getting the Data</a></li>
  <li><a href="#labelling-the-images-with-prodigy" id="toc-labelling-the-images-with-prodigy" class="nav-link" data-scroll-target="#labelling-the-images-with-prodigy">Labelling the images with Prodigy</a></li>
  <li><a href="#transferring-the-data-to-paperspace-with-magic-wormhole" id="toc-transferring-the-data-to-paperspace-with-magic-wormhole" class="nav-link" data-scroll-target="#transferring-the-data-to-paperspace-with-magic-wormhole">Transferring the data to Paperspace with <code>magic-wormhole</code></a></li>
  <li><a href="#using-the-labelled-data-in-our-training" id="toc-using-the-labelled-data-in-our-training" class="nav-link" data-scroll-target="#using-the-labelled-data-in-our-training">Using the labelled data in our training</a></li>
  <li><a href="#experimenting-with-augmentations" id="toc-experimenting-with-augmentations" class="nav-link" data-scroll-target="#experimenting-with-augmentations">Experimenting with augmentations</a></li>
  <li><a href="#experimenting-with-different-architectures" id="toc-experimenting-with-different-architectures" class="nav-link" data-scroll-target="#experimenting-with-different-architectures">Experimenting with different architectures</a></li>
  <li><a href="#hosting-the-model-with-mybinder" id="toc-hosting-the-model-with-mybinder" class="nav-link" data-scroll-target="#hosting-the-model-with-mybinder">Hosting the model with MyBinder</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next steps</a></li>
  <li><a href="#footnotes" id="toc-footnotes" class="nav-link" data-scroll-target="#footnotes">Footnotes</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>I am working my way through <a href="https://course.fast.ai">the fastai course</a> as part of an <a href="https://www.meetup.com/delft-fast-ai-study-group">online meetup group</a> I host.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’).</p>
<p><a href="https://twitter.com/jeremyphoward">Jeremy Howard</a>, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course <a href="https://sanyambhutani.com/how-not-to-do-fast-ai--or-any-ml-mooc-/">emphasise repeatedly</a>.)</p>
<p>I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not.</p>
<section id="the-problem-domain-image-redaction" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-domain-image-redaction">The Problem Domain: Image Redaction</h2>
<p>Under the <a href="https://en.wikipedia.org/wiki/Freedom_of_Information_Act_(United_States)">Freedom of Information Act</a> (FOIA), individuals can request records and information from the US government.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> This is <a href="https://www.esd.whs.mil/FOIA/Reading-Room/Reading-Room-List_2/">one collection</a> of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits <a href="https://www.esd.whs.mil/FOIA/Reading-Room/Reading-Room-List/UFO/">here</a>.</p>
<p>Quite often, however, these images are censored or redacted.</p>
<p><img src="redacted_sample.png" title="A name and an email address have been redacted here" class="img-fluid"></p>
<p>Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case.</p>
<p>It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless.</p>
<p>In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted.</p>
</section>
<section id="getting-the-data" class="level2">
<h2 class="anchored" data-anchor-id="getting-the-data">Getting the Data</h2>
<p>The first thing I did to gather my data was to download the PDF documents available on <a href="https://www.esd.whs.mil/FOIA/Reading-Room/Reading-Room-List_2/">this site</a>. I knew that they contained examples of redactions in FOIA documents. I used <a href="https://support.apple.com/en-gb/guide/automator/welcome/mac">Automator</a> to split the PDF files up into individual images.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training.</p>
<p>Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end.</p>
<p>At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with.</p>
</section>
<section id="labelling-the-images-with-prodigy" class="level2">
<h2 class="anchored" data-anchor-id="labelling-the-images-with-prodigy">Labelling the images with Prodigy</h2>
<p>I had used Explosion.ai’s <a href="https://prodi.gy">Prodigy data labelling tool</a> in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but <a href="https://twitter.com/_inesmontani">Prodigy co-creator Ines</a> helped me <a href="https://support.prodi.gy/t/labelling-a-set-of-images-classification/4608/1">work through those queries</a> and I was up and running pretty quickly.</p>
<p><img src="prodigy-interface.png" title="The interface for image classification looked like this" class="img-fluid"></p>
<p>It took about three hours to annotate some 4600+ images. Then I could export a <code>.jsonl</code> file that contained the individual annotations for whether a particular image contained a redaction or not:</p>
<p><img src="annotations_jsonl.png" class="img-fluid"></p>
<p>From that point it was pretty trivial to parse the file (using the <a href="https://pypi.org/project/json-lines/"><code>json-lines</code> package</a>), and to resize the images down further in order to separate redacted from unredacted:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json_lines</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_resized_image_file(location_path):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    basewidth <span class="op">=</span> <span class="dv">800</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> Image.<span class="bu">open</span>(record[<span class="st">'image'</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    wpercent <span class="op">=</span> (basewidth <span class="op">/</span> <span class="bu">float</span>(img.size[<span class="dv">0</span>]))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    hsize <span class="op">=</span> <span class="bu">int</span>((<span class="bu">float</span>(img.size[<span class="dv">1</span>]) <span class="op">*</span> <span class="bu">float</span>(wpercent)))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> img.resize((basewidth, hsize), Image.ANTIALIAS)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    img.save(location_path)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">'/my_projects_directory/redaction-model'</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>redacted_path <span class="op">=</span> path <span class="op">+</span> <span class="st">"/redaction_training_data/"</span> <span class="op">+</span> <span class="st">"redacted"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>unredacted_path <span class="op">=</span> path <span class="op">+</span> <span class="st">"/redaction_training_data/"</span> <span class="op">+</span> <span class="st">"unredacted"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path <span class="op">+</span> <span class="st">"/"</span> <span class="op">+</span> <span class="st">"annotations.jsonl"</span>, <span class="st">"rb"</span>) <span class="im">as</span> f:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> record <span class="kw">in</span> json_lines.reader(f):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> record[<span class="st">"answer"</span>] <span class="op">==</span> <span class="st">"accept"</span>:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>            save_resized_image_file(Path(redacted_path <span class="op">+</span> <span class="st">"/"</span> <span class="op">+</span> record[<span class="st">'meta'</span>][<span class="st">'file'</span>]))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            save_resized_image_file(Path(unredacted_path <span class="op">+</span> <span class="st">"/"</span> <span class="op">+</span> record[<span class="st">'meta'</span>][<span class="st">'file'</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="transferring-the-data-to-paperspace-with-magic-wormhole" class="level2">
<h2 class="anchored" data-anchor-id="transferring-the-data-to-paperspace-with-magic-wormhole">Transferring the data to Paperspace with <code>magic-wormhole</code></h2>
<p>Once I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer.</p>
<p>I used <a href="https://magic-wormhole.readthedocs.io"><code>magic-wormhole</code></a> to transfer the files over to my <a href="https://gradient.paperspace.com">Paperspace Gradient</a> machine. The files were only about 400MB in size so it took less than a minute to transfer the data.</p>
<p>Again, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then <a href="https://prodi.gy/docs/install#jupyterlab">use Prodigy from within my notebooks</a>.</p>
</section>
<section id="using-the-labelled-data-in-our-training" class="level2">
<h2 class="anchored" data-anchor-id="using-the-labelled-data-in-our-training">Using the labelled data in our training</h2>
<p>The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the <code>DataBlock</code> class instead of <code>ImageDataLoaders</code> for extra flexibility.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> Path(<span class="st">'redaction_training_data'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>foia_documents <span class="op">=</span> DataBlock(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    blocks<span class="op">=</span>(ImageBlock, CategoryBlock),</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    get_items<span class="op">=</span>get_image_files,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    splitter<span class="op">=</span>RandomSplitter(valid_pct<span class="op">=</span><span class="fl">0.2</span>, seed<span class="op">=</span><span class="dv">42</span>),</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    get_y<span class="op">=</span>parent_label,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    item_tfms<span class="op">=</span>Resize(<span class="dv">224</span>))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> foia_documents.dataloaders(path)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>foia_documents <span class="op">=</span> foia_documents.new(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    item_tfms<span class="op">=</span>Resize(<span class="dv">224</span>, method<span class="op">=</span><span class="st">'pad'</span>, pad_mode<span class="op">=</span><span class="st">'reflection'</span>),</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    batch_tfms<span class="op">=</span>aug_transforms(max_zoom<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> foia_documents.dataloaders(path)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>learn <span class="op">=</span> cnn_learner(dls, resnet18, metrics<span class="op">=</span>error_rate)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>learn.fine_tune(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The images get resized to 224x224 pixels, since this is the size that the <code>resnet</code> architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate.</p>
<p>I train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%.</p>
<p><img src="training_results.png" class="img-fluid"></p>
</section>
<section id="experimenting-with-augmentations" class="level2">
<h2 class="anchored" data-anchor-id="experimenting-with-augmentations">Experimenting with augmentations</h2>
<p>Initially I had been using the <code>RandomResizedCrop</code> transformation on the data, but I was reminded by someone in our group (<a href="http://fabacademy.org/2021/labs/ulb/students/jason-pettiaux/fastai/">Jason</a>) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found.</p>
<p>In the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of <a href="https://docs.fast.ai/vision.augment.html#Resize">the documentation</a> here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in.</p>
<p>I chose the <code>pad</code> method with the <code>reflection</code> mode since this seemed to give the best results. The <code>zeros</code> mode was too close to an actual redaction (i.e.&nbsp;a black box on the image) so I ruled that out pretty early on.</p>
</section>
<section id="experimenting-with-different-architectures" class="level2">
<h2 class="anchored" data-anchor-id="experimenting-with-different-architectures">Experimenting with different architectures</h2>
<p>The course mentions that architectures with more layers do exist. I saw that the next step up from <code>resnet18</code> was <code>resnet50</code>. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison.</p>
<p>The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with <code>resnet18</code>. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers.</p>
</section>
<section id="hosting-the-model-with-mybinder" class="level2">
<h2 class="anchored" data-anchor-id="hosting-the-model-with-mybinder">Hosting the model with MyBinder</h2>
<p>Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online.</p>
<p>Using <a href="https://mybinder.org">MyBinder</a> and the <a href="https://voila.readthedocs.io"><code>voila</code> library</a>, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit <a href="https://hub.gke2.mybinder.org/user/strickvl-binder-redaction-s1nr4p8k/voila/render/binder-redaction-classifier.ipynb?token=kReM2K-iSkmSjud5N28o8Q">this address</a> you’ll see an interface where you should first upload an image — i.e.&nbsp;a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true.</p>
<p><img src="mybinder_interface.png" title="Part of the MyBinder interface following a successful inference" class="img-fluid"></p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>I’m at the point in the course where I know enough to be dangerous (i.e.&nbsp;train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy:</p>
<ul>
<li>better augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied.</li>
<li>more labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me.</li>
<li>different redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong).</li>
</ul>
<p>Otherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted.</p>
<p>I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution.</p>
</section>
<section id="footnotes" class="level2">
<h2 class="anchored" data-anchor-id="footnotes">Footnotes</h2>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>You can find our thread in the fastai forum <a href="https://forums.fast.ai/t/virtual-study-group-delft-the-netherlands-europe-time-zone/90521">here</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Other countries have variations of this law, like <a href="https://www.gov.uk/make-a-freedom-of-information-request">this</a> from the United Kingdom.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>