[
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html",
    "href": "posts/2024-06-17-one-click-finetuning.html",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "",
    "text": "The last post in this series showed that finetuning an LLM needn’t be particularly difficult. I used axolotl to produce finetuned versions of Llama3, Mistral and TinyLlama models. During the course we were given a bunch of credits by various companies in the LLM and finetuning space. Among those were credits from some finetuning-as-a-service companies and I thought now might be a good time to try out these services now that I’ve done the process manually a few times.\nI picked three to try out: Predibase, OpenPipe and OpenAI. All were surprisingly similar in the approach they took. I’ll give a few details on the experience for each and how they compare to each other. With all the services, the process was roughly the same as when I did it manually:\nThe step I had the most trouble with was the custom data upload, since each provider wanted the data in a different format. Converting the data from the Pydantic models I had previously created was not a huge deal, but I wasn’t sure about the tradeoffs that I was making (or that were being made for me) by converting my data into these formats."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#predibase",
    "href": "posts/2024-06-17-one-click-finetuning.html#predibase",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "Predibase",
    "text": "Predibase\nI started with Predibase since I had enjoyed the talk Travis Addair had given during the course. Predibase is famous for their work on LORA adapters, particularly their demonstration of Lora Land where they gave some examples of how finetuned LORA models / adapters could outperform GPT-4.\nPredibase requires that the data you upload has certain column names depending on the task you select for the finetuning. At the moment they have instruction tuning and text completion as their two tasks, but it wasn’t clear to me which to select. (They also have a Colab notebook to help with constructing splits from your data.)\nOnce your data is ready and validated, you can select the model you want to finetune along with a few other hyperparameters. This is the full extent of what you can set from the web UI:\n\n\n\nScreenshot of Predibase website and the hyperparameters you can set\n\n\nThere’s also a helpful dataset preview pane to give a final sanity check for your data, to make sure that the inputs and outputs look what you’d expect:\n\n\n\nScreenshot of Predibase website and the dataset preview pane\n\n\nAs you’ll read in a little bit, this feature helps catch potentially costly errors before you start the finetuning process.\nOnce you click the button to start the training, there isn’t a great deal of information available to you beyond (eventually) a loss curve that you can see. I chose to finetune Qwen2 in Predibase and this took about 53 minutes using an A-100 GPU accelerator.\nOnce your model is ready, you can prompt the model in the UI, or using their REST API / Python SDK. They give code snippets prefilled with some dummy text that you can easily try out locally. Let’s show that here, but before you can run your inference query you have to first deploy the model. I hadn’t expected this extra step, and it takes a while to spin up since it’s deploying the adaptor along with the base model it was finetuned alongside. My Qwen2 model has a context window of 131072 tokens and supposedly would cost $3.90 per hour that it was up (as a dedicated deployment).\nLet’s show the results we got:\n\npr1 = \"\"\"2011-11-S-011 ISAF Joint Command - Afghanistan For Immediate Release\n      KABUL, Afghanistan (Nov. 7, 2011) — A combined Afghan and coalition\n      security force conducted an operation in search of a Haqqani facilitator\n      in Argo district, Badakshan province. The facilitator coordinates suicide\n      attacks with other insurgent leaders in the area. During the operation, a\n      local national male failed to comply with repeated verbal warnings and\n      displayed hostile intent toward the security force. The security force\n      engaged the individual, resulting in his death. The security force\n      confiscated a shotgun and intelligence linking the local national to the\n      Haqqani network. The security force also detained two suspected insurgents during the operation.\"\"\"\n\nprompt = f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\n\n### Instruction:\n\nPRESS RELEASE TEXT: '{pr1}'\n\n### Response:\n\"\"\"\n\n\nimport os\nfrom predibase import Predibase\n\npb = Predibase(api_token=os.getenv(\"PREDIBASE_API_KEY\"))\n# pb = Predibase(api_token=\"\")\n\nlorax_client = pb.deployments.client(\"isafpr\")\nprint(lorax_client.generate(prompt, max_new_tokens=100).generated_text)\n\nUnfortunately my Predibase model deployment was still ‘initializing’ after a couple of hours of spinning up. I didn’t want to leave that dedicated deployment up and running overnight, so I just deleted the deployment and I’ll try to get this going at a later date. So no inference sample to show you for this one. I’m very curious to see how Qwen2 did, though!"
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#openai",
    "href": "posts/2024-06-17-one-click-finetuning.html#openai",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "OpenAI",
    "text": "OpenAI\nI was actually surprised that this is even a thing that people do or that is offered by OpenAI. Currently you’re able to finetune three versions of GPT3.5 as well as babbage-002 and davinci-002. In the OpenAI presentation during the course they mentioned that they were working to make it possible to finetune GPT4 as well, but no timeline was given on this.\nSo why would someone want to finetune GPT3.5? I think there are some problems that are sufficiently complex or of a specific nature where the OpenAI GPT family shines where you might want to squeeze out a final last bit of performance and where the open LLMs just aren’t there yet.\nThe OpenAI models are sort of the antithesis of an ‘open’ model and nothing about the finetuning process lent itself to disabusing you of that idea. This was the UI to fill in in order to finetune a model and as you can see there aren’t really too many options available to you.\n\n\n\nOpenAI Finetuning UI\n\n\nSupposedly the data you upload (options for train as well as a separate test set here) will never be used by OpenAI to train their models but you have to just trust them on that front.\n \nAs with Predibase, during finetuning you don’t have access to any logs or even too much feedback during training. You get a loss curve and a few scraps of metadata and that’s it. The training took around 90 minutes to run and then you’re able to prompt the model to see how it works, using the standard OpenAI interface and methods you’re used to:\n\nfrom openai import OpenAI\nfrom rich import print\nimport json\nimport os\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nresponse = client.chat.completions.create(\n    model=\"ft:gpt-3.5-turbo-SOME_EXTRA_STUFF_HERE_FOR_MY_MODEL\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": pr1\n        }\n    ],\n    temperature=0\n)\n\nprint(json.loads(response.choices[0].message.content))\n\n{\n    'name': '1',\n    'start_date': '2011-11-07',\n    'event_type': ['captureandkill'],\n    'province': ['badakhshan'],\n    'target_group': ['haqqani'],\n    'min_killed': 1,\n    'min_captured': 2,\n    'killq': True,\n    'captureq': True,\n    'killcaptureraid': True,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 0,\n    'min_leaders_captured': 0\n}\n\n\n\nThey also give you an interface to see the response of the base model side-by-side against the finetuned model:\n\n\n\nSide-by-side UI of base model and finetuned model inference\n\n\nAs you can see, it’s done pretty well! It stuck to the JSON structure, and the extracted metadata looks good. Of course, since this is a GPT3.5 model, there’s no way to now download this model and run it locally. You’re hostage to OpenAI, to being online, etc etc. Not a scenario I’d like to be in, so I don’t think I’ll pursue this much further and rather use my OpenAI credits for other purposes.\nAll that said, I do think there might be some scenarios where only the OpenAI models are reliable enough to use (be that in terms of accuracy or with sufficient guardrails) and there were people in the course who were in this boat."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#openpipe",
    "href": "posts/2024-06-17-one-click-finetuning.html#openpipe",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "OpenPipe",
    "text": "OpenPipe\nThis was the last one-click provider I tried. As with the others, you upload your data first. When I tried this, I got a fairly opaque error message but I guess the format I’d used was incompatible. OpenPipe uses the same format as OpenAI does, it turns out, but it handles the train/test split itself so you just have to set your data up in a single file (unlike with OpenAI where they can take two separate files).\nThe interface for finetuning your model was somehow the most threadbare of all:\n\n\n\nOpenPipe finetuning UI\n\n\nMoreover, the selection of base models on which to finetune were also pretty slim: Llama3, Mistral, Mixtral and two OpenAI GPT3.5 models. I was surprised by the estimate of how much it’d cost to finetune the model (around $30 USD) but by limiting the number of options available to the user the path forward really was pretty easy.\nYou get no single morsel of information during the finetuning process and for me it took a while for the job to even start working, but after an hour or two (I can’t be sure as I left my desk) you get a model out the other end. At this point you can export the weights or just try out the model with a Python call.\nHelpfully, the web UI gives you code snippets you can use for Python, Javascript and cURL, and the snippets even have your prompt pre-filled with an example from your dataset. This was a nice touch.\n\n# pip install openpipe\n\nfrom openpipe import OpenAI\nfrom rich import print\nimport json\nimport os\n\nclient = OpenAI(\n  openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")}\n)\n\ncompletion = client.chat.completions.create(\n    model=\"openpipe:MY_MODEL_ID_WAS_HERE\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": pr1\n        }\n    ],\n    temperature=0,\n    openpipe={\n        \"tags\": {\n            \"prompt_id\": \"counting\",\n            \"any_key\": \"any_value\"\n        }\n    },\n)\n\nprint(json.loads(completion.choices[0].message.content))\n\n{\n    'name': '3 killed and 2 captured in Badakhshan',\n    'start_date': '2011-11-07',\n    'event_type': ['captureandkill'],\n    'province': ['badakhshan'],\n    'target_group': ['haqqani'],\n    'min_killed': 3,\n    'min_captured': 2,\n    'killq': True,\n    'captureq': True,\n    'killcaptureraid': True,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 0,\n    'min_leaders_captured': 0\n}\n\n\n\nAgain you can see we have a really nice result here: JSON is good and the content is solid too. This was a finetune of Llama3 so clearly the problem I noted in the previous blog post was a problem with how I’d set up my local finetune and not with Llama3 itself.\nI liked how OpenPipe automatically deployed my model for me once the finetune was complete. Moreover, there was no extra cost associated with this. (Since their base models are limited, I assume this means that they have lost of customers’ LORA adapters all connected to these base models and that’s how they’re able to keep all these deployments up and cost-effective.)\nThere was one final trick that OpenPipe had up its sleeve: an ‘evals’ interface. The interface is pretty simple again, but the gist is that you get to select OpenAI models to compare your finetune against a test dataset and get a comparison. You can select multiple models to run at the same time and the cost is pretty reasonable.\n\n\n\nOpenpipe eval UI\n\n\nThe evaluation is parallelised and you get a nice table with the aggregate results:\n\n\n\nOpenpipe eval results\n\n\nYou also (in the datasets tab) get a table with the individual responses for the test data:\n\n\n\nOpenpipe eval datasets\n\n\nLooking at the results you quickly become aware that this specific evaluation didn’t really make much sense. Comparing the same prompt between the finetuned model and GPT4 could never have been fair since my prompt never asks for the result back in a certain format, or that it should be JSON and so on.\nMoreover, you can see that the evaluation prompt itself doesn’t do a good job of picking up that the finetuned model really did a great job on the whole and so the aggregate comparison scores don’t really make much sense here.\nThat said, I found this feature a useful ‘nice-to-have’ and I can see how someone might find this helpful if they either wanted to run a quick experiment or weren’t particularly technically savvy."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#final-thoughts",
    "href": "posts/2024-06-17-one-click-finetuning.html#final-thoughts",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "Final thoughts",
    "text": "Final thoughts\nOverall I found this an interesting experience to do these finetunes in parallel. I suspect that I am not the core audience / market for these services. I was surprised how little customisation they offered, and I actually wonder who is using them. They were easy to use, however, and they do potentially open up the possibility for someone less technical to do something somewhat advanced with LLMs that they wouldn’t otherwise be able to do.\nThe moment you want to do something slightly custom, with your prompt template or with the architecture or try something new and cutting-edge, then immediately these services aren’t for you. Similarly, even though I think all of the services offer a Python SDK to replicate what I did in the web UI, I think you essentially have the same limited options available to you if you wanted to trigger these jobs programatically as part of a larger pipeline.\nFor the most part you never had the feeling that you were part of a wider ecosystem of these open models, with new techniques coming out all the time and new models as well. These are some of the things I missed from the experience, but as I mentioned before, I’m not the core audience here.\nI do appreciate the opportunity to try these out a few times and the companies for providing credits to do some meaningful attempts at doing something useful. I’ll try these a bit further down the road again and report back if my impression changes or if/when new features are added."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "",
    "text": "In the previous post we looked at a single example where a prompt could help us extract structured data from some text. Here were were relying on the inbuilt capabilities of LLMs to ‘reason’ over the text and the task. In this post we’ll try to get a sense of the overall / aggregate performance of gpt-4-turbo on the ISAF Press Releases data extraction task.\nThe first thing we need to do is to load and process the data. If you remember from last time, there were some fields that needed conversion so that we’re comparing apples to apples when we get back our result from the LLM."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html#loading-and-processing-the-data",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html#loading-and-processing-the-data",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "Loading and processing the data",
    "text": "Loading and processing the data\n\n# get data from datasets\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\nimport tqdm as notebook_tqdm\n\n# Load the dataset\ndataset = load_dataset(\"strickvl/isafpressreleases\", split=\"train\")\n\n# Convert the dataset to a pandas DataFrame\ndf = pd.DataFrame(dataset)\n\n\n\n\nI’ll output the column names as a reference since we’ll be using almost all of these in our evaluation.\n\ndf.columns\n\nIndex(['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province',\n       'citydistrict', 'village', 'targetgroup', 'commander', 'position',\n       'minkilled', 'mincaptured', 'capturedcharacterisation',\n       'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid',\n       'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta',\n       'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured',\n       'minfacilitatorscaptured', 'leaderq'],\n      dtype='object')\n\n\nWe set up the same objects to support the Pydantic IsafEvent model. This is the same as last time so I’ll hide the code but you can see it by clicking the popout arrow.\n\n\nCode\nfrom pydantic import BaseModel, Field\nfrom datetime import date\nfrom enum import Enum\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\n\n\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jawzjan = \"jawzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktia = \"paktia\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pol = \"sar_e_pol\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\n\n\nclass TargetGroup(str, Enum):\n    taliban = \"taliban\"\n    haqqani = \"haqqani\"\n    criminals = \"criminals\"\n    aq = \"aq\"\n    hig = \"hig\"\n    let = \"let\"\n    imu = \"imu\"\n    judq = \"judq\"\n    iju = \"iju\"\n    hik = \"hik\"\n    ttp = \"ttp\"\n    other = \"other\"\n\n\nOur IsafEvent is what we’re trying to get to. We want to turn an unstructured piece of text (a press release) into the structured format you see below.\n\nfrom typing import Set\n\n\nclass IsafEvent(BaseModel):\n    name: str = Field(\n        description=\"A title or name for the event which summarises the event as a headline\"\n    )\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    event_type: Set[EventType] = Field(\n        description=\"The event type. Can be multiple types.\"\n    )\n    province: Set[Province] = Field(\n        description=\"The province in which the event occurred. Can be multiple provinces.\"\n    )\n    target_group: Set[TargetGroup] = Field(\n        description=\"The group that was targetted during the event. Can be multiple groups.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\nAgain we can take a look at a single example to understand the task and to construct our first evaluation of this single example. The article looks like this:\n\narticle_id = 15\narticle_text = df[\"text\"][article_id]\nprint(article_text)\n\nDec. 11: Haqqani Facilitator Detained in Khowst; Security Discussed in Farah\nNEWS RELEASE ISAF Joint Command - Afghanistan   2009-12-CA-065 For Immediate Release  KABUL, Afghanistan (Dec. 11) - An\nAfghan-international security force detained a couple of militants in Khowst province today, one of whom was a \nsought-after Haqqani facilitator.  The facilitator is responsible for the shipment and distribution of weapons to \nother militant elements in the area.\n The joint security force searched a compound near the village of Badal Kalay in the Nader Shakhot district where \nintelligence sources indicated the facilitator was located.  The facilitator identified himself and surrendered \nwithout incident.  No shots were fired and no one was injured.\n\n\n\nAnd our prompt looks like this:\n\nquery = f\"\"\"\nThe following is a press release issued by ISAF (formerly operating in Afghanistan):\n{article_text}\n\nPlease extract the following information from the press release:\n- The name of the event (summarising the event / text as a headline)\n- The start date of the event\n- The event type(s)\n- The province(s) in which the event occurred\n- The target group(s) of the event\n- The minimum number of people killed during the event\n- The minimum number of people captured during the event\n- Whether someone was killed or not during the event\n- Whether someone was captured or not during the event\n- Whether the event was a so-called 'kill-capture raid'\n- Whether an airstrike was used during the event\n- Whether no shots were fired during the event\n- The minimum number of leaders killed during the event\n- The minimum number of leaders captured during the event\n\nAnnotation notes:\n- A 'faciliator' is not a leader.\n- If a press release states that 'insurgents' were detained without further details, assign a minimum number of two detained. Interpret 'a couple' as two. Interpret 'several' as at least three, even though it may sometimes refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a small group', and 'multiple' as denoting at least three, even if they sometimes refer to larger numbers. Choose the smaller number if no other information is available in the press release to come up with a minimally acceptable figure. Interpret 'numerous' and 'a handful' as at least four, and 'a large number' as at least five.\n\"\"\"\n\nSince we’re going to be doing a lot of LLM calls I took a moment to refactor the code to make it less verbose. Click through to see the full code for the query_llm but basically it’s just a wrapper around OpenAI, Anthropic and Ollama models and we route to one or the other of those depending on what model you say you want to use.\n\n\nCode\nimport instructor\nfrom anthropic import Anthropic\nfrom openai import OpenAI\n\n\ndef query_llm(message: str, model: str, response_model: BaseModel) -&gt; str:\n    if model in [\"gpt-4\", \"gpt-4-turbo-preview\", \"gpt-4-turbo\"]:\n        client = instructor.patch(OpenAI(), mode=instructor.Mode.JSON)\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": message,\n                },\n            ],\n            response_model=response_model,\n            max_retries=3,\n        )\n    elif model in [\"claude-3-opus-20240229\", \"claude-3-opus-20240229-preview\"]:\n        client = instructor.from_anthropic(Anthropic())\n\n        # note that client.chat.completions.create will also work\n        response = client.messages.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": message,\n                },\n            ],\n            max_tokens=4096,\n            response_model=response_model,\n            max_retries=3,\n        )\n    elif model in [\"mistral\", \"mixtral\", \"llama-3\", \"gemma\", \"gemma:2b\"]:\n        client = instructor.from_openai(\n            OpenAI(\n                base_url=\"http://localhost:11434/v1\",\n                api_key=\"ollama\",  # required, but unused\n            ),\n            mode=instructor.Mode.JSON,\n        )\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": message,\n                }\n            ],\n            response_model=response_model,\n        )\n    return response\n\n\nAnd here are the responses for gpt-4-turbo and claude-3-opus-20240229.\n\nopenai_response = query_llm(query, \"gpt-4-turbo\", IsafEvent)\nprint(openai_response)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    event_type={&lt;EventType.detention: 'detention'&gt;},\n    province={&lt;Province.khost: 'khost'&gt;},\n    target_group={&lt;TargetGroup.haqqani: 'haqqani'&gt;},\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\n\nclaude_response = query_llm(query, \"claude-3-opus-20240229\", IsafEvent)\nprint(claude_response)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    event_type={&lt;EventType.detention: 'detention'&gt;},\n    province={&lt;Province.khost: 'khost'&gt;},\n    target_group={&lt;TargetGroup.haqqani: 'haqqani'&gt;},\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\nUnfortunately I was unable to evaluate a local model for this task as I couldn’t get a response back. I just got a Pydantic validation error, suggesting to me that maybe we’re hitting the context length limits of those local models with all the extra metadata in the JSON schema that we’re passing in via instructor. At any rate for now it’s the first immediate validation of the idea to finetune the model if we’re already hitting the limits of raw prompting for those models."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-the-results",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-the-results",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "Evaluating the results",
    "text": "Evaluating the results\nWe’ll want some kind of object to store our evaluation results and you can see all the fields I selected below. For fields where we can say ‘true’ or ‘false’ then I just did that, but for fields with multiple options then I thought a float value from 0 to 1 representing how ‘correct’ the options were would be the best option.\nFor fields with numeric values (i.e. number of killed individuals etc) then I generally created two fields: one was just a boolean field for whether it was correct and then another which was the distance between the annotated value and the value we predicted.\n\nclass EvalResult(BaseModel):\n    start_date_correct: bool = Field(\n        description=\"Whether the start date of the event is correct\"\n    )\n    event_type_score: float = Field(\n        description=\"The score between 0 and 1 for the event type of the event\"\n    )\n    province_score: float = Field(\n        description=\"The score between 0 and 1 for the province of the event\"\n    )\n    target_group_score: float = Field(\n        description=\"The score between 0 and 1 for the target group of the event\"\n    )\n    min_killed_correct: bool = Field(\n        description=\"Whether the minimum number of people killed during the event is correct\"\n    )\n    min_killed_distance: int = Field(\n        description=\"The distance between the minimum number of people killed during the event and the annotated number of people killed during the event\"\n    )\n    min_captured_correct: bool = Field(\n        description=\"Whether the minimum number of people captured during the event is correct\"\n    )\n    min_captured_distance: int = Field(\n        description=\"The distance between the minimum number of people captured during the event and the annotated number of people captured during the event\"\n    )\n    killq_correct: bool = Field(description=\"Whether the 'killq' field is correct\")\n    captureq_correct: bool = Field(\n        description=\"Whether the 'captureq' field is correct\"\n    )\n    killcaptureraid_correct: bool = Field(\n        description=\"Whether the 'killcaptureraid' field is correct\"\n    )\n    airstrike_correct: bool = Field(\n        description=\"Whether the 'airstrike' field is correct\"\n    )\n    noshotsfired_correct: bool = Field(\n        description=\"Whether the 'noshotsfired' field is correct\"\n    )\n    min_leaders_killed_correct: bool = Field(\n        description=\"Whether the minimum number of leaders killed during the event is correct\"\n    )\n    min_leaders_killed_distance: int = Field(\n        description=\"The distance between the minimum number of leaders killed during the event and the annotated number of leaders killed during the event\"\n    )\n    min_leaders_captured_correct: bool = Field(\n        description=\"Whether the minimum number of leaders captured during the event is correct\"\n    )\n    min_leaders_captured_distance: int = Field(\n        description=\"The distance between the minimum number of leaders captured during the event and the annotated number of leaders captured during the event\"\n    )\n\nI then coded up some methods that would calculate the score for each of the fields. Once again, there’s a decent amount of code here so I’ll fold it away and you can unfold the code if you want to take a look.\n\n\nCode\ndef start_date_correct(start_date: date, correct_date: date) -&gt; bool:\n    return start_date == correct_date.date()\n\n\ndef event_type_score(event_type: Set[EventType], correct_event_types: str) -&gt; float:\n    # Convert the correct event types string to a set of EventType enum values\n    correct_event_types_set = set(EventType(t) for t in correct_event_types.split(\";\"))\n\n    # Calculate the number of correct predictions\n    correct_predictions = len(event_type.intersection(correct_event_types_set))\n\n    # Calculate the total number of correct event types\n    total_correct_types = len(correct_event_types_set)\n\n    # Calculate the accuracy percentage\n    if total_correct_types &gt; 0:\n        accuracy = correct_predictions / total_correct_types\n    else:\n        accuracy = 0.0\n\n    return accuracy\n\n\ndef province_score(province: Set[Province], correct_province: str) -&gt; float:\n    # Convert the correct province string to a set of Province enum values\n    correct_province_set = set()\n    invalid_provinces = set()\n    for p in correct_province.split(\";\"):\n        try:\n            correct_province_set.add(Province(p.lower()))\n        except ValueError:\n            invalid_provinces.add(p.lower())\n\n    # Calculate the number of correct predictions\n    correct_predictions = len(province.intersection(correct_province_set))\n\n    # Calculate the total number of provinces\n    total_provinces = len(correct_province_set) + len(invalid_provinces)\n\n    # Calculate the accuracy percentage\n    if total_provinces &gt; 0:\n        accuracy = correct_predictions / total_provinces\n    else:\n        accuracy = 0.0\n\n    return accuracy\n\n\ndef target_group_score(\n    target_group: Set[TargetGroup], correct_target_group: str\n) -&gt; float:\n    # Handle the case where correct_target_group is an empty string\n    if correct_target_group.strip() == \"\":\n        correct_target_group_set = set()\n    else:\n        # Convert the correct target group string to a set of TargetGroup enum values\n        correct_target_group_set = set(\n            TargetGroup(t.lower()) for t in correct_target_group.split(\";\")\n        )\n\n    # Calculate the number of correct predictions\n    correct_predictions = len(target_group.intersection(correct_target_group_set))\n\n    # Calculate the total number of correct target groups\n    total_correct_target_groups = len(correct_target_group_set)\n\n    # Calculate the accuracy percentage\n    if total_correct_target_groups &gt; 0:\n        accuracy = correct_predictions / total_correct_target_groups\n    else:\n        accuracy = 0.0\n\n    return accuracy\n\n\ndef min_killed_correct(min_killed: int, correct_min_killed: str) -&gt; bool:\n    return min_killed == int(correct_min_killed)\n\n\ndef min_killed_distance(min_killed: int, correct_min_killed: str) -&gt; int:\n    return abs(min_killed - int(correct_min_killed))\n\n\ndef min_captured_correct(min_captured: int, correct_min_captured: str) -&gt; bool:\n    return min_captured == int(correct_min_captured)\n\n\ndef min_captured_distance(min_captured: int, correct_min_captured: str) -&gt; int:\n    return abs(min_captured - int(correct_min_captured))\n\n\ndef killq_correct(killq: bool, correct_killq: str) -&gt; bool:\n    return killq == (correct_killq == \"true\")\n\n\ndef captureq_correct(captureq: bool, correct_captureq: str) -&gt; bool:\n    return captureq == (correct_captureq == \"true\")\n\n\ndef killcaptureraid_correct(\n    killcaptureraid: bool, correct_killcaptureraid: str\n) -&gt; bool:\n    return killcaptureraid == (correct_killcaptureraid == \"true\")\n\n\ndef airstrike_correct(airstrike: bool, correct_airstrike: str) -&gt; bool:\n    return airstrike == (correct_airstrike == \"true\")\n\n\ndef noshotsfired_correct(noshotsfired: bool, correct_noshotsfired: str) -&gt; bool:\n    return noshotsfired == (correct_noshotsfired == \"true\")\n\n\ndef min_leaders_killed_correct(\n    min_leaders_killed: int, correct_min_leaders_killed: str\n) -&gt; bool:\n    return min_leaders_killed == int(correct_min_leaders_killed)\n\n\ndef min_leaders_killed_distance(\n    min_leaders_killed: int, correct_min_leaders_killed: str\n) -&gt; int:\n    return abs(min_leaders_killed - int(correct_min_leaders_killed))\n\n\ndef min_leaders_captured_correct(\n    min_leaders_captured: int, correct_min_leaders_captured: str\n) -&gt; bool:\n    return min_leaders_captured == int(correct_min_leaders_captured)\n\n\ndef min_leaders_captured_distance(\n    min_leaders_captured: int, correct_min_leaders_captured: str\n) -&gt; int:\n    return abs(min_leaders_captured - int(correct_min_leaders_captured))\n\n\nI then wrote a function which could generate one of these EvalResult objects given a response from an LLM, the original dataframe (with the ground truth annotations) and the article id of the row.\nYou can see how Claude 3 Opus and GPT-4 Turbo performed on our single article below:\n\ndef evaluate_llm_response(\n    response: IsafEvent, df: pd.DataFrame, article_id: int\n) -&gt; EvalResult:\n    return EvalResult(\n        start_date_correct=start_date_correct(\n            response.start_date, df[\"StartDate\"][article_id]\n        ),\n        event_type_score=event_type_score(\n            response.event_type, df[\"eventtype\"][article_id]\n        ),\n        province_score=province_score(response.province, df[\"province\"][article_id]),\n        target_group_score=target_group_score(\n            response.target_group, df[\"targetgroup\"][article_id]\n        ),\n        min_killed_correct=min_killed_correct(\n            response.min_killed, df[\"minkilled\"][article_id]\n        ),\n        min_killed_distance=min_killed_distance(\n            response.min_killed, df[\"minkilled\"][article_id]\n        ),\n        min_captured_correct=min_captured_correct(\n            response.min_captured, df[\"mincaptured\"][article_id]\n        ),\n        min_captured_distance=min_captured_distance(\n            response.min_captured, df[\"mincaptured\"][article_id]\n        ),\n        killq_correct=killq_correct(response.killq, df[\"killq\"][article_id]),\n        captureq_correct=captureq_correct(\n            response.captureq, df[\"captureq\"][article_id]\n        ),\n        killcaptureraid_correct=killcaptureraid_correct(\n            response.killcaptureraid, df[\"killcaptureraid\"][article_id]\n        ),\n        airstrike_correct=airstrike_correct(\n            response.airstrike, df[\"airstrike\"][article_id]\n        ),\n        noshotsfired_correct=noshotsfired_correct(\n            response.noshotsfired, df[\"noshotsfired\"][article_id]\n        ),\n        min_leaders_killed_correct=min_leaders_killed_correct(\n            response.min_leaders_killed, df[\"minleaderskilled\"][article_id]\n        ),\n        min_leaders_killed_distance=min_leaders_killed_distance(\n            response.min_leaders_killed, df[\"minleaderskilled\"][article_id]\n        ),\n        min_leaders_captured_correct=min_leaders_captured_correct(\n            response.min_leaders_captured, df[\"minleaderscaptured\"][article_id]\n        ),\n        min_leaders_captured_distance=min_leaders_captured_distance(\n            response.min_leaders_captured, df[\"minleaderscaptured\"][article_id]\n        ),\n    )\n\n\nprint(\"Claude 3 Opus Scores\")\nprint(evaluate_llm_response(claude_response, df, article_id))\nprint(\"GPT-4 Turbo Scores\")\nprint(evaluate_llm_response(openai_response, df, article_id))\n\nClaude 3 Opus Scores\n\n\n\nEvalResult(\n    start_date_correct=True,\n    event_type_score=1.0,\n    province_score=1.0,\n    target_group_score=1.0,\n    min_killed_correct=True,\n    min_killed_distance=0,\n    min_captured_correct=True,\n    min_captured_distance=0,\n    killq_correct=True,\n    captureq_correct=True,\n    killcaptureraid_correct=False,\n    airstrike_correct=True,\n    noshotsfired_correct=True,\n    min_leaders_killed_correct=True,\n    min_leaders_killed_distance=0,\n    min_leaders_captured_correct=True,\n    min_leaders_captured_distance=0\n)\n\n\n\nGPT-4 Turbo Scores\n\n\n\nEvalResult(\n    start_date_correct=True,\n    event_type_score=1.0,\n    province_score=1.0,\n    target_group_score=1.0,\n    min_killed_correct=True,\n    min_killed_distance=0,\n    min_captured_correct=True,\n    min_captured_distance=0,\n    killq_correct=True,\n    captureq_correct=True,\n    killcaptureraid_correct=False,\n    airstrike_correct=True,\n    noshotsfired_correct=True,\n    min_leaders_killed_correct=True,\n    min_leaders_killed_distance=0,\n    min_leaders_captured_correct=True,\n    min_leaders_captured_distance=0\n)\n\n\n\nAt first glance it looks like our models performed pretty well. Both got the killcaptureraid status of the article wrong, but otherwise almost perfect across the board.\nDoing this one by one is tedious, of course, and really what we want is an evaluation across a random slice of our articles. So let’s do that next."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-in-aggregate",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-in-aggregate",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "Evaluating in aggregate",
    "text": "Evaluating in aggregate\nHere we’ll shuffle our dataframe to take a random sample of our articles, then we’ll iterate over 100 of our rows to get the generated responses for GPT-4. We’ll also time how long it takes to execute the cell so as to have some kind of benchmark for how long the inference takes. We can use that later on to try to beat this baseline.\n\n%%time\n\n# Shuffle the DataFrame\nshuffled_df = df.sample(frac=1, random_state=42)\n\nopenai_responses = {}\n\n# Iterate over the first 5 rows of the shuffled DataFrame\nfor index, row in shuffled_df.head(100).iterrows():\n    openai_responses[index] = query_llm(row[\"text\"], \"gpt-4-turbo\", IsafEvent)\n\nCPU times: user 2.53 s, sys: 34.4 ms, total: 2.57 s\nWall time: 10min 4s\n\n\nAs you can see, this took a total of 10 minutes to run so on average this was around 6 seconds per article. Some articles will be longer than others, of course, so perhaps the aggregate ‘10 minutes per 100 articles’ is a better metric.\nWe now have our results in memory in the openai_responses dictionary. We can now calculate the scores for the results and plot them.\n\nfrom typing import List\nimport matplotlib.pyplot as plt\n\n\ndef calculate_scores(eval_results: List[EvalResult]) -&gt; dict:\n    scores = {}\n    total_count = len(eval_results)\n\n    for attr in EvalResult.__fields__:\n        if attr.endswith(\"_score\"):\n            scores[attr] = (\n                sum(getattr(result, attr) for result in eval_results) / total_count\n            )\n        elif attr.endswith(\"_correct\"):\n            scores[attr] = (\n                sum(getattr(result, attr) for result in eval_results) / total_count\n            )\n        elif attr.endswith(\"_distance\"):\n            if attr == \"min_captured_distance\":\n                capped_values = [\n                    min(max(getattr(result, attr), 0), 1) for result in eval_results\n                ]\n                scores[attr] = 1 - (sum(capped_values) / total_count)\n            else:\n                scores[attr] = 1 - (\n                    sum(getattr(result, attr) for result in eval_results) / total_count\n                )\n\n    return scores\n\n\ndef plot_scores(scores: dict):\n    attributes = list(scores.keys())\n    values = [score * 100 for score in scores.values()]  # Convert scores to percentages\n\n    fig, ax = plt.subplots(figsize=(10, 8))  # Adjust the figure size\n\n    ax.barh(attributes, values)\n    ax.set_xlim(0, 100)  # Set x-axis range from 0 to 100\n    ax.set_xlabel(\"Score (%)\")\n    ax.set_ylabel(\"Attribute\")\n    ax.set_title(\"GPT-4 Turbo Evaluation Scores\")\n\n    for i, v in enumerate(values):\n        ax.text(v + 1, i, f\"{v:.2f}%\", va=\"center\")  # Add percentage symbol to labels\n\n    # Adjust the left margin to remove extra white space\n    plt.subplots_adjust(left=0.2)\n\n    plt.show()\n\n\neval_results = []\nfor article_id, response in openai_responses.items():\n    eval_results.append(evaluate_llm_response(response, df, article_id))\nscores = calculate_scores(eval_results)\nplot_scores(scores)\n\n\n\n\n\n\n\n\nThis gives us a nice overview of how a mostly-unoptimised approach that just pairs prompting with instructor (i.e. passing in the JSON schema for our expected results) performs on our data. Some scores are pretty good, particularly where it’s just a boolean we’re getting back. For others involving numbers (i.e. numbers of killed or captured) we have much lower scores.\nThe lowest performing score is the check as to whether the text describes a killcaptureraid. Our prompt doesn’t really give much explanation or context as to how we determined this in the original dataset, so maybe an example or two in the prompt would help there.\nSo overall, gpt-4-turbo performs moderately well, but this wouldn’t be acceptable for the purposes of the report that I wrote. The assumption there was that the data was labelled more or less perfectly since I used that data to make accusations and claims about the international military presence in Afghanistan. It simply wouldn’t be good enough to say that we ‘mostly’ got the right numbers when extracting the data from the text. We needed a really high accuracy across the board.\nIf I was doing this properly the obvious next step would be to try to split up the task into discrete / focused parts as I mentioned in the previous post. That way we could let the model take multiple shots at the problem and have less to handle overall in any one reading. I might return to that at a later stage because that might be the way forward with our finetuning work as well (to get best in class results).\nFor now, though, the next step is to prepare our data for finetuning and then to actually run the finetuning. My preference is to select a base model which I can train on my local machine, but since we do have lots of credits with a variety of cloud providers I’ll try to run my finetuning on more powerful machines in the cloud as well."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "",
    "text": "In this blog I want to walk through how I trained my first tokenizer(s) on a small Balochi language corpus. I used the Huggingface Tokenizers library and FastAI / Spacy to get a sense of the interfaces involved. There’s also some naive pre-processing I did to get the corpus into a format that the tokenizer could handle. I’m not sure if this is the best way to do it, but it worked for this first iteration.\nWe can get straight into the implementation details, but the general process was:\nI’ll go through each of these steps in turn.\nCode\n# !pip install datasets\n# !huggingface-cli login\n# from datasets import load_dataset\n# load_dataset(\"balochiml/balochi-language-data\", data_dir=\"data\", cache_dir=\"../data\")"
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#load-our-text-corpus",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#load-our-text-corpus",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Load our text corpus",
    "text": "Load our text corpus\nHere I walk through my .txt files and load the paths into a list. You can see we have 4294 files to work with.\n\nimport os\n\n\ndef get_txt_file_paths(directory):\n    txt_file_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".txt\"):\n                file_path = os.path.join(root, file)\n                txt_file_paths.append(file_path)\n    return txt_file_paths\n\n\n# Replace \"directory_path\" with the actual path of the directory you want to search\ndirectory_path = \"../data/raw_text\"\ntxt_paths = get_txt_file_paths(directory_path)\n\nlen(txt_paths)\n\n4294"
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#pre-process-the-texts",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#pre-process-the-texts",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Pre-process the texts",
    "text": "Pre-process the texts\nI still don’t fully have a good sense of the best ways to do this, not least of all because I’m not sure of the tradeoffs for decisions I take. For example, I frequently hear that people remove punctuation during pre-processing, but I’m not sure how that’s helpful. It feels like you’d be removing context more than anything else.\nI had similar thoughts on the removal of numbers, but in the end I removed them along with any a-z or A-Z English-language characters. I also removed excess whitespace.\n\nimport re\n\ndef clean_text(file_path):\n    # Open the file and read it into memory\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n\n    # Remove English-language characters and numbers\n    text = re.sub(r\"[a-zA-Z0-9]\", \"\", text)\n\n    # Remove any excess whitespace\n    text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n\n    return text\n\n\nfor path in txt_paths:\n    cleaned_text = clean_text(path)\n\n    # write the cleaned text to a new file with an incremented filename\n    # write the files all into the '../data/processed_text' directory\n    with open(\n        f'../data/processed_text/{path.split(\"/\")[-1]}', \"w\", encoding=\"utf-8\"\n    ) as file:\n        file.write(cleaned_text)"
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#lessons-learned",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#lessons-learned",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Lessons learned",
    "text": "Lessons learned\nThis first attempt at tokenisation was instructive in a number of respects.\nI didn’t show what was going on under the hood with the FastAI wrapper, but if you look at the source code you’ll see that the line spacy = WordTokenizer() assumes that the base language we’re dealing with is English. You can of course pass in a language code to the WordTokenizer initialization, but since it uses Spacy under the hood here and since Balochi isn’t represented as an official language supported by Spacy, when you’re basically out of luck. You hit an error and you can either continue using simplistic algorithms like the ones demonstrated above (essentially splitting on word delimiters) or you can abandon FastAI and dive into Spacy.\nAt that point, you’ll have to start implementing a whole bunch of things yourself in order to get going quickly. For example, you’ll ideally want to come up with all the list of punctuation marks, stop words, stemming rules and so on that I mentioned last time. (It might well be that it’s possible to get up and running faster for a non-standard language with Spacy, but it wasn’t clear to me how to do that.)\nI do actually now intend to make a contribution to the Spacy repo to have Balochi represented there, and to open the window for others to contribute to the language metadata directly, but that didn’t help me in the moment. You’ll notice that I didn’t show how you can save a serialized version of the Spacy/FastAI tokeniser because I wasn’t able to figure out how to get access to the underlying Spacy object. I’m sure it’s possible since I can read the Spacy API documentation showing which method to use but FastAI didn’t itself expose this functionality directly.\nMy initial impression from working with both libraries and spending some time with their documentation is that Spacy might end up being more useful for low-resource languages given the extent to which they support a more complete range of old-school NLP methods and techniques. That said, the 🤗 Tokenizers library was much easier to get up and running with and I think it’s a great option for anyone who wants to get started quickly with tokenization. They support most of the major algorithms you’d ever need to use and if they don’t you can always implement something yourself to extend it."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#balochi-tokenizers-on-huggingface-hub",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#balochi-tokenizers-on-huggingface-hub",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Balochi Tokenizers on Huggingface Hub",
    "text": "Balochi Tokenizers on Huggingface Hub\nI’m still working through a way to open up the core dataset (along with constructing as I work), but this first iteration of the tokenizer is now available over on the Huggingface Hub. You can load it for use with the single line:\ntokenizer = Tokenizer.from_file(\"../models/30k-balochi-tokenizer.json\")\nThe organisation is something I created together with some Balochi colleagues who expressed an interest in working together on this effort. I’m really happy to have made their acquaintance and I hope I’ll be able to make steady progress on this project with their help. (If you’re interested in contributing, please request access to the organization and/or contact me for more information.)\nWhile creating the tokenizer repository, I also noted how Balochi (as with Spacy) is not represented as a language recognised by the metadata tracking languages used on the Hub. Frustratingly, you’re asked to input an ISO-639-1 two-letter code to represent the language of the model, but of course Balochi doesn’t have one of those. Balochi only has an ISO-693-2 and ISO-693-3 code. I’ll have to see how we can get Balochi represented on the Hub given all this. It can’t be the first time that this has happened."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#next-steps",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#next-steps",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Next steps",
    "text": "Next steps\nNow that I have this first iteration complete, I want to reflect a bit on how to know when the tokenizer is ‘good enough’. In particular, how do you evaluate tokenisers? Are there ways of benchmarking this? There must have been work done on this and I want to understand both what the start of the art is as well as how to know when I’ve reached it.\nI also watched an extremely rewarding talk on low-resource languages (blog notes to follow!) and there was a section in that which stressed the foundational nature of tokenisation as part of language models. It also highlighted a failure mode where bad tokenisation made a model perform very badly on a certain kind of task. So based on this context I would like to understand how to evaluate tokenisers and how to know when I’ve reached a good enough point.\nI also have a grab-bag of odds and ends relating to tokenization (GPU-based tokenization! tiktoken! etc.) that I mean to write up alongside the above."
  },
  {
    "objectID": "posts/2023-03-18-stable-eights-part-two.html",
    "href": "posts/2023-03-18-stable-eights-part-two.html",
    "title": "Building Blocks For Better Stable Eights",
    "section": "",
    "text": "My last blogpost was about my attempt to try to generate images of handwritten ‘eight’ digits from random noise. It was an exploration of some of the process at work in diffusion models and the whole diffusion paradigm in general.\nIt’s come up since then during our Sunday morning ‘Delft FastAI Study Group’ sessions and we’ve been throwing around a few different ideas on how to improve the process to actually output eights. Two in particular seemed like things we’d want to try out:\nI thought I’d try to implement a starter version of both of these in order to learn what they are, and in order to present for our group discussion. Before we get started, we can get some boilerplate setup out of the way (i.e. the status quo by the end of the last post)."
  },
  {
    "objectID": "posts/2023-03-18-stable-eights-part-two.html#training-an-8-digit-classifier-initial-naive-approach",
    "href": "posts/2023-03-18-stable-eights-part-two.html#training-an-8-digit-classifier-initial-naive-approach",
    "title": "Building Blocks For Better Stable Eights",
    "section": "1: Training an ‘8’ digit classifier (initial / naive approach)",
    "text": "1: Training an ‘8’ digit classifier (initial / naive approach)\n\n\nCode\n!pip install -Uqq pip\n!pip install fastai torch datasets rich -Uqq\n# !pip install -Uqq timm\n\nimport IPython\n\n# automatically restart kernel\nIPython.Application.instance().kernel.do_shutdown(restart=True)\n\n\n\n^C\n\nERROR: Operation cancelled by user\n\n\n\n\n\n{'status': 'ok', 'restart': True}\n\n\n\nThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. View Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n\nCode\nfrom typing import Union, Callable\n\nfrom fastai.vision.all import *\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.models import vgg19\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# make sure the digits are human-readable\ntorch.set_printoptions(precision=6, sci_mode=False)\n\n# dataset patched together from the original MNIST dataset\npath = Path(\"./mnist_8_or_not/training\")\nfnames = get_image_files(path)\n\n\ndef label_func(x):\n    return x.parent.name\n\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func)\n# set environment based on hostname\nimport os\n\nenvironment_type = \"unknown\"\n\nif \"HOSTNAME\" in os.environ:\n    hostname = os.environ[\"HOSTNAME\"]\n    environment_type = \"local\" if hostname == \"localhost\" else \"cloud\"\n\nmodel_filename = \"eight_classifier.pkl\"\nmodel_base_path = Path(\"/home/\") if environment_type == \"cloud\" else Path(\"./\")\n\n# only train the model if we have no model already\nmodel_path = Path(f\"{model_base_path}/{model_filename}\")\n\nif not model_path.exists():\n    learn = vision_learner(dls, resnet34, metrics=error_rate)\n    learn.fine_tune(6)\n    # export our model so we don't have to retrain it every time from now on\n    learn.export(f\"{model_base_path}{model_filename}\")\nelse:\n    learn = load_learner(f\"{model_base_path}/{model_filename}\")\nan_eight = Path(path / \"8\").ls()[0]\nnot_an_eight = Path(path / \"not_8\").ls()[0]\n\n\ndef get_eight_probability(\n    image_pth: Union[Path, torch.Tensor], learner: Learner\n) -&gt; torch.Tensor:\n    _, _, probs = learner.predict(image_pth)\n    return probs[0]\n\n\n/Users/strickvl/.pyenv/versions/3.10.4/envs/mlops-blog/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# we generate a 3x28x28 tensor with random values assigned\n# we ensure that we can use PyTorch's autograd on the values\ndef get_noisy_starter_tensor() -&gt; torch.Tensor:\n    return torch.randn(3, 28, 28, requires_grad=True)\n\n\n# this will allow us to display the tensor as an image\ndef display_tensor(tns: torch.Tensor):\n    # Convert the tensor to a NumPy array\n    image_array = tns.detach().numpy()\n\n    # Clip the pixel values between 0 and 1\n    image_array = np.clip(image_array, 0, 1)\n\n    # Transpose the array to (28, 28, 3) shape\n    image_array = image_array.transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_array)\n    plt.show()"
  },
  {
    "objectID": "posts/2022-10-24-foundations-mnist-basics.html",
    "href": "posts/2022-10-24-foundations-mnist-basics.html",
    "title": "From the foundation up: Fashion-MNIST basics from Lesson 10",
    "section": "",
    "text": "In lesson 10 of the FastAI course, we began by examining some new research that had just been released (the iMagic paper) along with one of the key insights (‘progressive distillation’) of the paper. I’ll maybe return to that in a separate blogpost but I wanted to focus on the second half of the lesson where we start the work of actually building up our library of tools to replicate all the pieces of Stable Diffusion.\nTo make things more complicated, we’re allowed to use only a basic set of building blocks (at least in the first iteration) so as to really understand how things are working. This includes:\n\nPython\nThe Python standard library\nmatplotlib\nJupyter notebooks and nbdev\n\nThis notebook will showcase some of my experimentation and learnings around the things we went through in this lesson. I’ve also compiled a table showing the various pieces we got to in the lesson, pairing the specific task we were trying to do along with the associated skill or part of the Python standard library.\n\n\n\n\nTask\nSkill / Library\nDocs link\n\n\n\n\n1\nDownload Fashion MNIST\nurllib.requests.urlretrieve\nlink\n\n\n2\nUnzip & separate out the parts\ngzip\nlink\n\n\n\n\npickle\nlink\n\n\n\n\ndestructuring\n\n\n\n3\nTurn the list into a matrix\nchunking into a list of lists\n\n\n\n\n\nyield / next (generators)\nlink\n\n\n4\nPlot out the image using matplotlib\nmatplotlib\nlink\n\n\n5\nDefine a matrix class\n__init__\nlink\n\n\n\n\n__getitem__\nlink\n\n\n6\nAccess tensor values\nPytorch Tensor\nlink\n\n\n\n\nmap\nlink\n\n\n\n\ntensor().reshape()\nlink\n\n\n7\nFind the shape, min and max values\ntensor().shape\n\n\n\n\n\ntensor().min()\nlink\n\n\n\n\ntensor().max()\nlink\n\n\n8\nGenerate some random numbers\nrandom.random\nlink\n\n\n9\nProfile your code\n%timeit\nlink1 / link2\n\n\n\nAt the bottom of the post, I’ll also include my collation of the “Things Jeremy says to do” which has a bit of a precedent on the fastai forums so I thought I’d share them as well.\n\nDownloading MNIST\nWe start by downloading our dataset. I’m going to replicate the work we do in class while using Zalando’s alternative version of MNIST, entitled Fashion-MNIST. I previously used it on this blog (here, for example, but also elsewhere) while working through part 1 of the course, albeit at a much higher level using PyTorch to do similar things that we’ll be doing this time round. It’s comparable but also different, so I’m looking forward to working through whatever challenges are raised.\nThe URLs are specified on the Github Repo, so I’ll include those as constants here first:\n\nBASE_URL = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n\nTRAINING_IMAGES = BASE_URL + \"train-images-idx3-ubyte.gz\"\nTRAINING_IMAGES_LABELS = BASE_URL + \"train-labels-idx1-ubyte.gz\"\nTEST_IMAGES = BASE_URL + \"t10k-images-idx3-ubyte.gz\"\nTEST_IMAGES_LABELS = BASE_URL + \"t10k-labels-idx1-ubyte.gz\"\n\nWe now want a place where we can save them, so we’ll use pathlib to create a data directory in our current working directory, but only if it doesn’t already exist. We also set up the final locations of the data for when we download it:\n\n# create local path directory if it doesn't exist\nfrom pathlib import Path\n\nLOCAL_PATH = Path(\"data\")\n# check whether the path exists; create it if it doesn't exist\nif not LOCAL_PATH.exists():\n    LOCAL_PATH.mkdir(exist_ok=True)\n    \ntraining_images_path = LOCAL_PATH / \"train-images-idx3-ubyte.gz\"\ntraining_images_labels_path = LOCAL_PATH / \"train-labels-idx1-ubyte.gz\"\ntest_images_path = LOCAL_PATH / \"t10k-images-idx3-ubyte.gz\"\ntest_images_labels_path = LOCAL_PATH / \"t10k-labels-idx1-ubyte.gz\"\n\nThere’s probably a more elegant way to do this where we loop over a list, but the way I set up the URLs and the filenames didn’t lend itself to that approach so here I just download them one by one, but only if they don’t already exist.\n\n# download the raw data if it doesn't exist\nfrom urllib.request import urlretrieve\n\nif not training_images_path.exists():\n    urlretrieve(TRAINING_IMAGES, training_images_path)\nif not training_images_labels_path.exists():\n    urlretrieve(TRAINING_IMAGES_LABELS, training_images_labels_path)\nif not test_images_path.exists():\n    urlretrieve(TEST_IMAGES, test_images_path)\nif not test_images_labels_path.exists():\n    urlretrieve(TEST_IMAGES_LABELS, test_images_labels_path)\n\nHere you can see that we have downloaded four files as expected:\n\n!ls -l data\n\ntotal 60328\n-rw-r--r--  1 strickvl  staff   4422102 22 Oct 16:12 t10k-images-idx3-ubyte.gz\n-rw-r--r--  1 strickvl  staff      5148 22 Oct 16:12 t10k-labels-idx1-ubyte.gz\n-rw-r--r--  1 strickvl  staff  26421880 22 Oct 16:12 train-images-idx3-ubyte.gz\n-rw-r--r--  1 strickvl  staff     29515 22 Oct 16:12 train-labels-idx1-ubyte.gz\n\n\n\n\nUnzip the files and separate out the data\nIn the lesson, Jeremy downloads a pre-made MNIST file which contains a tuple of tuples. With Fashion-MNIST, we have four separate files representing our training and test data and our labels for each. Now our task is to unzip those files and grab the data as arrays.\nNote that there are convenience functions provided for both MNIST and Fashion-MNIST to do much of what we do below, since most people want the data as numpy arrays, not Python arrays.\n\n# import relevant standard library packages\nimport gzip\n\nWe have one elephant in the room which is this parse_idx function that I’m showing below in a collapsed cell. This was one part that I didn’t want to get too lost in so I simply went to the source code of one of these helper functions, specifically that written by datapythonista for MNIST.\nThis still all is standard library imports so I think technically I’m ok including this code here since the MNIST download Jeremy used in the lesson comes pre-parsed, it seems. After inspecting the data in the files, it seems that we have a series of bytes that need to be parsed somehow if they’re going to be used as an array. The original function returned a numpy array, but I modified it slightly such that it returns a nested array instead. I consider this not a very interesting part of the process so I’m hiding it away.\n\n#collapse-hide\n# taken from https://github.com/datapythonista/mnist/blob/master/mnist/__init__.py\nimport os\nimport functools\nimport operator\nimport struct\nimport array\nimport tempfile\n\ndef parse_idx(fd):\n    \"\"\"Parse an IDX file, and return it as an array of arrays.\n    \n    Parameters\n    ----------\n    fd : file\n        File descriptor of the IDX file to parse\n    endian : str\n        Byte order of the IDX file. See [1] for available options\n    Returns\n    -------\n    data : array\n        Numpy array with the dimensions and the data in the IDX file\n    1. https://docs.python.org/3/library/struct.html\n        #byte-order-size-and-alignment\n    \"\"\"\n    DATA_TYPES = {0x08: 'B',  # unsigned byte\n                  0x09: 'b',  # signed byte\n                  0x0b: 'h',  # short (2 bytes)\n                  0x0c: 'i',  # int (4 bytes)\n                  0x0d: 'f',  # float (4 bytes)\n                  0x0e: 'd'}  # double (8 bytes)\n\n    header = fd.read(4)\n    if len(header) != 4:\n        raise IdxDecodeError('Invalid IDX file, '\n                             'file empty or does not contain a full header.')\n\n    zeros, data_type, num_dimensions = struct.unpack('&gt;HBB', header)\n\n    if zeros != 0:\n        raise IdxDecodeError('Invalid IDX file, '\n                             'file must start with two zero bytes. '\n                             'Found 0x%02x' % zeros)\n\n    try:\n        data_type = DATA_TYPES[data_type]\n    except KeyError:\n        raise IdxDecodeError('Unknown data type '\n                             '0x%02x in IDX file' % data_type)\n\n    dimension_sizes = struct.unpack('&gt;' + 'I' * num_dimensions,\n                                    fd.read(4 * num_dimensions))\n\n    data = array.array(data_type, fd.read())\n    data.byteswap()  # looks like array.array reads data as little endian\n\n    expected_items = functools.reduce(operator.mul, dimension_sizes)\n    if len(data) != expected_items:\n        raise IdxDecodeError('IDX file has wrong number of items. '\n                             'Expected: %d. Found: %d' % (expected_items,\n                                                          len(data)))\n    return data\n\nThere’s one final piece of the process which comes from the fact that the array (i.e. the data local variable inside the parse_idx function) that gets returned is a single array (i.e. all the pixel values for all 60,000 images just listed one after another). We’ll want to split these up into separate chunks:\n\nsplitting each image into a separate array of 784 values (because our images are 28x28 size)\nthen splitting each image into an array of arrays giving us our 28x28 matrix.\n\nI’ll include the code that we use for this in the class, a chunks function, but I’ll return to how it works once everything is loaded.\n\n# chunk things together\ndef chunks(x, size):\n    for i in range(0, len(x), size): \n        yield x[i:i + size]\n\n\n\nLoad the data into memory\nWe use the chunks function here to turn our array of 47,040,000 values into matrices. To start with, something like x_train will have 60,000 separate arrays of 784 pixels, as we can see when we get the lengths of the initial array and then the length of the first item.\n\n# unzip the files and extract the images \nwith gzip.open(training_images_path, 'rb') as f:\n    pixels = list(parse_idx(f))\n    x_train = list(chunks(pixels, 784))\n\nwith gzip.open(training_images_labels_path, 'rb') as f:\n    y_train = list(parse_idx(f))\n    \nwith gzip.open(test_images_path, 'rb') as f:\n    pixels = list(parse_idx(f))\n    x_valid = list(chunks(pixels, 784))\n    \nwith gzip.open(test_images_labels_path, 'rb') as f:\n    y_valid = list(parse_idx(f))\n\n\nlen(x_train), len(x_train[0])\n\n(60000, 784)\n\n\nThe labels are in a slightly different format, containing single integer values indicating which item the image refers to. We can check and show that we have 60,000 of those values, as we’d expect, and we can even see what values those refer to with a simple conversion:\n\nlen(y_train)\n\n60000\n\n\n\ny_train[0:10]\n\n[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]\n\n\n\n# this list is taken from the README of the Fashion-MNIST repository\n# https://github.com/zalandoresearch/fashion-mnist\nindex_to_label = {\n    0: \"T-shirt/top\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\"\n}\n\nlist(map(lambda x: index_to_label[x], y_train[0:10]))\n\n['Ankle boot',\n 'T-shirt/top',\n 'T-shirt/top',\n 'Dress',\n 'T-shirt/top',\n 'Pullover',\n 'Sneaker',\n 'Pullover',\n 'Sandal',\n 'Sandal']\n\n\n\n\nPlot some examples using matplotlib\nWe can plot out one or two of our images to confirm that we’re seeing the right images and that everything’s as expected:\n\nimport matplotlib as mpl, matplotlib.pyplot as plt\n\nThis is the first image, which should be an ankle boot as per the labels above.\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x_train[0], 28)));\n\n\n\n\n\n\n\n\nThis is the fourth image, which should be a dress as per the labels above:\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x_train[3], 28)));\n\n\n\n\n\n\n\n\nEverything looks good!\n\n\nChunking with iterators and generators\nI promised I’d return to the chunks function defined above. There’s a way to define it in a simpler fashion without using generators that goes something like this:\n\ndef chunks(x, size):\n    result = []\n    for i in range(0, len(x), size):\n        subarray = x[i:i + size]\n        result.append(subarray)\n    return result\n\nFunctionally this does almost the same thing as what we have above, but this simpler version is less ideal because we’re iteratively building up our array in memory on the fly.\nThe advantage of using a generator (which we create when we use yield) is that it only grabs the number of items we need at once, so it’s far more memory efficient. (In other words, instead of grabbing all 60,000 images at once, we only make one image (i.e. 784 items / pixels) available at any one time.) With our generator, we can see the next set of items by calling next until we reach a StopIterationError.\nIn our implementation, we don’t get so many of those benefits since we’re still going to store everything in memory anyway. We wrap all our chunk generator functions in list() which causes the interpreter to unfurl or unwrap our chunks into an array of arrays.\n\n\nDefine a matrix class\nIn standard Python if we want to index into one of our array image we’d normally have to do something like image[0][15] whereas in machine learning we generally prefer to do something like image[0, 15]. So our next step in manually implementing these things would be to create a Matrix class that allows us to do this.\n\nclass Matrix:\n    def __init__(self, vals):\n        self.vals = vals\n        \n    def __getitem__(self, vals):\n        return self.vals[vals[0]][vals[1]]\n\n\nimg = list(chunks(x_train[3], 28))\nMatrix(img)[0, 10], img[0][10]\n\n(175, 175)\n\n\nNow we have a function that does what we want. You can see we used the built-in __getitem__ function which we over-wrote. This is what Python uses under the hood when you use square brackets to get values from an array, so by overwriting it we can achieve the behaviour we’re looking for.\nIn reality, this implementation probably leaves a bit to be desired, so now that we’ve implemented it from scratch, we can discard our own version and just use a tensor object from PyTorch:\n\nimport torch\nfrom torch import tensor\n\n\nt = tensor(list(chunks(x_train[3], 28)))\nt.shape\n\ntorch.Size([28, 28])\n\n\nNow that we know it works, we can map all our values and turn them into tensor objects.\n\nx_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\nx_train.shape\n\ntorch.Size([60000, 784])\n\n\nAs mentioned above, using tensors comes with a bunch of extra benefits.\n\n\nFind the shape, min and max values\nOur x_train is in the shape [60000, 784] and our y_train is just 60,000 individual values. So we still need to chunk our images into 28x28 matrices.\n\ny_train.shape\n\ntorch.Size([60000])\n\n\nFor the chunking process, instead of our own function we can use PyTorch’s reshape method. Note that we pass in the desired new shape, so in our case that could be (60000,28,28). It is more common, however, to use and see the number -1 for the first value, which implies that PyTorch should figure out what the value for that first dimension should be. In our case, we have 60,000 items so that’s how big it should be and these two options are both equivalent.\n\n# train_imgs = x_train.reshape((60000,28,28))\ntrain_imgs = x_train.reshape((-1,28,28))\ntrain_imgs.shape\n\ntorch.Size([60000, 28, 28])\n\n\nWe can find out the minimum and maximum value found inside one of our images by using Python’s min, but this is an inbuilt method on the tensor object as well so we can use that now:\n\ntrain_imgs[0].min(), train_imgs[0].max()\n\n(tensor(0), tensor(255))\n\n\nAs expected, our pixel values represent the 256 shades of grey available. Our labels represent the 10 possible clothing types listed above, so we’d expect our minimum and maximum to be zero and nine respectively:\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))\n\n\n\n\nGenerate some random numbers\nWe finished out the lesson with a brief discussion of random number generators, how to implement them and how this might be important for deep learning work. As we learned, there is no way for computers to generate truly random numbers. People go to interesting lengths to create their own random numbers, such as the infamous lava lamp wall at Cloudflare, but we are left with being able to create only pseudo-random numbers.\nIn the class we see a way based on the Wichmann Hill algorithm used before Python 2.3, which goes as follows:\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\nWe have two functions. It’s important to note that the seed is an important part of these pseudo-random number generators. The seed gives us a number that our generator starts with. Note also how we use global state to store the rnd_state variable, and how we are continually updating our values in that state as we generate. Also note that the process of generating the values is fairly simple, but also eminently reproducible.\n\nseed(2349873456787298) # some number I came up with by mashing my keyboard numbers\nrnd_state\n\n(24775, 23862, 14675)\n\n\n\nrand(),rand(),rand()\n\n(0.6580068826155674, 0.669616116835976, 0.9249613879702964)\n\n\nWe now have a function which generates pseudo-random numbers, but as we’ll see, it’s not the fastest bit of code around…\n\n\nProfile our code\nPython has a handy timeit module which allows you to calculate how long your code takes to run. What’s more, it’ll run your code hundreds or thousands of times to get average run times. Here we can see how fast our pseudo-random number generator takes to run, on average:\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n4.05 ms ± 500 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nIf we compare it against PyTorch’s random number generator, we can see a distinct difference, however:\n\n%timeit -n 10 torch.randn(784,10)\n\n56.1 µs ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nNow we’re talking about microseconds vs miliseconds, which is a big difference, so we should probably stick to how PyTorch does it.\n\ntorch.randn??\n\n\nDocstring:\nrandn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -&gt; Tensor\nReturns a tensor filled with random numbers from a normal distribution\nwith mean `0` and variance `1` (also called the standard normal\ndistribution).\n.. math::\n    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\nThe shape of the tensor is defined by the variable argument :attr:`size`.\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\nExample::\n    &gt;&gt;&gt; torch.randn(4)\n    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n    &gt;&gt;&gt; torch.randn(2, 3)\n    tensor([[ 1.5954,  2.8929, -1.0923],\n            [ 1.1719, -0.4709, -0.1996]])\nType:      builtin_function_or_method\n\n\n\n\nPyTorch has an interesting document entitled ‘Reproducibility’ in which they state the things that you can try to do to make your code as non-deterministic (i.e. non-random) as possible. This is important if you want to be able to reproduce your work. At the top, however, and as I learned during the Sunday Delft FastAI Study Group discussion, it’s not as simple as setting seed values; sometimes even the hardware used needs to be identical in order to achieve reproducibility:\n\n“Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.”\n\nBut why do we care about random values in deep learning to start with? We care because we are often doing things like augmentations in parallel. Augmentations are randomly generated, so if all the parallel processes use the same random number sequences then all the processes will generate the same augmented images.\nWe saw the nice example in the lesson of how our function runs into exactly this problem:\n\nif os.fork():\n    print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.17354408428968426\nIn child: 0.17354408428968426\n\n\nWe create two forked processes, which are identical copies of each other. Then we call our pseudo-random number generator. You would expect that these two values would be different, but because we’ve copied the entire state then we get identical values. What we’d need to do is to set a new seed at the beginning of each process.\n\nif os.fork():\n    seed(0)\n    print(f'In parent: {rand()}')\nelse:\n    seed(1)\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.01693090619965683\nIn child: 0.02258025041320865\n\n\n\n\n“Things Jeremy says to do”\nI thought I’d gather some of the core ‘things Jeremy says to do’ comments from the video lecture for this part of the lecture.\n\nshow the function signature with shift-Tab\nuse ? and ?? at the end of a method or function to show the docs and the source code respectively\nread all the docs for every Python function you use\n\nlook at all the arguments it takes\npractice with that function inside a notebook\n\n(sometimes) read the full source code\npause the video when something in Jeremy’s code is unfamiliar and experiment around with it in a notebook\n\nread the docs and example code for those new concepts\n\n(at some point) read through all the docs for the Tensor object / concept\n\nNote that there’s a bit more when it comes to the imagic demonstration which I’ll try to cover in a separate blogpost.\nIn any case, there was a lot in this lecture. This week is all about matrix multiplication which I’m looking forward to getting to implement myself!"
  },
  {
    "objectID": "posts/2022-05-15-fashion-mnist-neural-network.html",
    "href": "posts/2022-05-15-fashion-mnist-neural-network.html",
    "title": "A neural network for Fashion MNIST data",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntraining_dresses = [item[0][0] for item in training_data if item[1] == 3]\ntraining_pullovers = [item[0][0] for item in training_data if item[1] == 2]\ntest_dresses = [item[0][0] for item in test_data if item[1] == 3]\ntest_pullovers = [item[0][0] for item in test_data if item[1] == 2]\n\ntraining_dresses_tensor = torch.stack(training_dresses)\ntraining_pullovers_tensor = torch.stack(training_pullovers)\ntest_dresses_tensor = torch.stack(test_dresses)\ntest_pullovers_tensor = torch.stack(test_pullovers)\n\ntrain_x = torch.cat([training_dresses_tensor, training_pullovers_tensor]).view(-1, 28*28)\ntrain_y = torch.cat([torch.ones(len(training_dresses)), torch.zeros(len(training_pullovers))]).unsqueeze(1)\n\nvalid_x = torch.cat([test_dresses_tensor, test_pullovers_tensor]).view(-1, 28*28)\nvalid_y = torch.cat([torch.ones(len(test_dresses)), torch.zeros(len(test_pullovers))]).unsqueeze(1)\n\ntrain_dset = list(zip(train_x, train_y))\nvalid_dset = list(zip(valid_x, valid_y))\n\ntrain_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\n\ndls = DataLoaders(train_dl, valid_dl)\n\ndef initialise_params(size, std=1.0):\n    return (torch.randn(size) * std).requires_grad_()\n\ndef fashion_mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1 - predictions, predictions).mean()\n\ndef batch_accuracy(x_batch, y_batch):\n    preds = x_batch.sigmoid()\n    correct = (preds &gt; 0.5) == y_batch\n    return correct.float().mean()\n\n\nIn the previous post we used stochastic gradient descent to train a model to fit a linear function to our Fashion MNIST data, specifically the difference between a pullover and a dress.\nIn this final stage, we will take the next step to creating a neural network in code that will be used to detect that same difference between a pullover and a dress. The key difference here is that we will need to ‘add non-linearity’ to our function. I have no mathematics background so I have very little intuitive (or learned!) understanding of specifically what that means, but my current mental model as learned during the course is that linear functions just aren’t flexible enough to learn more complex patterns. In the end, what we want is a function that will fit to the patterns in our training data (as mapped to a multidimensional space). Simple linear functions aren’t going to cut it.\nWhat this looks like in code is this:\n\nweights1 = initialise_params((28*28, 30))\nbias1 = initialise_params(30)\nweights2 = initialise_params((30, 1))\nbias2 = initialise_params(1)\n\ndef simple_network(x_batch):\n    result = x_batch@weights1 + bias1\n    result = result.max(tensor(0.0))\n    result = result@weights2 + bias2\n    return result\n\nYou can see the three layers of our simple network pretty clearly in the code above. The middle layer is what otherwise is known as a ReLU or rectified linear unit. It basically means that negative values passing through that function become zero and all positive values are unchanged. When you plot the function it looks like this:\n\nplot_function(F.relu)\n\n\n\n\n\n\n\n\nWhen we put a non-linear function in between two linear functions, then this network is able to encode and express more complicated patterns. This is basically all we’re doing with deep learning: we stack these layers on to make the functions more and more capable of modelling and representing complex things.\nWe can express the above simple network in PyTorch-specific code (functionally it’s the same):\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\nAt this point, training a model is similar to what we did last time round:\n\nlearn = Learner(dls, simple_net, opt_func=SGD, loss_func=fashion_mnist_loss, metrics=batch_accuracy)\nlearn.fit(30, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.166980\n0.079354\n0.959000\n00:00\n\n\n1\n0.087369\n0.057471\n0.961000\n00:00\n\n\n2\n0.059999\n0.050108\n0.963500\n00:00\n\n\n3\n0.048050\n0.046509\n0.963500\n00:00\n\n\n4\n0.041847\n0.044165\n0.964000\n00:00\n\n\n5\n0.038310\n0.042841\n0.963500\n00:00\n\n\n6\n0.036514\n0.041464\n0.964500\n00:00\n\n\n7\n0.034653\n0.040640\n0.964500\n00:00\n\n\n8\n0.033204\n0.039827\n0.965000\n00:00\n\n\n9\n0.032370\n0.039344\n0.965000\n00:00\n\n\n10\n0.032060\n0.038778\n0.965000\n00:00\n\n\n11\n0.031814\n0.038854\n0.965500\n00:00\n\n\n12\n0.031212\n0.037872\n0.965000\n00:00\n\n\n13\n0.030837\n0.037836\n0.964500\n00:00\n\n\n14\n0.030193\n0.037372\n0.965000\n00:00\n\n\n15\n0.030213\n0.037250\n0.965000\n00:00\n\n\n16\n0.030058\n0.036885\n0.965000\n00:00\n\n\n17\n0.029849\n0.036862\n0.965500\n00:00\n\n\n18\n0.029551\n0.036518\n0.965500\n00:00\n\n\n19\n0.029381\n0.036236\n0.966000\n00:00\n\n\n20\n0.029358\n0.035996\n0.966500\n00:00\n\n\n21\n0.028987\n0.036024\n0.966000\n00:00\n\n\n22\n0.028436\n0.035731\n0.966500\n00:00\n\n\n23\n0.028494\n0.035813\n0.967000\n00:00\n\n\n24\n0.028174\n0.035535\n0.966000\n00:00\n\n\n25\n0.028089\n0.035361\n0.966500\n00:00\n\n\n26\n0.027961\n0.035035\n0.966500\n00:00\n\n\n27\n0.027761\n0.035248\n0.966000\n00:00\n\n\n28\n0.028016\n0.035006\n0.966500\n00:00\n\n\n29\n0.027430\n0.034841\n0.967500\n00:00\n\n\n\n\n\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\n\n\n\n\n\nlearn.recorder.values[-1][2]\n\n0.9674999713897705\n\n\nWe don’t actually emerge at the end of this with a vastly superior score to what we had at the end of the last notebook, but this basis (the simple neural network) has far more open vistas within which we can work and build upon.\nFinally, to round out my understanding, I put together a little diagram showing the various pieces that go into the Learner class when we instantiate it, adding some of the other concepts etc below it as I felt was appropriate. This isn’t a complete picture by any means, but I find it helpful to visualise how things are layered and pieced together:"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html",
    "href": "posts/2022-05-13-sgd-whole-game.html",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev\n\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#initialise-the-parameters",
    "href": "posts/2022-05-13-sgd-whole-game.html#initialise-the-parameters",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "1. Initialise the parameters",
    "text": "1. Initialise the parameters\nWe begin with random values. We also make sure to set up our Tensor so that we’re able to calculate the gradients.\n\nparams = torch.randn(3).requires_grad_()"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#calculate-the-predictions",
    "href": "posts/2022-05-13-sgd-whole-game.html#calculate-the-predictions",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "2. Calculate the predictions",
    "text": "2. Calculate the predictions\nWe make the calculations by passing our parameter values into our function f. We can visualise what our predictions would look like with those parameters.\n\npreds = f(time, params)\n\n\ndef show_preds(preds, ax=None):\n    if ax is None:\n        ax = plt.subplots()[1]\n        ax.scatter(time, speed)\n        ax.scatter(time, to_np(preds), color = 'red')\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\nYou can see that there’s a decent amount of difference between the curve denoting our predictions for the params (in red) and the actual function (in blue)."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#calculate-the-loss",
    "href": "posts/2022-05-13-sgd-whole-game.html#calculate-the-loss",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "3. Calculate the loss",
    "text": "3. Calculate the loss\nWe use the mean squared error as a way of calculating our loss.\n\nloss = mse(preds, speed)\nloss\n\ntensor(200.6502, grad_fn=&lt;SqrtBackward0&gt;)\n\n\nThis number is a measure of how far off our predictions are from the actual values. We want to improve this loss and drive it down even further, and for that we’ll need the gradients."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#calculate-the-gradient",
    "href": "posts/2022-05-13-sgd-whole-game.html#calculate-the-gradient",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "4. Calculate the gradient",
    "text": "4. Calculate the gradient\nAs described in the previous post, we use PyTorch’s inbuilt ability to calculate gradients.\n\nloss.backward()\nparams.grad\n\ntensor([166.3746,  10.6914,   0.6876])\n\n\nWe can update our parameters based on the learning rate. For our purposes here we can choose a really small learning rate: 0.00001 or 1e-5. This is what the values of our parameters would look like after that operation:\n\nparams * 0.00001\n\ntensor([ 1.2895e-05,  5.8427e-06, -2.3174e-06], grad_fn=&lt;MulBackward0&gt;)\n\n\n\nparams\n\ntensor([ 1.2895,  0.5843, -0.2317], requires_grad=True)"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#step-our-weights",
    "href": "posts/2022-05-13-sgd-whole-game.html#step-our-weights",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "5. Step our weights",
    "text": "5. Step our weights\nWe can step our parameters using the formula previously described: multiply the learning rate by the gradients.\n\nlearning_rate = 0.00001\nparams.data -= learning_rate * params.grad.data\nparams.grad\n\ntensor([166.3746,  10.6914,   0.6876])\n\n\nWe can visualise whether this has improved our function’s curve or not:\n\npreds = f(time, params)\nmse(preds, speed)\n\ntensor(200.3723, grad_fn=&lt;SqrtBackward0&gt;)\n\n\n\nshow_preds(preds)\n\n\n\n\n\n\n\n\nOur loss has gone from 268.4112 to 268.1312. An improvement, but it feels like a small improvement!"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#repeat-and-iterate",
    "href": "posts/2022-05-13-sgd-whole-game.html#repeat-and-iterate",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "6. Repeat and iterate",
    "text": "6. Repeat and iterate\nTo save ourselves some time, we can create a function that will help us in iterating through and repeating the above steps:\n\ndef repeat_sgd(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= learning_rate * params.grad.data\n    params.grad = None\n    if prn:\n        print(loss.item())\n    return preds\n\n\n# iterate a few times\nfor i in range(10):\n    repeat_sgd(params)\n\n200.3722686767578\n199.81639099121094\n199.53848266601562\n199.2605743408203\n198.98269653320312\n198.70480346679688\n198.4269561767578\n198.14910888671875\n197.87124633789062\n197.59344482421875\n\n\nWe see that our loss is going down, so everything is improving as we’d hope. The progress seems slow, but it’s progress nonetheless. I imagine we could increasing the learning rate to make the loss go down faster."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#stop-when-weve-iterated-enough",
    "href": "posts/2022-05-13-sgd-whole-game.html#stop-when-weve-iterated-enough",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "7. Stop when we’ve iterated enough",
    "text": "7. Stop when we’ve iterated enough\nWe’ve only iterated a few times here, but really what we’d want to do is keep going until we reached our stopping point (either we’ve taken too long or our model is ‘good enough’ for our needs)."
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#get-the-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#get-the-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Get the data",
    "text": "Get the data\nFirst thing to do: we’ll download the MNIST dataset for us to use as our example.\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n# what does this do?\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#3) [Path('valid'),Path('labels.csv'),Path('train')]\n\n\n\n# see what's inside the training folder\n(path / 'train').ls()\n\n(#2) [Path('train/7'),Path('train/3')]\n\n\n3 and 7 are the labels or targets of our dataset. We are trying to predict whether, given a particular image, we are looking at a 3 or a 7.\n\nthrees = (path / 'train'/'3').ls().sorted()\nsevens = (path / 'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]\n\n\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\n\n\n\n\nNow we’re getting a slice of the image to see how it is represented underneath as numbers.\nThe first index value is the rows we want, and then we get the columns.\n\narray(im3)[4:10,4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\narray(im3)[4:10,4:15]\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254],\n       [  0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253],\n       [  0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10],\n       [  0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0],\n       [  0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43]], dtype=uint8)\n\n\nWe can also have the same as a tensor. This is a PyTorch object which will allow us to get the benefits of everything PyTorch has to offer (plus speed optimisation if we use a GPU).\n\ntensor(im3)[4:10,4:15]\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254],\n        [  0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253],\n        [  0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10],\n        [  0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0],\n        [  0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43]], dtype=torch.uint8)\n\n\nIt basically looks the same except for the name of the object at this moment.\nIf we plot out the values of a different slice we can visualise how the numbers are used to output the actual image.\n\nim3_tensor = tensor(im3)\ndf = pd.DataFrame(im3_tensor[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0\n\n\n\n\n\nNote that the rest of this is 28x28 pixels for the whole image, and each pixel can contain values from 0 to 255 to represent all the shades from white to black."
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#approach-1-pixel-similarity",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#approach-1-pixel-similarity",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "APPROACH 1: PIXEL SIMILARITY",
    "text": "APPROACH 1: PIXEL SIMILARITY\n\nFind the average pixel value for each pixel of the 3s and 7s\nUse this ‘ideal’ 3 and 7 to compare a single image that we want to know whether it’s a 3 or a 7\n\n\nthree_tensors = [tensor(Image.open(img)) for img in threes]\nseven_tensors = [tensor(Image.open(img)) for img in sevens]\nlen(three_tensors), len(seven_tensors)\n\n(6131, 6265)\n\n\nWe can view these images with the fastai show_image function. This takes a tensor and makes it viewable in a Jupyter Notebook:\n\nshow_image(three_tensors[8]);\n\n\n\n\n\n\n\n\n\ntorch.stack(three_tensors)[0][4:15, 4:15]\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,  42, 118, 219, 166],\n        [  0,   0,   0,   0,   0,   0, 103, 242, 254, 254, 254],\n        [  0,   0,   0,   0,   0,   0,  18, 232, 254, 254, 254],\n        [  0,   0,   0,   0,   0,   0,   0, 104, 244, 254, 224],\n        [  0,   0,   0,   0,   0,   0,   0,   0, 207, 254, 210],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  84, 206, 254],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 209],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  91, 137, 253],\n        [  0,   0,   0,   0,   0,   0,  40, 214, 250, 254, 254],\n        [  0,   0,   0,   0,   0,   0,  81, 247, 254, 254, 254],\n        [  0,   0,   0,   0,   0,   0,   0, 110, 246, 254, 254]], dtype=torch.uint8)\n\n\nIt’s fairly common to have to convert a collection (i.e. in our case, a list) of tensors into a single tensor object with an extra dimension, so that’s why we have the torch.stack method which takes a collection and returns a rank-3 tensor.\n\n\n\n\n\n\nTip\n\n\n\nYou’ll see here that we also convert our values into floats and normalise them (i.e. divide by 255) since this will help us at a later stage.\n\n\n\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_sevens = torch.stack(seven_tensors).float()/255\n\nWe can see that the first axis of this tensor is our 6131 images, and each image has 28x28 pixels. The shape tells you the length of each axis.\n\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\nNote: the length of a tensor’s shape is its rank. We have a rank-3 tensor:\n\nlen(stacked_threes.shape)\n\n3\n\n\n\n# you can get the same value by using `ndim`\nstacked_threes.ndim\n\n3\n\n\n\nRank = the number of axes in a tensor.\nShape = the size of each axis of a tensor.\n\n\nGetting the ideal 3\nWe take our stack of threes (i.e. a rank-3 tensor) and we calculate the mean across the first dimension (i.e. the 6131 individual threes).\nBy the end, you can see that mean3 is actually a 28x28 tensor (i.e. a single image) that represents the ideal version of our threes data.\n\nmean3 = stacked_threes.mean(0)\n\n\nmean3.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean3);\n\n\n\n\n\n\n\n\n\n# repeat calculations for 7s\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\n\n\n\n\n\n\nComparing our ideal numbers with an individual sample\n\nsample_3 = stacked_threes[35]\nsample_7 = stacked_sevens[35]\nshow_image(sample_3);\n\n\n\n\n\n\n\n\nThere are two main ways we can measure the distance (i.e. similarity) in this context:\n\nthe Mean Absolute Difference aka L1 Norm – the mean of the absolute value of differences (our absolute value replaces negative values with positive values)\nthe Root Mean Squared Error (RMSE) aka L2 Norm – we square all the differences (making everything positive), get the mean of those values, and then square root everything (which undoes all the squaring).\n\n\n# in code it looks like this\n\ndef get_l1_norm(tensor1, tensor2):\n    return (tensor1 - tensor2).abs().mean()\n\ndef get_l2_norm(tensor1, tensor2):\n    return ((tensor1 - tensor2)**2).mean().sqrt()\n\n\nl1_norm_distance_3 = get_l1_norm(sample_3, mean3)\nl2_norm_distance_3 = get_l2_norm(sample_3, mean3)\n\nl1_norm_distance_3, l2_norm_distance_3\n\n(tensor(0.1401), tensor(0.2545))\n\n\n\nl1_norm_distance_7 = get_l1_norm(sample_3, mean7)\nl2_norm_distance_7 = get_l2_norm(sample_3, mean7)\n\nl1_norm_distance_7, l2_norm_distance_7\n\n(tensor(0.1670), tensor(0.3121))\n\n\nThe differences from our sample_3 to the mean3 is less than the differences between our sample_3 and the mean7. This totally makes sense and is what we were expecting!\n\nassert l1_norm_distance_3 &lt; l1_norm_distance_7\nassert l2_norm_distance_3 &lt; l2_norm_distance_7\n\n\n\nThe PyTorch built-in ways of calculating loss\nPyTorch exposes a variety of loss functions at torch.nn.functional which it recommends importing as F. These are available by default under that name in fastai.\n\n# get a sense of what options are available by inspecting the functions that are available to us with `rich`\n\n# !pip install -Uqq rich\n# from rich import inspect as rinspect\n# rinspect(F, methods=True)\n\n\npytorch_l1_norm_distance_3 = F.l1_loss(sample_3.float(), mean3)\npytorch_l1_norm_distance_7 = F.l1_loss(sample_3.float(), mean7)\n\nassert pytorch_l1_norm_distance_3 &lt; pytorch_l1_norm_distance_7\npytorch_l1_norm_distance_3, pytorch_l1_norm_distance_7\n\n(tensor(0.1401), tensor(0.1670))\n\n\nFor the L2 norm, the PyTorch function only calculates the mean squared error loss, so we have to add in the square root at the end ourselves:\n\npytorch_l2_norm_distance_3 = F.l1_loss(sample_3.float(), mean3).sqrt()\npytorch_l2_norm_distance_7 = F.l1_loss(sample_3.float(), mean7).sqrt()\n\nassert pytorch_l2_norm_distance_3 &lt; pytorch_l2_norm_distance_7\npytorch_l2_norm_distance_3, pytorch_l2_norm_distance_7\n\n(tensor(0.3743), tensor(0.4087))\n\n\nWhen to choose one vs the other?\n\nMSE penalises bigger mistakes more than the L1 norm\nMSE is more lenient with small mistakes than the L1 norm"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#load-the-fashion-mnist-dataset-with-pytorch",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#load-the-fashion-mnist-dataset-with-pytorch",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Load the Fashion MNIST dataset with PyTorch",
    "text": "Load the Fashion MNIST dataset with PyTorch\nThe Fashion MNIST dataset was created as a more advanced / difficult (and perhaps interesting) alternative to MNIST for computer vision tasks. It was first released in 2017 and I thought it might be interesting to try the same technique as we tried there on this new data set.\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\n\n\n\n\nI found it a bit hard to extract the data for the images, stored as it was together with the labels. Here you can see me getting the image and the label.\n\nshow_image(training_data[1][0][0]);\n\n\n\n\n\n\n\n\n\ntraining_data[1][1]\n\n0\n\n\nThere are 60,000 items in this training dataset, and 10 categories, so it makes sense that the set of images for the two categories I extracted would be 6000 each.\n\ntraining_dresses = [item[0][0] for item in training_data if item[1] == 3]\ntraining_pullovers = [item[0][0] for item in training_data if item[1] == 2]\n\nlen(training_dresses), len(training_pullovers)\n\n(6000, 6000)\n\n\n\ntraining_dresses_tensor = torch.stack(training_dresses)\ntraining_pullovers_tensor = torch.stack(training_pullovers)\n\n\ntraining_dresses_tensor.shape\n\ntorch.Size([6000, 28, 28])"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-mean-image-for-a-dress-and-a-pullover",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-mean-image-for-a-dress-and-a-pullover",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Calculate the mean image for a dress and a pullover",
    "text": "Calculate the mean image for a dress and a pullover\n\nmean_training_dress = training_dresses_tensor.mean(0)\nmean_training_pullover = training_pullovers_tensor.mean(0)\n\n\nmean_training_dress.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean_training_dress);\n\n\n\n\n\n\n\n\n\nshow_image(mean_training_pullover);\n\n\n\n\n\n\n\n\nI chose the two because I wondered whether there might be a decent amount of crossover between the two items. You can see even in the ‘mean’ / average versions of the items that they look fairly similar."
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#assemble-the-validation-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#assemble-the-validation-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Assemble the validation data",
    "text": "Assemble the validation data\n\ntest_dresses = [item[0][0] for item in test_data if item[1] == 3]\ntest_pullovers = [item[0][0] for item in test_data if item[1] == 2]\n\nlen(test_dresses), len(test_pullovers)\n\n(1000, 1000)\n\n\n\ntest_dresses_tensor = torch.stack(test_dresses)\ntest_pullovers_tensor = torch.stack(test_pullovers)\n\n\ntest_dresses_tensor.shape\n\ntorch.Size([1000, 28, 28])\n\n\nI also extract a single dress and a single pullover to check the loss in the next section.\n\nsample_test_dress = test_dresses_tensor[0]\nsample_test_pullover = test_pullovers_tensor[10]\n\n\nshow_image(sample_test_dress);\n\n\n\n\n\n\n\n\n\nshow_image(sample_test_pullover);"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-loss-comparing-a-random-pullover-with-validation-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-loss-comparing-a-random-pullover-with-validation-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Calculate the loss comparing a random pullover with validation data",
    "text": "Calculate the loss comparing a random pullover with validation data\n\ndef get_l1_norm(tensor1, tensor2):\n    return (tensor1 - tensor2).abs().mean()\n\ndef get_l2_norm(tensor1, tensor2):\n    return ((tensor1 - tensor2)**2).mean().sqrt()\n\n\nl1_norm_distance_dress = get_l1_norm(sample_test_dress, mean_training_dress)\nl2_norm_distance_dress = get_l2_norm(sample_test_dress, mean_training_dress)\n\nl1_norm_distance_dress, l2_norm_distance_dress\n\n(tensor(0.1134), tensor(0.1766))\n\n\n\nl1_norm_distance_pullover = get_l1_norm(sample_test_pullover, mean_training_pullover)\nl2_norm_distance_pullover = get_l2_norm(sample_test_pullover, mean_training_pullover)\n\nl1_norm_distance_pullover, l2_norm_distance_pullover\n\n(tensor(0.1713), tensor(0.2220))\n\n\nThe differences from our sample_test_dress to the mean_training_dress is less than the differences between our sample_test_dress and the mean_training_pullover. This totally makes sense and is what we were expecting!\n\nassert l1_norm_distance_dress &lt; l1_norm_distance_pullover\nassert l2_norm_distance_dress &lt; l2_norm_distance_pullover"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#using-broadcasting-to-check-our-predictions-on-our-validation-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#using-broadcasting-to-check-our-predictions-on-our-validation-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Using broadcasting to check our predictions on our validation data",
    "text": "Using broadcasting to check our predictions on our validation data\nThis function returns the L1 norm loss when calculated between two tensors. Because of the final tuple passed into .mean() (i.e. (-1, -2)), we can actually use this function to calculate the distance between a single image as compared to a full rank-3 tensor.\n\ndef fashion_mnist_distance(tensor1, tensor2):\n    return (tensor1 - tensor2).abs().mean((-1, -2))\n\n\nfashion_mnist_distance(sample_test_dress, mean_training_dress)\n\ntensor(0.1134)\n\n\n\nfashion_mnist_distance(sample_test_dress, mean_training_pullover)\n\ntensor(0.2864)\n\n\nAgain, our dress is ‘closer’ to the mean_training_dress than it is to the mean_training_pullover.\nWe can now create a function that predicts (by way of calculating and comparing these two losses) whether an item is a dress or not.\n\ndef is_dress(x):\n    return fashion_mnist_distance(x, mean_training_dress) &lt; fashion_mnist_distance(x, mean_training_pullover)\n\n\nis_dress(sample_test_dress)\n\ntensor(True)\n\n\n\nis_dress(sample_test_pullover)\n\ntensor(False)\n\n\n…as expected…\nFinally, we can get an overall sense of how well our prediction function is on average. We get it to calculate predictions for the entire test dataset and average them out.\n\ndress_accuracy = is_dress(test_dresses_tensor).float().mean()\n\n\npullover_accuracy = 1 - is_dress(test_pullovers_tensor).float().mean()\n\n\ncombined_accuracy = (dress_accuracy + pullover_accuracy) / 2\ncombined_accuracy, dress_accuracy, pullover_accuracy\n\n(tensor(0.9175), tensor(0.9730), tensor(0.8620))\n\n\nOverall, I think 91% accuracy using this fairly simple mechanism isn’t too bad at all! That said, in the fastai course we’re here to learn about deep learning, so in the next post I will dive more into the beginnings of a more advanced approach to making the same calculation."
  },
  {
    "objectID": "posts/2021-09-10-chapter1and2recall.html",
    "href": "posts/2021-09-10-chapter1and2recall.html",
    "title": "Retrieval Practice with fastai chapters 1 and 2",
    "section": "",
    "text": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover.\nWe start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I’ve seen much in Python imports.\n\n# !pip install fastai\nfrom fastai.vision.all import *\n\nThen we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not.\nThe simple assert testing was a little trick that I saw mentioned somewhere this past week. It’s not a full-fledged test suite, but it’s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else.\n\n# define the function that'll classify the images\ndef is_cat(string):\n    return string[0].isupper()\n\nassert is_cat(\"abs\") == False\nassert is_cat(\"Abs\") == True\n\nNow we have to import the data for the files and apply whatever custom transforms we want applied to them.\nI had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be.\nIt’s interesting that we actually don’t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that’s because the task is so close to that of the original resnet architecture.\n\n# import the data\npath = untar_data(URLs.PETS)/'images'\ndls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224))\n\nThen it’s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we’ll see displayed in the output.\n\n# instantiate a learner\nlearner = cnn_learner(dls, resnet34, metrics=error_rate)\n\n# fine-tune the model\nlearner.fine_tune(5)\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.140326\n0.019799\n0.008119\n00:19\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.046906\n0.021923\n0.006089\n00:24\n\n\n1\n0.041144\n0.009382\n0.004060\n00:25\n\n\n2\n0.028892\n0.004109\n0.002030\n00:25\n\n\n3\n0.008950\n0.002290\n0.001353\n00:25\n\n\n4\n0.004486\n0.002822\n0.001353\n00:25\n\n\n\n\n\nAnd here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad!\n\nlearner.show_results()"
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html",
    "title": "Trying to instrument an agentic app with Arize Phoenix and litellm",
    "section": "",
    "text": "It’s important to instrument your AI applications! I hope this can more or less be taken as given just as you’d expect a non-AI-infused app to capture logs. When you’re evaluating your LLM-powered system, you need to have capture the inputs and outputs both at an end-to-end level in terms of the way the user experiences things as well as with more fine-grained granularity for all the internal workings.\nMy goal with this blog is to first demonstrate how Phoenix and litellm can work together, and then to make sure that we are able to group all spans together under a single trace.\nI’ll write the blog as I work so at this point I’m not sure exactly how this will turn out."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#basic-logging-with-litellm-phoenix",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#basic-logging-with-litellm-phoenix",
    "title": "Trying to instrument an agentic app with Arize Phoenix and litellm",
    "section": "Basic logging with litellm + phoenix",
    "text": "Basic logging with litellm + phoenix\nAs a reminder, here’s how we make an LLM call with litellm:\nimport litellm\n\ncompletion_response = litellm.completion(\n    model=\"openrouter/google/gemma-3n-e4b-it:free\",\n    messages=[\n        {\n            \"content\": \"What's the capital of China? Just give me the name.\",\n            \"role\": \"user\",\n        }\n    ],\n)\nprint(completion_response.choices[0].message.content)\n# prints 'Beijing'\nThe Phoenix docs explain how to set up basic logging for litellm:\n\ninstall the following pip packages:\n\narize-phoenix-otel\nopeninference-instrumentation-litellm\n(litellm, obviously)\n\nset up the necessary environment variables with API key etc to ensure that traces get sent to the right account and endpoint\n\nLet’s assume we’re using the hosted Phoenix Cloud version for now. Then we can rerun our example, with some slight tweaks:\n\nimport litellm\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n    project_name=\"hinbox\",  # Default is 'default'\n    auto_instrument=True,  # Auto-instrument your app based on installed OI dependencies\n)\n\ncompletion_response = litellm.completion(\n    model=\"openrouter/google/gemma-3n-e4b-it:free\",\n    messages=[\n        {\n            \"content\": \"What's the capital of China? Just give me the name.\",\n            \"role\": \"user\",\n        }\n    ],\n)\nprint(completion_response.choices[0].message.content)\nSo we first register the Phoenix tracer, specify the project (already set up in Phoenix Cloud) and then run our litellm completion as previously. In the terminal we see he following logs:\n🔭 OpenTelemetry Tracing Details 🔭\n|  Phoenix Project: hinbox\n|  Span Processor: SimpleSpanProcessor\n|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n|  Transport: HTTP + protobuf\n|  Transport Headers: {'api_key': '****', 'authorization': '****'}\n|  \n|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n|  \n|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n|  \n|  `register` has set this TracerProvider as the global OpenTelemetry default.\n|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n\nBeijing\nSo immediately there are a lot of things to consider. It seems that we’ll want to use the BatchSpanProcessor that it suggests, and also it seems like I might not want to set this as the global tracing provider, too.\nIn Phoenix Cloud, I see this:\n\n\n\nBasic Phoenix tracing interface\n\n\nAs you can see, we’ve captured the input and output messages for the completion, it’s tracked the latency of the call (1.16s, which seems pretty slow actually!). There is also some sort of an annotation interface though I’ll explore that down the line maybe. I immediately notice that I’m missing things like the system attributes for where the call was made, also metadata like the temperature and other settings. I’d also like to see things like token counts (which you can get in Phoenix but they’re sort of buried) as well as the estimated cost of the call(s) and so on. We can see about adding some of that down the line."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#batchspanprocessor-for-production-usage",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#batchspanprocessor-for-production-usage",
    "title": "Trying to instrument an agentic app with Arize Phoenix and litellm",
    "section": "BatchSpanProcessor for production usage",
    "text": "BatchSpanProcessor for production usage\nLet’s next move on to adding BatchSpanProcessor as the message suggested, which is as simple as adding batch=True to the tracer provider registration code. What this does is make sure that spans are processed in batches before they’re exported to Arize. This takes away some of the network costs that you incur when sending the spans one by one. I’ve also made sure to turn off the registration of this tracing provider as the global one:\n\nimport litellm\nfrom phoenix.otel import register\n\n# configure the Phoenix tracer\ntracer_provider = register(\n    project_name=\"hinbox\",  # Default is 'default'\n    auto_instrument=True,  # Auto-instrument your app based on installed OI dependencies\n    set_global_tracer_provider=False,\n    batch=True,\n)\n\ncompletion_response = litellm.completion(\n    model=\"openrouter/google/gemma-3n-e4b-it:free\",\n    messages=[\n        {\n            \"content\": \"What's the capital of China? Just give me the name.\",\n            \"role\": \"user\",\n        }\n    ],\n)\nprint(completion_response.choices[0].message.content)\nAnd I get this in the terminal:\n🔭 OpenTelemetry Tracing Details 🔭\n|  Phoenix Project: hinbox\n|  Span Processor: BatchSpanProcessor\n|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n|  Transport: HTTP + protobuf\n|  Transport Headers: {'api_key': '****', 'authorization': '****'}\n|  \n|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n\nBeijing\nIt’s actually somehow a bit annoying to still see a message about the fact that I’m using a default SpanProcessor. It’s unclear to me why I need to care that this is a default one. The message is taking up real estate in the logs and it seems important (otherwise why would they have included it?) but it’s also unclear to me what the alternative is and why I’d want to overwrite the default. I think for now I’ll leave it."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#using-the-litellm-callbacks-as-an-alternative",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#using-the-litellm-callbacks-as-an-alternative",
    "title": "Trying to instrument an agentic app with Arize Phoenix and litellm",
    "section": "Using the litellm callbacks as an alternative",
    "text": "Using the litellm callbacks as an alternative\nIf we stray away from the official supported way to handle tracing with Phoenix, there’s also the community-supported in-built litellm option:\nimport litellm\n\nlitellm.callbacks = [\"arize_phoenix\"]\n\ncompletion_response = litellm.completion(\n    model=\"openrouter/google/gemma-3n-e4b-it:free\",\n    messages=[\n        {\n            \"content\": \"What's the capital of China? Just give me the name.\",\n            \"role\": \"user\",\n        }\n    ],\n    metadata={\"PROJECT_NAME\": \"hinbox\"},\n)\nprint(completion_response.choices[0].message.content)\nThis achieves a similar result, though I was unable to get the trace to land anywhere other than the default project. Arize’s docs mention a PHOENIX_PROJECT_NAME environment variable but it seems this isn’t respected or used by the litellm implementation. Indeed when I look at the implementation, I don’t see this being used anywhere, so it seems that the community-driven implementation isn’t really the way forward.\nI just wanted to mention it, however, since some of the ‘callback’ integrations for tracing in litellm are really nicely implemented (like the one for Langfuse, e.g.) so I wanted to try it out at least."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#one-trace-multiple-spans",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#one-trace-multiple-spans",
    "title": "Trying to instrument an agentic app with Arize Phoenix and litellm",
    "section": "One trace, multiple spans",
    "text": "One trace, multiple spans\nFor anything beyond a simple LLM call, which means most real-world LLM applications, we’ll want to be capturing multiple spans as part of a single trace.\n\nLLM Tracing Tools’ Naming Conventions (June 2025)\nSide-note: I dug into how some of the major LLM tracing providers name their primitives. I was reassured that we seem to have coalesced around ‘trace -&gt; span’ and that the OpenTelemetry way seems to have been adopted by most.\n\n\n\nTracing nomenclature (June 2025)\n\n\n\n\nGrouping spans under a single trace\nI updated the code such that we now have a function that makes two separate LLM calls. I’d want them to both be registered as spans under the same trace:\nimport litellm\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"hinbox\",  # Default is 'default'\n    auto_instrument=True,  # Auto-instrument your app based on installed OI dependencies\n    set_global_tracer_provider=False,\n    batch=True,\n)\n\ndef query_llm(prompt: str):\n    completion_response = litellm.completion(\n        model=\"openrouter/google/gemma-3n-e4b-it:free\",\n        messages=[\n            {\n                \"content\": prompt,\n                \"role\": \"user\",\n            }\n        ],\n    )\n    return completion_response.choices[0].message.content\n\ndef my_llm_application():\n    query1 = query_llm(\"What's the capital of China? Just give me the name.\")\n    query2 = query_llm(\"What's the capital of Japan? Just give me the name.\")\n    return (query1, query2)\n\nif __name__ == \"__main__\":\n    print(my_llm_application())\nBut these just get registered as two separate traces/calls. The key bit of the documentation is the ‘Using Phoenix Decorator’ section, it seems. If I add a decorator on top of my function and get the specific tracer, it seems I am able to start to group things together:\nimport litellm\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"hinbox\",  # Default is 'default'\n    auto_instrument=True,  # Auto-instrument your app based on installed OI dependencies\n    set_global_tracer_provider=False,\n    batch=True,\n)\ntracer = tracer_provider.get_tracer(__name__)\n\ndef query_llm(prompt: str):\n    completion_response = litellm.completion(\n        model=\"openrouter/google/gemma-3n-e4b-it:free\",\n        messages=[\n            {\n                \"content\": prompt,\n                \"role\": \"user\",\n            }\n        ],\n    )\n    return completion_response.choices[0].message.content\n\n@tracer.llm\ndef my_llm_application():\n    query1 = query_llm(\"What's the capital of China? Just give me the name.\")\n    query2 = query_llm(\"What's the capital of Japan? Just give me the name.\")\n    return (query1, query2)\n\nif __name__ == \"__main__\":\n    print(my_llm_application())\nThis works and I see this in the Phoenix Cloud dashboard:\n\n\n\nGrouped traces under a single span\n\n\nSee how it’s taken the function name as the name of the span. And it’s grouped those two LLM calls that happen within the function as we wanted. We can also update the decorator to denote different kinds of spans that we want to capture:\n\n\n\nThe kinds of spans you can choose from\n\n\nI’m immediately a bit confused by the interface again, because when you click on the ‘Traces’ tab in Phoenix Cloud you actually still just see ‘spans’:\n\n\n\nSpans in the Traces tab\n\n\nIn the documentation it isn’t clear to me how to create a trace that includes an llm span and an embedding span, for example. What’s even more frustrating is that the tracer decorator object doesn’t implement all the span types, just agent, chain and llm it seems. I tried something like this but it just ended up producing 3 separate traces in Phoenix Cloud.\nI looked at the documentation for using base OTEL instead of the Phoenix decorators, but there was also nothing in there on how to denote the trace instead of just the span.\nI was wondering if their ‘Sessions’ primitive was the way forward here, but they’re pretty clear in stating that a Session is a “sequence of traces”.\nSo I’m at a bit of a dead end with Phoenix for now. I might return to Braintrust or Langfuse since these seem to have better support for what I’m trying to do (i.e. group spans together underneath a trace). I’m really reluctant to try to instrument hinbox with Phoenix when I’m unable even to get this basic grouping working properly with some dummy code."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#update-solution-from-the-arize-team",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-arize-phoenix-and-litellm.html#update-solution-from-the-arize-team",
    "title": "Trying to instrument an agentic app with Arize Phoenix and litellm",
    "section": "Update: solution from the Arize team",
    "text": "Update: solution from the Arize team\nI posted this blog on the Arize slack and they got back to me with a solution:\nimport litellm\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"hinbox\",  # Default is 'default'\n    auto_instrument=True,  # Auto-instrument your app based on installed OI dependencies\n    set_global_tracer_provider=False,\n)\ntracer = tracer_provider.get_tracer(__name__)\n\n@tracer.llm\ndef query_llm(prompt: str):\n    completion_response = litellm.completion(\n        model=\"openrouter/google/gemma-3n-e4b-it:free\",\n        messages=[\n            {\n                \"content\": prompt,\n                \"role\": \"user\",\n            }\n        ],\n    )\n    return completion_response.choices[0].message.content\n\n@tracer.agent\ndef query_agent(prompt: str):\n    return \"I am an agent.\"\n\n@tracer.chain\ndef my_llm_application():\n    query1 = query_llm(\"What's the capital of China? Just give me the name.\")\n    query2 = query_llm(\"What's the capital of Japan? Just give me the name.\")\n    agent1 = query_agent(\"Who are you?\")\n    return (query1, query2, agent1)\n\nif __name__ == \"__main__\":\n    print(my_llm_application())\nAnd you can see how this looks in the Phoenix Cloud dashboard:\n\n\n\nGrouped spans\n\n\nJudging from the code it seems like the way the span is constructed simply depends on how you assemble the hierarchy of spans. For instance, if I wanted to consider the top-level entity for this ‘trace’ (i.e. a grouping of spans) then I could use this code:\nimport litellm\nfrom phoenix.otel import register\n\ntracer_provider = register(\n    project_name=\"hinbox\",  # Default is 'default'\n    auto_instrument=True,  # Auto-instrument your app based on installed OI dependencies\n    set_global_tracer_provider=False,\n    # batch=True,\n)\ntracer = tracer_provider.get_tracer(__name__)\n\n@tracer.llm\ndef query_llm(prompt: str):\n    completion_response = litellm.completion(\n        model=\"openrouter/google/gemma-3n-e4b-it:free\",\n        messages=[\n            {\n                \"content\": prompt,\n                \"role\": \"user\",\n            }\n        ],\n    )\n    return completion_response.choices[0].message.content\n\n@tracer.agent\ndef query_agent(prompt: str):\n    return \"I am an agent.\"\n\n@tracer.tool(name=\"query_embedding\", description=\"Query embedding\")\ndef query_embedding(prompt: str):\n    return [0.1, 0.2, 0.3]\n\n@tracer.agent\ndef my_llm_application():\n    query1 = query_llm(\"What's the capital of China? Just give me the name.\")\n    query2 = query_llm(\"What's the capital of Japan? Just give me the name.\")\n    agent1 = query_agent(\"Who are you?\")\n    embedding1 = query_embedding(\"What's the capital of China? Just give me the name.\")\n    return (query1, query2, agent1, embedding1)\n\nif __name__ == \"__main__\":\n    print(my_llm_application())\nAnd now instead of this trace being of kind ‘chain’, it’s now of kind ‘agent’, which some internal spans also being of kind ‘agent’. In a conversation in the Arize Slack I got the following clarification:\n\n“Traces as the concept under”signals” is basically a unique identifier of spans (think “span” of time). See https://opentelemetry.io/docs/concepts/signals/traces/ In most cases if you filter spans by “roots” (e.g. spans that don’t have parents) and or look at the collective set of “traces” they will roughly look the same. Most of the time this is the view you want when looking at telemetry. Spans are too noisy to be looking at in isolation. While the two tabs feel largely overlapping, it’s a bit intentional as there’s actually no real object called a trace - it’s just a series of spans. You will see these abstractions in most observability platform.”\n\nThe line that:\n\n“there’s actually no real object called a trace - it’s just a series of spans”\n\nWas extremely clarifying, actually. It explains the fuzziness between the spans and traces tab in the Phoenix dashboard.\nI also got some clarification around the missing @tracer.embbeding and @tracer.reranker decorators:\n\n“We emit spans for embedding text to vectors (like”adda”), guardrailing via thinks like guardrals or content moderation, and reranking things via things like cohere. However it’s sorta rare for people to manually write these. We will have decorators for them but right now they are typically emitted from autoinstrumentors like langgraph where there are common patterns for these things. We will have decorators for them very soon - but things like reranking are much more complex than things like tool calling so we are codifying these primitives now.”\n\nSo there you have it! Some clarity. I’ll have to play around to see whether I go with the Langfuse route or the Phoenix route and which feels most ergonomic in the hinbox codebase. Appreciate the quick feedback from the Phoenix team, though!"
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html",
    "title": "Error analysis to find failure modes",
    "section": "",
    "text": "I came across this quote in a happy coincidence after attending the second session of the evals course:\nIt’s obviously a bit abstract, but I thought it was a nice oblique reflection on the topic being discussed. Both the main session and the office hours were mostly focused on the first part of the analyse-measure-improve loop that was introduced earlier in the week.\nIt was a very practical session in which we even took time to do some live ‘coding’ (i.e. analysis + clustering) of real data. I’ll try to summarise the points I jotted down in my notebook and end with some reflection on how I will be applying this for an application I’ve been working on.\nA quick reminder of the context: we have an application of some kind, and we want to improve it. LLMs have lots of quirks that make them hard to narrow down exactly how they’re failing, so we’re working through a process that allows you to do just that. This was framed as a five-step process by Hamel + Shreya:\nFirst up, we need to look at some data to better understand the failure modes that our application might suffer from. If this application’s been in production for a while, you might well just have production data. If not, we’ll want to create a synthetic(-ish) dataset that allows us to get over the cold-start hump."
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#create-your-initial-dataset",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#create-your-initial-dataset",
    "title": "Error analysis to find failure modes",
    "section": "1. Create your initial dataset",
    "text": "1. Create your initial dataset\nThis process is fairly technical, but as we were introduced to this process, the aim is to end up with 100 inputs that span across different dimensions of use that your application / system might be exposed to.\nWhy 100? No reason. As Hamel explained, it’s just a magic number to get you started. We’re encouraged not to get too focused on the details of the process but rather to trust that we would get to where we wanted if only we had a little faith.\nThe idea is that we pass these 100 datapoints into our LLM-driven system in order to see what we get out at the other end, we analyse them iteratively until we’re not learning anything new by doing the iterative process.\nThe process is something like the following:\n\nyou want to sample among dimensions or facets of the use that your application could expect to experience, so come up with at least three of these. As a rule of thumb, perhaps think through the lens of features people might use, persona, query complexities or scenarios. It will differ per application, most likely.\nThen generate a number of combinations of these three dimensions. (So as an example: people who want to use a chatbot to buy a product, and these are all non-technical users who actually are non-native English speakers, and who don’t necessarily formulate their queries with full sentences because they’re being passed in by a voice transcription module). Generate 50 of these. (Then filter out the ones that don’t make sense.)\nThen either hand write or use an LLM to help you generate the full 100 realistic queries that would come from any of the particular tuple-combos that we created earlier. (Again, filter out the ones that don’t make sense.)"
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#look-at-your-data-open-coding",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#look-at-your-data-open-coding",
    "title": "Error analysis to find failure modes",
    "section": "2. Look at your data (‘open coding’)",
    "text": "2. Look at your data (‘open coding’)\nAt this point you’ll pass all these queries into your system and then you’ll have a pair of the initial query, together with the ‘full trace’ (which encompasses the final response along with all internal tool calls, retrieval and any other context or metadata).\nHere you assemble your traces and you write notes on each one. Basically you are looking at each of the 100 items of data and making observations on what failure modes you observe in the data. In the lesson we did this live through the Braintrust interface, but it was emphasised that custom vibe-coded interfaces were also recommended, especially when you have a lot of metadata and tool calling that you might want to present in a certain way to foreground certain elements etc.\nThis is where you’ll spend 80% of your time and for 100 traces could take something on the order of an hour. Read each trace. Write some brief descriptive notes about the observed problems or actions where things are going wrong or are unexpected.\nImportantly, you let the categories emerge from the data rather than coming in with pre-conceived ideas of what the categories already are.\nFor long traces, or ones with complex intermediary steps, focus on either the first upstream failure or the first independent failure that you come across. In the end, this process is an iterative one, so you’ll have a chance to repeat this a few times.\nNote also that we don’t really care about the root cause analysis (i.e. ‘why’ things are happening). We’re doing error analysis so what we care about is just the behaviour and patterns that we observe."
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#cluster-your-data-axial-coding",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#cluster-your-data-axial-coding",
    "title": "Error analysis to find failure modes",
    "section": "3. Cluster your data (‘axial coding’)",
    "text": "3. Cluster your data (‘axial coding’)\nAt this point you have a dataset of inputs, outputs and your notes on these 100 items. At this point you switch to a clustering effort where you are structuring the failure modes + merging them. You bring structure into your unstructured data by grouping similar failure modes into a sort of emergent failure taxonomy.\nThe process: you read the notes and then you cluster similar notes.\nIt’s possible to get some help from an LLM with this, for suggestions on how to group items, but there’s no way to automate yourself out of this process. You still need to make the final judgement and call, based on your understanding of the context of the application. “Always manually review, refine and define these failure modes yourself.”\nOne useful guidance was to try to have failure modes that are binary (i.e. observably yes or no) since this will help later on in the process but also it’s much easier to have clear definitions for yes and no. (The alternative, where you have grades between 1-5, for example, is too easy to be unclear.)"
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#label-more-traces-iterate",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#label-more-traces-iterate",
    "title": "Error analysis to find failure modes",
    "section": "4. Label more traces & iterate",
    "text": "4. Label more traces & iterate\nAnd then you’re repeating and iterating! During this process don’t be concerned that your failure mode naming or definitions might start to evolve. This is a known thing that happens when you annotate data, i.e. the criteria drifts as you review new outputs, and it’s actually something you should welcome because it is a reflection of you better understanding your data.\nYou’ll want to keep looping between open coding + axial coding stages until you are ‘saturated’ in terms of what you’re learning about the failure modes. You’ll be refining the definitions, merging similar categories, splitting ones that are different."
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#pitfalls-to-watch-out-for",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#pitfalls-to-watch-out-for",
    "title": "Error analysis to find failure modes",
    "section": "Pitfalls to watch out for",
    "text": "Pitfalls to watch out for\nWe skipped over this section fairly quickly, but there are a bunch of ways in which you can short-change yourself in this process and that are worth being aware of:\n\nyou might have underspecified or been too narrow in how you defined the tuple-combos at the beginning. i.e. your data that you generated didn’t end up covering wide dimensions of usage patterns.\nyou might skimp on the work, either only coding a few examples, or half-passing the effort to actually think through what an example or trace really represents\nyou might try to automate things too early, delegating your (expert) judgement to a machine that can’t represent your interests, at least not at this stage\nyou might skip the iteration loop of going back to the open coding after doing some axial coding\nfor complex domains, you might skip including experts as part of this process of annotation"
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#office-hours-discussions",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#office-hours-discussions",
    "title": "Error analysis to find failure modes",
    "section": "Office hours discussions",
    "text": "Office hours discussions\nThere were a few really interesting questions that were asked during the office hours.\nOne was about how to handle ‘complex’ pipelines (i.e. ones with many intermediary stages, possibly with lots of tool calling and iteration / reflection loops). Hamel suggested two ways of approaching this complexity:\n\nbuilding your own data viewer or annotator was one option since it allows you to customise exactly which bits of the complexity you’re exposed to. It’ll differ per application, but really you should focus on whatever is important to you based on the behaviour of the application, and an off-the-shelf tool — however good — can never be everything to everyone.\nlook at the final output instead of getting lost in all the intermediary details. You can see the errors in the output / final behaviour. Since this is an iterative process, if you observe errors in the output, that’s actually good enough. You don’t need to do a root cause analysis. Just code and cluster based on the failure modes you observe. You could also focus on the error type / pattern that seems most important or burning to you.\n\nIn general the emphasis was on finding ways to simplify things and not get lost in all the complexity of your system. This isn’t or won’t be the last time you see your system’s behaviour, so you don’t have to catch everything. Either picking the most glaring errors or sticking with upstream failures can be good ways of achieving this. “Find the one error that’s swamping out other errors.”\nAnother really interesting prompt from Hamel was to take on the mentality of a detective while working on this analysis stage. Think: “I’m going to find the failure nodes” and this mentality could carry you forward beyond all your doubts or hesitations or unsureness about the process.\nAnd in the end, as both Hamel and Shreya said, it might feel like taking a leap of faith to trust in the process, since it ultimately is quite an open-ended process. Sort of like the well-worn metaphor of driving at night through fog, where you can’t see more than ten metres in front of you, but still you are able to make forward progress.\nThere was also a question about how to generate synthetic inputs when the LLM-driven process to turn the inputs into outputs also involved some human intervention (perhaps human-in-the-loop responses etc). Two suggestions for this: possibly you could have a synthetic persona who could play the role that a human might have played in those cases, but alternative you could just find five real humans and ask them to run through the scenarios or workflows a dozen times each in order to get you enough data generated that you get past the cold-start problem."
  },
  {
    "objectID": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#reflections-what-ill-be-working-on",
    "href": "posts/2025-05-23-error-analysis-to-find-failure-modes.html#reflections-what-ill-be-working-on",
    "title": "Error analysis to find failure modes",
    "section": "Reflections & what I’ll be working on",
    "text": "Reflections & what I’ll be working on\nI was so struck during today’s session how much overlap there is in this work of evaluation with the work of a professional historian. The things I did when I wrote books, or my PhD, or just research reports, is really similar to this process. It actually made me a bit sad that there are aren’t more ways for people with a humanities background to be involved in the work of LLM application development. Not only are people with humanities backgrounds often trained to be good writers — important in the domain of prompting as we learned on Tuesday — but they have spent their whole career trying to find ways to get their heads around unwieldy unstructured data.\nI have a project which is an agentic workflow / pipeline to ingest primary source or raw data from newspapers or books and iteratively improve and populate a sort of wikipedia based on what gets learned from each source. It’s a sister project to my source translation repo, tinbox (‘translator in a box’) and so this one’s called hinbox (i.e. ‘historian in a box’). I have a working prototype but it still needs a bit of work before I’m happy going into more detail about it works. I’ll make the repo public soon I hope. Needless to say, I am using this course as a way of developing evals as a way of improving it and iterating on its failure modes.\nI might only get round to doing some deep practical work on that next week or the week after, but I’ll be sure to keep up the notes and reflections on the course sessions here as we go."
  },
  {
    "objectID": "posts/2025-04-09-first-impressions-of-the-new-gemini-deep-research-with-2-5-pro.html",
    "href": "posts/2025-04-09-first-impressions-of-the-new-gemini-deep-research-with-2-5-pro.html",
    "title": "First impressions of the new Gemini Deep Research (with 2.5 Pro)",
    "section": "",
    "text": "Google released an updated iteration of their Deep Research tool that uses the new 2.5 Pro model. This was taken from a post originally made on Twitter, so please excuse the terseness.\nFirst impressions:\n\na bit too eager to jump into a deep research task even when I just ask a clarifying question\nquite verbose, just like the OpenAI version. Not sure why both play this up a lot. It looks impressive but in practice I think we need more entry points into this. The ‘Executive Summary’ and other concluding headers are nice touches but I feel maybe there should be some more adherence to user requests for short reports. (I get that as UI it’s maybe weird to think for 10 mins and then spit out a very concise version, but it might actually be more useful.)\nI continue to be annoyed about how these Gemini DR reports handle footnotes (i.e. as endnotes whacked on at the end of the report). Almost a deal-breaker IMO.\nIt’s almost like GDR tries to show how scholarly and serious it is by giving you these walls of prose (vs OpenAI DR which throws in a lot more bullet points). Not sure one is better than the other but would appreciate a bit more flexibility!\nThe portability of these reports has always been not great. Yes you can export them to Google Docs but markdown (+ other options) would have been much better. In practice, this means that whenever I use GDR the report stays stuck there and I’m far less likely to share it with anyone, whereas the OpenAI DR reports I drop parts/all into a Github Gist etc.\nThese reports have been getting better and better, all things considered. I’ve been following along and using GDR from the early days (even pre-OpenAI DR) and this latest version is the best version of it so far (as you’d hope!)\n(It’s also a little bit annoying that GDR has removed any way to use the older versions of GDR with Gemini Pro 2.0 and 1.5 etc. Makes it harder to actually compare these things.)\nPlease let’s get an ipad version of the Gemini iPad app soon, too? Feels a bit regressive to have to use GDR on the web interface always.\nFor serious research (as opposed to simply generating a nice report on some area where you don’t know much about already), all these tools remain hamstrung by the quality of the sources. In areas where I am (or very recently used to be) a leading scholar / researcher, the difference between what I’d expect (in terms of taste / discernment for picking out these sources) is especially egregious. Make the models better, yes, but have better filters + retrieval.\n\nSo yeah, these tools are getting good! Kudos to the teams who are implementing this stuff. Hard to make it perform reproducibly well on so many open-ended uses. But more work to be done!\nIMO the really great implementations of this ‘deep research’ pattern will all be in-house where you can have control over:\n\nsource selection (i.e. high-quality inputs only, not just some random things on the internet)\nhow long it spends thinking about a particular area / loop of the research (or decides to backtrack and dig deeper etc)\noutput types / templates / length\ndifferent modalities of Q&A (sometimes you want reports, other times you want a quick question answered, other times you want visual guides etc etc.)\ndifferent models for different kinds of tasks\npossibly you have little sub-research agents / processes which will go off and work on some hypothesis, possibly involving actual datasets / analysis of tabular data etc, something clearly missing from the current versions we have\n\nA few other things:\n\nGDR’s ‘clarification step’ (which I’ve heard them discuss on podcasts etc) is not as good or useful as the OpenAI DR clarification questions. In practice, because it’s buried under a concealment button that you have to click etc, and where the entire UI seems to be screaming at you to ‘Start Research’, you basically never update or amend the research plan. And when you do, it’s really not clear what’s changed because you don’t get some feedback or diff that your comments were understood; you just get an entire new research plan (again buried under the concealment button)\nGoing forward we’re probably going to want / need ways of navigating the layers to this research. A global overview report will have subsections that (should you wish) can be expanded into their own more detailed or granular reports. This is how research works, after all. Not just endless new reports all trailed one after another pointed in the same direction.\n\nThe other thing that I think we’re really going to need to work on is research taste. Like the LLMs that power them, GDR and OpenAI DR offer a level of research taste developed to the mean. (I know people are thinking about this since it came up on Dwarkesh’s podcast with the AI 2027 guys, but they were focused on scientific research.)\nI think there’s not a single answer for this which is, again, why I see the end result as people bringing these things in-house where they get to develop and refine what makes their particular flavour of research unique. (In the human-generated research world this is very much the case, where certain institutions (or even particular authors) are known for how deep they go, or what kinds of sources they prefer, or how they choose to feature or highlight the primary sources they access, and so on.) There are many possible variations of how this manifest, and I hope that we’re headed into a world where all the AI ‘deep researchers’ will be unique and quirky in all the best senses of that word."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html",
    "href": "posts/2025-02-21-beeminder-mcp.html",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "",
    "text": "I spent the morning building an MCP server for Beeminder, bridging the gap between AI assistants and my personal goal tracking data. This project emerged from a practical need — ok, desire :) — to interact more effectively with my Beeminder data through AI interfaces like Claude Desktop and Cursor."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#understanding-beeminder",
    "href": "posts/2025-02-21-beeminder-mcp.html#understanding-beeminder",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Understanding Beeminder",
    "text": "Understanding Beeminder\nFor those unfamiliar with Beeminder, it’s a tool that combines self-tracking with commitment devices to help users achieve their goals. The platform draws what they call a “Bright Red Line” – a visual commitment path that shows exactly where you need to be to stay on track. What makes Beeminder unique is its approach to accountability: users pledge real money to stay on their path, and there’s a seven-day “akrasia horizon” that prevents immediate goal changes, helping to overcome moments of impulsivity.\nI’ve written a lot about Beeminder over on my personal blog in the past so do go check that out if you’re interested to learn more about how I use it. I can attest that if it clicks with you, you’ll find it incredibly valuable. I have used in the past to write books, learn languages, finish my PhD and many many other things."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#the-role-of-mcp",
    "href": "posts/2025-02-21-beeminder-mcp.html#the-role-of-mcp",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "The Role of MCP",
    "text": "The Role of MCP\nThe Model Context Protocol (MCP) serves as a standardised way for AI assistants to interact with various data sources and tools. Think of it as a universal adapter that allows AI systems to directly access and manipulate data in your applications. Instead of copying and pasting information between your AI assistant and Beeminder, MCP creates a secure, direct connection.\nThis standardisation is particularly valuable because it means you can build one interface that works across multiple AI platforms. Whether you’re using Claude Desktop, Cursor, or other MCP-compatible tools, the same server provides consistent access to your Beeminder data."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#building-the-server",
    "href": "posts/2025-02-21-beeminder-mcp.html#building-the-server",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Building the Server",
    "text": "Building the Server\nThe development process was surprisingly straightforward, largely due to two factors: the well-documented MCP specification from Anthropic and an existing Python client for Beeminder’s API by @ianm118. Most of the implementation work involved mapping Beeminder’s API endpoints to MCP’s expected interfaces and ensuring proper error handling.\nAnd obviously, much of the code was actually written by Claude itself. After providing the initial structure, writing a couple of tools the way I wanted them and providing documentation, I found that Claude could generate the remainder of the code, requiring only minor adjustments and debugging from me."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#using-the-beeminder-mcp-server",
    "href": "posts/2025-02-21-beeminder-mcp.html#using-the-beeminder-mcp-server",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Using the Beeminder MCP Server",
    "text": "Using the Beeminder MCP Server\nHaving an MCP server for Beeminder opens up several practical possibilities. You can have natural conversations with AI assistants about your goals, analyse patterns in your data, and even update your tracking information – all while the AI has direct access to your actual Beeminder account. This direct connection means the AI can provide more contextual and accurate assistance, whether you’re adjusting goal parameters or analysing your progress trends.\nI’ve found that sometimes Claude needs a bit of coaxing to display the information it’s getting back from the Beeminder API in appropriate formats, which is to say, in table format. I will probably update my Claude settings so that it knows it should use tables (either Markdown or React components) to display Beeminder results that would benefit from such a presentation."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#looking-forward",
    "href": "posts/2025-02-21-beeminder-mcp.html#looking-forward",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Looking Forward",
    "text": "Looking Forward\nNow that I have my Beeminder MCP server, I also want one for Omnifocus, my task management app of choice. That’ll probably have to wait since it doesn’t appear that they offer a REST API, but it’ll be great when I can mash up the results of those two tool queries as that’s what I currently do manually as part of my process.\nThe ease of building this MCP server suggests an interesting future where more of our tools and services become directly accessible to AI assistants. The real value isn’t in any single connection, but in the potential for creating a network of interconnected tools that AI can help us manage more effectively.\nIf you’re interested in trying this out yourself, you can find the code and setup instructions in the GitHub repository. While this implementation focuses on Beeminder, the same principles could be applied to create MCP servers for other services and tools."
  },
  {
    "objectID": "posts/2025-02-12-i-forgot-how-powerful-old-school-nlp-is.html",
    "href": "posts/2025-02-12-i-forgot-how-powerful-old-school-nlp-is.html",
    "title": "I forgot how powerful old-school NLP is",
    "section": "",
    "text": "[This is a long one. Sorry / not sorry. TL;DR: I automated the process of curating articles to be included in the a popular database I maintain by using Spacy to create a classifier that’s only 2 MB in size!]\nA few weeks back I created and launched a big database that curated implementations of GenAI use cases and LLMOps out in the wild. I’d been keeping a list of these interesting blogs for a while and it occurred to me that it might be useful for others to share the full list.\nEach entry in the database has some metadata and tags attached to it (to help with search / discoverability) as well as a comprehensive summary of the key points of the link’s contents. (Links were most often blogs, but sometimes YouTube videos where the task was then to grab the transcript and summarise that.)"
  },
  {
    "objectID": "posts/2025-02-12-i-forgot-how-powerful-old-school-nlp-is.html#the-problem-it-takes-time-to-curate-the-database",
    "href": "posts/2025-02-12-i-forgot-how-powerful-old-school-nlp-is.html#the-problem-it-takes-time-to-curate-the-database",
    "title": "I forgot how powerful old-school NLP is",
    "section": "The Problem: it takes time to curate the database",
    "text": "The Problem: it takes time to curate the database\nThe database has grown quite a bit since we launched and now is approaching 600 entries (!). At ZenML, we’ll continue to maintain and update it but the process of finding new entries is non-trivial:\n\nFinding good use cases: where possible we try to meet some kind of technical depth standard where we’re not just republishing some marketing press release. The gold standard is the engineering team of some company writing a deep dive into what they learned from implementing feature X or Y, ideally with code samples and/or architecture diagrams.\nStaying fresh: things change fast in the world of LLMOps, so we want to stay more or less up to date with whatever people are building now and not just the fresh trends of a year ago.\nNot just the big names: there are well-known and (rightfully) acclaimed engineering blogs from mega-companies but there are also smaller groups doing interesting work that is at least as worthy of inclusion in the database.\n\nWe have a ‘submit your case study’ link at the top of the database but to be honest so far it’s been a flytrap for spammers rather than a go-to destination for the people writing their technical accounts. All of of the above means that, till now, my process for updating the database looks something like this:\n\nIn general, I keep an eye out on social media (mostly Twitter, but occasionally LinkedIn and/or Bluesky) for new posts or videos that get shared. This is low-effort but also low-frequency.\nI have RSS subscriptions (yay Newsblur!) to some blogs and YouTube channels that consistently have good content and I group them together so I can process that queue every few days.\nAs mentioned above, we have a link to submit technical write-ups which I check whenever someone makes a new submission.\nSearch with exa.ai: this is the most high-value thing I do to populate the list. (Exa.ai is an embeddings-based search engine that excels at finding ‘other articles like this one’ and I’d guess that 70% of the blogs (i.e. non-YouTube links) in the database were found with it.)\n\nFor all of this, usually ‘processing’ means clicking through the URL, giving the article a quick skim read. For videos, it means a mix of skipping through the contents directly in video form or passing it off to an LLM using my custom YouTube summarisation tool. I have a ‘I know it when I see it’ set of criteria that determines whether an article is worthy of inclusion in the database. That mostly boils down to:\n\nDoes the article actually match up with the subjects we’re trying to cover? i.e. GenAI uses in production and not just some experiment that someone tried locally or some pushing of some benchmark score forward by 0.2% etc. The focus is on production use cases, so how these things work as a system and how people are ensuring reliability and what are they learning along the way.\nDoes the article have some depth to it? There is a lot written about AI use cases these days and (aside from the content that’s auto-generated purely for SEO) not all of it actually contains useful insights or implementation details. I’m not saying we need to see your code, but we’re really looking for something more than just ‘we made this amazing system and it uses these 3 tools’.\n\nI haven’t properly written down my acceptance criteria yet because so far it was only me working on the project and it’s also a pretty hard one to summarise as a text spec. It seems like there’s many edge cases, and when I’m checking the links I often have to stop, think and weigh the value of what I’m consuming.\nLong story short: it takes up quite a bit of time to sift through the internet in the hope that someone’s written a new technical blog describing how they built something or what lessons they learned along the way. So far it’s been a pretty manual process taking a non-zero amount of time. (Not hours and hours, but perhaps 20 minutes every two or three days. ‘It’s not nothing’ would be how I’d quantify it!)"
  },
  {
    "objectID": "posts/2025-02-12-i-forgot-how-powerful-old-school-nlp-is.html#lets-train-a-model-to-pick-the-articles",
    "href": "posts/2025-02-12-i-forgot-how-powerful-old-school-nlp-is.html#lets-train-a-model-to-pick-the-articles",
    "title": "I forgot how powerful old-school NLP is",
    "section": "💡 Let’s train a model to pick the articles",
    "text": "💡 Let’s train a model to pick the articles\nI read Daniel van Strien’s article on distilling DeepSeek down into a ModernBERT classifier with interest and I wondered whether I could also use a model to help process the incoming articles. This seems like an obvious next step in retrospect, but I think I also needed the months of drip-drip work on curating the articles till this point for me to have arrived at a solid collection of validated examples.\nOn the other hand, I hadn’t been"
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "",
    "text": "Chapter 10 of Chip Huyen’s “AI Engineering,” focuses on two fundamental aspects: architectural patterns in AI engineering and methods for gathering and using user feedback. The chapter presents a progressive architectural framework that evolves from simple API calls to complex agent-based systems, while also diving deep into the crucial aspect of user feedback collection and analysis."
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html#progressive-architecture-patterns",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html#progressive-architecture-patterns",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "1. Progressive Architecture Patterns",
    "text": "1. Progressive Architecture Patterns\nThe evolution of AI engineering architecture typically follows a pattern of increasing complexity and capability. Each stage builds upon the previous one, adding new functionality while managing increased complexity.\n\nBase Layer: Direct Model Integration\n\nThe simplest architectural pattern begins with direct queries to model APIs. While straightforward, this approach lacks the sophistication needed for most production applications.\n\n\nEnhancement Layer: Context Augmentation\n\nThe first major enhancement comes through Retrieval-Augmented Generation (RAG). This layer enriches model responses by incorporating custom data and sources into LLM queries, significantly improving response quality and relevance.\n\n\nProtection Layer: Guardrails Implementation\n\n\nGuardrails: Protective mechanisms that filter both inputs and outputs to ensure system safety and reliability.\n\nThe protection layer implements two types of guardrails:\n\nInput Guardrails: Filter sensitive information before it reaches the LLM, such as:\n\nPersonal customer information\nAPI keys\nOther confidential data\n\nOutput Guardrails: Monitor and manage model outputs for:\n\nFormat compliance (e.g., valid JSON)\nFactual consistency\nHallucination detection\nToxic content filtering\nPrivacy protection\n\n\n\n\nRouting Layer: Gateway and Model Selection\n\nThis layer introduces two key components:\n\nAI Gateway: A centralized access point for LLM interactions that manages costs, usage tracking, and API key abstraction.\n\n\nModel Router: An intent classifier that directs queries to appropriate models based on complexity and requirements.\n\nThe routing layer enables cost optimization by directing simpler queries (like FAQ responses) to less expensive models while routing complex tasks to more sophisticated systems.\n\n\nPerformance Layer: Caching Strategies\n\nThe architecture implements two distinct caching approaches:\n\nExact Caching:\n\nStores identical queries and their responses\nParticularly valuable for multi-step operations\nRequires careful consideration of cache eviction policies:\n\nLeast Recently Used (LRU)\nLeast Frequently Used (LFU)\nFirst In, First Out (FIFO)\n\n\nSemantic Caching:\n\nUses embedding-based search to identify similar queries\nDepends on high-quality embeddings and reliable similarity metrics\nMore prone to failure due to component complexity\n\n\n\nSecurity Note: Cache implementations must carefully consider potential data leaks between users accessing similar queries.\n\n\n\nAgent Layer: Advanced Functionality\n\nThe final architectural layer introduces agent patterns, enabling:\n\nRetry loops for reliability\nTool usage capabilities\nAction execution (email sending, file operations)\nComplex workflow orchestration"
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html#monitoring-and-observability",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html#monitoring-and-observability",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "Monitoring and Observability",
    "text": "Monitoring and Observability\nThe complete architecture requires robust monitoring systems tracking key metrics:\n\nMean Time to Detection (MTTD): Time to identify issues\nMean Time to Response (MTTR): Time to resolve detected issues\nChange Failure Rate (CFR): Percentage of deployments requiring fixes\n\nThe monitoring system should track:\n\nFactual consistency\nGeneration relevancy\nSafety metrics (toxicity, PII detection)\nModel quality through conversational signals\nComponent-specific metrics (RAG, generation, vector database performance)\n\n\nAI Pipeline Orchestration\na discussion of AI pipeline orchestration, addressing the trade-offs between using existing frameworks (Langchain, Haystack, Llama Index) versus custom implementations. This decision should be based on specific project requirements, team expertise, and maintenance considerations."
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html#user-feedback-systems",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html#user-feedback-systems",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "2. User Feedback Systems",
    "text": "2. User Feedback Systems\nThe second major focus of the chapter explores comprehensive user feedback collection and utilization strategies.\n\nFeedback Collection Methods\n\nDirect Feedback:\n\nExplicit mechanisms (thumbs up/down)\nRating systems\nFree-form comments\n\nImplicit Feedback:\n\nEarly termination patterns\nError corrections\nSentiment analysis\nResponse regeneration requests\nDialogue diversity metrics\n\n\n\n\nFeedback Collection Timing\nFeedback can be gathered at various stages:\n\nInitial user preference specification\nDuring negative experiences\nWhen model confidence is low\nThrough comparative choice interfaces (e.g., ChatGPT’s response preference selection)\n\n\n\nFeedback Limitations\n\nFeedback Bias: User feedback systems inherently contain various biases that must be considered when making system improvements.\n\nKey limitations include:\n\nNegative experience bias (users more likely to report negative experiences)\nSelf-selection bias in respondent demographics\nPreference and position biases\nPotential feedback loops affecting system evolution\n\n\n\nImplementation Considerations\nThe implementation of feedback systems requires careful attention to:\n\nUI/UX design for feedback collection\nBalance between different user needs\nMonitoring feedback impact on system performance\nRegular inspection of production data\nDetection of system drift (prompts, user behavior, model changes)"
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "",
    "text": "Finally back on track and reading the next chapter of Chip Huyen’s book, ‘AI Engineering’. Here are my notes on the chapter."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#overview-and-core-philosophy",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#overview-and-core-philosophy",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Overview and Core Philosophy",
    "text": "Overview and Core Philosophy\n\n“Data will be mostly just toil, tears and sweat.”\n\nThis is how we start the chapter :) This candid assessment frames dataset engineering as a discipline that requires both technical sophistication and pragmatic persistence. While the chapter’s placement might have been suitable earlier in the book, its position allows it to build effectively on previously established concepts."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-curation-the-foundation",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-curation-the-foundation",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Data Curation: The Foundation",
    "text": "Data Curation: The Foundation\nData curation addresses various use cases including fine-tuning, pre-training, and training from scratch, with specific considerations for chain of thought reasoning and tool use. The process addresses three fundamental aspects:\n\nData Quality: The equivalent of ingredient quality in cooking\nData Coverage: Analogous to having the right mix of ingredients\nData Quantity: Determining the optimal volume of ingredients\n\n\nQuality Criteria\nData quality encompasses multiple dimensions:\n\nRelevance to task requirements\nConsistency in format and structure\nSufficient uniqueness\nRegulatory compliance (especially critical in regulated industries)\n\n\n\nCoverage Considerations\nCoverage involves strategic decisions about data proportions:\n\nLarge language models often utilize significant code data (up to 50%) in training, which appears to enhance logical reasoning capabilities beyond just coding\nLanguage distribution can be surprisingly efficient (even 1% representation of a language can enable meaningful capabilities)\nTraining proportions may vary across different stages of the training process\n\n\n\nQuantity and Optimization\nA key phenomenon discussed is ossification, where extensive pre-training can effectively freeze model weights, potentially hampering fine-tuning adaptability. This effect is particularly pronounced in smaller models.\nKey quantity considerations include:\n\nTask complexity correlation with data requirements\nBase model performance implications\nModel size considerations (OpenAI notes that with ~100 examples, more advanced models show superior fine-tuning performance)\nPotential for using lower quality or less relevant data for initial fine-tuning to reduce high-quality data requirements\nRecognition of performance plateaus where additional data yields diminishing returns\n\n\n\nData Acquisition Process\nThe chapter provides a detailed example workflow for creating an instruction-response dataset:\n\nInitial dataset identification (~10,000 examples)\nLow-quality instruction removal (reducing to ~9,000)\nLow-quality response filtering (removing 3,000)\nManual response writing for remaining high-quality instructions\nTopic gap identification and template creation (100 templates)\nAI synthesis of 2,000 new instructions\nManual annotation of synthetic instructions\n\nFinal result: 11,000 high-quality examples"
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-augmentation-and-synthesis",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-augmentation-and-synthesis",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Data Augmentation and Synthesis",
    "text": "Data Augmentation and Synthesis\n\nSynthesis Objectives\n\nIncreasing data quantity\nExpanding coverage\nEnhancing quality\nAddressing privacy concerns\nEnabling model distillation\n\n\nNotable Research: An Anthropic paper (2022) found that language model-generated datasets can match or exceed human-written ones in quality for certain tasks.\n\nNote that some teams actually prefer AI-generated preference data due to human fatigue and inconsistency factors.\n\n\nSynthesis Applications\nThe chapter distinguishes between pre-training and post-training synthesis:\n\nSynthetic data appears more frequently in post-training\nPre-training limitation: AI can reshape existing knowledge but struggles to synthesize new knowledge\n\n\n\nLLaMA 3 Synthesis Pipeline\nA comprehensive workflow example:\n\nAI generation of problem descriptions\nSolution generation in multiple programming languages\nUnit test generation\nError correction\nCross-language translation with test verification\nConversation and documentation generation with back-translation verification\n\nThis pipeline generated 2.7 million synthetic coding examples for LLaMA 3.1’s supervised fine-tuning.\n\n\nModel Collapse Considerations\nThe chapter addresses the risk of model collapse in synthetic data usage:\n\nPotential loss of training signal through repeated synthetic data use\nCurrent research suggests proper implementation can avoid collapse\nImportance of quality control in synthetic data generation\n\n\n\nModel Distillation\nNotable example: BuzzFeed’s fine-tuning of Flan T5 using LoRa and OpenAI’s text-davinci-003 generated examples, achieving 80% inference cost reduction."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-processing-best-practices",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-processing-best-practices",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Data Processing Best Practices",
    "text": "Data Processing Best Practices\n\nExpert Tip: “Manual inspection of data has probably the highest value to prestige ratio of any activity in machine learning.” - Greg Brockman, OpenAI co-founder\n\n\nProcessing Guidelines\nThe chapter emphasizes efficiency optimization:\n\nOrder optimization (e.g., deduplication before cleaning if computationally advantageous)\nTrial run validation before full dataset processing\nData preservation (avoid in-place modifications)\nOriginal data retention for:\n\nAlternative processing needs\nTeam requirements\nError recovery\n\n\n\n\nTechnical Processing Approaches\nDeduplication strategies include:\n\nPairwise comparison\nHashing methods\nDimensionality reduction techniques\n\nMultiple libraries are referenced (page 400) for implementation.\n\n\nData Cleaning and Formatting\n\nHTML tag removal for signal enhancement\nCareful prompt template formatting, crucial for:\n\nFine-tuning operations\nInstruction tuning\nModel performance optimization\n\n\n\n\nData Inspection\nThe chapter emphasizes the importance of manual data inspection:\n\nUtilize various data exploration tools\nDedicate time to direct data examination (recommended: 15 minutes of direct observation)\nConsider this step non-optional in the process"
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "",
    "text": "This chapter was all about RAG and agents. It’s only 50 pages, so clearly there’s only so much of the details she can get into, but it was pretty good nonetheless and there were a few things in here I’d never really read. Also Chip does a good job bringing the RAG story into the story about agents, particularly in terms of how she defines agents. (Note that the second half of this chapter, on agents, is available on Chip’s blog as a free excerpt!)\nAs always, what follows is just my notes on the things that seemed interesting to me (and a high-level overview of the main points of the chapter just for future reference). YMMV!"
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#chapter-structure-and-framing",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#chapter-structure-and-framing",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Chapter Structure and Framing",
    "text": "Chapter Structure and Framing\nThis chapter undertakes the ambitious task of unifying two major paradigms in AI engineering: Retrieval-Augmented Generation (RAG) and Agents. At first glance, combining these topics might seem surprising given their scope and complexity. However, Chip creates a compelling framework that positions both as sophisticated approaches to context construction.\nThe unifying thesis presents RAG as a specialised case of the agent pattern, where the retriever functions as a tool at the model’s disposal. Both patterns serve to transcend context limitations and maintain current information, though agents ultimately offer broader capabilities. This framing provides an elegant theoretical bridge between these technologies while acknowledging their distinct characteristics."
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#retrieval-augmented-generation-rag",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#retrieval-augmented-generation-rag",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Retrieval-Augmented Generation (RAG)",
    "text": "Retrieval-Augmented Generation (RAG)\n\nCore Concepts and Context Windows\nThe discussion begins with a fundamental examination of RAG’s purpose: enhancing model outputs with query-specific context to produce more grounded and useful results. Chip introduces a fascinating variation on Parkinson’s Law:\n\nContext Expansion Law: Application context tends to expand to fill the context limits supported by the model.\n\nThis observation challenges the common assumption that RAG might become obsolete with infinite context models. Chip argues that larger context windows don’t necessarily solve the fundamental challenges RAG addresses, particularly noting that models often struggle with information buried in the middle of large context windows.\n\n\nRetrieval Architecture and Algorithms\nThe retrieval architecture discussion introduces two primary paradigms:\n\nSparse Retrieval: Term-based approaches that rely on explicit matching of terms between queries and documents. The primary example is the TFIDF (Term Frequency-Inverse Document Frequency) algorithm, which evaluates term importance based on frequency patterns.\n\n\nDense Retrieval: Embedding-based approaches that transform text into vector representations, requiring specialised vector databases for storage and sophisticated nearest-neighbour search algorithms for retrieval.\n\n\n\nCost Considerations and Trade-offs\nA striking revelation emerges regarding the cost structure of RAG systems: vector database expenses often consume between one-fifth to half of a company’s total model API spending. This cost burden becomes particularly acute for systems requiring frequent embedding updates due to changing data. Chip notes that both vector storage and vector search queries can be surprisingly expensive operations.\n\n\nRetrieval Optimisation Techniques\n\nThe chapter presents several sophisticated approaches to optimisation:\nChunking Strategies: While the section is brief, it addresses the critical trade-offs in how documents are segmented for retrieval.\nQuery Rewriting: A powerful but potentially complex technique that enhances initial queries with contextual information. For example, transforming a query like “how about her?” into “how about Aunt Mabel from the previous question?” Chip notes this can introduce latency issues and suggests careful consideration before implementation.\nContextual Retrieval: Introduces the innovative “chunks-for-chunks” approach, where each retrieved chunk triggers additional retrievals for supplementary context. This might include retrieving related tags or associated metadata to enrich the initial results.\nHybrid Search: Combines term-based and embedding-based retrieval, typically implementing a re-ranking process. A common pattern involves using term-based retrieval (like Elasticsearch) to obtain an initial set of ~50 (or however many!) documents, followed by embedding-based re-ranking to identify the most relevant subset.\n\n\nEvaluation Framework\nThe evaluation framework centres on two primary metrics:\n\nContext Precision: The percentage of retrieved documents that are relevant to the query. Generally easier to measure and optimise.\n\n\nContext Recall: The percentage of all relevant documents that are successfully retrieved. More challenging to measure as it requires comprehensive dataset annotation."
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#agents",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#agents",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Agents",
    "text": "Agents\n\nFoundational Definition\nChip provides a clear definition of an agent:\n\nAgent Definition: An entity capable of perceiving its environment and acting upon it, characterised by: - The environment it operates in (defined by use case) - The set of actions it can perform (augmented by tools)\n\n\n\nTool Types and Capabilities\nThe chapter delineates three primary categories of tools:\nKnowledge Augmentation Tools: - RAG systems - Web search capabilities - API calls for information retrieval\nCapability Extension Tools: - Code interpreters - Terminal access - Function execution capabilities These have been shown to significantly boost model performance compared to prompting or fine-tuning alone.\nWrite Actions: - Data manipulation capabilities - Storage and deletion operations\n\n\nPlanning Architecture\nThe planning process emerges as a four-stage cycle:\n\nPlan Generation: Task decomposition and strategy development\nInitial Reflection: Plan evaluation and potential revision\nExecution: Implementation of planned actions, often involving specific function calls\nFinal Reflection: Outcome evaluation and error correction\n\nChip includes an interesting debate about foundation models as planners, noting Yan LeCun’s assertion that autoregressive models cannot truly plan, though this remains a point of discussion in the field.\n\n\nPlan Execution Patterns\n\nThe execution of agent plans reveals a fascinating interplay between computational patterns and practical implementation. Chip identifies several fundamental execution patterns that form the backbone of agent behaviour, each offering distinct advantages and trade-offs in different scenarios.\n\nExecution Paradigms: The core patterns through which agents transform plans into actions, ranging from simple sequential execution to complex conditional logic.\n\nThe primary execution patterns include:\nSequential Execution: The most straightforward pattern, where actions are performed one after another in a predetermined order. This approach offers predictability and simplicity but may not maximise efficiency when actions could be performed concurrently.\nParallel Execution: Enables multiple actions to be performed simultaneously when dependencies permit. While this pattern can significantly improve performance, it introduces complexity in managing concurrent operations and handling potential conflicts.\nConditional Execution: Implements decision points through if statements, allowing agents to adapt their execution path based on intermediate results or environmental conditions. This pattern introduces crucial flexibility but requires careful handling of branch logic and state management.\nIterative Execution: Utilises for loops to handle repetitive tasks or process collections of items. This pattern is particularly powerful when dealing with datasets or when similar actions need to be performed multiple times with variations.\n\nPattern Selection: The choice of execution pattern often emerges from the intersection of task requirements, system constraints, and performance goals.\n\nThe effectiveness of these patterns depends heavily on the underlying system architecture and the specific requirements of the task at hand. For instance, parallel execution might offer theoretical performance benefits but could introduce unnecessary complexity for simple, linear tasks. Similarly, conditional execution provides valuable flexibility but requires robust error handling and state management to maintain system reliability.\nChip emphasises that these patterns aren’t mutually exclusive - sophisticated agent systems often combine multiple patterns to create more complex and capable execution strategies. This hybrid approach allows for the development of highly adaptable agents that can handle a wide range of tasks while maintaining system stability and performance.\n\n\nPlanning Optimisation\nThe chapter provides several practical tips for improving agent planning:\n\nEnhance system prompts with more examples\nProvide better tool descriptions and parameter documentation\nSimplify complex functions through refactoring\nConsider using stronger models or fine-tuning for plan generation\n\n\n\nFunction Calling Implementation\nThe function calling architecture requires:\n\nTool inventory creation, including:\n\nFunction names and entry points\nParameter specifications\nComprehensive documentation\n\nTool usage specification (required vs. optional)\nVersion control for function names, parameters, and documentation\n\n\n\nPlanning Granularity\nChip introduces an important discussion of planning levels, analogous to temporal planning horizons (yearly plans vs. daily tasks). This presents a fundamental trade-off:\n\nPlanning Trade-off: Higher-level plans are easier to generate but harder to execute, while detailed plans are harder to generate but easier to execute.\n\n\n\nTool Selection and Evaluation\nThe chapter provides a systematic approach to tool selection:\n\nConduct ablation studies to measure performance impact\nMonitor tool usage patterns and error rates\nAnalyze tool call distribution\nConsider model-specific tool preferences (noting that GPT-4 tends to use a wider tool set than ChatGPT)\n\n\n\nMemory Systems\nThe memory architecture comprises two core functions:\n\nMemory Functions: - Memory management - Memory retrieval\n\nThe system supports three types of memory:\n\nInternal knowledge\nShort-term memory\nLong-term memory\n\nThese systems prove crucial for:\n\nManaging information overflow\nMaintaining session persistence\nEnsuring model consistency\nPreserving data structural integrity\n\n\n\nEvaluation and Failure Modes\nThe comprehensive evaluation framework considers:\n\nPlanning effectiveness\nTool execution accuracy\nSystem latency\nOverall efficiency\nMemory system performance"
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#conclusion",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#conclusion",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Conclusion",
    "text": "Conclusion\nThe unifying thread of context construction provides a compelling framework for understanding these technologies not as separate entities, but as complementary approaches to extending model capabilities."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "",
    "text": "Really enjoyed this chapter. My tidied notes from my readings follow below. 150 pages in and we’re starting to get to the good stuff :)"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#overview-and-context",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#overview-and-context",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Overview and Context",
    "text": "Overview and Context\nThis chapter serves as the first of two chapters (Chapters 3 and 4) dealing with evaluation in AI Engineering. While Chapter 4 will delve into evaluation within systems, Chapter 3 addresses the fundamental question of how to evaluate open-ended responses from foundation models and LLMs at a high level. The importance of evaluation cannot be overstated, though the author perhaps takes this somewhat for granted. The chapter provides a comprehensive framework for understanding various evaluation methodologies and their applications."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#challenges-in-evaluating-foundation-models",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#challenges-in-evaluating-foundation-models",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Challenges in Evaluating Foundation Models",
    "text": "Challenges in Evaluating Foundation Models\nThe evaluation of foundation models presents several unique and complex challenges that make systematic assessment difficult:\n\nExisting benchmarks become increasingly inadequate as models improve in their capabilities\nAs models become better at writing and mimicking human-like responses, evaluation becomes more complex and nuanced\nMany foundation models are API-driven black boxes, limiting access to internal workings\nModels continuously develop new capabilities, requiring constant adaptation of evaluation methods\nThere has been notably limited investment in evaluation studies and technologies compared to the extensive resources devoted to enhancing model capabilities\nThe improvement in model performance necessitates the continuous development of new benchmarks\nWithout a systematic approach to evaluation, progress can be hindered by various headwinds"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#language-model-metrics",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#language-model-metrics",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Language Model Metrics",
    "text": "Language Model Metrics\nThe chapter includes a technically detailed section on understanding language model metrics, which while math-heavy, provides fundamental insights into model capabilities:\n\nEntropy\nCross-entropy\nPerplexity\n\nThese metrics serve as underlying measures to understand what’s happening within the models and assess their power and conversational abilities. While this section spans 4-5 pages of technical content, it provides some useful foundational understanding of how we can measure a language model’s intrinsic capabilities."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#downstream-task-performance-measurement",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#downstream-task-performance-measurement",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Downstream Task Performance Measurement",
    "text": "Downstream Task Performance Measurement\nThe chapter transitions from intrinsic metrics to evaluating actual capabilities, dividing evaluation into exact and subjective approaches.\n\nExact Evaluation\nThere are two principal approaches to exact evaluation:\n\nFunctional Correctness Assessment\n\nEvaluates whether the LLM can successfully complete its assigned tasks\nFocuses on practical capability rather than theoretical metrics\nExample: In coding tasks, checking if generated code passes all unit tests\nProvides clear, objective measures of success\n\nSimilarity Measurements Against Reference Data Four distinct methods are identified:\n\nHuman Evaluator Judgment\n\nRequires manual comparison of texts by human evaluators\nHighly accurate but time and resource-intensive\nLimited scalability due to human involvement\nOften considered the gold standard despite limitations\n\nExact Match Checking\n\nCompares generated response against reference responses for exact matches\nMost effective with shorter, specific outputs\nLess useful for verbose or creative outputs\nProvides binary yes/no results\n\nLexical Similarity\n\nEmploys established metrics like BLEU, ROUGE, and METEOR\nFocuses on word overlap and structural similarities\nKnown to be somewhat crude in their assessment\nWidely used despite limitations due to ease of implementation\n\nSemantic Similarity\n\nUtilizes embeddings for comparing textual meaning\nLess sensitive to specific word choices than lexical approaches\nQuality depends entirely on the underlying embeddings algorithm\nMay require significant computational resources\nGenerally provides more nuanced comparison than lexical methods\n\n\n\nThe chapter includes a brief but relevant sidebar on embeddings and their significance in evaluation, though this digression seemed a bit out of place in the overall flow."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#ai-as-judge",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#ai-as-judge",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "AI as Judge",
    "text": "AI as Judge\nThis section explores the increasingly popular approach of using AI systems to evaluate other AI systems.\n\nBenefits\n\nSignificantly faster than human evaluation processes\nGenerally more cost-effective than human evaluation at scale\nStudies have shown strong correlation with human evaluations in many cases\nAI judges can provide detailed explanations for their decisions\nOffers greater flexibility in evaluation approaches\nEnables systematic and consistent evaluation at scale\n\n\n\nThree Main Approaches\n\nIndividual Response Evaluation\n\nAssesses response quality based solely on the original question\nOften implements numerical scoring systems (e.g., 1-5 scale)\nEvaluates responses in isolation without comparison\n\nReference Response Comparison\n\nEvaluates generated response against established reference responses\nUsually produces binary (true/false) outcomes\nHelps ensure responses meet specific criteria\n\nGenerated Response Comparison\n\nCompares two generated responses to determine relative quality\nPredicts likely user preferences between options\nParticularly useful for:\n\nPost-training alignment\nTest-time compute optimization\nModel ranking through comparative evaluation\nGenerating preference data\n\n\n\n\n\nImplementation Considerations\n\n\nTable 3-3 (page 139) provides an overview of different AI judge criteria used by various AI tools\nNotable lack of standardization across different platforms and approaches (see above)\nVarious scoring systems available, each with their own trade-offs\nAdding examples to prompts can improve accuracy but increases token count and costs\nCareful balance needed between evaluation quality and resource consumption\n\n\n\nLimitations and Challenges\n\nAI judges can show inconsistency in their judgments\nCosts can escalate quickly, especially when using stronger models or including more context\nEvaluation criteria often remain ambiguous and difficult to standardize\nSeveral inherent biases identified:\n\nSelf-bias: Models tend to favor responses generated by themselves\nVerbosity bias: Tendency to favor longer, more detailed answers\nOther biases common to AI applications in general"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#specialized-judges",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#specialized-judges",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Specialized Judges",
    "text": "Specialized Judges\nThis section presents an innovative challenge to the conventional wisdom of using the strongest available model as a judge. The author introduces a compelling alternative approach:\n\nSmall, specialized judges can be as effective as larger models for specific evaluation tasks\nMore cost-effective and efficient than using large language models\nCan be trained for highly specific evaluation criteria\nDemonstrates comparable performance to larger models like GPT-4 in specific domains\n\nThree types of specialized judges are identified: 1. Reward models (evaluating prompt-response pairs) 2. Reference-based judges 3. Preference models\nThis represents a novel approach that could significantly impact evaluation methodology in the field."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#comparative-evaluation-for-model-ranking",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#comparative-evaluation-for-model-ranking",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Comparative Evaluation for Model Ranking",
    "text": "Comparative Evaluation for Model Ranking\n\nMethodology\n\nFocuses on binary choices between two samples\nSimpler for both humans and AI to make comparative judgments\nUsed in major leaderboards like LMSIS\nRequires evaluation of multiple combinations to establish rankings\nVarious algorithms available for efficient comparison\n\n\n\nAdvantages\n\nMore intuitive evaluation process\nOften more reliable than absolute scoring\nReduces cognitive load on evaluators\nProvides clear preference data\n\n\n\nChallenges\n\nHighly data-intensive nature affects scalability\nLacks standardization across implementations\nDifficulty in converting comparative measures to absolute metrics\nQuality control remains a significant concern\nNumber of required comparisons can grow rapidly with model count"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#key-takeaways-and-future-implications",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#key-takeaways-and-future-implications",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Key Takeaways and Future Implications",
    "text": "Key Takeaways and Future Implications\n\nThe emergence of smaller, specialized judge models represents a significant shift from the traditional approach of using the largest available models\nComparative evaluation offers promising approaches but requires careful consideration of scalability and implementation\nThe field continues to evolve rapidly, requiring flexible and adaptable evaluation strategies\nSets up crucial discussion for system-level evaluation in Chapter 4\nHighlights the ongoing tension between evaluation quality and resource efficiency\n\nThe chapter effectively establishes the foundational understanding necessary for the more practical, system-focused evaluation discussions to follow in Chapter 4."
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "",
    "text": "Here are the final notes from ‘Prompt Engineering for LLMs’, a book I’ve been reading over the past few days (and enjoying!)."
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-10-evaluating-llm-applications",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-10-evaluating-llm-applications",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "Chapter 10: Evaluating LLM Applications",
    "text": "Chapter 10: Evaluating LLM Applications\nThe chapter begins with an interesting anecdote about GitHub Copilot - the first code written in their repository was the evaluation harness, highlighting the importance of testing in LLM applications. The authors, who worked on the project from its inception, emphasise this as a best practice.\n\nEvaluation Framework\nWhen evaluating LLM applications, three main aspects can be assessed:\n\nThe model itself - its capabilities and limitations\nIndividual interactions with the model (prompts and responses)\nThe integration of multiple interactions within the broader application\n\nAs a general rule of thumb, you should always track and record:\n\nLatency\nToken consumption statistics\nOverall system approach metrics\n\n\n\nOffline Evaluation\n\nExample Suites\nThe foundation of offline evaluation is creating example suites - collections of 10-20 (minimum) input-output pairs that serve as test cases. These should be accompanied by scripts that apply your application’s logic to each example and compare the results.\nExample sources come from three main areas:\n\nExisting examples from your project\nReal-time user data collection\nSynthetic creation\n\nWhen using synthetic data, it’s crucial to use different LLMs for creation versus application/judging to avoid potential biases.\n\n\nEvaluation Approaches\n\nGold Standard Matching\n\n\nCan be exact or partial matching\nParticularly effective for binary decisions or multi-label classification\nCan leverage “logical frogs” tricks from Chapter 7 to assess model confidence\nFree-form text requires more creative evaluation approaches\nTool-use scenarios may be easier to evaluate, especially in agent-driven applications\n\n\nFunctional Testing\n\n\nA step up from unit tests but not full end-to-end testing\nFocuses on testing specific system components\n\n\nLLM as Judge\n\n\nCurrently trendy but requires careful implementation\nShould include human verification loop, preferably multiple humans\nKey insight: Always frame the evaluation as if the LLM is grading someone else’s work, never its own\nRecommendations for quantitative measures:\n\nUse gradient and multi-aspect coverage (MA)\nImplement 1-5 scales with specific criteria\nPlace all instructions and criteria before the content to be evaluated\nBreak down “Goldilocks” questions (was it just right?) into separate questions about whether it was enough and whether it was too much\n\n\n\n\n\nOnline Evaluation\nThe chapter transitions to discussing why we need online testing despite having offline evaluation capabilities. While offline testing is safer and more scalable, real human interactions are unpredictable and require live testing.\nKey points about online evaluation:\n\nAB testing is the standard approach\nExisting solutions include Optimizely, VWO Consulting, and AB Tasty\nApplications need to support running in two modes (A and B)\nConsider rollout timing and users on older versions\n\nFive main metrics for online evaluation (from most to least straightforward):\n\nDirect feedback (user responses to suggestions)\nFunctional correctness\nUser acceptance (following suggestions)\nAchieved impact (user benefit)\nIncidental metrics (surrounding measurements)\n\nDirect feedback data is particularly valuable as it can later be used for model fine-tuning. It’s recommended to track more incidental metrics rather than fewer, both for quality indicators and investigating unexpected changes."
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-11-looking-ahead",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-11-looking-ahead",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "Chapter 11: Looking Ahead",
    "text": "Chapter 11: Looking Ahead\nThe final chapter covers several forward-looking topics:\n\nMultimodality in LLMs\nUser experience and interface considerations\nPublished artifacts from Anthropic\nRisks and rewards of custom interfaces\nTrends in model intelligence, cost, and speed\n\n\nBook-Level Conclusions\nTwo main lessons emerge from the book:\n\nLLMs as Text Completion Engines\n\nThey fundamentally mimic training data\nSuccess comes from aligning prompts with training data patterns\nParticularly relevant for completion models\n\nEmpathy with LLMs\n\n\nThink of them as mechanical friends with internet knowledge\nFive key insights:\n\nLLMs are easily distracted; keep prompts focused\nIf humans can’t understand the prompt, LLMs will struggle\nProvide clear instructions and examples\nInclude all necessary information (LLMs aren’t psychic)\nGive space for “thinking out loud” (chain of thought)"
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#personal-reflections",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#personal-reflections",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "Personal Reflections",
    "text": "Personal Reflections\nThe book, while not revolutionary, provides valuable insights and is a recommended read at 250 pages. It can be completed in about 10-11 days. The heavy focus on completion models versus chat models is interesting, likely due to the authors’ experience with GitHub Copilot. While some points were novel, none were completely mind-blowing. The book’s emphasis on completion models versus chat models is both intriguing and occasionally confusing, though this perspective is understandable given the authors’ background with GitHub Copilot."
  },
  {
    "objectID": "posts/2025-01-12-prompt-content-notes-on-chapter-6-prompt-engineering-for-llms.html",
    "href": "posts/2025-01-12-prompt-content-notes-on-chapter-6-prompt-engineering-for-llms.html",
    "title": "Prompt Content: Notes on ‘Prompt Engineering for LLMs’ ch 5",
    "section": "",
    "text": "Chapter 5 of ‘Prompt Engineering for LLMs’ tackles the kinds of things you might want to include in your prompt. (Chapter 6 thinks through the order, structuring and weighting of these different pieces of content, so this is purely about the ‘what’ and not the ‘how’).\nWe split the kinds of content up into static and dynamic content. For static you can think of fixed instructions (‘always respond politely, and in the first person’) whereas dynamic content is assembled on the fly and is (potentially) custom to each user or query. The classic example of dynamic content insertion is your bog standard RAG app.\nThis was a very tactical chapter and I think the rest of the book will keep this up. Most of what gets discussed usually has a little bitesize example to illustrate which I found helpful. This illustration was used to illustrate the differences between static and dynamic context inclusion.\n\nFor static content, the book explores two types: lists of instructions and few-shot prompting. For instructions, we get some useful rules of thumb:\n\nask for positives vs negatives and does instead of don’ts.\ngive reasons for the things you’re asking\navoid absolutes\n\n(and use the system message for these kinds of instruction as most LLMs have been trained to follow them)\nFor few-shot prompting, we dive into all the tradeoffs around how many does ‘few’ mean, what order they should be included, how to get a representative sample and so on. We also consider the tradeoffs and biases that can be subtly introduced with few-shot prompts: issues around scaling the # of examples and accidental picking up of spurious patterns.\nOne thing I’ve often done is to have a ‘best examples first, then the edge cases’ pattern for how I include these examples but we learn how this can bias the LLM to be unduly pessimistic and cautious. (All this stuff is really super tactical / in the weeds, but it’s what I was hoping for…)\nTL;DR: use static prompting where it’s appropriate. use few-shot prompts as well, but be SUPER careful about how these get used and make sure to run evals to see if there are not better orders and amounts of those few-shot examples.\nFor dynamic context, the chapter first thinks through how we might think through which dynamic examples or context to include. This will be influenced by latency requirements, cost considerations + how much can be prepared ahead of time as well as thinking about how to decide which parts of a theoretically infinite amount of extra context you could provide.\nThe bulk of the dynamic context discussion focuses around RAG, and we even get a tiny POC RAG implementation using FAISS as vector storage. We read about the tradeoffs of choosing lexical retrieval (e.g. ElasticSearch + some naive algorithm like Jaccard similarity) vs neural search (e.g. embeddings-driven retrieval with cosine similarity).\nFinally, the chapter closes with a discussion of summarization. What if, e.g., you have so much context that you want to include but you hit the limit? Then you might want to compress it somehow. We read about hierarchical summarisation (which sometimes has to be recursive if there’s too much context).\nWe also get a nice warning about the ‘rumour problem’ which I’ve personally experienced when you summarise a summary (and maybe you summarise a summary of a summary) and things get lost or misrepresented along the way. But for just one level of summarisation that shouldn’t be too big an issue with modern LLMs.\nWe also get into general vs specific summaries. In other words, when you ask the LLM to summarise some text, do you do it with a certain task in mind or do you get a general summary? A general summary is more flexible and can be used in many places, but while specific summaries might give better results for a specific task, you might end up having to rerun your summarization for different tasks.\nNext up: chapter six which tackles exactly how you put all these pieces of context and content together in your prompt."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html",
    "title": "All the things I learned while trending on Hacker News",
    "section": "",
    "text": "My previous two blog posts — here and here — were trending / on the front page of Hacker News, driving over 20,000 new visitors to this blog. Welcome! I learned a few new tricks (and some mistakes I’d made) during the ensuing discussion so I thought I’d share some of these here. Some of them might trigger some mini side-investigations into certain hypotheses, too, which is even more exciting. Let’s dive in."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#temperature-0",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#temperature-0",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Temperature = 0",
    "text": "Temperature = 0\nSome commenters rightly pointed out that setting the temperature to 1 for some of the OpenAI inference meant that I was more likely to have less stable and less factually consistent responses. I also heard back from the OpenPipe team that there was maybe no hard and fast rule on this but that I should experiment around for my specific use case.\nThere was enough strongly-voiced opinions on this that I might see if I can rerun the evals using 0 as the temperature to see how much of a difference it makes."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#function-calling-vs-json-mode-vs-prompt-vs-some-schema-forcing-library",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#function-calling-vs-json-mode-vs-prompt-vs-some-schema-forcing-library",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Function calling vs JSON mode vs prompt vs some schema-forcing library",
    "text": "Function calling vs JSON mode vs prompt vs some schema-forcing library\nIn a previous baseline eval for OpenAI’s models I used instructor to coerce the output into Pydantic objects. This time round I just used a strongly-worded request in the prompt to request a JSON response and turned on JSON mode (with the response_format={\"type\": \"json_object\"} passed into the create method). That was enough to ensure that every response I got back was valid JSON.\nI’ve since been reading about the performance differences between these different responses, and how certain models (like the OpenAI GPT class) do much better with function-calling than with just a prompt and/or JSON mode.\nI’ll be blogging about the differences between these options (and how exactly they work) but I think there’s also enough potential here for me to try this as well in a separate round of reruns of the evals.\nSpecifically: what’s the difference in performance between the prompt that I used (which effectively stuffed the schema into the prompt) and using a more formalised function-calling approach? Given what I’ve read, I suspect function-calling will prove superior, but by how much?"
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#llama3-eos-and-pad-tokens",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#llama3-eos-and-pad-tokens",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Llama3 EOS and PAD tokens",
    "text": "Llama3 EOS and PAD tokens\nI had some helpful comments suggesting there was maybe something untoward going on with these tokens during my Llama3 local finetune. I did set them in my axolotl config, but it’s well possible that something went wrong there. I’m planning to return to some local finetunes (since my credits on the one-click providers is not infinite and I want to make the local setup work) so I will dive into this soon. Llama3 performed really well so it seems there’s just some small bug here."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#the-one-click-finetuned-models-can-be-run-locally",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#the-one-click-finetuned-models-can-be-run-locally",
    "title": "All the things I learned while trending on Hacker News",
    "section": "The one-click finetuned models can be run locally",
    "text": "The one-click finetuned models can be run locally\nI found out that it is possible to download the adapters from places like Predibase and OpenPipe and just set things up to run them locally, but it’s just buried a bit in the docs (or not documented at all.)\nPart of the difficulty with documenting how users can do this is that (thanks to CUDA setup intricacies) there isn’t really an easy one-approach-fits-all option. Docker is maybe the closest to this, but at the moment you have to do some of the legwork yourself."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#others-have-had-similar-success-with-finetuned-models",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#others-have-had-similar-success-with-finetuned-models",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Others have had similar success with finetuned models",
    "text": "Others have had similar success with finetuned models\nThere were a few other links to successes that others had with finetuning models for structured data extraction posted in the HN thread. See this doctoral dissertation. Also this article in Nature. And of course the OG LoRA Land paper."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#controversial-content-means-openai-tries-less-hard",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#controversial-content-means-openai-tries-less-hard",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Controversial content means OpenAI tries less hard?",
    "text": "Controversial content means OpenAI tries less hard?\nOne commenter suggested that the nature of the content might be the reason why OpenAI’s GPT performance wasn’t as good as the finetuned models. My experience of this is that it’s binary — either you get a real response or you get a canned ‘this is too sensitive a topic’ reply — but (s)he suggested that instead of getting rejected I might just get a degraded-in-quality response.\nThis would need some further testing to confirm or deny. A nice little experiment for someone."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#what-about-anthropics-claude-models",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#what-about-anthropics-claude-models",
    "title": "All the things I learned while trending on Hacker News",
    "section": "What about Anthropic’s Claude models?",
    "text": "What about Anthropic’s Claude models?\nI should probably have done this as well, but I just hit up against the timebox I allocated for the evaluation work. I’ll try to do some experiments with Haiku and the new Sonnet 3.5 to see if a mixture of tool use (aka function calling) and stuffing the prompt with more examples might be able to get us to feature parity with the finetuned models. Watch this space."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#data-labelling-issues",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#data-labelling-issues",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Data labelling issues",
    "text": "Data labelling issues\nOne commenter found some inconsistencies in the data labelling around dates. I’ll admit to not really having a good answer around this, but also I didn’t dive into the issues raised too deeply. They showed some examples of where the ‘ground truth’ date assigned to a press release was wrong. There’s of course the possibility I may have made mistakes while doing the labelling, and there might be some cases where press releases were emailed out earlier than when they were published on the website, but that’s much harder to show. I’ll dig into this a bit at some point, though this is lower priority on the ‘next steps’ list."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#you-cant-predict-what-people-want-to-read",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#you-cant-predict-what-people-want-to-read",
    "title": "All the things I learned while trending on Hacker News",
    "section": "You can’t predict what people want to read!",
    "text": "You can’t predict what people want to read!\nThe title I chose was clearly designed to be a bit provocative and/or draw readers in, but it wasn’t hyperbolic. My evals did actually show my finetuned models ‘beating’ GPT-4o. That said, the blog before it, setting up the evals I did, was written in a fairly general way and I was really surprised that people enjoyed reading that one so much. It just goes to show that you just need to keep showing up, writing and publishing and you don’t know what people will like. Most of the time I’m writing just for me anyway, so anything on top is just a bonus.\nAlongside this is the understanding that the things that a Hacker News audience enjoys are not necessarily the same things that the wider world and readership enjoys. That’s worth bearing in mind."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#github-pages-hosting-holds-up",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#github-pages-hosting-holds-up",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Github Pages hosting holds up!",
    "text": "Github Pages hosting holds up!\nMy blog is a Quarto blog hosted on Github Pages. As such I don’t pay anything for this hosting. I was pleasantly surprised that Github Pages did well in scaling up in response to the traffic. There was no slowness or downtime on the site. Good to know that just because you’re using open-source software and free tools that you’re not penalised."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#next-steps",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#next-steps",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Next Steps",
    "text": "Next Steps\nMy next effort will be to dive into the deployment side of these finetuned LLMs along with some of the low-hanging fruit mentioned above."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "",
    "text": "I am excited to announce the release of a new dataset on the Hugging Face Hub: the Afghanwire Dataset. This dataset is a comprehensive collection of translated Afghan media articles from the period of May 2006 to September 2009, created by the Afghanwire media agency, which I co-founded together with Felix Kuehn.\nDuring the years that Afghanwire was active, our team of Afghan translators worked diligently to translate articles from Dari and Pashto media sources into English. The dataset includes translated newspaper and magazine articles, as well as summaries of radio and television content. As most of the original media from this period is no longer available online, and certainly not in English, this dataset represents the largest publicly available trove of translated Afghan media for the 2006-2009 period.\nThe primary motivation for releasing this dataset is to serve as a historical artefact, preserving the voices and perspectives of Afghan civil society during a critical period in the country’s history. By making these translated articles accessible to researchers, historians, and the general public, we aim to shift the focus from foreign powers and military forces to the diverse opinions and discussions within Afghan society."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#dataset-overview",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#dataset-overview",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe Afghanwire Dataset consists of 7,990 translated articles, covering a wide range of topics and sourced from media outlets across Afghanistan. The dataset is provided as a single large collection, with no predefined splits. Each article is accompanied by metadata such as the publication date, author (if available), translator, topic, and language.\nOne of the strengths of this dataset is its geographical diversity. Although the Afghanwire office was based in Kabul, efforts were made to obtain newspapers and magazines from the provinces to ensure a representative collection. This inclusivity is particularly valuable for regions like Dai Kundi province, whose media coverage might have otherwise been lost and that we sought to represent."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#potential-applications",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#potential-applications",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Potential Applications",
    "text": "Potential Applications\nWhile the primary purpose of the Afghanwire Dataset is to serve as a historical resource, it also presents opportunities for various Natural Language Processing (NLP) tasks. These include:\n\nNamed Entity Recognition (NER) for entities that may be underrepresented in standard or smaller models.\nSentiment analysis to gauge public opinion on various issues during the covered period.\nTopic modelling to identify the main themes and concerns in Afghan media discourse.\nComparative analysis with other datasets or media sources to identify unique perspectives or biases."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#accessing-the-dataset",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#accessing-the-dataset",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Accessing the Dataset",
    "text": "Accessing the Dataset\nThe Afghanwire Dataset is now available on the Hugging Face Hub under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. Researchers and interested parties can access the dataset here."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#next-steps",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#next-steps",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Next Steps",
    "text": "Next Steps\nI’ve had these files sitting on my hard drive for over a decade now. The website is barely accessible via archive.org snapshots (see here, for example) but since the newsletter and most articles were behind a login screen (though no paywall 🤟) the only source of the data was a database dump we made before we switched off the servers. (As a side note, this was a raw MySQL dump compatible with v4.1.21 which is actually so old that you can’t find Docker images to support it. I was saved by this amazing repo from @andrebossi which gave me everything I needed to rescue the data and port it out into Parquet files.)\nI hope that researchers, historians and other interested parties find this a useful collection of the voices of Afghan civil society during a critical period in the country’s history. By making these translated articles accessible to a wider audience, as always I hope to foster a deeper understanding of the complex issues and diverse perspectives that shaped Afghanistan during the 2006-2009 period.\nI would like to express my gratitude to the dedicated team of translators at Afghanwire, particularly Hamid Stanikzai, Atif Mohammadzai, Abdul Hassib Rahimi, and Hamid Safi, for their tireless efforts in selecting and translating these articles. Their work has made this dataset possible and ensures that the voices of Afghan civil society will not be forgotten."
  },
  {
    "objectID": "posts/2024-03-24-publishing-afghanistan-dataset-huggingface.html",
    "href": "posts/2024-03-24-publishing-afghanistan-dataset-huggingface.html",
    "title": "Publishing the ISAF Press Releases dataset",
    "section": "",
    "text": "Yesterday I published two datasets to the Hugging Face Hub and I wanted to briefly add some context to them and what they might be useful for.\n\nTL;DR:\n\nI wrote a paper in 2011 that used international military forces’ press releases about Afghanistan military operations to gain an understanding of what was going on on the ground.\nThe paper was covered in the Guardian and on the BBC and elsewhere and subsequently lead to a significant scaling back of press release coverage.\nThe data is hard to come by now since ISAF and NATO took down the original website and files. I’ve had researchers contact me asking for the data over the years so now I’m making it available as a public dataset on the Hugging Face Hub.\nThe core dataset consists of expert-labelled data pairs (an article with the metadata extracted from the article about how many people were killed and captured and so on) so I’m thinking that it might unintentionally be a useful evaluation task for LLMs. The Hugging Face dataset cards provide more information about what tasks might be appropriate.\n\n\nThe Datasets\nThe ISAF Press Releases dataset contains data used as the basis for the research paper “A Knock on the Door: 22 Months of ISAF Press Releases”, originally published in October 2011. The dataset provides a comprehensive collection of press releases issued by the International Security Assistance Force (ISAF) in Afghanistan from December 1, 2009, to February 21, 2013. The press releases were collected, processed, and annotated to extract information about kill-capture missions carried out by ISAF during this period. The dataset offers valuable insights into the nature and extent of these operations, providing a historical record of ISAF’s activities in Afghanistan. It consists of 4822 press release reports, each labelled with information about the event, including the date, location, target group, and the number of people killed or captured (as represented in the data). The Guardian’s datablog team also published a series of accompanying visualisations at the time.\nThe second dataset is a variation on the same data. It contains the raw HTML files of press releases issued by the International Security Assistance Force (ISAF) in Afghanistan, covering a broader period than the original dataset (spanning 2009-2016). In addition to the raw HTML files containing the news reports, the dataset provides a Parquet file that contains all the data parsed from the HTML files and API requests. This Parquet file serves as the primary resource for researchers and organizations interested in using the dataset. The dataset offers a comprehensive collection of press releases. The HTML files are organized by year and month for archival purposes, while the Parquet file provides a structured and easily accessible format for data analysis.\n\n\nMotivation for the core dataset\nThe dataset was created to provide a comprehensive and publicly accessible record of ISAF’s kill-capture missions in Afghanistan, as reported in their press releases. The motivation was to enable scholars, legal teams, and others to analyse and understand the nature and extent of these operations, as the original ISAF website no longer exists in the same form. The dataset serves as an important historical artifact for Afghan history and provides a means to reverse-engineer the minimum numbers of people killed and captured by ISAF during the specified period.\nThe initial data collection involved manually copying the text of press releases from the now defunct ISAF website at http://www.isaf.nato.int/article/isaf-releases/ into Tinderbox software. The press releases were collected from December 1, 2009, to February 21, 2013, covering a period of over 21 months. All available press releases during this period were included in the dataset.\nThe collected press releases were then processed to split them into individual incident reports. If a press release mentioned multiple incidents, they were separated into distinct entries. The text of the press releases was not modified and remains in its original form.\nThe annotation process involved reading each press release and evaluating it against a set of variables. The annotations were performed using Tinderbox software. The variables included:\n\nBasic data: Incident name, reference number, date of the incident.\nLocation data: Province, district, village name (if provided).\nTarget data: Target group, claimed capture of a “leader” or someone in a leadership position, specific position of the target.\nNumerics: Indication of someone being killed or detained, minimum number killed or detained, exact terms used to refer to those detained or killed, numbers of “leaders” and “facilitators” claimed to be killed or captured, classification as a “capture-or-kill” raid, involvement of an airstrike. The annotator used a fixed list of interpretations for certain terms when estimating the minimum numbers of people killed or detained. Detailed explanations of the annotation process and variable definitions are provided in the associated research paper.\n\n\n\nThe report’s conclusions\nI’ll just quote from the report’s press release for this so as not to get any details wrong:\n\n“ISAF officials have long presented the recently stepped‐up capture‐or‐kill operations as one of the most effective parts of the military mission in Afghanistan. They regularly release large figures describing the number of ‘leaders’, ‘facilitators’ and ‘insurgents’ that were killed or captured, to illustrate the success of the campaign. A closer examination of the information that is publicly available, however, reveals some important inconsistencies, particularly surrounding the classification of who is considered an insurgent ‘leader’.”AAN’s latest report, by Alex Strick van Linschoten and Felix Kuehn, is based on an analysis of all ISAF press releases over the last 22 months. The report provides important baseline data, as well as an insight into how ISAF sees the success of their operations. Alex Strick van Linschoten: “Because there is no solid data and no transparency, the debate [with regard to the capture‐or‐kill strategy] tends to be either emotional or anecdotal. If anything goes wrong, ISAF often dismisses the incident as an exception to a successful strategy. Without proper data, you can’t really have this discussion.” “The research covers the period from 1 December 2009 to 30 September 2011 and included 3,771 ISAF press releases, which reported a total of 3,157 incidents (including 2,365 capture‐or‐kill raids). During this period (at least) 3,873 individuals were reported killed and 7,146 detained. The two peaks of ISAF activity were in September 2010 and June 2011. The numbers show a steady general increase in reported kills and captures each month until June 2011, with a slight decrease over the winter (2010—11). The number of ‘leaders’ and ‘facilitators’ that were reported killed amounted to approximately 5 per cent of the total number of deaths, while the number of ‘leaders’ and ‘facilitators’ detained consists of approximately 13 per cent of the total number of reported detentions.”\n\n\n\nFurther research using this data\nI originally undertook this project and research work because I was sick and unable to do much onerous fieldwork at the time on the ground in Afghanistan. I was surprised at how much value could be extracted from a seemingly benign set of data as a series of press releases. The report I originally wrote (together with Felix Kuehn) barely scratches the surface in terms of possible analyses, and I hope that, by publishing the raw data, others might follow in my footsteps and produce further analysis. Some possible ideas I had for projects that would be interesting:\n\ncorrelate the press releases with the data in the Wikileaks Afghanistan war logs to see the extent to which press releases were reporting the reality of what was going on.\ncorrelate the press releases to local news stories\ncompare the public claims about the kill capture raids with the data shared in press releases. I did a bit of this in the original report, but there’s a lot more to be done here.\n\nSince I’m also sharing this data with the view that it’s also an interesting machine learning dataset, some further ideas include:\n\nuse it for structured data extraction (fine-tune or train a language model that can reproduce the expert labels that I originally made)\nfine-tune a smaller model that can label the full dataset, given the labelled subset (i.e. the original dataset)\nfine-tune a model to write the press releases, given some summary of the days’ activities\nstatistical analysis of the full dataset, esp following LLM-driven labelling\napply the same methodology to the press releases of the Afghan Ministry of Defence, who took over the mantle of providing these daily summaries of kill-capture missions around the country. (I hope to present a separate dataset containing these press releases soon as well, which is important since these have also been taken offline.)\n\n\n\nSome takeaways from the project\nFirstly, data labelling is hard! There are 4822 events/items in the core dataset and I read and labelled every single one of them. This took me weeks to carry out and it’s worth reading the report’s section on data labelling to get a sense of why labelling this particular data was hard.\nSecondly, data loss is a real thing! Even though we have amazing resources like archive.org which preserve old sites, it’s still striking that a military operation that took years and that cost so much money and lives didn’t see it as important to preserve the archival record of its actions. Publishing these press releases as an open dataset is part of other work I’ve done to try to preserve these kinds of documents. I encourage others to do the same.\nFinally, there was an unexpected amount of value in data that at first sight was fairly anodyne and mundane. It’s a good reminder that small projects with focused datasets can provide a lot of value."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "",
    "text": "TL;DR:\nLast week I received the results for the MU123 module of my Open University Maths degree. I was really happy to have achieved a distinction. The course starts off slowly — one of the reasons I went with the OU — but there was still a lot of work that went into that grade. I used a combination of study techniques to revise and cement my understanding along the way, but a core tenet of how I study anything is spaced repetition. (I actually care so much about spaced repetition that I’m one of the co-founders of the Spaced Repetition Foundation and have written about it a fair bit over on my other blog.) For my maths work thus far this has meant creating a not insignificant quantity of Anki cards. (Just checked and I have 673 cards that I created for that first module.)\nWhen I posted about my degree on LinkedIn, Matt Squire from the always-worth-following FuzzyLabs (seriously, check out their blog!) replied:\nHe was referring to CoachBot, another web tool I built to scratch a personal itch around getting study prompts when learning a language outside a classroom/group environment.\nAlongside my use of Anki to cement the more fact-y parts of the things that I’m studying, I’ve been a consistent user of ChatGPT to supplement the practice questions that my course gives me. As part of the Open University course, you get access to some practice questions that (I think) are generated from templates. The advantage of using those is that the system will grade your answer, but it’s pretty rigid / inflexible to use and you can only practice in the sandbox of one particular unit or topic so you miss out on all the very real benefits of interleaved practice. (Interleaved practice is making sure that you mix up the topics and things that you’re studying so that you switch e.g. from your Chinese vocabulary to your maths to whatever else it is that you’re studying. You’ll learn the materials far better if you do your retrieval practice using interleaving than if you study just one topic in isolation, then another topic in isolation and so on.)\nThere was also no way in the OU system to really get a sense of how strong you are in the different areas beyond just the static once-done-instantly-forgotten end of unit exams and tests. Thus the tool available to me didn’t handle spaced repetition or interleaving, two core parts of what have been shown to be good study techniques / foundations.\nWhat I really wanted was:"
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#introducing-mathsprompt",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#introducing-mathsprompt",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "🧮 Introducing MathsPrompt",
    "text": "🧮 Introducing MathsPrompt\nSo that’s what I built :) MathsPrompt has three core modes. Firstly we have the core, a place you go to review the problems you’re studying:\n\nThis pretty closely replicates the interface you see when using Anki. You get a question to solve and then when you’ve solved it you grade how easy you found the exercise. (Note that all we have here are prompts for things to solve. There aren’t any generated solutions to check against; most of the time all of this can be looked up fairly easily if need be.) Then depending on how easy or hard you found it, you’ll see that prompt or exercise later or sooner. More on that later.\nTo populate the database of questions / prompts, we have the input screen which was actually the first thing I built.\n\nYou type or copy-paste in some maths exercise, give it some tags (per topic name or unit) and it’ll add that question to the database. Then it’ll send that question (embedded in a lovingly-crafted prompt) to ChatGPT to get 5 more similar questions (but with different values / variables) and it’ll then add those (with an autogenerated flag) to the database as well. This adds a little bit of variation into the mix and ensures that I’m actually getting to practice things I haven’t seen before.\nFinally, we have a ‘Focus’ mode where you can pick one of the tags that you want to specifically focus on (in a non-interleaved way) and you’ll just get infinite random questions relating to that particular topic area:\n\nThere are ways I’d like to improve this going forward (see below for those) but for now I’m happy with having this functional prototype to use in my studies. The data (in the form of my database) is completely separated out from the web interface so I can update and improve this without worrying that the questions I’ve created and the answers I’ve inputted will need to be restarted from scratch going forward."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#why-rust",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#why-rust",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "Why Rust?",
    "text": "Why Rust?\nI chose to build the project using Rust as I’ve been reading a couple of books over the past few months and I wanted a way to get my hands dirty. (Those two books, Rust in Action by Tim McNamara and Command-Line Rust by Ken Youens-Clark, were really great in getting me going / started.) Rust comes with a considerable reputation for its steep learning curve but I found that the passive understanding of syntax / workflows I’d developed by reading in those books coupled with ChatGPT was enough to get this project off the ground at a fast pace.\nIt certainly helped that I knew what I wanted to build and had thought through what the database schema as well as the core functionality on a few long train journeys recently with paper and pen in hand. It also probably helped that I’d had some previous exposure to and experience with Golang and that our Python development at work happens in the context of mypy strict mode which is as close as Python gets to being forced to think through your code in terms of types."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#chatgpt-to-generate-question-variations",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#chatgpt-to-generate-question-variations",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "🤖 ChatGPT to generate question variations",
    "text": "🤖 ChatGPT to generate question variations\nI considered going down the path of generating question templates for every question, and then having some random numbers get inserted into those templates, but for a lot of questions that wouldn’t have worked. For instance, within trigonometry there are lots of ways that the variables are somehow interrelated or some values wouldn’t make sense at all. The angles inside a triangle still need to sum up to 180 degrees, for example.\nDespite a few misgivings and hesitations around GPT-4’s mathematical abilities (which have been widely noted) I figured it’s a quicker way to get going with this project than any other. Moreover, if I find I’m having issues with the quality of the autogenerated questions I can also just remove them from the set that get shown to me. It also added an extra piece of complexity to the project (and thus something new to learn) since I had to make those calls to the OpenAI API as part of the app."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#frontend-web-ui",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#frontend-web-ui",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "Frontend / Web UI",
    "text": "Frontend / Web UI\nI studied web development / engineering for 3 years at Launch School yet have very little interest in importing some framework behemoth into this project. Where possible I’m pragmatically and philosophically aligned towards simplicity so I chose just to make my site in raw HTML, CSS and JavaScript. Really nothing special there. Probably lots of things I could do to make the site look unique, but that can come later if at all. I was pleased that I got to use many of the things I’d learnt in the past for the JavaScript part, however!"
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#rust-server-backend",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#rust-server-backend",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "🖥️ Rust Server / Backend",
    "text": "🖥️ Rust Server / Backend\nAll the server logic is captured in a single file and this was a strong GPT-4 collaboration piece. I knew enough to tweak things once I had some basic code, but writing it from scratch is far beyond my personal abilities right now so I just iterated many times to get to this.\nIt’s all pretty simple. Most of the logic is around sending the text of the input question to OpenAI and then parsing whatever gets returned back. The server logic is all standard. I have no idea if actix is the standard for web servers in the Rust world but it was what ChatGPT recommended so I went with it and it was easy to get my heard round setting up the endpoints and plugging in the logic for what happens when requests came in."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#postgresql-database",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#postgresql-database",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "💾 PostgreSQL Database",
    "text": "💾 PostgreSQL Database\nI started out with SQLite but then I wanted a few extras that I knew weren’t built in with SQLite (particularly some types) so I just created a local Postgres database, created the tables and populated it all with some dummy data to get started.\nAgain, nothing fancy here. Four tables to hold the questions, the tags for the questions as well as metadata around how well I did when answering each question.\nI considered hosting this online somewhere but that wasn’t necessary either and keeping everything local kept my iteration speed fast. Towards the end I refactored the Rust code to allow you to use any database you want (local or managed/hosted) so this gives me a way to put it all online should I want."
  },
  {
    "objectID": "posts/2023-06-04-token-odds-and-sods.html",
    "href": "posts/2023-06-04-token-odds-and-sods.html",
    "title": "Tokenizer Links",
    "section": "",
    "text": "This is just a collection of various links and observations that I came across while learning about tokenisation during the past week that would otherwise have no other home."
  },
  {
    "objectID": "posts/2023-06-04-token-odds-and-sods.html#more-questions",
    "href": "posts/2023-06-04-token-odds-and-sods.html#more-questions",
    "title": "Tokenizer Links",
    "section": "More Questions",
    "text": "More Questions\nAnd some other questions (beyond my larger questions around how to evaluate tokenisers):\n\nHow useful (or not) is data augmentation when it comes to training a tokenizer?\nIs a list of dictionary words useful for training a tokenizer?"
  },
  {
    "objectID": "posts/2023-05-22-balochi-language-model-harms.html",
    "href": "posts/2023-05-22-balochi-language-model-harms.html",
    "title": "The Risks of Language Models in Minority Languages",
    "section": "",
    "text": "In thinking about my work to put together a language model or some utilities relating to the Balochi language, I thought a fair bit about whether I should even start. At a very high level, we can look at general risks that comes from language models, as highlighted in the 2022 Deepmind paper entitled “Taxonomy of Risks posed by Language Models” which covers\n\n“six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms” (p.214)\n\nI’ll leave you to check out the paper if you wish. I can think of a number of specific risks and harms that could be connected to developing a language model for the Balochi language, most of which relate to state actors and their desire for more power and better surveillance capabilities.\nThe communities speaking Balochi have historically and currently been subject to more monitoring than many, either from central governments in Iran and Pakistan or from European and American intelligence agencies, for a variety of reasons. In this context, language models can potentially fit into a system which is geared towards maximising power and influence among the powerful, enhancing state control and surveillance. In the longer term — and I haven’t seen to much by the way of research on this, but I’m going to take a leap — I can imagine that language models could well have the effect of creating a kind of linguistic monoculture. (Just think about the kind(s) of language that you read in default responses from ChatGPT and extrapolate from there.)\nMy assumption is that large, well-funded intelligence agencies already have strong capabilities for Balochi. Indeed, Balochi is one of the language specified as qualifying for the CIA’s ‘Foreign Language Incentive Program’ that offers cash bonuses for new and current employees with foreign language skills in Balochi. There is almost certainly a team working on — among other things — language models that allows for the better monitoring of communications in the Balochi language. (To get more of a sense of what such models and capabilities might be used for, check out this article.)\nWhat, then, are the positive uses of such technology? In no particular order, some things I thought of in the context of Balochi:\n\ndisaster monitoring and outreach following natural disasters (NGOs and aid organisations (or even governments) are sometimes blocked by their inability to effectively communicate with those they are trying to help)\ntranslation and accessibility (heavily caveated, but there is the potential for positive action here as long as technology is responsibly deployed)\nequal access to technologies\naggregation and summarisation tools\nlanguage models as cultural artifacts (in a context of an incremental cultural erasure)\nenhancing the state of the art when it comes to training language models for low resource languages has the potential to support efforts being undertaken for many other languages\nwork done to enhance datasets and the raw materials for language models might potentially be incorporated in bigger efforts by larger organisations (who generally would never think or make the effort to cover a language like Balochi)\nlocal and/or community ownership of the models (vs that of corporations)\na way to reverse linguistic monoculture\nprobably better results and performance for their specialised tasks than generalised mega-large language models\n(and finally, a cause close to my heart) language models can facilitate language learning by those learning it as a second language.\n\nI don’t believe that there’s a simple calculation than can be made, putting potential harm on one side and potential benefits on the other. Given that these models surely do already exist or are being developed by state actors, I also don’t think it’s a matter of staying away from the area entirely. That said, I do think it’s important to have these questions present when doing this kind of work as well as to involve and work within the context of pre-existing community efforts. I’ll turn to other resources and previous work in my next post to give a sense of what low-hanging fruit remains for the Balochi language."
  },
  {
    "objectID": "posts/2023-05-14-mu123-complete-maths.html",
    "href": "posts/2023-05-14-mu123-complete-maths.html",
    "title": "Finishing MU123",
    "section": "",
    "text": "I completed and submitted the final exam for the MU123 module that I’ve been studying since October last year. This module is the first step on my journey towards a BSc Mathematics degree from the Open University, something I’m really happy I have the time to do on the side of my full-time job.\nMathematics was always something I enjoyed studying at school, but for various reasons I stopped taking it as a subject by the time I was 15 or so. This means that I skipped many (most) important skills and subjects like linear algebra, calculus and more. (As an aside, looking back now I find it amazing that it was even possible for me to emerge from a supposedly fancy school with such a shoddy grasp — if at all — of basic techniques and concepts.)\nNow I get the chance to fill in the missing pieces, pieces that feel increasingly fundamental the further I progress down the rabbit hole(s) of both software engineering and machine learning. The Q31 degree programme is very slow to start, mainly because the early modules take a good amount of time to address people like me who maybe never studied things previously at school. The loose mental model you hear thrown about is that the first set of modules should bring you to a solid A-Level standard, and from then on things get a bit more serious in terms of the topics covered.\nThis matches my experience so far with MU123. For those for whom mathematics doesn’t feel as distant a memory, there is a fast-track approach where you start a bit further down the tracks, but eventually the idea is that everyone meets up together later on.\nI really enjoyed the topics covered in MU123. There was a good mix of variety as well as some depth, especially as we started to reach the later units. This module is often taken by students working towards other degrees (in the sciences, perhaps) so it is purposefully open and welcoming. It was clear that a lot of effort had been taken to make everything approachable and engaging, even if some of the ‘multimedia’ felt a bit old.\nI wonder whether the more advanced Maths modules will dispense with the hand-holding and friendly tones of the excellent written materials issued for MU123. For now, though, I don’t have any new classes starting until October, when I’ll be taking a double module together. (Results from MU123 will be issued at some point, though I’m not sure exactly when.) I plan on doing some personal exploration in a couple of areas of mathematics as well as working on some machine learning projects. More on that here in the coming weeks!"
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "",
    "text": "I’ve taken on some different responsibilities at work and part of this involves me diving into a repository filled with Terraform HCL code. I have some idea of how Terraform works, how the declarative paradigm is different and how things connect to one another, but getting up to speed and effective in a somewhat complex codebase is the goal here so I’ll need more than just high-level talking points.\nIn this post, I share my journey of understanding and working with an existing Terraform codebase, focusing on initial exploration, key concepts, and useful commands. I start with an introduction to basic Terraform commands like init, plan, apply, and destroy. Then, I dive into the structure of a Terraform codebase, discussing root modules, entry points, providers, variables, outputs, and data blocks. I also cover validation and formatting commands, as well as tools to help visualize and document your infrastructure. This guide is ideal for those who, like me, are new to Terraform or tasked with diving into a complex codebase, providing a roadmap for learning and making changes effectively."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#start-at-the-root-modules",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#start-at-the-root-modules",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🎬 Start at the root & Modules",
    "text": "🎬 Start at the root & Modules\nMost Terraform code is structured into modules, which are simply groups of multiple .tf and/or .tf.json files in the same directory. The root module is where you’ll want to start, as specified in the docs:\n\n“Every Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the .tf files in the main working directory.”\n\nYou can have child modules within that root module (i.e. local files that define other separate modules) or you can load modules from a registry (like the Terraform Registry)."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-entrypoint",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-entrypoint",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🚪 Finding our entrypoint",
    "text": "🚪 Finding our entrypoint\nMost root modules will have a main.tf which we can think of as the main entrypoint for the logic of the code. (Child modules can and will likely have their own set of main.tf as well as variable definitions (see below).)\nI found it hard to reason about declarative code of the kind that you find in terraform codebases, because it’s not simply a question of procedurally running through a series of steps one by one. The work that terraform is actually doing is resolving all the interconnections, dependencies and diffs between the entirety of the code all at once, so it’s counterproductive to try to think of it as happening sequentially."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#providers",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#providers",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🏥 Providers",
    "text": "🏥 Providers\nYou’ll likely see some providers being defined in your main.tf file. These are the infrastructure providers that you’re interfacing with when you run apply for your terraform code. The specific logic and definitions for those providers are what gets loaded when you initialise your repository with terraform init. These will look something like this:\nprovider \"kubernetes\" {\n        ...details go here...\n}"
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#variables",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#variables",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "✏️ Variables",
    "text": "✏️ Variables\nInput variables are usually defined in a variables.tf file, which can specify their type, any tags needed to be applied as well as optional defaults. If you define them without any defaults, then running terraform apply will see you being queried about what the values for those variables should be.\nYou can pass them in via the command line, for example:\nterraform apply -var \"server_port=8080\"\nWith more than a few variables it’s unwise to have that be the way you pass those values in, so you can also optionally set those variables using a terraform.tfvars file. This file is where you’d set specific assignments and you can then pass that in via the command line explicitly or (if it’s named terraform.tfvars) these will just get automatically loaded and applied.\nterraform apply -var-file=\"testing.tfvars\"\nTo use these variables elsewhere in the code, simply reference the variable name after var.. For example, if your variable is called server_port as mentioned earlier, you would reference var.server_port when you wanted to access the value for that variable.\nYou can also specify and declare local variables (accessed through the local. prefix) by declaring a locals block within a specific file. For example:\nlocals {\n  service_name = \"forum\"\n  owner        = \"Community Team\"\n}"
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-outputs",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-outputs",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📤 Finding our outputs",
    "text": "📤 Finding our outputs\nOutputs are values that will be printed in the console output at the end of whatever happens as part of terraform apply. They will also be accessible and available to you later on to query. For example, let’s say you spin up an instance of a Linux machine somewhere. You’ll want to have the IP address of that machine if you want to SSH into it, so you could set that as one of the outputs. Most likely your terraform codebase will have some outputs, usually stored in an outputs.tf file that is pretty straightforward to read."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#data-blocks",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#data-blocks",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🧱 Data blocks",
    "text": "🧱 Data blocks\nData blocks are special kinds of resource that allow you to access (read-only) information from some source. It is a way to query a provider’s API for some data. Most providers expose various pieces of data, and these can range from simple things to IP address ranges or AMI IDs and so on. Even the current user’s identity can be accessed through data blocks.\nThese are defined inside a data block. For example, if you’re using the aws provider you could get the data for your default VPC with the following:\ndata \"aws_vpc\" \"default\" {\n    default = true\n}\nThen to use that data elsewhere, you can just reference it using the data. prefix."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#listing-state",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#listing-state",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📦 Listing State",
    "text": "📦 Listing State\nThe terraform state list command shows the resource addresses for every resource Terraform knows about in a configuration, optionally filtered by partial resource address.\nThese values are managed and stored in a terraform.tfstate file which is either stored locally or in a shared location (perhaps inside an S3 bucket that your team all have configured to connect to). I won’t get into all the details that surround how to handle state (shared or otherwise); for that, see chapter 3 of Brikman’s excellent Terraform Up & Running.\nThe CLI command is your way to access information about resources without needing to wade through the raw state file itself, and might be interesting to inspect at some point once you have something deployed."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#graphing-your-infrastructure",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#graphing-your-infrastructure",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📈 Graphing your infrastructure",
    "text": "📈 Graphing your infrastructure\nTo view a diagram of the infrastructure defined in your code, run the following:\nterraform graph -draw-cycles | dot -Tpng &gt; graph.png\nYou’ll probably also need Graphviz to be installed for this to work.\nThis will create a PNG file which allows you to visualise the infrastructure defined in all your terraform code. For simple deployments this can be useful, but in my case this was less useful, as you can see from the following section of the image:\n\n\n\nPart of a terraform graph output\n\n\nAs with all these things, try it out and see if it works for you. You can also just export the data in other formats and manipulate them using your preferred graph data structure visualisation tools.\nYou can also use something like the terraform graph beautifier to make it more comprehensible."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#auto-generated-deployment-documentation",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#auto-generated-deployment-documentation",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📄 Auto-generated deployment documentation",
    "text": "📄 Auto-generated deployment documentation\nIf you’re lucky and whoever wrote the terraform code has taken care to write descriptions for variables and the various modules being used, the next tool might prove quite useful. terraform-docs is a way to generate module documentation via a CLI command. Once you’ve installed it, running the following will give you a markdown file that you can preview to learn more about your codebase:\nterraform-docs markdown . --output-file docs.md\n\n\n\nAn example of output from terraform-docs\n\n\nThe file that gets output links to the code and underlying files, so it’s a handy way to sort through exactly what is defined from your root module. I’ve personally found this to be really useful and it’s even more so if the description parameters are defined."
  },
  {
    "objectID": "posts/2023-03-25-tricks-are-the-thing.html",
    "href": "posts/2023-03-25-tricks-are-the-thing.html",
    "title": "The Trick Is The Thing, Part II",
    "section": "",
    "text": "I’m approaching the end of the MU123 ‘Discovering Mathematics’ module and have really been enjoying some of the recent areas we’ve studied. In particular, quadratic equations really captured my attention and imagination. I’m starting to work on the trigonometry unit and it looks set to be equally amazing. What’s even better is that I think I’m starting to see (the beginnings of) some of the connections between concepts and units that I previous wrote about which makes everything that much more compelling.\nTo broaden those connections even further, in the FastAI course we often are reminded of how lots of parts of deep learning are (just) clever tricks. Those tricks might take the form of how you process the data, or how one thing is combined with another, but together these things combine together to form a more meaningful whole. So it is, I am discovering, with mathematics.\nIn a degree format like the one I’m working my way through, you often don’t encounter the insights and ‘tricks’ in a similar context to when and how they were first discovered. Instead of encountering problems that require solutions, you first cover the solutions and then apply those to some problems. I was struck today, though, at the cumulative weight of these incremental improvements.\nTo take one example, we have trigonometry which is — as I currently understand it, one day of study into the unit (please don’t email me) — the study of how angles and lengths relate to one another. We learn how we can use sin and cos and tan to calculate lengths and angles when we don’t have all the information about a particular geometric shape. When we start off, we’re only talking about right-angled triangles, but then later on we start thinking about all kinds of triangles and the trick is to drop a perpendicular so that we are still actually talking about right-angled triangles. So we’ve made this mental leap which allows us to solve more interesting problems.\nThere are many other examples of this from how we work with quadratic equations, to how something like the VAE (variational autoencoder) helps us train faster in the world of deep learning and Stable Diffusion.\nAs someone watching the breathtaking pace of developments in generative AI, I’m struck by how we can observe the same thing there, but at an accelerated pace compared to the world where many of these mathematical techniques were discovered. The internet combined with a number of smart people thinking about this problem space are proving fertile ground.\nFrom trigonometry to quadratic equations to deep learning, the insights and tricks developed along the way build upon each other to form a more meaningful whole. This idea of clever tricks is not unique to mathematics or deep learning, but it is a universal concept that applies to many areas of human knowledge and understanding. These tricks may take the form of new methods or approaches that allow us to solve problems more efficiently or effectively."
  },
  {
    "objectID": "posts/2023-01-01-on-mathematical-literacy.html",
    "href": "posts/2023-01-01-on-mathematical-literacy.html",
    "title": "On mathematical literacy",
    "section": "",
    "text": "I’m at the half-way point in MU123, the first module in the mathematics degree I’m currently working towards. So far we’ve covered models, properties of numbers, some basic statistics, basic algebra and a starter kit on how to think about graphs.\nAs the first thing you do in the BSc Mathematics (Q31) The module is intended as a way for people who haven’t thought about or used mathematics for many years to re-engage. It gives a common vocabulary and shared understanding of several topics from different areas, such that eventually those with more (recent) experience can join classes with the late-starters (like myself).\nAll of the above preamble is a way of saying that I understand why MU123 exists and have some sense of why it’s structured and programmed the way it is, but as a student, it can feel a little scattered. Specifically, it feels like we are being armed with a large bag of tricks. If you show me a graph, I can find the gradient and the y-intercept. If you give me a surd to simplify, I know what to do. Asked to interpret the standard deviation summary for a data set, I have an intuition for what those numbers mean.\nAll that is good and well, and there’s something satisfying to mastering the individual techniques, much like the early days of learning a new programming language. Some context to the use of the technique is given, but for the most part you are using the techniques to solve specific questions and problems.\nThe missing piece is the sense of how all the various parts, all the tricks, connect together into the whole game of mathematics. For now, I’m writing this post to acknowledge the sense of there being a ‘lack’ in the programme but with the hope and anticipation that this will improve the further down the road we go.\nI should also probably take some responsibility for my own education. In particular, there are probably things I can be doing to make the connections and links myself based on how I’m understanding things as we go. The Open University programme for Maths very much seems to take an ‘a la carte’ approach which means that the onus is more on the student to fill in the gaps. This blog should probably serve as a record of my efforts to make those connections and fill in those gaps."
  },
  {
    "objectID": "posts/2022-10-16-notational-precedence.html",
    "href": "posts/2022-10-16-notational-precedence.html",
    "title": "Avoiding BIDMAS, or how J does notation",
    "section": "",
    "text": "One of the topics that comes up early on in Open University’s MU123 mathematics course is precedence. Those who grew up in English-speaking countries will probably know this as BODMAS or BIDMAS. The order of precedence for execution of a mathematical expression gives us an idea for how to resolve expressions that don’t make sense. For example, 3 + 1 x 4 can either amount to 7 or 16, depending on how when you do the multiplication step.\nBrackets are one way to make things more precise, and that’s probably why they’re the B in BIDMAS and that they go first. We could write 3 + (1 x 4) to make it really clear that we wanted the 1 x 4 sub-expression to be evaluated first.\nWith the rules of precedence, we technically wouldn’t need to add in any brackets because we could (likely) assume that people would follow the standard rules and they would know that we have to evaluate multiplications before we evaluate the additions. So we have a way, but it maybe feels a bit unsatisfactory.\nSome languages or domains, however, have notational rules which don’t rely on a meta-schema of precedence rules like BIDMAS to tell you which expressions should be evaluated first. Instead, the order is determined in other ways, with the option of brackets when needed.\nSeveral of the languages in the APL family, like J, simply evaluate from right to left in the order that expressions are encountered. See this example in J:\n   3 + 1 * 4\n7\n   4 * 3 + 1\n16\nThe order in which the expressions are evaluated determines the answer.\nThinking and reading a bit about these orders of precedence brought me to learn a bit about other traditions of mathematical notation. The one most used and that you’ll be most familiar with is called infix notation i.e. 3 + 4.\nPrefix notation (AKA Polish notation) is when we write + 3 4 (to the same end) and postfix notation (AKA reverse Polish notation) is when we write 3 4 +. (The Polish part relates back to Jan Łukasiewicz, who invented it in 1924.) These kinds of notation are used in Lisp and Clojure, for example.\nWhy would you want to use a notation style like this? Some possible reasons:\n\nthe operands in the expressions can handle arbitrary numbers of arguments, making them more efficient to write\nthey are consistent with the syntax used for functions in computer programming (which can be easier to get your mind round)\nthey’re clearer to read and (mostly) unambiguous, unlike infix notation which (see above) requires a whole order of precedence if you’re not using brackets\nthere’s no confusion or need for precedence rules\nit’s faster for a machine to evaluate, since the way expressions are formulated is much easier to translate into computer code.\n\nSo there you go. I’m unclear whether there are more fundamental benefits to living in the world of post-/prefix notation, and perhaps it’s a little like the people who argue that we’d all be better off if we lived in a base-12 world instead of base-10, but that’s beside the point for now.\nI’ll try to share some more diversions from my mathematics study along the way, hopefully powered by J which I’m trying to get back into."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html",
    "href": "posts/2022-05-31-redaction-production-introduction.html",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nSo we’ve trained our model… now what? Are we done?\nIf we’re just exercising our curiosity or have a tight focus for our work, then we might well be done with our work. Our responsibility within a larger team might only be for this specific step of training the model, for example. For many cases, however, we’re going to want to do something with our model, perhaps making it available to others via some web interface or an online API.\nThe next blog posts in this series will focus on the challenges and practicalities of getting a model ‘in production’. I’ll sidestep the nuances of exactly what we mean by ‘in production’ for the moment, but suffice it to say that the end goal is to have a way to not only make our model available to other consumers but also to deal with re-training and/or re-deploying new models to take the place of older or stale models. There is a whole spectrum of variety in this context that goes by the name “MLOps”. This blog post will try to provide a high level overview of some of the basic elements relevant to getting my redaction model into production."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#tldr-what-will-you-learn",
    "href": "posts/2022-05-31-redaction-production-introduction.html#tldr-what-will-you-learn",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🚦 TL;DR: What will you learn?",
    "text": "🚦 TL;DR: What will you learn?\n\n🤖 Deploying a model and automating everything on the way requires a decent number of steps and tools to do it properly.\n👀 I take you through the core steps you need to think about when working with a continuously deployed object detection model.\n💪 I end by outlining the specific pieces I will need to build as I get my own model out into the world and ‘in production’."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#what-is-in-production-for-the-redaction-model",
    "href": "posts/2022-05-31-redaction-production-introduction.html#what-is-in-production-for-the-redaction-model",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🚀 What is ‘in production’ for the redaction model?",
    "text": "🚀 What is ‘in production’ for the redaction model?\n“Production” relates heavily to the use case. An image classification model used across the United States to identify dents on rental cars is going to have a very different profile to a privately hosted language model being used as part of an internal company chatbot interface. In the case of our redaction model, there are two main scenarios that I’m interested in supporting:\n\nan online API which can power other services building on top of the functionality the model enables, albeit document-by-document. A user could upload a specific document and they’d receive specific predictions and results for just that document.\nan offline-first model which handles large volumes of input data and that does not require internet connectivity. For example, legal teams trying to get a sense of what kinds of documents are redacted as part of a case (and to what extent) could run an extended inference process over an entire set of documents.\n\nThese two scenarios have different requirements. Of the two, the online version is perhaps slightly more complex given the more complex serving needs and potential guarantees around speed of execution and so on. The second use offline / on-prem scenario has its own challenges around making sure that inference is fast enough to be able to work over massive document collections, but I’m not sure I’d consider that an MLOps challenge so I will mostly be focusing on the online deployment in this series."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#handling-failure-and-success",
    "href": "posts/2022-05-31-redaction-production-introduction.html#handling-failure-and-success",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "⚖️ Handling failure and success",
    "text": "⚖️ Handling failure and success\nOne way to think about what’s required to get a model in production is to think of the answers to the following two questions:\n\nwhat could go wrong with my deployed online model?\nwhat are the possible consequences if everything went really well?\n\nIn terms of the redaction model, there are lots of potential complications:\n\nour model could be slow and therefore users wouldn’t want to hang around for inference to take place\nthe data on which our model was originally trained could start to show its age and so maybe our model wouldn’t perform as well on newer documents being released (perhaps with a different style or method of redaction)\nsome software bug or managed infrastructure outage could bring our hosting down and we’d have to redeploy everything\nthere could be a legal or ethical challenge to our deployed model, and we’d perhaps be required to show the exact process used that resulted in a particular model\nmaybe the way we choose to make our model available is expensive and/or unreliable\nand so on…\n\nIf things went really well, we have a different set of problems: perhaps a million people would be interested in using the service at the same time. This scalability problem could bring down the model inference service completely. Or perhaps the model would be adopted for use in a legal setting and people would start to trust its predictions blindly without taking into account the fact that its performance was starting to decline the further away we got from when it was originally trained. Maybe some legal institution would start using it to make decisions making assumptions about the accuracy of the model’s predictions, or maybe even the various government departments responsible for creating and applying the redactions in the first place would use it as a way of more efficiently or voluminously adding redactions, an unintended consequence that could potentially be harmful.\nAll the above scenarios and more are some of the reasons why MLOps exists. We care about having repeatable and robust processes for getting our models out in the world because the use cases are themselves often complex. We also care that our models are actually a net positive when released into the world rather than just some process that happens after model training is completed from which we’re completely disconnected."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#basic-mlops-building-blocks",
    "href": "posts/2022-05-31-redaction-production-introduction.html#basic-mlops-building-blocks",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🧱 Basic MLOps building blocks",
    "text": "🧱 Basic MLOps building blocks\n\nThese are some of the tools available to those trying to put their models into production. This ‘landscape’ showcases both the explosion of options for various parts of the full lifecycle as well as the way that this space hasn’t yet settled on a set of best-in-class tools. For my redaction project, there are a few basics that will seek to have in place in order to meet the needs of the use case(s):\n\n⎂ Code standardisation\nPerhaps not even worth mentioning, but having some standards around how the code looks and having processes to enforce this is important. Using pre-commit alongside tools like black, isort, darglint, pydocstyle and so on will take you a long way in this direction.\nThis would be especially important if I were working as part of a team. These tools would ensure some kinds of standards and baseline uniformity within the codebase. In my case, I’m doing everything on my own so it matters less, but these tools all help me collaborate with my future self, several months from now, perhaps when I need to fix some bug or add a new feature. Having code that reads clearly and cleanly goes a long way to getting started on that work.\n\n\n📸 Code Versioning\nVersioning your code with a tool like git is also another basic requirement of any process involving software. I’d almost say this barely requires mentioning, but I know that the use of regular atomic git commits is by no means standard practice in the world of data science so it bears restating here.\nWe version our code because we want to be able to go back to earlier points in our code’s history. If we wanted to see the difference between the code used to train our model today and the code used last week, we’d require a tool like git to help us with that. (Code versioning tools also help tremendously when collaborating with a larger team or to an open-source community project.)\nThere are various options for what tool to use but the vast majority of people use git for this as of 2022.\n\n\n🧳 Data, Model and Artifact Versioning\nLast week I wrote about DVC and the ways it can be used as a lightweight way to add data versioning to a project. Since data is as important to a model as the code used to train it, we want ways to step backwards and forwards with our data. This will not only enable us to debug and retrain older models but it will help in general with managing our assets such that we don’t just have a sprawling series of folders with names like training_data_FINAL_july_2021 or validation_data_minus_synthetic_FINAL_FINAL_march_2020.\nNot only does this make sense from the perspective of productivity, but for sensitive or popular ML applications there are increasing legal requirements around this kind of flexibility to introspect how you trained your models.\nDVC is commonly used for this kind of use case, but there are other alternatives such as Pachyderm or LakeFS that might be worth considering if you have larger amounts of data.\n\n\n🧪 Testing\nThis testing is primarily around ensuring that the code does what you think it’s doing, but we also care about preventing regressions in one part of the codebase when you change something somewhere else.\nThere isn’t much in the way of rocket science to testing, but it is a whole world unto its own. For my project, there is a decent amount of code that handles somewhat complicated conversions between different kinds of image formats, multiple competing ideas for how bounding boxes or BBoxes should be handled as data structures and so on. Having a way to be sure that my code is actually doing what I intended is a surefire way to letting me sleep better at night if I intend my model to be used by others.\n\n\n👁 Data Validation\nJust like our code needs to have some kinds of checks and balances, so does the lifeblood of our project: our data. I recently finished a three-part series on data validation in the context of this project, and both Great Expectations and Evidently are excellent options worth considering, depending on your exact requirements.\n\n\n📝 Experiment Tracking\n{% twitter https://twitter.com/bernhardsson/status/1526635195243409408 %}\nWhen starting a project from scratch, you want to iterate quickly, trying out lots of training options or combinations of features and/or data. Not only do you want to be able to replicate the precise combination of data and code used to train a particular model, but you also want to be able to compare these various efforts with one another.\nExperiment trackers like Weights & Biases, MLflow, Tensorboard and Neptune enable you to compare the results of your models as well as practically any combination of the hyperparameters used to train them. I’ve used charts from my own (Weights & Biases-powered) experiment tracker to showcase the different results obtained as part of my process. Not only is it useful for outward-facing engagement with users, stakeholders or other third-parties, but it can be useful to step back from your experiments and evaluate where your model is performing well and how you might improve it.\n\n\n📺 Monitoring\nThere’s a lot of potential complexity packed into the simple term ‘monitoring’. (I’d recommend you check out Shreya Shankar’s four-part series on monitoring if you’re curious to learn more.) For our purposes, this will mainly involve making sure our model doesn’t drift or become stale. We’ll need to make sure that the data used to periodically (re)train or fine-tune our model is somewhat within the original parameters of the original training data. We’ll also want to be monitoring the kinds of predictions that our deployed model is making to make sure that they’re more or less within the reasonable ranges of values that we’d expect. If we start to regularly diverge from these kinds of boundary values it probably should prompt an examination of what’s going on and why we’re overstepping. This is an essential part of what it means to robustly deploy a model in production; you need to know when things are going wrong.\n\n\n🏁 Automated Model Deployment\nAutomation is a big part of what people generally consider to be ‘mature’ MLOps practices. It is useful to have an automated way to take your model from when training is complete to having it deployed and user-facing. Perhaps you know you need to retrain or fine-tune your model once a week because the data context is continually changing. Perhaps (or likely!) you have rigorous monitoring processes and in the event of a failure or series of non-sensical / out-of-bounds predictions you want a way to revert the model used in production to something more stable.\nThis is where the process of putting a model in production resembles the processes and values of DevOps most closely. For my redaction model, I’ll want to have a way to handle the two cases mentioned above as a starting point along with more complex versions of those cases. I’ll also want to automate the process of converting my IceVision VFNet model into something that can be used in the offline ‘on-prem’ use case I described at the beginning of this post.\n\n\n🃏 DAG Cards, Model Cards, Data Cards, All the Cards\nThe basic idea is that you write some notes on the context surrounding your data, or your model or the pipelines you’re using as part of your overall workflow. Your processes and artifacts will likely change with the project, and I know from bitter experience that it’s easy to forget the reasoning behind why you chose to do one thing or another. So you write notes to describe what you were thinking when you created or modified this or that asset. You describe the decisions you made and what tradeoffs and downstream effects this might have. Not only is this a good practice that benefits FutureYou™️ and your project, but anything developed in the open will maybe have users or contributors and they’ll also benefit from these notes.\nThis is the only part of my ‘requirements’ that is (at least currently) a bit more of a ‘fad’ and I wouldn’t say was commonly found. Even five years from now, I imagine that we’ll have more sophisticated or standardised ways of achieving what cards bring, but for now they resonate strongly with some processes I used when working as a historian, archivist and researcher in my previous life. Some tools like the Huggingface Model Hub offer this as a built-in standard."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#mlops-maturity-models",
    "href": "posts/2022-05-31-redaction-production-introduction.html#mlops-maturity-models",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "👴 MLOps Maturity Models",
    "text": "👴 MLOps Maturity Models\nThe pieces I described above relate to my particular use case. Different project will require different levels of automation, or even potentially other additional stages or toolkits. There is a vast spectrum of options and decisions to be made and now is probably a good time to mention that various players have tried to define what it means to do the whole ‘putting a model into production and keeping it healthy’ thing in a good way. These “MLOps Maturity Models” are not completely without value, but remember that what works for Google may not be (or is probably not) applicable to you as an individual working on a small side-project. I wrote an overview of the two most commonly cited maturity models (from Microsoft and Google) over on the ZenML blog and I encourage you to give that a read if you want to learn more.\nBut what does this all mean for my project? What specifically will it all look like and how am I implementing it? I’ll get into some of the details in the coming weeks, but for now let me just outline my two main workflows."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-1-annotation",
    "href": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-1-annotation",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "✍️ Redaction Project Workflow #1: Annotation",
    "text": "✍️ Redaction Project Workflow #1: Annotation\n\nThere are two main pieces to this pipeline that happens before we train our model. My model is still thirsty for annotations and data, so from the very beginning I want to integrate the annotation process in as part of how I set up the workflows. This way, I make it as easy as possible to annotate data and use that data for subsequent training or fine-tuning.\n\nIngestion / Import\n\nHere I will check a series of pre-defined URLs to see if there are any new PDF files available for download. If new files are available (and we’ve confirmed that we haven’t already downloaded them, I can download those files. Those PDFs then get converted into image files and the metadata for each image gets saved centrally so we have that to hand when annotating the files. This point is a good one to save a checkpoint version of our data using DVC.\n\nAnnotation\n\nWe only want to annotate images that haven’t been annotated, so that check is the first to be performed before spinning up Prodigy to randomly select pages from the PDFs (now in the format of image files) to be annotated. 10% of the images that are annotated get saved in a separate ‘test data’ location. This test data is never used in training and is simply held out for a more realistic final validation of the project. We version the annotations file whenever we are done annotating for the day."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-2-continuous-training-continuous-deployment",
    "href": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-2-continuous-training-continuous-deployment",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🐙 Redaction Project Workflow #2: Continuous Training, Continuous Deployment",
    "text": "🐙 Redaction Project Workflow #2: Continuous Training, Continuous Deployment\n\nThis longer pipeline contains the core value and most compute-intensive processes like training. We take the data from the raw state as annotations and go all the way to deployment.\n\nAnnotation Checker\n\nWe first check to see if there are any new annotations available since we last ran the pipeline. We will probably need some kind of threshold number of annotations which will make it worth our while to trigger the retraining process.\n\nData Validation\n\nWe’ll use Great Expectations to validate the incoming new annotation data.\n\nSynthetic Data Generation\n\nIf / as we hit certain thresholds, we might want to generate more synthetic data and add it to the dataset.\n\nTraining\n\nWe train or fine-tune the model for a certain number of epochs. We log our experiment metadata with Weights & Biases.\n\nDeployment Trigger / Decision\n\nAt this point we need to decide whether to deploy the model or not, based on some evaluation criteria. Our decision will determine the path of the rest of the workflow.\n\nDeployment\n\nWe take the trained model and make it available for online inference. We save a version of the model using DVC, and we also package it up for use in on-prem / offline settings.\n\nInference & Monitoring\n\nThis step is crucial. We monitor the performance of our deployed model along with the predictions it is making. We want to be able to catch any cases where we notice the predictions to start to drift, or be aware of sluggish response times from our server and so on."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#final-thoughts",
    "href": "posts/2022-05-31-redaction-production-introduction.html#final-thoughts",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nNice work on making it all the way to the end! We took a long tour through the various considerations you need to bear in mind when deploying a model, and finished off with a preview of the kinds of things I’ll be building over the coming weeks to actually put my own object detection model in production.\nIf you have parts of this that you’d like me to cover in more detail, or questions based on what you read here today, please leave a comment below!"
  },
  {
    "objectID": "posts/2022-05-07-redaction-mvp-huggingface.html",
    "href": "posts/2022-05-07-redaction-mvp-huggingface.html",
    "title": "A painless way to create an MVP demo using computer vision models",
    "section": "",
    "text": "After the second class of the fastai course, we’re encouraged to create mini-projects that result in models we can deploy online. Deployment is a huge field with its own complexities, of course, but having an option to get something out in the world that’s visible and usable is extremely useful.\nIn this post, I will walk you through how I built a super quick MVP of my redacted document detector project. I used:\n\nfastai to classify and extract redacted pages extracted from PDFs\nicevision (@ai_fast_track) to detect the redacted areas\nHuggingFace Spaces (with Gradio and Streamlit) to deploy my MVP\n\nThe post shows how I went about thinking through the task, showcasing some examples of small prototypes I built along the way, including the final stage where I built: - an app including everything that would be needed by a final ‘deployed’ use case of my model - two models working in tandem in the same app (one classification, one object detection) - optional PDF generation of items detected by the model (!)\nI also explore why you might want to have a minimal deployed version of your application in the first place!"
  },
  {
    "objectID": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-gradio",
    "href": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-gradio",
    "title": "A painless way to create an MVP demo using computer vision models",
    "section": "📐 When to use Gradio",
    "text": "📐 When to use Gradio\n\nif you have a simple use case that you want to highlight\nif your inputs and outputs are clearly defined\nif you have a single model to showcase\nif you want to get something quickly deployed"
  },
  {
    "objectID": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-streamlit",
    "href": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-streamlit",
    "title": "A painless way to create an MVP demo using computer vision models",
    "section": "🌊 When to use Streamlit",
    "text": "🌊 When to use Streamlit\n\nif your use case is more interactive or less simple than just basic input-then-output\nif you want more control on how your demo application is displayed\nif you enjoy a more imperative style of programming\n\nGiven how much inference is going on behind the scenes, I’m surprised that these applications run as fast as it does. For a document with 4 or 5 redacted pages, it takes around 10 seconds to do all the steps described above. 10 seconds is still far too long for a scenario where you wanted to run inference over millions of pages, but in that scenario you wouldn’t be manually uploading them on a web app either.\nIt’s extremely gratifying to have these kinds of tools available to use for free, and really exciting that you get to build out prototypes of this kind after just two weeks of study on the fastai course."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nThe previous two posts in this series have made the case for why you might want to consider adding a Great Expectations step or stage to your computer vision project, particularly once it becomes something you’re going to want to iterate on a few times.\nThis post begins by showcasing how you can use Evidently’s open-source library to calculate and visualise comparisons between your data. I list some of the lighter alternatives to Great Expectations and Evidently, concluding with some thoughts on when you might use it as part of your computer vision pipeline."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#tldr-alternatives-for-data-validation-using-python",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#tldr-alternatives-for-data-validation-using-python",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "TL;DR: Alternatives for data validation using Python",
    "text": "TL;DR: Alternatives for data validation using Python\n\n🛠 Data validation tools come in many flavours, from full-featured libraries like Great Expectations down to the humble assert statement in Python.\n⚠️ The tool you choose should be appropriate to your particular use case and situation. You might not need or want to add a large dependency or take on extra code / project complexity, in which case there are alternative options available to you.\n⏰ You’ll also want to think about when you’re doing your validation. Two key moments stand out for machine learning projects: when you’re ingesting data prior to training or fine-tuning a model, and at the moment where you’re doing inference on a trained model.\n📃 For my project, I’m using a variety of tools as part of my process because I’ve found it gives me confidence in the predictions my model is making and it gives me freedom to experiment and iterate, without needing to also worry that I’m silently breaking something with downstream effects on my model performance."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-using-evidently-for-drift-detection",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-using-evidently-for-drift-detection",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "Alternatives: Using Evidently for drift detection",
    "text": "Alternatives: Using Evidently for drift detection\nI’ve previously written about why Evidently is a great tool to use for drift detection and data monitoring over on the ZenML blog. At its core, Evidently takes two chunks of data and compares them. The statistical comparisons going on under the hood are quite sophisticated, but as an interface to be used it is extremely trivial to get going.\nIn the case of my redaction project data, I did the work of transforming my annotation and image metadata into Pandas DataFrames for Great Expectations already, so using it with Evidently at this point is trivial:\nfrom evidently.dashboard import Dashboard\nfrom evidently.dashboard.tabs import DataDriftTab\nfrom evidently.pipeline.column_mapping import ColumnMapping\n\nreal_annotations = main_annotations_df[['area', 'category_name', 'top_left_x', 'top_left_y', 'width', 'height', 'orientation']]\neasy_synth_annotations = easy_synth_annotations_df[['area', 'category_name', 'top_left_x', 'top_left_y', 'width', 'height', 'orientation']]\nhard_synth_annotations = hard_synth_annotations_df[['area', 'category_name', 'top_left_x', 'top_left_y', 'width', 'height', 'orientation']]\n\ncolumn_mapping = ColumnMapping(\n    numerical_features=[\"area\", \"width\", \"height\", 'top_left_x', 'top_left_y'],\n    categorical_features=[\"category_name\", 'orientation'],\n)\n\ndrift_report = Dashboard(tabs=[DataDriftTab()])\ndrift_report.calculate(real_annotations, hard_synth_annotations, column_mapping=column_mapping)\ndrift_report.save(\"reports/my_report.html\")\nIn this code, I’m comparing between the real (i.e. manually annotated) annotations and the ‘hard’ synthetic annotations that I created (and blogged about recently). I choose the columns I care about, tell Evidently which columns are numerical vs categorical features and save the report. (I can also display the report directly within a Jupyter notebook.) When I open the report, I see this:\n\nYou can unfold the graphs to dive into the details for specific features, as in the following example where I take a look at the orientation of my annotations and see the difference between my manual annotations and the synthetically generated ‘hard’ batch:\n\nIt doesn’t surprise me too much that we have this disparity, since the only annotations that are portrait in the synthetically-generated set are those for the content box around the whole page. All the rest are landscape, and that’s by design. (Note: you can make the comparisons using different statistical tests depending on your use case. I’m told that the next Evidently release will increase the number of available options for this.)\nI can repeat the same test for the image DataFrame. I’ve included some metadata for each image such as how many annotations are associated with the image, or how many redaction vs content annotations are associated and so on. The code is basically the same, except now taking into account the different columns and their types:\n# comparing between real images and hard_synth images\n\ncolumn_mapping = ColumnMapping(\n    numerical_features=[\"area\", \"width\", \"height\", 'annotation_count', 'content_annotation_count', 'redaction_annotation_count', 'area', 'file_size_bytes'],\n    categorical_features=['orientation', 'format', 'mode'],\n)\n\ndrift_report = Dashboard(tabs=[DataDriftTab()])\ndrift_report.calculate(main_images, hard_synth_images, column_mapping=column_mapping)\ndrift_report.save(\"reports/my_report-real-vs-hard-images.html\")\nAnd we get this report:\n\nYou can immediately see how certain things like the number of annotations and the number of redactions in an image was a bit different when comparing the two. We also seem to have a far more even distribution of file sizes in the synthetically generated images and that makes sense since that was essentially randomly determined.\nNote that all the data that goes into making these reports can be accessed programatically as a Python object or JSON through Evidently’s Profile feature, which is probably what you’re going to want when assessing for drift as part of a continuous training / continuous deployment cycle.\nIf you change just a few things once more, you get a really useful data quality report showing distributions, correlations, and various other features of your data at a single glance:\n# profiling data quality\n\nfrom evidently.dashboard.tabs import DataQualityTab\n\nquality_dashboard = Dashboard(tabs=[DataQualityTab()])\nquality_dashboard.calculate(main_images, hard_synth_images, column_mapping=images_column_mapping)\nquality_dashboard.save(\"reports/quality-report.html\")\nYou can get an idea of the report that it produces in the following screen recording from my browser:\n\nAs a place to get started with understanding a dataset, this is a pretty nice visualisation and report to have in your toolkit, but even after immersion in your data it can be useful to take a step back with something like this data quality overview. For instance, it reveals quite clearly how the average number of annotations in my manually annotated dataset is quite a bit lower than that of my synthetically generated examples. Of course, that was by intention, but it is nice to see that confirmed in the data.\nOnce you have your model ready, there are other reports that Evidently offers which perhaps I’ll return to in a subsequent blogpost but for now I hope this has given you a flavour of the tool and how easy it is to get going with it.\n(As a side-note, Evidently’s community is friendly, welcoming and filled with interesting people thinking about these issues. I find it a welcome breath of fresh air when compared with some other tools’ forums or chat platforms, so it also has that going for it!)"
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-some-other-options",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-some-other-options",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "Alternatives: some other options",
    "text": "Alternatives: some other options\nWith Evidently, we drifted a little into the ‘visualise your data’ territory which wasn’t really the point of this post, but you can see how they combined clear visualisation with the statistical validation working underneath to calculate whether data was drifting. The following are some other tools I’ve come across that might help you in validating data in a computer vision context. I haven’t found a use for them in my project, but it’s possible that they might gel with what you’re doing:\n\nTensorFlow Data Validation (TFDV) — This is a part of TensorFlow and tfx which uses schemas to validate your data. If you’re using TensorFlow, you might have heard of this and might even be using it already, but I don’t get the sense that this is often much recommended. I include it as it is a prominent option available to you.\nDeepchecks — Deepchecks is adjacent to what Great Expectations offers, albeit with an emphasis on the kinds of tests you might want to do for ML model training code. It has some features and documented use cases for computer vision (object detection and classification) but I haven’t used it myself. Feels like a tool worth keeping your eye on, however. (Works on Pandas dataframes and numpy arrays.)\npandera — This is a statistical tool for validating data inside dataframes, and it overlaps quite a bit in its functionality with Great Expectations, particularly with the hypothesis testing functionality. Worth checking out.\nCerberus — Offers a lightweight schema-based validation functionality for Python objects.\njsonschema — similar in approach to Cerberus, above, this is a lightweight way to test your JSON files based on how they conform to a defined schema. Useful in the case of annotations files, perhaps, if you really want something minimal.\nschema — More of the same: a Pythonic way to validate JSON or YAML files based on schema.\nassert — We shouldn’t forget the humble assert statement, which I have sprinkled in various places within my code where it makes sense to make sure that data flowing through conforms to whatever implicit or explicit contracts exist.\n\nI mention these various options not to suggest that you should use them all, but rather to state that you have options ranging the whole spectrum of complexity and dependency."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#when-to-do-data-validation-in-your-project",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#when-to-do-data-validation-in-your-project",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "When to do data validation in your project",
    "text": "When to do data validation in your project\nRegularly! I’ve written previously about how you can think about data validation as testing for your data. Just like many (most?) engineering teams run their tests every time you add a new commit to the codebase, it’s worth thinking of these kinds of tests as something that get run at any point where the underlying data gets updated.\nThere are three points where it might make sense to do some data validation:\n\nat the point of data ingestion\nat the point just prior to training a model, i.e. after your data has been split into training and validation sets\nat the point of inference (i.e. using the data being passed into the trained model)\n\n\nThe first (at data ingestion) is essential, especially if you have any kind of continuous training or continuous deployment loop going on. You don’t want to be training on data that clearly is unsuitable for training, or where the distribution has shifted so much that it’s going to cause hidden problems down the line.\nThe second (at training-validation split time) may or may not be important depending on your use case. For my redaction project I don’t think there is a great deal of benefit from this and so haven’t incorporated it as such.\nThe third (at inference time) is quite important to have, even though the behaviour when an anomaly is detected might be different from if you were to detect issues earlier on in the process. You might choose to just log the result of your validation check internally, or you could potentially also feed the result back to a user in the terms of some sort of warning (i.e. if the image that they were uploading was a very different kind of image from the data that had been used to train the model)."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#what-im-using-for-my-redaction-project",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#what-im-using-for-my-redaction-project",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "What I’m using for my redaction project",
    "text": "What I’m using for my redaction project\nI don’t have any general advice as to which tool you should use as part of your computer vision model training pipeline. It’s likely to be heavily context-dependent and will differ based on the particular use case or problem you’re trying to solve. For my own project, however, I can be more specific.\nI’m using plain assert statements liberally scattered through my code, in part leftover from when writing the code but also as a failsafe should strange or anomalous data make its way into my functions. I’m not sure if this is a best practice or not — I could imagine someone telling me that it’s not advised — but for now it’s helpful, especially as I continue to change things in the innards of various parts of the process.\nI’m using Great Expectations as a general-purpose validation tool to lay out my ‘expectations’ of my data in a assertive and declarative way, and even though it took a little longer to wrap my head round how it worked, I’m glad I made the effort as it seems really helpful.\nI’m using Evidently to do similar things as Great Expectations, but I find they have different strengths and benefits even as they serve the same purpose. Evidently is a bit more of a lighter piece in the process, I feel, and as such it’s a bit more flexible and you can iterate faster with it. I am not quite at the point where I’m serving my model to accept inference requests from outside, but Evidently will be part of that process when I do, for sure.\nFinally, FiftyOne is also somehow part of the validation process. (I’ve written about that previously.) Having visual tools that allow you to quickly test out a hypothesis or debug something unexpected in your training results is an essential part of the work of developing computer vision models.\nThis brings my short series on data validation for computer vision to a close. I’m fully conscious that I might have missed some obvious opportunities, tricks or workflows that may be widely used in this field, so I welcome any comments and feedback that you might have."
  },
  {
    "objectID": "posts/2022-04-19-data-validation-great-expectations-part-1.html",
    "href": "posts/2022-04-19-data-validation-great-expectations-part-1.html",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nData validation is a process of verifying that data is accurate and consistent. It plays a crucial role in end-to-end machine learning pipelines.\nThere is a lack of validation tools in Computer Vision (CV) due the complexity of the data used by the domain.\nIn this series of articles, I will show you how to leverage the Great Expectations open-source library to validate object detection data. This will help you to feed your model with data less prone to break your model performance.\nWhen something goes wrong with a newly trained or newly deployed version of your model, where do you look first? Where does your gut tell you the bug or issue is likely to be found? For me, knee deep into my redaction model project, I immediately think of my data. For sure, I could probably have more in the way of testing to be sure my code is working how I expect it to work, but issues with the data are far more likely to be silent killers. Issues with data are unlikely to raise a loud exception and suddenly bring my training to a stop. Instead, my training will continue, but I’ll either get really unsatisfactory results or I’ll get results that are underperforming the real potential of the data I’m feeding into my model. That’s the scary part: I most likely won’t even know that my data is broken or faulty.\n\n\nFor software engineers, testing your code is a tried-and-tested way to find some confidence in what you’re trying to accomplish. (I am exploring some of these best practices in my review series about Patrick Viafore’s excellent Robust Pythonbook, which covers testing along with typing and various other patterns.) For critical systems, testing is one of the things that allows you to sleep soundly. For those living in the world of machine learning or data science, data validation is like writing tests for your data. You can be confident that your data looks and has the shape of what you feel it should when you address the data quality issue head-on.\nIf you think of your model training workflow as a pipeline, there are certain places where it makes sense to do some kind of data validation:\n\nat the very beginning, when you’re seeing your data for the first time: a lot of exploration and basic analysis really helps at this point. It will help you build up intuition for the general patterns and boundaries of the data you’re going to use to train your model.\nany time you do some kind of conversion: perhaps you have to — as I do with my project — convert from one image annotation format into another and you’re juggling x and y coordinates constantly, or maybe you’re using different image formats at different points?\nprior to training your model: ‘garbage in, garbage out’ as the saying goes… You probably want to make sure that you only have high quality data passing through into your model training pipeline step.\nas part of a continuous training loop: perhaps you’ve trained and deployed a model, but now a few months have passed, you have more data and you want to retrain your model. Are you confident that the new data retains the same characteristics and qualities of your original data?\n\nAs you can see, there are many different approaches that you might take. To discuss where you might want to validate your data is to discuss where your processes might be flawed in some way. For most projects of any size or significance, you probably will find that taking the care with your data inputs will pay dividends.\n\nData validation and computer vision\nIt often seems like computer vision exists in a world unto its own, particularly when it comes to the data used to train models. These idiosyncrasies amount to a strong case for some kind of data validation:\n\nimage data isn’t always easily introspectable, especially on the aggregate level (i.e. what is the ‘average’ of a series of images, or how to think of the standard deviation of your images?)\nfor something like object detection, the annotations are stored in a separate location from the images to which they correspond, leaving the door open for a creeping data drift between the original image locations and what is listed in the annotations.\nFor massive data sets, the original data will likely not be stored in the environment where you’re doing some fine-tuning with new data.\nDifferent model architectures require different kinds of pre-processing for your data and sometimes annotations need converting into slightly different formats (perhaps for your evaluation metric)\nThe pure images (or image-adjacent objects like medical scans) contain a lot of sub-surface metadata that isn’t easily accessed and isn’t automatically used as criteria for comparison or error detection.\n\nIn short, there are lots of ways that training a computer vision model can go wrong, and implementing even basic safeguards against this can give you confidence in the data you’re using. Unfortunately, the landscape of tooling for data validation in the computer vision space feels like it’s lagging behind what exists for tabular data, for example, but that’s almost certainly because it’s just a harder problem. The big data validation libraries don’t really cater towards computer vision as a core domain, and (as you’ll see below) you’ll probably have to crowbar your data into the formats they expect.\n\n\nBig picture: what might this look like for my project?\nAs I outlined above, there are lots of different places where you might want to use some kind of data validation strategy. At the level of code, you might want to make your input and output validation a bit more solid by using type annotations and a type checker like mypy. You can add tests to ensure that edge cases are being handled, and that your assumptions about your code behaviour are proven. You also have your tests to ensure that changes in one function or area of your codebase don’t break something somewhere else.\nAt the level of your data, you can of course use simple assert statements within the functional meat of your codebase. For example, at the point where you’re ingesting data pre-training you could assert that each image is of a certain format and size, and perhaps even that annotations associated with that image ‘make sense’ as per the context of whatever problem you’re solving. You can handle some of these assertions and checks with simple conditionals, perhaps, earlier on in the process when you are ingesting or pre-processing your data.\nA significant benefit of having these simple assertions inside your core functions is that you are handling the ways things can go wrong at the same time as you’re writing the functionality itself. A disadvantage is that your code can easily become cluttered with all this non-core behaviour. It feels a little like the validation can become an afterthought in this scenario. For this reason, it seems to make sense to me that you’d want to have one or more dedicated checkpoints where your data undergoes some kind of validation process. In the context of a pipeline, this means you probably will want one or more steps where this happens.\n\n\nTradeoffs\nFor tiny throwaway projects, or for proof-of-concept experimentation, it might not make sense to start off by working up a massive data validation suite. A really rigorous validation process early on might slow you down more than is useful. Instead, simple assert statements coupled with type annotations on your functions might be the way to go for safeguards and will-this-be-readable-in-the-future sanity checks.\nIdeally, you’ll want to create some kind of end-to-end pipeline or workflow at the beginning of your process, since this will allow you to iterate faster in a manner that’s meaningful for whatever you’re trying to solve. With a basic pipeline in place, data validation can be added as a stage of its own without too much disruption once you have an initial working prototype. As with most things in life, investing for the longer term is going to take a bit more upfront effort but that shouldn’t be too much an issue as long as your project has that kind of a horizon to it.\n\n\nWhat kind of validation does Great Expectations offer?\nGreat Expectations is an open-source data validation tool. It is somewhat agnostic as to what specific use case you have, but I don’t think it’d be wrong to say that it isn’t primarily developed for those working on computer vision problems; tabular data seems to be a much cosier fit.\nI stated above that Great Expectations could be used as if you were adding tests for your data. At a very high level, you can think of it as a fancier way of adding assertions about your data. The ‘expectations’ in the title are like those assertion statements, only in this case there are dozens of different pre-made ‘expectations’ you can choose from. For example, you could assert that you expect that the values of a particular column of a Pandas DataFrame be between 0 and 100, and that if they exceeded those boundaries then it would be only a very small proportion that did so.\nYour expectations make up a ‘suite’, and you run your suite of expectations against a batch or data asset. There are another 10 or 20 concepts or terms that I’d need to define and connect together in a mental map before we covered everything about how Great Expectations works. Unfortunately, this is one of the things I found most confusing about getting to know the library through its documentation. From the outside, it appears that they had one set of terminology, but now it’s partially changed to a different set of terms or abstractions. Presumably for reasons of backwards compatibility, some of the old abstractions remain in the documentation and explanations, which makes it not always clear to understand how the various pieces fit together.\n\nYou can read the glossary over at their documentation site if you want to learn more, but for now everything I explained above should suffice.\nThere seem to be two main ways of setting up and using Great Expectations. One is heavily interactive and driven by executing cells in a series of notebooks. The other is as you’d expect — code-based using a Python library, backed by some external configuration files and templates. I didn’t find the notebook-based configuration and setup very compelling, but it is the one emphasised in the documentation and in online materials, so I will give it due attention in the next part of this blog series. For now, it might suffice to show a very simple version of how the code-based use works:\n\n\nA simple example of using Great Expectations for data validation\nThe first thing I did was to convert my annotations data into a Pandas DataFrame. You can use Pandas, SQL and Apache Spark as sources for your data to be validated through Great Expectations, and luckily my COCO annotations file was just a JSON file so it was easily converted. While doing the conversion, I made sure to add some extra metadata along the way: a column noting whether an image or a redaction was horizontal or vertical in its orientation, for example, or splitting the bbox array into its four constituent parts.\nimport great_expectations as ge\n\nannotations_df = ge.from_pandas(pd.DataFrame(annotations))\n\nfeature_columns = ['area', 'iscrowd', 'image_id', 'category_id', 'id', 'synthetically_generated', 'category_name']\nfor col in feature_columns:\n    annotations_df.expect_column_to_exist(col)\n    \nannotations_df.expect_column_values_to_be_in_set(\n    \"category_name\",\n    [\"content\", \"redaction\"]\n)\nGreat Expectations wraps the Pandas library, so importing the data was easy. Then adding the expectations (methods beginning with expect…) was trivial. Below you can see the result from the second of the expectations. All of the column values were in that set, so the test passed.\n{\n  \"success\": true,\n  \"result\": {\n    \"element_count\": 6984,\n    \"missing_count\": 0,\n    \"missing_percent\": 0.0,\n    \"unexpected_count\": 0,\n    \"unexpected_percent\": 0.0,\n    \"unexpected_percent_total\": 0.0,\n    \"unexpected_percent_nonmissing\": 0.0,\n    \"partial_unexpected_list\": []\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nIn the second part of this series, I’ll explore how the interactive way of using Great Expectations works, and I’ll also show the web results interface for your expectations suite. It’s much fancier than the dictionary / object that was output above, and what’s even better is that you can have Great Expectations make some of its own guesses about what the right expectations for your particular dataset might be.\nI hope for now that I’ve made the case for why data validation is probably worth doing, and started you thinking about how that might apply to a computer vision use case."
  },
  {
    "objectID": "posts/2022-04-04-ml-portfolio-best-practices.html",
    "href": "posts/2022-04-04-ml-portfolio-best-practices.html",
    "title": "Some characteristics of best-in-class ML portfolio projects",
    "section": "",
    "text": "Ekko was the last time I worked on a big project that would be presented publicly. An open-source framework that provided realtime infrastructure and in-transit message processing for web applications was a group project that I worked on together with three other colleagues, and we took the time to really make the how and the why really explicit. We made animations, diagrams, charts, and I learned a lot about what’s hard when explaining technical projects, even when the audience is expected to be (mostly) technically literate.\nI’ve been working on my redaction project since December and slowly but surely I’m tying the ends together and getting ready for it to come to a close. As part of the final touches, I want to offer something equivalent to how we presented Ekko. From reading around and exposure to various projects over the years, it seems to me that machine learning projects sometimes have different emphases and conventions. This blog post is my attempt to list some of the characteristics of really great ML portfolio projects, with an emphasis on how the project is presented."
  },
  {
    "objectID": "posts/2022-04-04-ml-portfolio-best-practices.html#some-top-projects",
    "href": "posts/2022-04-04-ml-portfolio-best-practices.html#some-top-projects",
    "title": "Some characteristics of best-in-class ML portfolio projects",
    "section": "Some top projects",
    "text": "Some top projects\n\nHealthsea by Edward Schmuhl (@aestheticedwar1) is my current favourite project writeup, blending amazing visuals, full explanation and a clear overview\nThis project (by @ahmed_besbes_) was recommended to me and although it’s more of a step-through of how the project works and was created, it also is clearly presented and very visual.\nFor computer vision projects, Hugging Face Spaces is a great place to find interesting Gradio demos, though after a while they blend into each other a little. HF Spaces also doesn’t seem like it gets used for full project explanation that often."
  },
  {
    "objectID": "posts/2022-04-04-ml-portfolio-best-practices.html#characteristics-of-top-projects",
    "href": "posts/2022-04-04-ml-portfolio-best-practices.html#characteristics-of-top-projects",
    "title": "Some characteristics of best-in-class ML portfolio projects",
    "section": "Characteristics of top projects",
    "text": "Characteristics of top projects\nSome things I think make a great portfolio project stand out:\n\nvisual design — looks count for a lot, for better or for worse.\ninteractivity — if there is some kind of a demo or application that I can play around with in order to relate to concepts being written about, that’d be great.\nvisual explanations alongside pure text — a diagram or animation can really help bring explanations to life.\na clear overview — the structure of the writeup should be clear and readers should be able to get a high-level overview first without necessarily needing to read through every last detail.\nexplain what problem you’re solving — spend (probably) more time than you think is necessary to explain what problem you’re solving and set up the context for the work you did.\ncode snippets are ok, but don’t just dump your source code.\npresent your dead ends — don’t just present the happy path; feel free to present things that didn’t work out as well. Readers will want to know that you encountered difficulties and there are benefits from seeing how you made decisions along the way.\npresent further work and next steps — offer hints at what other work could be done on the project, even if you’re done with it for now.\ndon’t lose track of the use case — show that you thought about the specific problem you were solving and not just as a technical problem in a void. (Real-world use cases have constraints, and your solution should live within a universe where those constraints directed you).\nFeel free to link out — you can easily link to other places where you’ve gone into the details about a particular problem you encountered. No need to cram every single last detail into the project portfolio.\nDon’t forget the purpose of the portfolio — it doesn’t need to be an exhaustive catalogue of every last detail; it just needs to offer a compelling overview that is understandable as an independent entity.\n\nThere are other aspects which are more table stakes for anything you write online — no typos, clear writing and so on.\nI took the time to step back from the project to write this down as I move into a phase where I’ll increasingly focus on the full writeup and I wanted to have a list to remind me of the things I valued in these kinds of projects.\nIf you have good examples of ML portfolio projects (or really great blog write-ups with interactivity and so on), please let me know in the comments!"
  },
  {
    "objectID": "posts/2022-03-21-docker-in-a-month.html",
    "href": "posts/2022-03-21-docker-in-a-month.html",
    "title": "Starting Docker In A Month Of Lunches",
    "section": "",
    "text": "As far as software engineers go, I’m still barely a spring chicken, six months into my job with ZenML. Working at a startup is fairly fast-paced and the ability to get going quickly with any number of libraries and tools is a requirement of the work. The number of things I could study, learn or practice is vastly larger than the amount of time I have. Accordingly, it helps to try to pick things that will be long-lasting, teach a cross-cutting skill or that are somehow fundamental.\nTwo closely-connected technologies that I’ve realised I can no longer avoid are Docker and Kubernetes. I have some high-level knowledge of both, having worked with Docker images on Ekko and having encountered Kubernetes in recent months, but it’s become clear in the last few weeks that they aren’t going away. More than that, it seems that not having a better (practical) grasp of some of the ins and outs of both is holding me back from grasping more complex decisions that are being made at work.\n[Side-note: I’m very curious about Podman as a Docker-adjacent alternative, but I need to understand Docker better first before I can make comparisons. I’d also note that I’m pretty sure that there are lots of cases where Kubernetes is overkill, and where it doesn’t make much sense to add all that complexity, particularly for smaller teams and projects. It’s nevertheless a feature of life in the MLOps space, it seems, so I must understand it.]\nI’ve had my eye on two Manning books by Elton Stoneman for a while, and now seems the perfect time to dive in. Learn Docker in a Month of Lunches and Learn Kubernetes in a Month of Lunches are very practical introductions to their subjects, come with good reviews and feedback and were published relatively recently. I’m especially happy that both books are extremely hands-on and even though I won’t in any way be an expert in either technology by the end, I’ll at least have some experience of having encountered the core use cases of both and maybe have a strong idea of what I do and don’t know.\nI’m not sure whether I’ll complete each one in exactly a month, but I’ll try to fast-track my reading. The chapters are written in such a way as to be digestible (including exercises) in around an hour. Stoneman says in the introduction to the Kubernetes book that it’s best to start with the Docker one, which I suppose makes sense given that one builds on the other.\nJust like my posts as I read through Robust Python (which I haven’t stopped doing), I’ll write up various things that I learn along the way, mainly as notes for myself but perhaps it will have value beyond this limited goal. So far I’ve read through the first three chapters of the Docker book, so what follows are some notes on the key points from that."
  },
  {
    "objectID": "posts/2022-03-21-docker-in-a-month.html#dockerfile-layout",
    "href": "posts/2022-03-21-docker-in-a-month.html#dockerfile-layout",
    "title": "Starting Docker In A Month Of Lunches",
    "section": "Dockerfile layout",
    "text": "Dockerfile layout\nDockerfiles seem to have some commonalities in terms of the structure:\n\nyou start with a base image on which you’re building. These seem usually or often to be a base image containing a runtime for whatever language your application uses\nThen there’s often environment variables afterwards\nThen you can specify a WORKDIR which is the working directory for the application\nThen you can COPY files from your local filesystem into that working directory\nThen at the end you specify which CMD needs to be run in order to execute the application.\n\nOnce you’re done with writing your Dockerfile, use the following command to build your image:\ndocker image build --tag SOMENAME .\nNote that final . trailing that command. The . states that the current directory is the ‘context’ and thus is used for when you’re copying in files using the COPY command.\nYou can view the layers of your Docker image with the docker image history IMAGENAME command (which will output them to the terminal).\nTo see how much disk space your containers and images are taking up, type docker system df.\nWhen you rebuild an image, you can specify this with a :v2 after the name, as in this command:\ndocker image build -t web-app:v2 .\nWhen it comes to optimising your Dockerfile, bear the following in mind:\n\n“Any Dockerfile you write should be optimised so that the instructions are ordered by how frequently they change — with instructions that are unlikely to change at the start of the Dockerfile, and instructions most likely to change at the end. The goal is for most builds to only need to execute the last instruction, using the cache for everything else. That saves time, disk space, and network bandwidth when you start sharing your images.” (pp. 42-43)\n\nSome command tips and tricks:\n\ncombine multiple commands onto the same line\nput the CMD instruction early on as it’s unlikely to change"
  },
  {
    "objectID": "posts/2022-03-03-model-improvements.html",
    "href": "posts/2022-03-03-model-improvements.html",
    "title": "Incremental Improvements to my Redaction Detection Model",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nLast time I wrote about my work on this project, I’d just finished creating synthetic image data to supplement my manual annotations. Before integrating those into the model training, I wanted to make changes to a few hyper parameters to ensure that I’m getting as much out of the current configuration as possible.\nI focused on three ways of improving the model’s performance, each of which ended up having an effect albeit in different ways.\n\nIncreasing the image size\nWhen I started the project, I set the size of the images that would be passed into the training as the training dataset to 384x384 pixels. (It is a convention relating to how some of the older pre-trained models (like EfficientDet) were such that the image size must be divisible by 128.) This turned out to be too small.\nThe next steps up were 512 and 640. The GPU / hardware on which I was training my model seemed to have no problem with either of these image sizes and the performance increased as I worked with either 512 or 640 as the base image sizes.\n\n\nIncreasing the batch size\nAnother important lever at my disposal was either to increase the batch size (the number of images that are used in each epoch) or to decrease the learning rate. (A useful Twitter thread by Boris Dayma explains some of the tradeoffs for one versus the other, along with some references to things to read).\nI had started off with a batch size of 8, but increasing to 16 and then 32 had a big effect on my model’s performance:\n\nBatch sizes of both 16 and 32 eventually converged on more or less the same COCOMetric score of around 74%. The validation loss rate showed pretty clearly that the highest (32) batch size overfit far faster than for 16. It seems that 16 is the best choice for now.\n\n\nBackbone model size\nThe way I’ve set things up to train this object detection model requires two main choices in terms of architecture: a particular pre-trained model and a backbone. VFNet (as mentioned previously) outperformed basically everything else I’ve tried and I think it seems to be a clear best choice for the model. In terms of the backbone, I’d been using resnet50 until now, but following some of the above experiments, it made sense to try increasing the backbone size as well. (An obvious disadvantage to this approach was slower training times and a larger final model size.)\n\nIn this image you can see the stages of improvements we made throughout this whole process. vfnet-pre-synthetic-base was the lowest performer at the beginning, then doubling the batch size gave another boost of almost 8% to our model performance. Then the final increase to the backbone size added another 4% increase bringing us to a score of around 78% for the COCOMetric.\nIt remains to be seen how much of these changes will make sense when I introduce the synthetic data, or if there are more effective boosters to the model performance in the form of adding annotations to areas where the model struggles the most."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html",
    "href": "posts/2022-02-10-synthetic-image-data.html",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "",
    "text": "This blog outlines my process (and a few false starts) for generating a series of synthetic images (and corresponding annotations) to supplement training data used in a machine learning project. This problem is one for which there aren’t many (any?) pre-existing data sets that I can repurpose so I’ve been trying to find ways to bootstrap and improve the performance of the model I’m training.\nBefore I dive into the details, I wanted to include a little context on the wider project and what I’m seeking to accomplish. It is a relatively common practice for documents released as part of FOIA requests to contain redactions. With so many documents being released — and perhaps in specific cases where legal teams are dealing with huge numbers of those redacted documents — it can be useful to identify which documents are redacted and/or to get a sense of just how much has been redacted. If you have 10 or 20 documents you can fairly easily get that overview, but if you have 10,000 or a million documents? That’s where my project comes in: I want to train a model to make it easy to detect redactions in a document and to generate statistics on what proportion of a document or documents have been redacted.\nYou can read more about the problem domain, about my initial forays into annotating a dataset for this problem, as well as view some examples of these redactions (and perhaps why they’re not as easy to identify as you might think). You can even try out a demo showing some of what the model can identify here. Note that this isn’t the latest version of the model so it’s not the absolute best performance."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#whats-the-deal-with-synthetic-images",
    "href": "posts/2022-02-10-synthetic-image-data.html#whats-the-deal-with-synthetic-images",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "What’s the deal with synthetic images?",
    "text": "What’s the deal with synthetic images?\nIt’s a truism that in computer vision projects you probably need or want a lot of data to get good results. For the Facebooks and Bytedances of the world this perhaps isn’t an issue: they have access to a ton of data, for better or for worse. But for me, I don’t have teams of data annotators or millions of users generating all this data. This is probably the norm for small- to medium-sized computer vision problems being solved out in the world, especially with more non-traditional entrants into the field who are just trying to do things with the skills instead of generating research and so on.\nInstead of using huge amounts of data, we need to be smarter about how we work, levelling ourselves up with whatever tricks of the trade we can muster. The fastai course contains a great number of these best practices, perhaps unsurprisingly since it is in some way targeted at individuals seeking to solve their domain-specific problems. One of the key insights I took away from earlier parts of the fastai book was the benefits of using pre-trained models. With a wealth of these models available and accessible, you don’t need to start your work from scratch. Instead, fine-tune your model and benefit from the expertise and hard work of others.\nYou do need some data to get started with fine-tuning a pre-trained model, however. That’s why I took a bit of time to make some initial annotations. I currently have annotated 2097 images, labelling where I have found redactions on the images as well as a box to show which parts of the image contain text or content. That approach has done pretty well so far, with in the low to mid seventies in terms of a % COCO score. (This is a commonly-used metric to assess the performance for object detection problems.) I want to go further, though, which is where synthetic images come in.\nThe big bottleneck in the annotation process is, of course, me. Depending on how many redactions any particular image contains, it could take me 5-10 minutes for a single image’s worth of annotations. This does not scale. Part of the speedup for this process is to use self-training, but I’ll write about that separately. Another option that has is often used is to generate images which approximate (to a greater or lesser degree) the actual real images. The useful thing about generating the images yourself is that you know where you placed the redactions, so you have the annotations at the same time.\nMy overall goal here was to boost my model’s performance. I didn’t know how how well these synthetic images would contribute, or even if they’d contribute to any boost at all. I was also quite conscious of the fact that you could probably spend a year generating pixel-perfect synthetic redacted documents. I didn’t want to waste too much time doing that, so at various points I had to make decisions as to whether a particular stage was good enough."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-1-get-a-baseline-naive-trial",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-1-get-a-baseline-naive-trial",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 1: Get a Baseline / Naive Trial",
    "text": "Phase 1: Get a Baseline / Naive Trial\nWhen I started this, I didn’t know how hard or easy it was going to be, so I set myself a low bar. I knew it was theoretically possible to create images with Python, but I’d never done it before so didn’t have a sense of the range of possibilities.\nIn situations like this, I find Jupyter notebooks really reveal their strengths. Experimentation is easy and the pace of iteration can be really high. A few minutes of searching around and it seemed like Pillow (aka ‘PIL’) was probably the best option to go with. I noted that you could edit, resize, copy and paste images. For my basic version of a synthetic image generator, that’s most of what I needed to do:\n\nTake an image that we know contains no redactions.\nGet a separate image file that is of a redaction box / squiggle or shape.\nRandomly resize the redaction shape.\nPaste the redaction shape at a random location on top of the base unredacted image.\n\nAnd voila! Finding unredacted images was easy since I had previously used fastai to build a model that could detect to ~95% accuracy whether an image contained a redaction or not. For the redactions, it took me about an hour with Pixelmator Pro and its ‘quick selection’ tool to extract 100 examples of various kinds of redaction that I knew were commonly found in the data set. You can see some of this variety in the illustration that follows, though note that each individual redaction snippet was its own separate image for the purposes of my synthetic generation.\n\nI found that it was pretty trivial to generate images of the kind I proposed above. The placement of the redactions didn’t always make sense, and sometimes the random resize that the redaction underwent meant that it was either far too small or far too large. I also hadn’t included any steps to capture the annotation in this prototype, but I knew it was possible so continued onwards."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#detour-get-stuck-pretty-quickly-experience-bbox-sprawl",
    "href": "posts/2022-02-10-synthetic-image-data.html#detour-get-stuck-pretty-quickly-experience-bbox-sprawl",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl",
    "text": "Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl\nBuoyed by my success in the prototype stage, I immediately added a bunch of improvements and features to what I wanted to achieve. I knew I wanted to make sure that the redaction stayed within the boundaries of the original base image. I also wanted to ensure that it stayed within the boundaries of the content of the base image — i.e. redactions generally tend to be made on top of content which tends not to be right on the outer margins.\nI rushed into things too fast without thinking the problem through and quite quickly got into deep waters as all the various pieces started to overlap. I was somehow still in notebook mode, passing various objects through various other custom libraries, not sure what I was passing where. In short: it was a mess.\nOne thing that tripped me up really fast was bboxes. (A bbox, in case this means nothing to you, is a data structure or type that allows you to represent where a box is positioned if you were to paste it on top of a base image (for example). It seems that there are different conventions about how to represent this concept of the location of a box on top of some other larger space. Some people represented it with pairs of coordinates, such that for each of the four corners of the box you’d have an [x, y] pair to represent each point. Others took this bbox type to contain references to the xmin, ymin, xmax, and ymax values of the box. In this way you could reconstruct the various corners since you had two opposite corners specified. Another option was that used by COCO, which was [xmin, ymin, width, height]. And yet another option was to represent a bounding box by [x_center, y_center, width, height]. (This is a useful article that details some of these representation differences.)\nI’m sure there are people who are really good at keeping multiple types of x and y coordinates, each with slightly different nuances, in their heads. I am not such a person and after an hour or two of struggling in these deep waters I realised I needed to regroup.\nMy notebook experiments had been good for uncovering the range of possibility, but now that I had a better sense of the edges of the problem — and the twists and turns of dealing with bounding boxes — I had to take a more systematic approach. I spent some time with pen and paper thinking through the flow that this synthetic generation process would have to include. I thought through what the various independent parts of this could be, and how data would flow through this set of steps."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-2-generate-my-own-base-images",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-2-generate-my-own-base-images",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 2: Generate My Own Base Images",
    "text": "Phase 2: Generate My Own Base Images\nThe first part of this process was to generate my own base images. In general, the types of base unredacted images in the core data set were relatively unremarkable. These were mostly letters, reports or some kind of form / table. I figured I could approximate this pretty quickly. By chance, that very weekend I happened to listen to an episode of the Real Python podcast which interviewed the creator of borb, a Python package for creating and manipulating PDFs. I knew I wanted images in the end, but I had already created a tool to extract images from PDFs and I figured borb would probably save me time, even if it meant I had to do some converting back and forth between images and PDF files.\nThe great thing about borb is that it offers an easy abstraction with which to reason about creating PDF documents. Have some text and want it to be displayed on a page? Done. Want that text to be displayed in three columns? Done. Want do insert some images and have the text flow round it? Done. Have styling requirements? Done. And on and on. I figured that this was just the level of abstraction I needed — rather than staying in the world of pixel primitives like lines and boxes.\nOnce I got going it was easy to generate base images with multi-column text and some random coloured shapes thrown in here and there. (I used lorem-text to generate random Latin texts.) After I created the PDF I then had to convert it into an image format for use elsewhere in the generator pipeline but I think that speed hit was a price worth paying."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-3-generate-my-own-redactions",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-3-generate-my-own-redactions",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 3: Generate My Own Redactions",
    "text": "Phase 3: Generate My Own Redactions\nThe redactions weren’t quite as easy as the base images. The easiest version of a redaction box was literally that: a black box that sits on top of the base image. That much was easy to create. Pillow had some useful interfaces that I could use to quickly create randomly sized boxes. I could even add text to them in the upper left corner as I’d noticed that many of the real redactions did that.\nIt was less clear to me how I’d go about generating the other kinds of redactions, particularly ones that resembled a handwritten mark in thick black marker over the top of a document. In the end, I decided not to go any further with anything that wasn’t a box, but I did make the redaction boxes more varied. I set it such that the box would be filled with a random colour. If the colour was dark enough, I made sure that the text was in a light (contrasting) colour. And ensure that there wasn’t always a text on the box.\nNot perfect, but still it gave me a way to move forward."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#the-big-picture-bringing-it-all-together",
    "href": "posts/2022-02-10-synthetic-image-data.html#the-big-picture-bringing-it-all-together",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "The Big Picture: Bringing It All Together",
    "text": "The Big Picture: Bringing It All Together\nWith these pieces complete, I had the basics of the next version of my synthetic image generation. You can see the flow and progression of my script in the following diagram:\n\nYou’ll note that there were a number of other steps that supported the image creation. I did again descend into bbox hell when calculating exactly where to paste the redaction image, but with a much more modularised approach to my code I didn’t get lost. Type hints also kept me honest about what variables I was passing in and out of the functions I’d created.\nI ended up using the initial model I’d trained so far in the step that figured out where the content of the image was. You’ll recall that this was one of the annotations I’d already been generating when I annotated my data, and since it’s a fairly simple computer vision task I was already seeing excellent performance from that specific class in object detection. IceVision, a library that I’m using for the computer vision and deep learning parts of this project, allowed me to fairly easily make this inference on the images and extract the bbox coordinates for the content box.\nI made sure to include a lot of random variation in the first two steps where the base and redaction images were created. I didn’t remove the original naive approach completely. Instead, I made it 50% likely that we’d generate an image versus just picking one of the unredacted images from our store. Then I gave the same chance for the redaction as to whether we’d use an actual redaction snippet or one of the computer-generated boxes. There was lots of resizing and colouring and various other randomisation that was also included."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-5-make-the-images-look-old-and-worn",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-5-make-the-images-look-old-and-worn",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 5: Make The Images Look Old and Worn",
    "text": "Phase 5: Make The Images Look Old and Worn\nOnly one step remained. I realised that when I generated the images completely from scratch, not using any of the real base images or redaction snippets, that they looked very new and unrealistic. A significant proportion of the documents in the collection looked like they’d been photocopied a thousand times and in general had seen better days. Sometimes the quality was such to make them unreadable. I realised if I was going to get good results with the overall goal (i.e. improve my model’s performance) I’d have to make the synthetic creations look old somehow.\nAfter some exploration I settled on augraphy as how I’d process the newly generated images to look old and worn. Luckily for me, this package seems to have been created explicitly to support machine learning workflows for synthetic data creation, and it seemed to be (somewhat) actively maintained. There was a default set of so-called ‘augmentations’ that Augraphy suggested I apply to my image. Unfortunately it was simply too aggressive. I guess for some workflows it would have been great, but the page ended up looking somewhat unintelligible by the end. Compare these two examples:\n\nNot only did the default Augraphy transforms often make the redaction indistinguishable, it shifted parts of the image around on the page for these crinkle and scrunch effects, which would have rendered my annotations inaccurate.\nThat said, as you can see from the left image, it was pretty easy to switch out the default for a set of random transforms to be applied that wasn’t quite so aggressive. I’m thankful that tools like this exist out in the open-source space and that allow me to get on with the work of solving the actual problem I’m interested in working on."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#final-results-2097-synthetic-images",
    "href": "posts/2022-02-10-synthetic-image-data.html#final-results-2097-synthetic-images",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Final Results: 2097 Synthetic Images",
    "text": "Final Results: 2097 Synthetic Images\n\nThis gif gives you a brief sense of some of the images I generated as a result of the process I’ve detailed above. They’re not perfect, and as I write I currently don’t know how well they will perform when training my model.\nI have 2097 real annotated images, so I’m going to combine them with a maximum of an equal number of synthetic images. I’ll try out different proportions of real to synthetic, but that’s also a topic for another blogpost to follow. Stay tuned!\nIt took about three and a half hours to create these 2000+ images on my laptop. There are LOTS of places where I could have made speed improvements, notably all the conversion between PDF and image objects, the inference for the content box and also the fact that the pipeline wasn’t performed in parallel on all my CPU cores. I spent about 30 minutes exploring Ray as a means to getting this process to be executed in parallel but it ended up being not as simple as I’d initially thought so I’ve left that to one side for now. In any case, I won’t be creating so many synthetic images at once so often, so it wasn’t a real blocking point for my work.\nNote, too, that the annotations get created as part of the same script. I append them to a synthetic annotations file at the same time as the synthetic images is generated, and the file is subject to being combined with the real annotations at a later stage.\nThere are obviously lots of ways this synthetic data creation process could be optimised, but I was recently reminded that it’s also important not to lose momentum and not to let the perfect be the enemy of the good.\nThe next step is to carry out an experiment to see the effect of adding in the synthetic annotations on model performance. There are a bunch of really tricky aspects to this (most notably finding ways to make sure not to allow my training data to leak into the validation data) but I’ll save all that for my next blogpost.\n(If you got all the way to the end, well done!)"
  },
  {
    "objectID": "posts/2022-02-05-robust-python-9.html",
    "href": "posts/2022-02-05-robust-python-9.html",
    "title": "Upgrade your Python dicts with data classes",
    "section": "",
    "text": "I’ve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn’t taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore’s book, ‘Robust Python’, specifically chapter nine.\nAn example upfront might help ground the conversation. Here is a data class in action:\nimport datetime\nfrom dataclasses import dataclass\nfrom typing import Literal\n\n@dataclass\nclass CatPassport:\n  name: str\n  breed: CatBreed\n  issue_date: datetime.date\n  expiry_date: datetime.date\n  gender: Literal['male', 'female']\n\naria = CatPassport(\"Aria\", CatBreed('bengal'), datetime.date(2022, 01, 05), datetime.date(2025, 01, 04), 'female')\nprint(aria.name) # prints 'Aria'\nFrom this you can see that it’s an easy way to represent structured data made up of different types. Where it excels over simply using a dict or a class you write yourself is the fact that it auto-generates a number of __ dunder helper methods. You get __str__ and __repr__ to handle what this object looks like when you try to print() it. It also creates an __eq__ method which allows you to check for equality between two objects of the same type with the == comparison operator.\n(If you want to add a way to compare between your data class objects, you can add arguments to the @dataclass decorator like @dataclass(eq=True, order=True) which will handle the creation of the relevant dunder methods.\nThe fact that data classes are just classes at heart mean that you can also add behaviours to these collections of values, something that isn’t possible with a plain dict.\nYou can specify that your data class should be frozen (@dataclass(frozen=True)) which effectively makes it an immutable data store, though taking note that objects stored as values on the data class’ properties might themselves not be immutable (think lists and dicts).\nAfter reading the chapter in ‘Robust Python’, I read around a little to get a sense of this concept. I read the official docs which were fairly helpful, but in fact it was the PEP document (557) that I found most interesting. I haven’t previously taken the time to dive into the specifics of PEP specifications before, but I discovered that they are pretty readable and you get a real sense of what problem a particular feature or addition to the language was trying to solve.\nPEP 557 explains some of the alternatives and why it might be useful to include this new feature. I also learned about the attrs package and how data classes are actually just a subset of what attrs offers. (As a side note, I was surprised that attrs seems to have been mentioned nowhere in ‘Robust Python’, even in the context of the upcoming Pydantic chapter. Perhaps it was just too confusing to have all these things alongside one another.)\nOther options to consider alongside data classes when dealing with heterogenous data inside a single object or structure include TypedDict and namedtuple, but it seems like the default for this kind of scenario should probably just be a data class, though I should add that it is only part of the standard library for Python 3.7 and above."
  },
  {
    "objectID": "posts/2022-01-22-robust-python-6.html",
    "href": "posts/2022-01-22-robust-python-6.html",
    "title": "Using mypy for Python type checking",
    "section": "",
    "text": "The final two chapters of part one of Patrick Viafore’s ‘Robust Python’ cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase.\nmypy is the most commonly used option for type checking in Python and it does most of what you probably need it for. You can run it via the command line, inline as part of your IDE, or as part of a CI/CD pipeline. At work we do all three.\nYou can configure mypy to your heart’s desire either with inline comments in your code, or via a configuration file. A configuration file is probably the way to go, particularly if you’re versioning your code and sharing these kinds of settings across a team.\nChapter 6 goes into detail about some of the specific options or settings you can tweak to make mypy more or less sensitive to certain kinds of errors. For example, in a previous post we mentioned how you can implicitly accept None as a type with the Optional type annotation wrapper. But maybe you don’t want to allow this behaviour because it’s generally not a great idea: if so, you can use the —strict-optional flag to get notified whenever you’re using that particular construction.\nmypy also allows for the export of its results to html and xml, and you can run it in the background as a daemon which (particularly for large code bases) might speed it up.\nWe also learn about some alternatives to mypy, namely Pyre and Pyright. Pyre runs as a daemon in the background and allows you to run queries relating to type usage in your codebase. It also includes a static code analyser called Pysa that runs a kind of security analysis on your code called ‘taint analysis’. A quick summary of this would be to say that you can specify specific kinds of security flaws that you want to address and/or prevent being part of your codebase.\nPyright is interesting since it has a useful VS Code integration (via the Pylance extension). You get all sorts of autocompletion and tooltip goodness by using Pyright/Pylance.\nFinally, chapter 7 thinks through how you might want to approach actually using type checking and type hints in a larger codebase, perhaps one that already exists. It’s useful this was included as I imagine these sorts of practicalities are much more of a blocker to adoption than any technical issues. After a brief discussion of tradeoffs, we learn about some different options for where you might want to start with introducing types to a legacy codebase.\n\nFocusing on the pain points — i.e. where the lack of type hints has already seen bugs emerge in the past\nor perhaps adding them to new code only\nor perhaps type annotating the pieces of the codebase that actually drive the product or business’ profits\nor maybe whatever is complex to understand\n\nAll of these are options and it will definitely depend on your particular situation.\nWe also learn about two tools that might help get you started with type annotation: MonkeyType and Pytype. Both auto-generate type hints for your codebase. MonkeyType does so dynamically, so it only generates type hints for parts of your code that it accesses while running the code. Pytype does so by static analysis. Both deliver some kind of output that you can then use (perhaps) as the basis of some annotations of your codebase. My instinct is that these two tools feel like they might lead to some faulty assumptions or errors if you rely on them too much and that in fact it would be better to just methodically go through your code and incrementally add type hints as suggested above.\nThis concludes the type hints part of the book. I feel like I really got a solid overview of why type hints are used in large or complex Python codebases as well as how to implement this practically. I will be writing separately about how we use mypy and type hinting at ZenML as I think it offers an interesting case study on some of the benefits and tradeoffs that we’ve observed on a day-to-day basis.\nNext up in Robust Python: defining your own types with Enums, data classes, classes and how this fits into libraries like Pydantic."
  },
  {
    "objectID": "posts/2022-01-16-midway-report-redaction-project.html",
    "href": "posts/2022-01-16-midway-report-redaction-project.html",
    "title": "A Midway Report on my Computer Vision Project",
    "section": "",
    "text": "(This post is adapted from a twitter thread, so is a bit more terse than usual.)\nI recently switched what I spend the majority of my professional life doing (history -&gt; software engineering). I’m currently working as an ML Engineer at ZenML and really enjoying this new world of MLOps, filled as it is with challenges and opportunities.\nI wanted to get some context for the wider work of a data scientist to help me appreciate the problem we are trying to address at ZenML, so looked around for a juicy machine learning problem to work on as a longer project.\nI was also encouraged by Jeremy Howard’s advice to “build one project and make it great”. This approach seems like it has really paid off for those who’ve studied the fastai course and I wanted to really go deep on something myself.\nFollowing some previous success working with other mentors from SharpestMinds on a previous project, I settled on Computer Vision and was lucky to find Farid AKA @ai_fast_track to mentor me through the work.\nIn the last 6 weeks, I’ve made what feels like good progress on the problem. This image offers an overview of the pieces I’ve been working on, to the point where the ‘solution’ to my original problem feels on the verge of being practically within reach.\n\nAfter just a few lessons of the FastAI course, I trained a classification model to ~95% accuracy to help me sort redacted images from unredacted images.\nI used Explosion’s Prodigy to annotate an initial round of data to pass into the next step, enjoying how the labelling process brought me into greater contact with the dataset along the way.\nI switched to using IceVision to help me with the more complicated object detection problem, using MMDetection and VFNet to get pretty good results early on.\nI’m currently in the process of creating my own synthetic images to boost the annotations I’ve manually made. (I’ll be writing about this process soon as well, as I’m learning a lot about why this is so important for these kinds of computer vision problems.)\nI’ve also been amazed at the effectiveness of self-training (i.e. using my initial model in my annotation loop to generate an initial set of annotations which I can easily amend as appropriate, then feeding those annotations in to create a better model and so on). More to follow on that step, too.\nI started using Evidently to do some drift detection, inspired by some work I was doing for ZenML on adding Evidently as an integration to our own tool. This helped me think about how new data was affecting the model and the training cycle. I feel like there’s a lot of depth here to understand, and am looking forward to diving in.\nI made a tiny little demo on HuggingFace Spaces to show off the current inference capabilities and to see the model in a setting that feels close to reality. This is a simple little Gradio app but I liked how easy this was to put together (a couple of hours, mainly involving some build issues and a dodgy requirements.txt file)\nAlong the way, I found it sometimes quite painful or fiddly to handle the PDF files that are the main data source for the project, so I built my own Python package to handle the hard work. I used fastai’s nbdev to very quickly get the starters of what I’m hoping might be a useful tool for others using PDF data for ML projects.\nThroughout all this, Farid has been patiently helping guide me forward. He saved me from going down some dark rabbit holes, from spending too long studying skills and parts of the problem that needed relatively little mastery in order to get to where I am.\nFarid has been a consistently enthusiastic and kind advocate for my work, moreover, and this has really helped me stay the course for this project that takes a decent chunk of my time (especially seeing as I do it completely aside / separately from my day job).\nI feel like I’m consistently making progress and learning the skills of a data scientist working in computer vision, even though I have so much left to learn! My project still has a ways to go before it’s ‘done’, but I’m confident that I’ll get there with Farid’s support. (Thank you!)"
  },
  {
    "objectID": "posts/2022-01-06-nbdev-early-impressions.html",
    "href": "posts/2022-01-06-nbdev-early-impressions.html",
    "title": "Learning about ‘nbdev’ while building a Python package for PDF machine learning datasets",
    "section": "",
    "text": "While working to develop a computer vision model that detects redactions in documents obtained as a result of FOIA requests, I have encountered some tasks that I end up repeating over and over again. Most of the raw data in the problem domain exists in the form of PDFs. These PDF files contain scanned images of various government documents. I use these images as the training data for my model.\nThe things I have to do as part of the data acquisition and transformation process include the following:\n\ndownloading all the PDF files linked to from a particular website, or series of web pages\nconverting and splitting all the downloaded PDF files into appropriately sized individual image files suitable for use in a computer vision model\ngenerating statistics on the data being downloaded and processed, as well as (further down the line) things like detecting data drift for incoming training data\nsplitting up data as appropriate for train / validation / test data sets\nextracting text data from the images via an OCR process\nversioning, syncing and uploading those images to an S3 bucket or some other cloud equivalent for use in the overall workflow\n\nIt’s not hard to see that many of these things likely apply to multiple machine learning data acquisition scenarios. While writing the code to handle these elements in my specific use case, I realised it might be worth gathering this functionality together in an agnostic tool that can handle some of these scenarios.\nI had wanted to try out nbdev ever since it was announced back in 2019. The concept was different to what I was used, but there were lots of benefits to be had. I chose this small project to give it an initial trial run. I didn’t implement all of the above features. The two notable missing parts are text extraction and data versioning and/or synchronisation.\npdfsplitter is the package I created to scratch that itch. It’s still very much a work in progress, but I think I did enough with nbdev to have an initial opinion.\nI think I had postponed trying it out because I was worried about a steep learning curve. It turned out that an hour or two was all it took before I was basically up and running, with an understanding of all the relevant pieces that you generally use during the development lifecycle.\nBuilt in to nbdev in general is the ability to iterate quickly and driven by short, small experiments. This is powered by Jupyter notebooks, which are sort of the core of everything that nbdev is about. If you don’t like notebooks, you won’t like nbdev. It’s a few years since it first saw the light of day as a tool, and as such it felt like a polished way of working, and most of the pieces of a typical development workflow were well accounted for. In fact, a lot of the advantages come from convenience helpers of various kinds. Automatic parallelised testing, easy submission to Anaconda and PyPi package repositories, automatic building of documentation and standardising locations for making configuration changes. All these parts were great.\nPerhaps the most sneakily pleasant part of using nbdev was how it encouraged best practices. There’s no concept of keeping test and documentation code in separate silos away from the source notebooks. Following the best traditions of literate programming, nbdev encourages you to do that as you develop. Write a bit of code here, write some narrative explanation and documentation there, and write some tests over there to confirm that it’s working in the way you expected. When Jeremy speaks of the significant boost in productivity, I believe that a lot of it comes from the fact that so much is happening in one place.\nWhile working on pdfsplitter, I had the feeling that I could just focus on the problem at hand, building something to help speed up the process of importing and generating images from PDF data for machine learning projects.\nNot everything was peaches and roses, however. I ran into a weird mismatch with the documentation pages generated and my GitHub fork of nbdev since I was using main as the default branch but nbdev still uses master. I will be submitting an issue to their repository, and it was an easy fix, but it was confusing to struggle with that early on in my process. I’m also not sure how well nbdev will gel with large teams of developers, especially when they’re working on the same notebooks / modules. I know reviewnb exists now and even is used within fastai for code reviews, but I would imagine an uphill battle trying to persuade a team to take a chance with that.\nI’ve been using VSCode at work, supercharged with GitHub Copilot and various other goodies, so it honestly felt like a bit of a step back to be forced to develop inside the Jupyter notebook interface, absent all of my tools. I also found the pre-made CLI functions a little fiddly to use — fiddly in the sense that I wish I’d set up some aliases for them early on as you end up calling them all the time. In fact, any time I made a change I would find myself making all these calls to build the library and then the documentation, not forgetting to run the tests and so on. That part felt a bit like busy work and I wish some of those steps could be combined together. Maybe I’m using it wrong.\nAll in all, I enjoyed this first few hours of contact with nbdev and I will continue to use it while developing pdfsplitter. The experience was also useful to reflect back into my current development workflow and environment, especially when it comes to keeping that close relationship between the code, documentation and tests.\n[Photo by Laura Ockel on Unsplash]"
  },
  {
    "objectID": "posts/2022-01-01-counter.html",
    "href": "posts/2022-01-01-counter.html",
    "title": "Counter: a shortcut to counting iterables in Python",
    "section": "",
    "text": "I came across this special dictionary type while reading an earlier chapter of ‘Robust Python’ the other day. It’s perhaps best illustrated with an example:\nfrom collections import Counter\nCounter([1,1,2,3])\n# returns Counter({1: 2, 2: 1, 3: 1})\n\nCounter('The Netherlands'.lower())\n# returns Counter({'e': 3, 't': 2, 'h': 2, 'n': 2, ' ': 1, 'r': 1, 'l': 1, 'a': 1, 'd': 1, 's': 1})\nI had no idea this existed, and of course usually default to some kind of a cookie-cutter loop when trying get counts of elements and put those counts into a dict.\nTo get the inividual elements, just call the elements method on the Counter object. To get the most common n elements, call the most_common(n) method. To get the total number of counts inside the dictionary, use the total method. To reset all the counts, use the clear method.\nJust a nice little set of functionality, hiding in plain sight inside the Python standard library.\nPhoto by Ibrahim Rifath on Unsplash"
  },
  {
    "objectID": "posts/2021-12-29-robust-python-1.html",
    "href": "posts/2021-12-29-robust-python-1.html",
    "title": "What makes code robust?",
    "section": "",
    "text": "We use a lot of modern Python idioms, libraries and patterns at work, so I’ve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, Robust Python: Write Clean and Maintainable Code by Patrick Viafore, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I’ll be working my way through it in the coming months and reflecting on things as I encounter them.\nThe first chapter is mainly about setting the scene for all the technical pieces that follow. Patrick asks the core questions: what is robust code and why do we even care? What problems does it solve to think about code in this way.\nWhat I took away was that a robust codebase emphasises good communication as well as avoiding accidental complexity. A lot has been written about ‘clean code’ and how to achieve this, but it seems that ‘Robust Python’ is arguing for looking a bit further into the future, when you have to come back to refactor your code three months after you wrote it, or when your colleague needs to do the same.\n\n“Writing robust code means deliberately thinking about the future.” (p. 3)\n\nYou write robust code, in other words, because you know that the codebase is going to be changing and shifting and that whatever you write today may need to be modified at a later date:\n\n“A robust codebase is resilient and error-free in spite of constant change.” (p. 4)\n\nWe’re trying to solve for the way that code is often hard to reason about or understand when you’re outside the original moment when it was written. Accordingly, it pays dividends to take a bit of extra time upfront to write code such that it does communicate intent well, and that you haven’t made things more complicated than they need to be.\nMoreover, the communication of intent needs to be done in a way that is asynchronous. The book goes into a bit more detail about why communication practices that require minimal cost and minimal proximity are to be preferred. These include: the code itself, in-code comments, tests, version control history, wikis, and in-project documentation.\nThe first part of the book is all about type annotation, using mypy, and how working with types helps makes your code more robust. We use a lot of this at work so I’m excited to take a deep dive into this."
  },
  {
    "objectID": "posts/2021-12-15-redaction-taxonomy.html",
    "href": "posts/2021-12-15-redaction-taxonomy.html",
    "title": "A Taxonomy of Redaction",
    "section": "",
    "text": "One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn’t always share redaction practices.\nI took a bit of time to try to understand the different kinds of redactions in my (pretty huge) data set. I didn’t have any special process for selecting these images; I randomly sorted the immediate ~70,000 images I have collected and looked through to try to identify some patterns.\nTaking a close look at the actual parts of images that contain redactions gives me a better sense of the challenges involved in detecting those redactions. As I iterate through my collection of images, I can start to build up an intuitive sense of where class imbalances might exist. Among the images that contain redactions, for example, which ones are most represented and which contain fewer examples? In general, where do I need to focus my efforts when it comes to improving my model?\nThe first easy distinction to draw is that between digital and hand-applied redactions.\n\nIt seems that the trend in this is towards digital redactions. Perhaps it is seen as less reliable, or perhaps it’s more time consuming to attach the reasons for redactions having happened. Perhaps, too, there are some legal reasons why each redaction needed to start having a specific reason applied to it.\nAt first glance, no pun intended, it would appear that digital redactions are much easier to recognise. They’re often uniform in how they are applied and are usually pretty blunt in their appearance. There are some non-redaction uses for totally black or grey boxes laid on top of text, but they aren’t common and it’s a pretty strong feature to have to predict.\nHandwritten redactions are also easy to recognise, but potentially the borders are harder to make out. Sometimes having a thinner pen with which redactions are applied might make it slightly less accurate.\nIt is more practically important to distinguish between redactions that are easy to recognise vs ones that take some time to notice. I can use my own speed at noticing the redaction on a page as a gauge. It’s not a perfect analogy, but Jeremy Howard’s adage that if a human can reliably do some kind of classification or object detection, then probably a computer can as well. I guess the inverse is also true: if a human will find it hard to recognise a particular feature in an image, then a computer will probably also find it hard.\nThere isn’t much point spending too long with the ‘easy’ redactions. These are usually whatever is boxy and blunt. It’s the stereotype of a redacted document, one like what was used as the cover art on the (much-censored) Guantánamo Diary by Mohamedou Ould Slahi.\n\nSometimes you see that the entire page has been redacted with some kind of a coloured box. Other times entire columns of information has been redacted from a table. These definitely feel like they are the more recent types of redactions.\nOne thing that makes detecting redactions hard, on the other hand, is if the number of redactions is small. It stands to reason that lots of small redactions can stand out at first glance, whereas a single small redaction on one corner of the page is maybe harder to notice.\nThe hardest of redactions seems like it is in examples like this:\n\nA white box on top of other white boxes! I often have to look quite closely at these to distinguish what is normal text and what is a redaction box. Some of them have a faint thin grey boundary box around them, which I guess ends up being pretty useful as a way to make that distinction. Surprisingly, the model that I’ve trained so far is not terrible at making these kinds of distinctions.\n\nI have a few hundred annotated images so far, but I now have an intuitive sense of the hard parts of the object detection test. I also have a sense of how represented I feel like those hard parts are — not very.\nAs I wrote in my previous update on my progress in this project, the next step is very much to find ways to increase the volume of good training data that I’m using to train my model. Part of that will involve creating synthetic data, part of that will be using self-training to speed up my annotation, and of course another part will just be doing more manual annotation. I’ve already started work on creating the synthetic data. More on that next time!"
  },
  {
    "objectID": "posts/2021-11-30-vfnet-basics.html",
    "href": "posts/2021-11-30-vfnet-basics.html",
    "title": "What is VFNet?",
    "section": "",
    "text": "VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been other improvements.\nThe original paper is here. The implementation of this model is here.\nThe problem it solves is that when we’re training a model, we have a large number of possible options for objects detected in an image. What we need to do is rank these options in order of likelihood of being a correct bounding of a box.\nIt is based on and draws on the MMDetection model/toolbox. MMDetection is a Pytorch library for object detection. It is modular, allowing for greater customisability."
  },
  {
    "objectID": "posts/2021-11-30-vfnet-basics.html#other-resources",
    "href": "posts/2021-11-30-vfnet-basics.html#other-resources",
    "title": "What is VFNet?",
    "section": "Other resources",
    "text": "Other resources\nAirctic Presentation on VFNet"
  },
  {
    "objectID": "posts/2021-11-27-safety-vulnerability-checker.html",
    "href": "posts/2021-11-27-safety-vulnerability-checker.html",
    "title": "Check your security vulnerabilities with safety",
    "section": "",
    "text": "safety is a tiny tool that checks your package’s dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check.\nIt checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can subscribe via pyup.\nWith that caveat, it’s not perfect, but it’s better than nothing. An easy CI win for open-source projects.\n[I first learned of this tool here. Many thanks to calmcode for continuing to make these really useful videos.]"
  },
  {
    "objectID": "posts/2021-11-26-environment-variables.html",
    "href": "posts/2021-11-26-environment-variables.html",
    "title": "How to set and get environment variables using Python",
    "section": "",
    "text": "If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this:\nimport os\n\nos.environ['SOME_ENV_VARIABLE'] = 13.5\nAnd to access an environment variable, there are actually a number of different ways. All these three are essentially the same:\nos.getenv('SOME_ENV_VARIABLE')\nos.environ.get('SOME_ENV_VARIABLE')\nos.environ('SOME_ENV_VARIABLE')\nFor the final one (os.environ('SOME_ENV_VARIABLE')), if the variable doesn’t exist, it’ll return a KeyError, whereas the first two will just return None in that case."
  },
  {
    "objectID": "posts/2021-11-21-on-failure.html",
    "href": "posts/2021-11-21-on-failure.html",
    "title": "On failure",
    "section": "",
    "text": "I’ve been working as a machine learning engineer now for a few months now. If there’s one thing that I have found characterises my experience so far, it’s failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand.\nThere hasn’t been a week that’s gone by since I started where I didn’t encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. My last post was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process.\nFailure isn’t fun. My initial reaction to hitting something I don’t understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it’d be a different kind of work.\nTwo posts by Julia Evans are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‘Life in Code’ and ‘Close to the Machine’.\nThe point is this: we are paid to confront this failure. This is the work. Thinking that it’s a distraction from the work — some kind of imaginary world where there are no blockers or failing tests — is the real illusion."
  },
  {
    "objectID": "posts/2021-09-18-writing-code.html",
    "href": "posts/2021-09-18-writing-code.html",
    "title": "Writing Code",
    "section": "",
    "text": "I read Daniel Roy Greenfeld’s post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I’ve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time.\nJust like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.)\nFor me, this looks like the following:\n\ncoding at work during the week\nsmaller focused exercises from PythonMorsels, Exercism, LeetCode and AlgoExpert\ncode written while working my way through the fastai course; this will probably manifest as blog posts here as well, outlining some small project I completed along the way.\na bigger project, perhaps a package, that I’ll start building at some point. I have some ideas for things I want to implement. I’ll pick one soon. It’ll probably be related in some way to the fastai coding. I’m thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words.\nsmaller scripts to solve daily problems in my digital life. I’ll store those on my GitHub somewhere and write up the design decisions around the more interesting ones here.\n\nOne thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it."
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html",
    "href": "posts/2021-09-14-python-versioning-package-managers.html",
    "title": "A Baseline Python Development Setup",
    "section": "",
    "text": "The world of Python versioning (and the downstream package versioning) is wild. This StackOverflow thread gives you a sense of some of the core issues at play. (As an indication of the importance of the issue, even BDFL Guido van Rossum himself has the current second most upvoted answer.)\nFor a really vanilla and close-to-core-python setup, a combination of venv and pip seem to be the way to go. venv is part of the standard library and as such is pretty close to a default option.\nFor something a bit more involved, that handles dependencies and package installation in a slightly more deft manner, the combination of pyenv, pyenv-virtualwrapper and poetry works really well. I’ll detail some of the setup gotchas and usage patterns below."
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-for-versioning-python-itself",
    "href": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-for-versioning-python-itself",
    "title": "A Baseline Python Development Setup",
    "section": "pyenv for versioning Python itself",
    "text": "pyenv for versioning Python itself\npyenv lets you install multiple versions of Python on the same machine. The interface to switch between local versions and whatever you’ve decided will be your global option is pretty intuitive.\nVisit the pyenv github page for more on installation. (If you’re on a Mac you can simply do a brew install pyenv.)\nTo see which versions of Python you have installed locally:\npyenv versions\nTo see versions of Python which are available for installation:\npyenv install —list\nNote that, as I understand it, these versions are not dynamically updated. You get an updated list of new Python versions by updating pyenv, in other words.\nTo install a specific version of Python, and to make it available for use:\npyenv install 3.9.1\nTo set that version of Python as the global version (i.e. running python will use this version by default):\npyenv global 3.9.1\nIf you are in a project directory and wish to only use a particular version of Python in that directory (and its subdirectories):\npyenv local 3.8.2\nThis creates a .python-version file in that directory with the desired local version."
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-virtualenv-for-managing-virtual-environments",
    "href": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-virtualenv-for-managing-virtual-environments",
    "title": "A Baseline Python Development Setup",
    "section": "pyenv-virtualenv for managing virtual environments",
    "text": "pyenv-virtualenv for managing virtual environments\npyenv-virtualenv is a plugin that connects the work of selecting which version of Python to use (through pyenv, which we’ve previously installed) to the work of creating and running virtual environments to keep code contained in quasi-sandbox environments. When you install packages in virtual environments they don’t conflict with other locations where you might have conflicting versions of those same packages installed.\nRead installation instructions and the docs here. (If you installed pyenv with homebrew, be sure to do the same with pyenv-virtualenv).\nTo create a virtual environment for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtual environment directory:\npyenv virtualenv 3.8.2 my-virtual-env-3.8.2\nThis will create a virtual environment based on Python 3.8.2 under $(pyenv root)/versions in a folder called my-virtual-env-3.8.2.\nTo list what virtual environments have been created and are available to use:\npyenv virtualenvs\nAs a common workflow pattern, you’d create your directory and cd into it, and then you can set the virtual environment you just created as the one to use for that directory:\nmkdir test-project && cd test-project\npyenv local my-virtual-env-3.8.2\nThis should change the prompt in your terminal window and you’ll thus know that you’re now working out of that virtual environment. Any time you return to that folder you’ll automatically switch to that environment.\nThe manual way of turning on and off virtual environments is:\npyenv activate env-name\npyenv deactivate env-name\nTo remove a virtual environment from your system:\npyenv uninstall my-virtual-env\n(This is the functional equivalent of removing the directories in $(pyenv root)/versions and $(pyenv root)/versions/{version}/envs.)"
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html#poetry-for-handling-package-installation-and-dependencies",
    "href": "posts/2021-09-14-python-versioning-package-managers.html#poetry-for-handling-package-installation-and-dependencies",
    "title": "A Baseline Python Development Setup",
    "section": "poetry for handling package installation and dependencies",
    "text": "poetry for handling package installation and dependencies\npython-poetry is the latest standard tool for handling package installations and dependency management.\nYou can use poetry without the previous two tools, but really they work best all together. Follow the installation instructions documented on their page to get it going.\nThen update poetry:\npoetry self update\npoetry is one of those tools that’s able to update itself.\nFor basic usage for a new project, you can follow the following workflow. There are two ways to start a new project using poetry: using new or init. For example:\npoetry new some-project-name\nThis will kickstart your new project by creating a bunch of files and a directory structure suitable for most projects, like so:\nsome-project-name\n├── pyproject.toml\n├── README.rst\n├── some-project-name\n│   └── __init__.py\n└── tests\n    ├── __init__.py\n    └── test_some-project-name.py\nYou might want to use a src folder (above the some-project-name in our example) which is fairly commonly used, in which case amend the command as follows:\npoetry new --src some-project-name\npoetry init doesn’t do all the extra work of creating a directory and file structure. It merely creates a pyproject.toml file interactively, using some smart defaults. For a minimal use of poetry, this is definitely the way to go.\nThe add command adds required packages to your pyproject.toml and installs them (along with all their dependencies). It does a lot under the hood to make sure that dependencies are correctly resolving before installing. For example:\npoetry add zenml\nTo add packages only to be used in the development environment:\npoetry add --dev zenml\nTo list all installed packages in your current environment / project:\npoetry show\nTo uninstall a package and remove it (and its dependencies) from the project:\npoetry remove zenml\nTo install all relevant packages and dependencies of a project that you’ve newly cloned into:\npoetry install\nNote that it is possibly worth creating some custom scripts to handle some of the overhead of using these tools, depending on your common development workflows."
  },
  {
    "objectID": "posts/2021-09-10-python-environments.html",
    "href": "posts/2021-09-10-python-environments.html",
    "title": "Managing Python Environments with pyenv and pipenv",
    "section": "",
    "text": "It’s hardly news that that managing multiple versions of Python in a development environment is hard. Adding in dependency management on top of that makes everything harder."
  },
  {
    "objectID": "posts/2021-09-08-baseline-mlops-understanding.html",
    "href": "posts/2021-09-08-baseline-mlops-understanding.html",
    "title": "A Baseline Understanding of MLOps",
    "section": "",
    "text": "Next week I’m due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It’s a new field to me. I’ve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there.\nA top-down explanation is probably the best way to think of what we’re doing when we talk about ‘doing MLOps’: we’re doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‘in production’. It isn’t just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve.\nThe kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‘full stack machine learning engineer’, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "",
    "text": "I’ve been thinking about what it actually means to create something “with AI” versus creating something “yourself.” The usual framing treats this as binary: either you wrote it, or the machine did. Tools like Pangram try to detect AI-generated text. People unfollow accounts that seem to be posting slop. The implicit question is always: was this made by a human or not?\nI don’t think that’s the right question."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html#the-choice-space",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html#the-choice-space",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "The choice space",
    "text": "The choice space\nHere’s a different way to look at it. Imagine life as an infinite branching choice space. Every choice you make opens up a new infinite set of choices. When you create something – a blog post, a Slack message, a piece of code – you’re picking a specific line through that space.\nThe question isn’t whether AI was involved. The question is: how many choices did a human make?\nI’ve started calling this “choice density.” A piece of writing with high choice density has been shaped by many human decisions: what to include, what to cut, how to phrase something, when to push back on a suggestion, when to add personal context. A piece with low choice density is the result of minimal human input – prompt in, output out, copy, paste, done."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html#both-kinds-of-slop",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html#both-kinds-of-slop",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "Both kinds of slop",
    "text": "Both kinds of slop\nThe discourse around AI tends to focus on “AI slop” – content that’s obviously machine-generated, generic, soulless. But there’s another kind of slop that doesn’t get talked about as much: what I’d call “brain slop.” A human sitting down and writing a first draft with no iteration, no editing, no refinement. Stream of consciousness dumped onto the page.\nBoth can be low-choice. Both can be generic. The presence or absence of AI isn’t what determines quality or authenticity. What matters is how many choices someone made along the way.\nWhen I think about the things I’ve written that felt most like mine, they weren’t necessarily the ones I wrote entirely by hand. They were the ones where I made the most decisions. Where I pushed back, iterated, added context, cut things that didn’t fit, injected my own experience and taste."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html#this-applies-everywhere",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html#this-applies-everywhere",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "This applies everywhere",
    "text": "This applies everywhere\nThe choice density lens isn’t just about blog posts or tweets. It applies to any domain where AI is showing up.\nWork communication. When a colleague asks a question in Slack, do you copy the thread into ChatGPT, paste back whatever it says, and move on? Or do you actually think about the response – curate it, add your own understanding, make sure it fits the context of your team and the person asking?\nCode. Agentic coding tools can now generate entire pull requests. You can say “fix this bug” and watch the agent make changes across multiple files. But there’s a spectrum here too. On one end: fully autonomous, you don’t even look at what it did. On the other end: you review the plan, you push back on architectural decisions that don’t fit your team’s approach, you catch the places where the model doesn’t understand your codebase’s conventions. Same tool, wildly different choice density.\nThe pattern is the same across all of them. It’s not about whether AI was used. It’s about how many human choices shaped the outcome."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html#the-enabling-side",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html#the-enabling-side",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "The enabling side",
    "text": "The enabling side\nThere’s something that often gets left out of these conversations: for some people, AI tools aren’t just convenient. They’re enabling.\nNot everyone has the same cognitive resources available on any given day. Chronic illness, mental health conditions, neurodivergence, caregiver responsibilities, burnout – there are lots of reasons someone might have limited capacity to wrestle their thoughts into polished prose. The “spoon theory” from the chronic illness community captures this well: you only have so many spoons each day, and when they’re gone, they’re gone.\nFor someone with limited spoons, the choice might not be between “write it yourself” and “use AI.” It might be between “use AI to help get your thoughts out” and “don’t express yourself at all.”\nThis doesn’t mean the choice density question disappears. It still matters how much you engage with the process, how many decisions you make. But it reframes the stakes. AI assistance isn’t inherently a shortcut or a cheat. For some people, it’s the difference between participating and staying silent."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html#the-slippery-slope",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html#the-slippery-slope",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "The slippery slope",
    "text": "The slippery slope\nHere’s where I don’t have clean answers.\nThese tools are designed to reduce friction. They’re trained to be helpful, agreeable, generative. They say yes. That’s what makes them useful, but it’s also what makes them dangerous.\nA human collaborator might push back. They might say “I don’t think you’ve thought this through” or “that’s not how we do things here.” They have their own stakes in the work. An AI assistant, by default, doesn’t. It’s optimized to help you get to an output, not to question whether the output is actually good or actually yours.\nSo the responsibility for maintaining choice density falls entirely on the user. And that’s a lot to ask – especially when your spoons are low, especially when the tool is designed to make acceptance easy and pushback hard.\nI don’t think there’s an out-of-the-box way to use these tools well. Just like social media platforms are designed to be addictive, AI assistants are designed to be frictionless. Using them in a way that preserves your humanity requires building your own guardrails. Deliberate friction. Processes that force you to make choices. People who can tell you when you’re slipping.\nIt’s a discipline. I’m not sure it’s a discipline I’ve mastered."
  },
  {
    "objectID": "personal/2026-01-21-choice-density-ai-authenticity.html#what-im-still-figuring-out",
    "href": "personal/2026-01-21-choice-density-ai-authenticity.html#what-im-still-figuring-out",
    "title": "Choice Density: A Better Way to Think About AI and Authenticity",
    "section": "What I’m still figuring out",
    "text": "What I’m still figuring out\nI don’t have a formula for “enough” choices. I can’t tell you where the line is between high-choice and low-choice use. I’m not even sure the line is stable – it probably depends on the context, the stakes, what you’re trying to create.\nWhat I do think is that “was AI involved?” is the wrong question. It collapses a rich spectrum into a binary, and it misses what actually matters.\nThe better question is: how many choices did you make? How much of your taste, your experience, your judgment, your context went into shaping this thing? If you think of the output as a path through an infinite choice space, how many forks did you consciously navigate?\nThat’s what I’m trying to pay attention to now. Not whether I used AI, but how many choices I made along the way.\n\nI’m still working through these ideas. If you’ve thought about this differently, I’d genuinely like to hear it."
  },
  {
    "objectID": "personal/2023-08-10-language-study-zero-to-hero.html",
    "href": "personal/2023-08-10-language-study-zero-to-hero.html",
    "title": "Language Learning Crash Course: from slightly more than zero to slightly less than advanced",
    "section": "",
    "text": "A colleague at work asked me for some tips on learning German and her situation and goals were fairly common so I thought I’d write up some notes here.\nFor the specific scenario under consideration, and to ground what follows: my colleague’s mother tongue is not English, but she wants to learn German since she now lives there. She’s interested in all the skills and wants to be able to read complex literature and philosophy, for example, but in the short-term building up spoken abilities is the main focus and intention. She knows a few words / phrases here and there but not too much more than that."
  },
  {
    "objectID": "personal/2023-08-10-language-study-zero-to-hero.html#how-what-basics",
    "href": "personal/2023-08-10-language-study-zero-to-hero.html#how-what-basics",
    "title": "Language Learning Crash Course: from slightly more than zero to slightly less than advanced",
    "section": "How / What?: Basics",
    "text": "How / What?: Basics\nThere are many (many) options at the total beginner level. My go-to sources / methods are some combination or selection of the following. It’s really hard to give generic advice here so YMMV, but all these are suitable for this level:\n\nMichel Thomas course (if it exists for your language, do all courses / extensions that exist (usually 2 levels plus a vocab extension pack)). These are audio courses that will give you a solid basic foundation. As an alternative or sometimes additional option, you might want to try Pimsleur courses (again, if they exist). These are not cheap, but they’re a good way to get going.\nFrequency dictionary and/or lists: Gabe Wyner’s Fluent Forever app is based around the concept of frequency dictionary lists so if it supports your language then you can either use that app or just get the raw word list and create your own flashcards + study those basic words.\nSomething like a Duolingo / Drops / Language Transfer course if it exists for your language. These are a double-edge sword since Duolingo by now has quite extensive courses available for popular languages. I wouldn’t recommend using it as your core source of studies because the likelihood is that you’ll drop away from your studies before completing it. That said, they do have mostly solid materials and they have spaced repetition (of some kind) built in so if you’re someone who lives to complete things and can invest a bit of time into moving through the materials quickly, then these kinds of courses aren’t the worst idea. (But definitely see them as a means to the wider end and not the core of what you do.) If you’re someone who where possible prefers to study / learn away from technology / apps, try the excellent ‘Assimil’ books / course as the equivalent for this. (Note that this section potentially takes you very far off from the 2-5 weeks estimate I mentioned above. Only you know the extent to which you can afford to really invest in foundations.)\nVery easy readers. Again, if your language is popular then you might have access to these kinds of readers. Sometimes they’re developed for children, but other times they’re also available for adult students too. You’re looking for something (it’ll often be advertised as such) with a small / limited vocabulary size of words used. This is on the border with the next phase of study, so this can and will also be part of what you do next."
  },
  {
    "objectID": "personal/2023-08-10-language-study-zero-to-hero.html#how-what",
    "href": "personal/2023-08-10-language-study-zero-to-hero.html#how-what",
    "title": "Language Learning Crash Course: from slightly more than zero to slightly less than advanced",
    "section": "How / What?",
    "text": "How / What?\nWhat does this mean in specific? Some variation or combination of the following would be common:\n\nreading a lot in graded readers (hopefully these exist for your target language)\ngraded listening: again, these hopefully exist for your language\nitalki lessons or targeted spoken practice with a teacher (depending on what specific approaches you take)\nClozemaster or Glossika: these are two tools that allow you to work your way through the most frequently used words using spaced repetition and context-rich language selections. Both also offer some kind of audio component. You might not need this if you have a solid method / rhythm already going with your use of Anki, but it’s worth trying out at least to see if it clicks.\nwriting sentences / paragraphs, then having them corrected, and then using the corrected versions of the language in Anki along with cloze deletion (a variation of what I showed above with the sentence and then a word or phrase missing that you have to guess) to practice the language used in a particular sentence."
  },
  {
    "objectID": "personal/2023-08-10-language-study-zero-to-hero.html#how-what-1",
    "href": "personal/2023-08-10-language-study-zero-to-hero.html#how-what-1",
    "title": "Language Learning Crash Course: from slightly more than zero to slightly less than advanced",
    "section": "How / What?",
    "text": "How / What?\nAlmost everything here is driven by specific needs, but it will involve and include:\n\nlots of learner-driven comprehensible input\nneeds-driven activities (whether you’re still focused on spoken prowess vs reading vs listening vs writing)\nopen-ended practice like the kinds of tasks recommended by my (free) CoachBot self-study tool."
  },
  {
    "objectID": "personal/2023-07-15-automating-social-media-posting.html",
    "href": "personal/2023-07-15-automating-social-media-posting.html",
    "title": "Automating social media posting for my new blogposts",
    "section": "",
    "text": "I love blogging and I’ve benefitted a lot from what it’s done for me ever since I started my first Geocities page in the mid 1990s. I maintain a technical blog at mlops.systems and a somewhat less technical blog at alexstrick.com/blog, though hope at some point to merge these together.\nIn the past I would have been content with ensuring that my blog published an RSS feed and known that anyone wanting to follow what I was writing could do so simply by connecting their feed reader and subscribing. I’ve become more conscious in recent years of a healthy brew of ambivalence, ignorance or even outright hostility to even the idea of RSS feeds and readers. It seems many people don’t have RSS as an essential part of their informational hygiene any more. (I’ll put my sadness / confusion about this to one side for now.)\nAnd if I love blogging, I really dislike having to post my new blog posts to social media one by one, coming up with some catchy yet not overtly breathless summary of what I wrote, since this is apparently what many people use instead of RSS.\nI’ve been grumbling under my breath about this situation for this for a few years now, but when ChatGPT came out it seemed like an obvious use: summarise my blogpost and repost to all my social media accounts taking into account their particular needs. (Mastodon uses hashtags more than the others, whereas LinkedIn posts can be a bit longer, vs Twitter which needs to be a bit shorter and so on.)\nI held off, thinking I’d want to set up some system fully under my control involving serverless function calls and so on, but then I was reminded that I already use Zapier for some other administrative tasks. So this afternoon I set up and turned on some automation for social media posting to my Mastodon, Twitter and LinkedIn accounts. Posting happens at one step removed since I queue my posts in Buffer so that they go out at a time when people are more likely to see them. I apologise / don’t apologise for this. My blog writings remain wholly un-automated; it would completely remove the point of ‘learning through writing’ if I were to automate the things that I blog about. My social media postings (just one post per blogpost so as not to spam you all) are from now on automated. As an additional courtesy / discourtesy, I’ve tweaked the prompt such that the social media posts should always read just slightly ‘off’ and will be labelled with an #automated hashtag."
  },
  {
    "objectID": "personal/2022-12-27-2022-readings.html",
    "href": "personal/2022-12-27-2022-readings.html",
    "title": "2022 Readings",
    "section": "",
    "text": "I read 75 books this year (over 22,000 pages), and a few days still remain. Looking back over the full list, I’m both surprised at how many were only of middling reward. I think the key is to take one’s sweet time on the true gems and speed through the should-have-been-a-blog-post dross. Out of the gems, the following stand out:\n\nSabine Hossenfelder’s Lost in Math: How Beauty Leads Physics Astray. We’re told so often about how mathematics and physics is beautiful, how it makes sense and that finding those kinds of beauty-centric explanations is what we should be aiming for, so I was surprised and invigorated by a book about how the opposite might well be the case. It’s a few years old and I’m sure it’s part of a wider argument and debate in the field, but without knowing much more it certainly opened my mind.\nEmmanuel Ameisen’s Building Machine Learning Powered Applications: Going from Idea to Product was one of the first things I read as I transitioned into a new field and I often return to it. Lots of hard-won wisdom about the ‘whole game’ of machine learning in the real world, and a story economically-told.\nI didn’t read as much science fiction as I’d planned this year, but of the ones that I did, Samuel R. Delany’s Babel-17 blew my mind. It’s a classic for a reason, but still amazing to think that it was first published in 1966!\nFor strong emotions and characters that keep you rooting for them, look no further than Shobha Rao’s Girls Burn Brighter. Not a light read by any means, but a strong showing from start to finish.\nI devoured the eight currently-available parts of Martha Wells’ Murderbot Diaries over the course of a few weeks. They’re all fast-paced and once you’ve read the first the general conceit of the series wears off, but the character is so incredibly enjoyable. Come for the smart science fiction, stay for the strong character development. Definitely my favourite find of the year.\nKate Crawford’s Atlas of AI: Power, Politics and the Planetary Costs of Artificial Intelligence was a recommendation from a friend and it was a sharply sobering read in a discipline where ML boosterism is all too common. Nothing in there was particularly news, per se, but the picture as a whole raised important questions. Moreover, the balance between fine-grained detail and the big-picture worked well in this book.\nI reread Chanel Miller’s Know My Name this year. Still as devastating and powerful as when I first read it, and a searing indictment of the criminal process for cases of sexual assault in the United Status. Required reading.\nDostoevsky’s Crime and Punishment, a late entry this year, was my first Dostoevsky and I’m still thinking about quite a few of the scenes, weeks after putting the book down. In particular, the last time Raskolnikov meets with his mother.\nSome books are unexpected gifts, like Sarah Polley’s collection of stories-almost-not-told entitled Run Towards the Danger: Confrontations with a Body of Memory. Each essay is unexpected, brave and deftly constructed. I’ve watched most of what she’s been involved in since The Sweet Hereafter and this book certainly didn’t disappoint.\nFinally, I read through the two books written by David Goggins, Can’t Hurt Me and Never Finished and something about his take on suffering and the lessons it has to teach resonates a lot with me. I’ll be returning to these to sort through his experience at a later date."
  },
  {
    "objectID": "personal/2021-05-27-fastai-lesson-zero.html",
    "href": "personal/2021-05-27-fastai-lesson-zero.html",
    "title": "FastAI Lesson Zero: video notes",
    "section": "",
    "text": "[These are mainly notes for myself, based off Jeremy Howard’s ‘Lesson 0’ video that was recently posted. It doesn’t capture the entirety of what was said during the course, but it includes pieces that I felt are relevant to me now and that might be relevant to me in the future.]\n\ndecide when you’re studying\n\nbe precise about how much time you’re going to spend\nthink about how it’s going to fit into your day or your life\ngive yourself deadlines and goals, perhaps, but also don’t worry if disruptions happen.\nMainly make sure that if something does come up, make sure you get back on the horse and keep going. (Tenacity counts for a lot)\n\nFinish. The. Course.\n\nmake a commitment to complete the course, and make sure you actually do that.\nIf you’re attending the course and working through it, you should follow through on your original commitment and actually work through the course.\n\nFinish a Project\n\nbuild a project and make it really great.\nYou’ll probably have several projects here and there that you work on during the course of the fastai course, but at a minimum make sure you pick one of those and make it really great.\n(It doesn’t have to be unique or world-changing. Even replicating something that’s already in existence can still be worth it).\n\nFind good role models\n\nJeremy raises up the example of Radek Osmulski (who recently published an ebook called Meta Learning).\n(Jeremy himself is a good role model too).\n\nLearn by doing. Use what you learn and figure out the rest as you go. (Don’t get paralyzed by trying to learn ‘pre-requisites’ like complex mathematics topics, esp since most of them aren’t actually needed to become a practitioner).\nShare and communicate your work\n\n(Jeremy doesn’t mention the book, but I’ll insert here that the book “Show Your Work” by Austin Kleon is a great starter on this point).\nIf you consistently blog during your studies, at the end of it you’ll likely have a huge collection of artefacts of that study, showing what you’ve learned and accomplished.\nAlongside that, being a good citizen and contributing in the forums etc is also a really solid way to extend whatever knowledge you have to others, and quite possibly cement things in your own mind as you reply.\n\nHow to do a lesson\n\nwatch the video / read the chapter\nRun the notebook & experiment — play around with things + make sure you actually understand what’s happening\nReproduce the notebook from scratch — (and really start with nothing here, and try to reimplement whatever was done during the lesson. From previous experience, this work will be hard, but it’s super worth it. Recall learning is the best kind of learning)\nRepeat with a different dataset — use the techniques you learned in the course on a dataset of your own / or solve some related problem using these techniques\n\nUsing a notebook server vs a full linux server\n\nthe notebook server allows you to get going much faster\nA full linux server is more ‘your own’ and you get to also practice a bunch of other not-specifically-deep-learning skills along the way\nWith the fastsetup library, Jeremy has made getting going with an EC2 instance pretty easy.\nthe video spends a fair amount of time showing how to do this with Colab Noteboks and a AWS EC2 instance. Refer to the FastAI website and the full video for more details.\n\nGet better as a developer\n\njust doing the course, you’ll also work on your development skills along the way\nTwo important things to do to help with this:\n\nRead a lot of code\nWrite a lot of code\n\n\nStart with a simple baseline & get a basic end-to-end solution up and running\n\nWhen you’re working on a project, it’s a really good idea to start with a naive / super-basic baseline so that you know whether you’re making progress or whether you’re achieving anything with the work you’re doing.\nSuccessful ML projects that Jeremy has seen start with the simplest possible end-to-end solution and then incrementally grow from there.\nThe work of getting your pipeline working / your data imported etc will take a bit of time, and if you get that all sorted upfront it’ll help you focus on the actual work you want to be focused on.\n\n(At some point during the course) join a Kaggle competition and make a serious attempt to do your best\n\njust getting a model on the leaderboard tests your knowledge and your skills\njust work regularly on things, show up every day, try to make your model a little better each day\nDo these things:\n\n\n\n\nFor getting a job in the space\n\nhaving a public-facing portfolio of writings and projects will take you a really long way\nSome companies are more interested in people having the right credentials etc and will never choose you.\nStartups are a great place where this matters less.\n\nTry to take the second course\n\nThe first course gets you going as a practitioner of deep learning, but the second course allows you to implement algorithms and models from scratch and digs far more into the depths of the subject.\nJeremy wishes more people would take part two + encourages them to do so.\n\nThe fastsetup library is great for installing everything on a Ubuntu machine (like an AWS EC2 instance)\nExperiment tracking software\n\nThe two big players are TensorBoard and Weights & Biases.\nJeremy doesn’t use these. Finds it too tempting to spend your time watching your models train instead of doing something else that is probably more valuable.\nThere are some cases where it might help to use this software.\nWeights & Biases seems like a good company to work for & they’ve hired FastAI grads in the past."
  },
  {
    "objectID": "personal/2021-05-26-arthur-samuel-and-the-frontier-of-automation.html",
    "href": "personal/2021-05-26-arthur-samuel-and-the-frontier-of-automation.html",
    "title": "Arthur Samuel and the ‘Frontier of Automation’",
    "section": "",
    "text": "The use of neural networks / architectures is a powerful pattern, but it’s worth remembering that this pattern is part of the broader category of machine learning. (You can think of ‘deep learning’ as a rebranding of neural networks or what was once more commonly referred to as connectionism).\nIn a classic essay published in 1962, an IBM researcher called Arthur Samuel proposed a way to have computers ‘learn’, a different process from how we normally code things up imperatively (see my previous post for more on this):\n\n“Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximise the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would”learn” from its experience”\n\nWithin this essay and this quote specifically, we can find some of the key building blocks of machine learning:\n\nWe have our inputs (our data) and our weights. Our weights (or the weight assignments) are variables that allow for different configurations and behaviours of our model. Our results are what the computer has assumed based on the weights and the model, and we have some kind of a metric (our performance) to judge whether this model was accurate or not. The computer then updates the weights based on that performance, tweaking it such that it tries to get better performance.\nThis is a slightly amended version which language or jargon that are more commonly found today. As you might expect would happen, the language used in the 1960s is in many cases different from what gets used today:\n\nThe main difference here is that we have some labels which are used to know whether the predictions are correct or not. The loss is a way of measuring the performance of our model that is suited for updating our parameters (that used to be referred to as weights)."
  },
  {
    "objectID": "personal/2021-05-23-small-unexpectedly-powerful-boxes.html",
    "href": "personal/2021-05-23-small-unexpectedly-powerful-boxes.html",
    "title": "Small, unexpectedly powerful boxes",
    "section": "",
    "text": "Graphics Processing Units or GPUs are what your computer uses to quickly display your screen. Most computers (desktop or laptop) have one of these, and they are used to good effect to keep the screen refreshed and display everything in effectively realtime speed. The world of gaming is also, perhaps unsurprisingly, quite dependent on fast GPU performance, with Nvidia as the lead provider of these hardware units.\n\n\n\nnvidia gpu\n\n\nIt was discovered a while back that GPUs are also pretty great at performing certain kinds of computation at incredible speed. Certain calculations which, if you would do them on a standard CPU, would take ages to complete are much faster when run on a GPU. For this reason, they’re the hardware of choice for training deep learning models.\nGPUs also happen to be heavily used (for similar reasons) for cryptocurrency mining and accordingly there has been a worldwide shortage for some time. Between the crypto bros and the deep learning practitioners, the price got inflated for a while. Nvidia has made some attempts to limit crypto miners from using their hardware, but to inconclusive effect."
  },
  {
    "objectID": "personal/2021-05-23-removing-barriers-deep-learning-edition.html",
    "href": "personal/2021-05-23-removing-barriers-deep-learning-edition.html",
    "title": "Removing Barriers: Deep Learning Edition",
    "section": "",
    "text": "I’ve been re-reading Jeremy Howard & Sylvain Gugger’s Deep Learning for Coders with Fastai and PyTorch and I really appreciate the reminder that a lot of barriers to entry into the Deep Learning space can be productively put to one side.\nGatekeepers make four big claims:\n\nYou need lots of maths to use Deep Learning to solve problems\nYou need lots of data (think prodigious, Google-sized quantities) to use Deep Learning\nYou need lots of expensive computers and custom hardware to use Deep Learning\nYou need a PhD, preferably in Maths or Physics or some computation-heavy science\n\nNeedless to say, it’s not that maths or more data or better hardware isn’t maybe going to help or improve your experience. But to say that if you don’t have those things then you shouldn’t start is also (seemingly) inaccurate or not helpful.\nIf you are a domain expert in something that has nothing to do with Deep Learning or data science, you probably have a lot of problems that are like low-hanging fruit in terms of your ability to use powerful techniques like Deep Learning to solve them."
  },
  {
    "objectID": "personal/2021-05-23-held-back-by-misunderstanding.html",
    "href": "personal/2021-05-23-held-back-by-misunderstanding.html",
    "title": "Held back by misunderstanding",
    "section": "",
    "text": "The field of deep learning seems to have had a rough journey into public consciousness and adoption. In particular, two theoretical misunderstandings lead to funding being pulled and energy and attention moving away from the field:\n\nMinsky/Papert’s book Perceptrons showed how a neural network using only one layer was unable to learn some critical functions like XOR. Later in the same book, they show how using more layers addresses this problem completely, but for some reason the ‘fix’ to the problem was ignored and people fixated on the problem with using a single layer and its drawbacks.\nBy the 1980s, many people were using two layers in their neural networks, and while this did solve the problems identified in ‘Perceptrons’ and people were using neural networks to solve real problems, it was unwieldy in that form. Yes, you could theoretically approximate any mathematical function with two layers, but it was impractical and slow to do so. People thought that this meant that the principle was broken, whereas really the misunderstanding was that two layers were just not enough and that the number of layers could continue to increase.\n\nThese are two key misunderstandings identified by the Howard/Gugger short introduction and I’m sure I’ll read more of these in Genius Makers. It’s amazing, but not entirely surprising, that a non-generous and unimaginative misreading of the literature could be responsible for such an effective trashing of a research path."
  },
  {
    "objectID": "personal/2020-12-10-how-to-use-jquery-and-handlebars-in-your-website.html",
    "href": "personal/2020-12-10-how-to-use-jquery-and-handlebars-in-your-website.html",
    "title": "How to use jQuery and Handlebars in your website",
    "section": "",
    "text": "jQuery and Handlebars are both external to the core functionality of JavaScript. Both are libraries that we can use and include when making websites. Doing so is very simple. We include &lt;script&gt;s in the head of our HTML file, as in the following example:\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/handlebars@latest/dist/handlebars.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Hello, World&lt;/h1&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nNote that jQuery is now modular, so you may want to consider whether you want to include the entire library. Above I chose to download it from Google’s CDN, but there are other options listed here."
  },
  {
    "objectID": "personal/2020-12-04-using-apis-to-make-things-happen-on-the-web.html",
    "href": "personal/2020-12-04-using-apis-to-make-things-happen-on-the-web.html",
    "title": "Using APIs to make things happen on the web",
    "section": "",
    "text": "Towards the end of the Launch School core syllabus, we start to work with API calls. These allow us to pass information between servers. It turns out, this is really useful to be able to do.\nMany if not most of the things you do online involve API calls. That little widget on the side of the web page that shows today’s weather: an API call. Even things like Siri which aren’t exactly web pages: using API calls.\nBeing able to make those calls and to interact with the services available through the internet gives users all sorts of power. The creativity comes, then, in how these things are all combined together.\nThat said, the dreams of the connected web have been somewhat oversold in the past. What are we on now, web 4.0?.\nFrom a technical perspective, in this part of the course I enjoyed seeing how simple text in the form of JSON objects was behind so much of our communication and interactivity online these days. (All the more reason to have more secure ways of exchanging those plain text data).\nI was also conscious of how much creativity and widened thinking has gone into expanding the possibilities of what HTML, CSS and a bit of JavaScript can do over the internet. Some of these capabilities mean that we’re straining the possibilities in this area or that, but above all I take away some inspiration in how people made do with what they had instead of feeling like they needed to reinvent the wheel."
  },
  {
    "objectID": "personal/2020-12-02-the-four-positions-for-inserting-elements-into-the-dom.html",
    "href": "personal/2020-12-02-the-four-positions-for-inserting-elements-into-the-dom.html",
    "title": "The Four Positions for Inserting Elements into the DOM",
    "section": "",
    "text": "As part of my Launch School studies, I’m revising the ways we can use JavaScript to insert nodes into the DOM. These can be simple text nodes, or they could be elements, but the difficult part I’ve found is recalling the arguments for position. This is an argument you add into your method call which states where the node, for example, should be inserted. It looks something like this in action:\nlet newNode = document.createElement('p');\ndocument.body.insertAdjacentElement('beforebegin', newNode);\nWhere you see beforebegin is where you can include one of four different text strings:\n\nbeforebegin — before the calling element\nafterbegin — immediately inside the element, before its first child\nbeforeend — inside the element, just after its last child\nafterend — after the element itself\n\nThe words chosen for these position arguments never felt fully clear to me, so writing it out has been useful to clarify their meaning. This is also a useful code excerpt, from the MDN docs:\n&lt;!-- beforebegin --&gt;\n&lt;p&gt;\n  &lt;!-- afterbegin --&gt;\n  foo\n  &lt;!-- beforeend --&gt;\n&lt;/p&gt;\n&lt;!-- afterend --&gt;"
  },
  {
    "objectID": "personal/2020-11-12-using-css-selectors-with-javascript-dom-methods.html",
    "href": "personal/2020-11-12-using-css-selectors-with-javascript-dom-methods.html",
    "title": "Using CSS selectors with JavaScript DOM methods",
    "section": "",
    "text": "I’ve been using JavaScript methods that interact with the DOM all week as part of my Launch School course. Among them, document.querySelector() and document.querySelectorAll() seem really useful. I realised I didn’t fully understand the selectors that you were supposed to pass in as an argument, so I’m writing up some of what I discovered about them here. (See here for the documentation).\nThe simple part is when you have a single selector. Three important selectors are:\n\nid selectors (#) — so if the html contains an id attribute of ‘some-id’, then you could write #some-id.\nclass selectors (.) — so if the class was ‘special-style’, then you can write .special-style.\ntag selectors — for these, you just write the page itself\n\nWhen combining tags, there is a complex set of options depending on whether things are siblings or descendants or children. For the most part that is TMI. The important ones to remember are:\n\ndescendant selector combinations — place a space between elements — so if you want to select all &lt;div&gt; tags that descend (however far down in the tree) from a &lt;p&gt; tag, then you can write p div.\nchild selector — place a &gt; between the elements — this allows you to find elements that are direct children (i.e. no intermediary levels) of other elements. p &gt; div will find all div elements that are the direct children of paragraph elements.\n\n(For a more complete exploration of this topic, specifically the combination of selectors, read this blogpost.)"
  },
  {
    "objectID": "personal/2020-10-15-understanding-the-use-cases-for-closures-in-javascript.html",
    "href": "personal/2020-10-15-understanding-the-use-cases-for-closures-in-javascript.html",
    "title": "Understanding the use cases for Closures in JavaScript",
    "section": "",
    "text": "I spent today revising some of my Launch School JS225 topics. Among those was the idea of closures, or in other words how functions can capture and package away any variables or ‘state’ in scope at the point of function definition.\nI hadn’t fully realised just how useful that can be. Other languages have other ways of realising the same functionality. In JavaScript, it turns out this is a really useful feature because it effectively allows you to keep some variables, functions and whatever else available to our objects or functions, but we can control access to them. They are accessible via whatever interfaces we provide, but to an outsider trying to access those elements via our API or function calls, they are effectively private.\nThere are a bunch of ways that this is useful in things that I don’t yet understand and haven’t studied yet (async or promises seem high up that list) but closures play a role in those as well.\nI write all of this mainly because it’s useful from time to time to step out of the weeds and remind oneself why any or all of this is actually useful."
  },
  {
    "objectID": "personal/2020-10-12-working-with-javascripts-object-prototype-model.html",
    "href": "personal/2020-10-12-working-with-javascripts-object-prototype-model.html",
    "title": "Working with JavaScript’s Object Prototype model",
    "section": "",
    "text": "Today I worked on some more exercises relating to implementing object-oriented design patterns in JavaScript.\nI’m taking the position that a lot of my difficulty in absorbing the materials is because JavaScript itself wasn’t designed with this in mind. Any implementations that look or seem intuitive are probably going to be hacky, or hide complexity under some syntactical sugar.\nThe exercises revealed some models for how to work with OLOO code (see this for an example), particularly when you want to conceal some kind of ‘private’ data along with your encapsulation of methods. For this latter implementation, an IIFE seems to do the trick, though it also feels a bit of a hack.\nTomorrow I will try to complete the rest of my exercises for the module and then get some more practice of these paradigms / models in examples of my own creation."
  },
  {
    "objectID": "personal/2020-06-11-kapoor-amp-sons.html",
    "href": "personal/2020-06-11-kapoor-amp-sons.html",
    "title": "Kapoor & Sons",
    "section": "",
    "text": "A bittersweet family drama set in Coonoor, Tamil Nadu. This was a picture of India I wasn’t familiar with, and purely as backdrop this went a long way for me. The story with Tia (played by Alia Bhatt) seemed vastly underfunded in the plot, and the stumbling block was artificial. A number of strong ensemble scenes, though, and just enough grit in the ending to make sense to this viewer."
  },
  {
    "objectID": "personal/2020-02-12-two-ruby-patterns-around-map-and-reduce.html",
    "href": "personal/2020-02-12-two-ruby-patterns-around-map-and-reduce.html",
    "title": "Two Ruby patterns around map and reduce",
    "section": "",
    "text": "When you’re going to choose a method to work to transform a group of n things in Ruby, there are two broad patterns you can choose: work with a map function or work with a reduce / inject function.\nThis pattern choice was recently explained to me as part of my Ruby education at Launch School. I hadn’t fully grokked that the choice around how you transform a bundle of things (an Array, perhaps) really can be summarised by those two options.\nYou use a map method when you want to transfer your array into another array of (transformed) things. (For example, you wanted to transform an array of lowercase names into an array of uppercase names).\nexample_array = ['alex', 'john', 'terry', 'gill']\ntransformed_array = example_array.map(&:upcase) # =&gt; [\"ALEX\", \"JOHN\", \"TERRY\", \"GILL\"]\nYou use a reduce method when you want to transform n number of things (inside your array) into a single thing. (For example, you wanted to sum up the values of an array).\nexample_array = [1, 3, 5, 7, 9]\ntransformed_array = example_array.reduce(:+) # =&gt; 25"
  },
  {
    "objectID": "personal/2019-09-10-taliban-mutmain-preorder.html",
    "href": "personal/2019-09-10-taliban-mutmain-preorder.html",
    "title": "New book, new ways to order",
    "section": "",
    "text": "“I was around three or four years old when the Communists led the bloodiest coup in Afghanistan. KhAD personnel were arresting the faithful. One day, a few ugly moustached men knocked on our door. My father left with them and then he never came back. We never saw him again.\n“After a year, I began to understand that this kind person was no longer with me. Poverty, a cold fireplace, and my old clothes made it evident – I was an orphan. Every man with a moustache looked like my father’s murderer. My uncle took us with him to another village, and we no longer had a home of our own.”\n\nIn this way Abdul Hai Mutma’in begins his memoir of time alongside the senior leadership of the Afghan Taliban movement. First published in Afghanistan a couple of years ago, Taliban: A Critical History from Within is now available for pre-order in an English translation.\nMutma’in served as a political advisor to Taliban leader Mullah Muhammad Omar and as spokesperson. He worked in the media section of Kandahar’s Culture and Information Ministry and from 2013 onwards served as a political and humanitarian affairs advisor to Mullah Akhtar Mansour from 2013. In short: he spent a good deal of time around the senior leadership and was privy to the internal workings and machinations of the Taliban movement at its highest levels.\nAt First Draft Publishing, the small publishing house I started five years ago together with Felix Kuehn, our explicit agenda is to publish books that will help “give researchers, professionals and the interested public access to primary and secondary sources”. This book falls firmly into this remit. The list of primary sources relating to the Taliban (or primary-source-adjacent) is exceedingly thin, even all these years since the movement first burst onto the national and international stage. From our perspective as researchers, the more such memoirs get written, the more we are able to attempt a critical unpicking of narratives and myths that have driven both conflict and efforts towards integration. Without these raw materials, it is impossible to begin the slow and methodical work of scholarship: triangulation, verification, context, synthesis and so on.\nA bit of additional housekeeping: if you want to (pre-)order Mutma’in’s book, we have made some changes to how we’re producing and delivering books. We’re moving away from Amazon as the delivery system for our content and will simply process orders manually. For hardcopy purchases, we’ll be printing copies on demand. For ebooks, we’ll distribute DRM-free copies upon receipt of payment. If you’re interested in purchasing any of our books, please visit our website to learn more about our titles and email us to place an order."
  },
  {
    "objectID": "personal/2019-03-12-sources-and-methods-does-technology.html",
    "href": "personal/2019-03-12-sources-and-methods-does-technology.html",
    "title": "Sources and Methods Does Technology",
    "section": "",
    "text": "Episode one of Sources and Methods’s new season is out today. In this interview, Matt and I spoke with the two creators of an online programme to teach the abacus.\nI’ve been studying the abacus over the past few months using the RightLobeMath course and can attest to how thoroughly they build up the skill progressions. Using the web app doesn’t always feel completely polished as an experience, but it’s clear that a lot of thought has gone into the pedagogy. I’m extremely glad that it even exists at all as a service!\nThe rest of the season has all been recorded and edited and will be released at regular intervals. We got to talk to some amazing guests, some of whom are personal heroes / inspirations of mine from the world of technology. For now, though, you can find the first episode here."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html",
    "title": "Solid Study Habits for Coders",
    "section": "",
    "text": "I’ve been working to diversify my skills over the past three years, in particular to grow as a software developer / computer programmer. To that end, I started working my way through the Launch School syllabus a few months back. I’m at an inflection point in the course right now so I thought it worth reflecting on what study habits have served me well as well as which ones I still haven’t quite managed to adopt.\nBefore I started, I exhaustively read through their forum comments and in-house blog posts to see how I might best approach my study time. They recommend you spend 20+ hours per week working on their course materials. With such a large investment of my time, I wanted to make sure I was getting the best return."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#core-principles-mental-models",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#core-principles-mental-models",
    "title": "Solid Study Habits for Coders",
    "section": "Core principles / Mental Models",
    "text": "Core principles / Mental Models\n\n‘Get Good at the Fundamentals’\n\nThis is a bit of a mantra at Launch School and I find it greatly appealing as a principle to drive how I study. It’s also useful as a heuristic to decide whether to dive deep on something, to read an article, to attend a meetup and so on. (See more here.)\n\n‘Process, not goals’\n\nI’m not sure I saw this explicitly enunciated somewhere, but given the fact that it can take around two years to work your way through the syllabus it seemed useful to keep in mind. I already have adopted this thanks to Beeminder and a really excellent piece by Scott Adams from 2013. With a somewhat long-term goal, arbitrary goal-focused time deadlines (‘finish the 202 course by May’) are less useful than something where you specify the amount of time you’d like to regularly invest into the study process.\n\n‘Finishing is not the goal, mastery is.’\n\nThere’s a little bit of fetishising the idea of ‘mastery’ at Launch School, but this principle reminds me not to be in so much of a rush while working through the course. As Chris Lee mentioned in the forums, “if you’re in a rush, my question is: “where are you going?””\n\n(Find ways to) enjoy what is hard\n\nThis one I find quite difficult, lazy and averse to struggling with difficult things as I am. The whole skill/career of coding and wrestling with these abstract concepts in your head is one where you’re constantly upgrading your baseline of difficulty. This is rewarding in that there are always challenging problems to chew down on. This is also frustrating, though, because there’s always somewhere harder / out of reach to work towards. A number of students and TAs on the forums mentioned that having a mindset where you actively seek out those moments as opportunities for growth was a game-changer for them. (Note: strong intersections with Dweck’s growth mindset here)."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#mindset",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#mindset",
    "title": "Solid Study Habits for Coders",
    "section": "Mindset",
    "text": "Mindset\n\nAbandon artificial timelines\n\nDon’t work to get through a course in x or y number of months ‘just because’. Rather, only advance if you’ve actually mastered the things in that course. This is a freeing perspective to take if you can find a way to adopt it. Society / culture encourages always keeping an eye on the clock, so this is a hard one (for me at least).\n\nComparisons are odious\n\nIt doesn’t help to compare timelines. Everyone is coming at things with a different background and set of experience behind them. It also doesn’t help to compare competency / progress through the course for the same reason. It adds very little to my ability to grasp a particular topic to know where I lie on the bell curve. The only slight exception to this is knowing that the full core curriculum will take somewhere between 6-24 months to complete; this way if I am still studying it 20 years from now I can maybe recalibrate.\n\nLean in to hard problems\n\nI covered this earlier. If I have it as a sort of rubric which reminds me to lean into the difficulty whenever I encounter such a patch, then that’s useful.\n\nMaster the current problem in front of you, right at this moment, without anxiety\n\nBooks are written one word at a time. Similarly with logic problems, or broken code or whatever, taking a calm look at something that looks knotted and gnarly is the way to approach it. There’s no need for anxious sudden movements, or huge massive changes. Work your way through things one step at a time.\n\nBe clear about the ‘why’\n\nFor something that isn’t completed quickly (i.e. within a few days), it pays to remember why you’re doing it. This helps with motivation, it helps reorient you to what kinds of choices you have to make, and it (often) helps remind you of some best practices around working towards that mastery.\n\n(Be specific)\n\nThis is sort of a sub-point relating to specifying the reason why you’re doing something. Try to specifically connect with some kind of emotion, visualise the outcome in some kind of image etc; this will help get the most of your reasons why in terms of it translating into motivation and clarity of purpose."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#practice",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#practice",
    "title": "Solid Study Habits for Coders",
    "section": "Practice",
    "text": "Practice\n\nStudy ‘to depth’\n\nMake sure to study the exercises or problems to their full depth rather than just completing them for the sake of moving on. A useful image to have in your mind is that of squeezing an orange. You don’t want to abandon the orange (i.e. the opportunity to study a particular problem) until you’ve squeezed every last drop out of it. There’s a point at which you’ve spent too much time, of course, but for many / most people you’re likely erring on the side of ‘making progress’ vs getting lost going too deep.\n\nDo code exercises\n\nThere’s lots of options for this. You probably don’t want to start out doing this when you’re still figuring out what a variable is, but fairly soon it pays to get into the habit of doing an exercise or three each day. Think of it almost like your daily workout. You’re building your muscle memory for solving problems, as well as cementing various syntactic and idiomatic phrasings. Of course, Launch School has you doing a mountain of exercises as well, so don’t worry too much about supplementing from outside the course, at least not initially.\n\nReinforce concepts / theories using active recall\n\nThis is learning meta-methods 101, but always worth reminding oneself to actively test yourself. Don’t just read through notes, but keep a list of higher-level concepts or method names and then take 30 minutes or an hour once a week to type out an example or two using each of those concepts and methods. (More on this later).\n\nDeliberate practice\n\nThis builds on the previous concept of active recall. For me, this often takes the following shape: I’ll complete an exercise or a challenge. Perhaps I’ve solved the problem fine using my own code / approach, but when I finish and look at the ‘official’ solution it differs from mine in some (big or small) way. I’ll take a look, make sure I understand the principles around which it’s organised, then I’ll shut that page / window and try to replicate that approach myself. Sometimes, if a problem was hard enough to solve, I’ll even leave it a few days and just try to work the problem from scratch in any possible way that I can think of. I have enough of a goldfish brain that sometimes this will feel like I’ve never solved the problem in the past.\n\nCompare solution tradeoffs\n\nAlongside just trying to code it from scratch yourself, it sometimes helps to compare the various approaches (perhaps adding in solutions given by other students) and seeing which might be optimum. Asking ‘why’ at this point is often a useful place to stretch towards, even though the answer to that ‘why’ is often a few modules ahead of where you are. I keep a log of these bigger picture ‘why’ questions.\n\nBuild things\n\nPerhaps slightly controversial amongst those taking Launch School. They don’t encourage a ‘hack and slash’ mentality but rather direct towards a more methodical and deliberate approach. That said, once you’re at a certain point, it can be useful, motivating and just practically beneficial in terms of the reps it gives you, to build something that has meaning for you or for others. At the moment I’m just working on somewhat more pure Ruby code in the terminal / via the command line, but later on I imagine this will become more relevant. (To that end, perhaps keep a list of project ideas)."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#taking-notes-studying",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#taking-notes-studying",
    "title": "Solid Study Habits for Coders",
    "section": "Taking Notes / ‘Studying’",
    "text": "Taking Notes / ‘Studying’\nThis is the one that people seem to have the most thoughts about on the Launch School forums. That’s possibly explained by the fact that people will know what works for them — or at least will have some patterns that they feel have worked for them in the past, in somewhat analogous situations.\n\nType out all code examples manually\n\nThis one is easy to forget. When you’re following along with a video or textbook, make sure to type out the examples, however easy or seemingly comprehensible they seem. Typing not only engages your muscles, giving you an additional hook for memory, but it forces you to slow down and gives an opportunity to question what each character of a code snippet is doing.\n\nTake notes with pen and paper first time round\n\nThis is especially true for things that I’m not fully comfortable with. For something where I have thousands of hours of experience under my belt — the modern political history of Afghanistan, let’s say — I have more of a structure in my head and so can get away with typing notes. For the rest, like all these new things I’m learning about Ruby, pen and paper is pretty unbeatable. There’s a bunch of good science confirming that that’s the case, and it aligns with my experiential sense as well.\nSome people in the forums noted how one of their preparatory tasks prior to an assessment was to type up their handwritten notes into a more formal digital reference of the course materials. I’m in two minds as to whether I think that’s going to be useful, mainly because the online documentation exists to serve that purpose. But I’m at the point where I have to decide about that so watch this space…\n\nAnki for things you need to remember\n\nIf I think a particular method / fact / morsel of knowledge is something I’m going to want to actively recall from memory in the future, I’ll add it to Anki. I try to follow the principles outlined in Piotr Wozniak’s essential piece entitled “Twenty rules of formulating knowledge”. (Seriously if you use Anki and you haven’t read it, stop right now and go read it).\nI use a mix of cloze-deletion cards and custom templates for coding.\n\nThis card is a simple cloze deletion card. I wanted to make sure I was learning the options for file permissions as part of the command line portion of the prep course.\n\nThis card tests my ability to produce an example where I’m using the .reduce method. I try to include a bunch of these production cards since they mimic the kinds of situations / circumstances where I’d need to recall this piece of information in real life.\n[Even though comparisons are odious (see above), in case it’s of interest to others, I’m just through RB101 and about to begin RB109 and I have 536 cards that I generated during the course of my studies.]\n\nAsk questions\n\nAsk lots of questions. The advice given by Launch School instructors is to focus on the why questions when asking others. (The how questions are usually a matter of looking something up). I found this was broadly true. I have a reserved set of pages at the end of my notebook where I write down these questions that seem a bit outside my comfort zone.\nSome of these get answered in the course of the programme. For example, I see that one of my questions was about why symbols are often preferable to non-symbols when used as hash keys. This was answered (sort of / enough to give me a sense of the answer) a few lessons later. Others are bigger issues for which there probably is no final answer. For example: “doesn’t dynamic typing force us to spend a lot of time validating input? isn’t static typing safer?”\nAsking ‘why’ something behaves they way it does allows you to better develop your mental models and can be an effective way to grasp what’s going on.\n\nFeynman technique\n\nI use a slight variant of this much-lauded technique. It consists of something approximate to the following steps:\n\nWrite the title of a topic that you want to study / test yourself on\nWrite or map out an explanation of that subject intelligible / appropriate to a non-specialist. Do this from memory.\nIdentify any gaps in your explanation / understanding.\nRelearn / restudy / interrogate to fill in the gaps.\n\n[caption id=“” align=“alignnone” width=“600”] Part of one of my Feynman reviews that I do on Saturday afternoons [/caption]\nYou can use narrative / diagrams to condense and clarify your explanation. For my Launch School studies, I do this once a week on Saturdays. I keep a list of new methods that I’m learning about during the week. Particularly towards the end of RB101 these started to mount up. Then on the weekend, I’d take the list of methods and test myself by writing out (by hand, with pen and paper) code that illustrated how to use each method. If needed, I’d write or say out loud an explanation relating to that method. This is a humbling exercise; you realise that you don’t know the things that you thought you knew.\n\nWrite blog posts to explain anything that feels unclear\n\nMy last blog post is an illustration of this. I was having trouble getting what the .zip method could be used for and how it transformed things to which it was applied. So I wrote up some notes to myself in the form of a blog post.\nI sometimes worry that too many of these posts — written purely for me to understand something — are alienating for those who subscribe to the blog via something intrusive like the email newsletter. I considered putting my ‘writing for the purposes of understanding’ posts in a separate location. In the end, though, I’m probably overthinking it. Like it or lump it!\n\nMake mental models of how things work\n\nA lot is made of this, both at Launch School and in the wider world of study techniques. I’m not sure I have too many examples of where I’ve formally had to think through something using a mental model of how it works. I imagine this will start to be more applicable in later courses. Or perhaps it’s just that I already have some mental models for how the code behaves owing to my previous studies in coding. At any rate, I’m keen to get the most out of this but so far haven’t found it to apply too much.\n\nMake a cheat sheet at the end of a topic\n\nI haven’t done this yet, though I can see how it’d be useful. For my current course and assessment, I think it’s probably hard to do — i.e. condensing the Ruby language to a single sheet of paper. But I’m reserving the right to do this at a later stage.\n\nSpend time reading the code of others\n\nI tried to do this a little bit while going through the exercises. My hesitation was initially that the code hadn’t been reviewed or formally assessed and I didn’t want to absorb bad patterns from others who — like me — were likely at the beginning of their journey and who couldn’t be expected to know if they were writing something that should be emulated or not.\nIn the end, I think it’s still useful to read the code of others. It’ll usually be somewhat clear or not whether an answer is overly complicated. And answering why you think that is in itself a revealing process."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#review",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#review",
    "title": "Solid Study Habits for Coders",
    "section": "Review",
    "text": "Review\nThe Launch School syllabus takes a year or two to get through. You start off with Ruby, but later on you’re working on Javascript or whatever. The concepts and the methods and the details could easily start to get forgotten if you’re not regularly reviewing and practicing things you studied in the past.\n\nAnki as the cornerstone for spaced repetition\n\nAs evidenced by the large number of cards in my Anki deck, Anki occupies a central place in my strategy to outsource the need to worry about reviews and recall over the long-term. If there’s something I want to be able to recall a year from now and it’s not something I’m using literally every day or two, then it’s going to end up in Anki.\nAn important note, though: learn the material FIRST, before adding it to Anki. I’ve learned this the hard way in the past; if you don’t learn it before adding to Anki, you’ll find Anki becomes slow and filled with a sense of drudgery. It’s also inefficient to group those two very separate tasks into a single moment at the point where you’re reviewing a card.\nThe great thing about Anki is that as a particular fact passes from your short term memory to your long-term memory, the need for reviews becomes less frequent, so you’ll see it less often. Trust the spaced repetition algorithm. It knows what it’s doing.\n\nExplain a concept out loud\n\nThis overlaps with the Feynman technique (see above). You can do this on your own, or to a friend / captive audience. Doing this with people who are much older or younger than you can be instructive as to whether you actually understand a particular topic.\nI sometimes build this in as part of my Anki studies. I’ll have a card where the front says “What is [concept x or whatever]? Try explaining it out loud.” Then I’ll either find someone to do this with or I’ll just do it myself. Then on the back of the card it’ll either have a small list of key sub-concepts to make sure I got it right or it’ll say “Go look at your notes to confirm that you covered everything important”. This way I’m getting prompts to review old material, but they’re reoccurring at regular enough intervals that I don’t have to worry about having to do this too often.\n\nKeep Ruby / other fundamentals sharp\n\nPeople in the forums mentioned that when they moved on to Javascript or other parts of the course, they found their Ruby skills starting to atrophy. Many said they wished they’d been more formal about reviewing old materials and keeping those skills fresh.\nI think this is best solved by doing regular code challenge exercises as provided by sites like those listed above. I’ll use Beeminder to keep me honest and doing at least one or two every few days.\n\nReview old materials\n\nPeople suggested that roughly 20% of your weekly study time should be devoted to reviewing old materials. I wonder if that’s a little high, but for me that might translate to keeping a solid hour or two on Saturdays for review of those old materials. And by review I mainly mean active recall using some of the techniques mentioned above, and then reading through notes to confirm whether I still knew something or not."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#getting-stuck",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#getting-stuck",
    "title": "Solid Study Habits for Coders",
    "section": "Getting Stuck",
    "text": "Getting Stuck\nThis is where I struggle the most with coding — the mental game of failure. But having a process to deal with those moments — because they will come — addresses a good chunk of the issue when it comes to getting stuck. The following are possible options for working through it, as suggested by fellow students / instructors:\n\nRead the error messages thoroughly\nFirst try it with pseudocode\nSolve problems on your own before looking at solutions or asking others for help. (If you can’t solve it, and depending on the size of the problem, still don’t look at the solution until a day or two has passed).\nUse the rubber ducky method of talking through a problem. (This can be translated to writing as well, where just the act of writing up where you’re stuck can often be enough to get you unstuck)\nDon’t cheat yourself of the opportunity to learn if the problem is difficult. Work through it and take your time.\nUse the PEDAC system / process"
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#community",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#community",
    "title": "Solid Study Habits for Coders",
    "section": "Community",
    "text": "Community\nEveryone studying at Launch School is doing so remotely. We’re all scattered around the world, but doing thing with others can be really helpful with motivation. I don’t always find it the most time-efficient way of studying, but working with others in a structured manner seems to be strongly recommended by posters in the forums. For me I think this will take two forms: attending the group study sessions appropriate to my particular level, and interacting with discussions on the forums and Slack channel.\nI might supplement this with ad hoc study groups depending on my need for that during assessment preparation. At any rate, there seems to be a good deal of opportunity for that kind of thing among fellow students."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#habits-meta-structure",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#habits-meta-structure",
    "title": "Solid Study Habits for Coders",
    "section": "Habits / Meta-Structure",
    "text": "Habits / Meta-Structure\nA lot of this comes under the rough category of ‘life fundamentals’. Most shouldn’t need too much explanation.\n\nStop using / viewing social media or meaningless input — for me that’s Twitter and YouTube. I have turned this off completely outside specific windows.\nNo email — I also have email turned off everywhere except outside certain windows. On my phone, I’ve set it up that I don’t have access to IMAP mail via apps or the standard ‘Mail’ app, but I am able to send mails through SMTP email.\nBe careful with caffeine / tea — I don’t drink coffee, but some varieties of tea cause me to get a little too edgy and scatter-brained. So this is just a reminder to be conscious of that and always err on the side of caution. Also, periodically taking 1-3 weeks completely without caffeine (as with salt) allows me to reset my baseline, needing less to cause the stimulating effect it has.\nKeep the house / room clean\nGo for a walk every day\nExercise — do it.\nSleep — this is maybe the most important item in this entire post for me."
  },
  {
    "objectID": "personal/2019-02-16-solid-study-habits-for-coders.html#scheduling-study-core-routine",
    "href": "personal/2019-02-16-solid-study-habits-for-coders.html#scheduling-study-core-routine",
    "title": "Solid Study Habits for Coders",
    "section": "Scheduling Study / Core Routine",
    "text": "Scheduling Study / Core Routine\n\nStudy Every Day\n\nI try to study a minimum of two hours every day on average, moving up to a stretch goal of three or four if I am able. This allows me to make meaningful progress in my studies. Ideally, I try to have at least one day / week which is a 3-4 hour uninterrupted slot. But probably four hours is my maximum when it comes to focused, productive work.\n\nPick a sustainable pace\n\nI could probably do a few days of full-on / flat-out work, but eventually I’d burn out. The trick here is to do enough that you can always work the next day. This means not working from a place of exuberance or excitement. Learning and coding doesn’t have to be high-octane. That doesn’t mean it can’t be enjoyable or satisfying, needless to say.\nThis means starting with reasonable weekly hours and slowly increasing them rather than jumping straight into an intense pace of 5 hours a day, let’s say.\nFor me, this initially looked like starting with 15 hours per week and I’m currently at around 18. Reaching somewhere around 25-30 would be my ideal, but I have to listen to how my mind and body responds, and have to balance other work to support (read: pay for) this course of study.\n\nCreate a routine / habit around Launch School study\n\nFor me, this means having my studies as the first thing I do in the day. I want to give it the time when I’m at my best during the day, so that means first thing in my morning. I potter around for 30-60 minutes after getting up, and then I immediately start work on my studies.\n\nBlock things\n\nI covered this above a little, but basically for me it means using tools like Freedom and Focus apps to keep me honest and not distracted. I’m fairly good about using these tools, but not always and it’s easy to notice the difference.\n\nTake breaks\n\nI usually segment my studies with inbuilt short breaks. These can be anywhere from 5-10 minutes and I try to get up, stretch or do some kind of physical exercise / movement. For Macs, the Move! app is a great utility. I like to take somewhere between one and two breaks per hour on average.\n\nThink about posture early on\n\nThis is something which I haven’t thought too much about, but people on the Launch School slack channel and elsewhere have suggested it’s important. If you’re going to spend hours behind some sort of laptop, getting posture and ergonomics right seems like a good thing to aim for. I’ve had good experiences with the Ninja Standing Desk (sadly not being produced any more, it seems!) though I don’t currently have it with me. This is something to return to in a few months.\n\nTake a day off\n\nI take one day off each week which is for non-tech things, or family or friends. This is usually Saturdays for me but not always. I turn off phones and laptops and don’t let myself use them for the whole twenty-four hour period. I sometimes lose this habit when I have too many things going on and am overcommitted — witness that I’m currently writing this blogpost on my supposedly sacred Saturday ‘day off’ — but in any given year I’ll stick to it perhaps 80% of the time. The more I work on Launch School the more I’m reminded that it’s a good thing to disconnect from digital input and reconnect with the world around me.\n\nA routine for the end of the day\n\nCurrently the two key parts to this involve writing down what’s coming up tomorrow and where to start with them. It also includes making my environment conducive to just starting work the next day — i.e. cleaning my desk."
  },
  {
    "objectID": "personal/2019-01-31-pain-love-story.html",
    "href": "personal/2019-01-31-pain-love-story.html",
    "title": "Pain: A Love Story",
    "section": "",
    "text": "To go in the dark with a light is to know the light.\nTo know the dark, go dark. Go without sight,\nand find that the dark, too, blooms and sings,\nand is traveled by dark feet and dark wings.”\n\n— Wendell Berry\n\n“We have been conditioned to think of the darkness as a place of danger, even of death. But the darkness can also be a place of freedom and possibility, a place of equality. For many, what is discussed here will be obvious, because they have always lived in this darkness that seems so threatening to the privileged. We have much to learn about unknowing. Uncertainty can be productive, even sublime.”\n\n— James Bridle\nThis is a story of pain. (The title sort of gives it away). As with babies or dreams, telling stories about pain is primarily of interest to the person directly involved. This is also not the story I thought I was writing. A few months back, I was starting to feel much better, as if the story that I began writing had changed. That one was a story of recovery, a story of lessons learned. It wasn’t a story dictated from the trenches. Since then, I have been plunged back a few times into the experiential side of things. It’s only a temporary setback, I keep telling myself. Nothing is permanent, especially pain.\nFor the past five years, I have living my days in varying levels of pain. It seems to have started with some routine wisdom teeth surgery. Or a period of intense stress in my life. I have no real answers about any of this, least of all about the beginning of it all.\nWhat does this mean, practically speaking? What do you have? What did the doctor say? (Yes, I hear you speaking as I write). The doctors say nothing. Or they say many things, which is the equivalent of saying nothing in practical terms.\nThis thing I’m writing here, this is an attempt to carve out some meaning from my experience. I initially wanted to write something more abstract about the medical system, or about the aetiology of stomach pain, but what I’ve settled on is something a lot more personal. I’ve tried to capture what it’s like to be me at this precise moment in time. I have some conclusions and a sense of some pathways that have opened up ahead of me towards the end, but that’s all quite provisional.\nA year or two ago I came across something called a keukegen. See the picture to the side. These are Japanese folkloric creatures. They are, in the words of one online source:\n\n“particularly filthy monsters commonly found in populated areas. They are the size of a small dog, and appear simply as a mass of long, dirty hair. They make their homes in cool, damp, dark places, and are particularly fond of living under floorboards and around run-down homes, where stuffiness, moisture, and lack of human activity create the perfect breeding place for sickness.”\n\nThey are often cited as explanations for complicated illnesses that appear without a clear reason or pathway.\n\nI grabbed on to the idea of externalising what I was feeling — or the cause of what I was feeling — into this shaggy creature. I loved too how this somehow allowed me to feel some kind of compassion for whatever was going on. The creature perhaps can seem a little sinister, but mostly it doesn’t seem like what you’d consider to be a scary depiction of illness or pain. There is somehow some kind of reassurance in the keukegen. It’ll stick around as long as it needs; while it’s here I might as well feed it and take care of it.\nA lot of the experience of pain is a mental game. You get quite good at the somatic dialogue, listening to what the body has to say, sometimes reacting, sometimes choosing to just be there with it, sometimes trying to talk back. Dialogue is the ideal, you see, and not always achieved. Sometimes it’s just the somatic lecture, with me trying to interpret the signals that my body has to communicate to me.\nYou want these signals to have meaning. You desperately want all of this to have some kind of meaning or deeper purpose. You need it not just to be empty sensation. In my more wise moments, or when I am able to step back, I realise or understand that there is no such thing as ‘empty sensation’. Sensation is all there is. But at other times it feels like a disappointment.\nYou look to the past, casting glances at the wreckage of your life, all the things you haven’t been able to do, everything you’ve had to turn down. You look to the future, always assessing the ways in which some decision will complicate your life in the future, the ways where you’ll get something wrong or have to pay a price. These can be a bit like visiting a museum — glimpses of the lost life you’ll never get to experience, everything encased in glass cases, all the plans you have to say no to, all the things you’re too scared to accept for fear of what the consequences may be.\nYou also lose your memory of a pain-free body. I sometimes try to imagine myself into that life, into a body where I did not feel these sensations, but it is like trying to visualise negative space. It feels like a logical impossibility.\nWhere to start? I have been in some degree of pain for much of the past three to five years. Memory of pain is notoriously (and, experientially) fickle, hence the huge variance in that estimate for how long this has been going on. I only started to really get serious about addressing the pain once it started to significantly affect my life and my ability to go about everyday tasks and my work.\nTo talk about pain is to enter a universe of metaphor and simile. There is nothing to put your hand on directly, and the measurement of pain itself is notoriously difficult to achieve in any kind of objective manner.\nWhat does it mean for me and my day-to-day life? In the last 9 months I’ve often been confined to home, stuck lying in bed or on the sofa. Things that normally bring me pleasure became out of reach. They became impossible to do because I didn’t have the energy, the mental space, the focus. I lost my creativity. My sense of possibility. My options. Simple things like planning out the coming month with travel for work became weighed down with what sometimes feel like astrological predictions. Nothing beyond the next hour or two seemed to exist with any certainty. I was forced to abandon any sense of the long-term future, existing instead in a tiny pod made up of the short-term.\nA few months back ago, it began to seem as if things had taken a turn for the better. I had stopped taking medications for the pain. I was on a new treatment regimen. My body felt better. I started thinking that my body and my gut — and my ability to eat any kind of food — was more resilient. It’s hard to convey how this felt in words — it was far more like a feeling of confidence than a particular thought or any objective measure of ‘being better’. I felt like I could relax back into the experience of my life once again.\nI started saying ‘yes’ more often. I started participating in more things outside the house. I started coming up with plans for work, for my personal and intellectual development. I started to think about travel in a timeframe looking six months ahead.\nBut the pain returned. I knew that feeling, the way it melded with my thoughts and my attitudes. What it did to my ability to plan, to think about the future, to move forward in my life. My vision closed down, it curtailed itself.\nInstead of fanning out, opening to the world, I closed various doors open around me. I retreated to the few rooms that I knew well, and where I had some sense of control and familiarity. I adjusted my work schedule. I adjusted what passed through my mouth. I ramped up my self-care protocol, much-honed after months of trial in the trenches. This is a familiar place. I know what works — mostly — and what doesn’t. I say no more often. I don’t do things. I stay at home.\nI put the walls back up around me. I closed myself off to experience, to the world. I returned to focusing more on what is happening right now instead of what will or might happen in the future, or in any kind of future that I want to construct for myself. This is a retreat. But it’s ok, it’s a retreat that is familiar, where I know what’s going to happen, what kinds of pain and sensations are likely. I rest. I watch, wait and listen. What’s happening in my body right now? Oh, that sensation. I know you. I’ve met you before. I know you.\nPain is notoriously hard to describe to those not experiencing it directly. There’s an interesting history to the different attempts that have been made by medical professionals in an attempt to get a gauge of their patients’ pain levels, but the two more common methods are to make an estimate from one to ten (with ten being the most intense pain you’ve ever experienced in your life), or choosing between a series of smiling (or crying) faces on a piece of paper. As the subject experiencing pain, neither feels very satisfying or communicative.\nThe experience of pain in the moment is also quite different from the experience of pain as a long-term phenomenon. One is (mostly sensation) mixed in with some thoughts that are reactive in kind. The latter is mostly thoughts, or thoughts about thoughts — swiftly a recursive and enfolding phenomenon — where the moment-to-moment sensation falls to the background and a wider picture emerges.\nTwo useful words to add into the mix at this point: tolerance and threshold. Tolerance is the point at which pain becomes intolerable. Your threshold is the point at which pain breaks into your consciousness (possibly as contrasted to a kind of background feeling that’s possible to ignore). I seem to have developed a high tolerance for pain over the past few years. I have grown to tolerate far more pain than other people would be able, especially moment to moment. My threshold shifts over time. I couldn’t really say I have a fixed idea as to whether my threshold is high or not.\nThe more time you spend with pain, the more you realise how the usual metaphors start to break down. Pain is like a scale, yes; there is something that we can equate to more and less pain, but there are also degrees or different flavours of the same number on the scale. Spectrum is perhaps a better word — you can be more or less intense, but you can also be different colours on this intensity scale.\nI often find that the different flavours of pain come with a colour or mood or shade association. The more time I’ve spent with pain — the hours really spent with the focus of my attention directed towards whatever sensations are inhabiting my body — the more I realise how many different gradations and flavours there are.\nThe smiley faces card method of grading your pain is more or less useless. For starters, the scale of the smiley pictures often doesn’t make any sense. Why does the frown have to come before the grimace? Why is the wide-open mouth with tears the final position? It doesn’t correspond to my pain scale, at any rate.\n\nI tried to express a little of what my body feels sometimes in this drawing. It’s not always like this — either the intensity or the precise flavour — but it gives you an idea. Like this piece, the drawing was just a snapshot in time. Pain never stays the same; it is always changing. This is one of the things I’ve learnt and that I find somehow reassuring; nothing is for ever after all. There is reassurance in that.\nMost of my pain is felt or begins as a sensation in my abdomen. Following my own observations plus the investigations of a series of doctors, it seems that this is related to an imbalance of gut bacteria and some infections and weaknesses of that gut. Pain is sometimes caused by an excess buildup of gas in the gut — trapped there by who knows what — or sometimes just a phantom pain all of its own. Figuring out the precise pieces and tessellation of this cause-and-effect puzzle is frustratingly hard to do, and I gave up any hope of real answers a couple of years ago.\nIf it’s caused by something I ate, or if I’m experiencing some kind of flare that that moment, then I experience that as a sort of torture from the inside. The slow passage of food through my intestines is accompanied by an intense sharp pain, as if a ball wrapped in razor blades is attempting to pass through my digestive tract. Everything is inflamed, raw and highly sensitive to every single inch traversed.\nThat pain sometimes becomes a whole-body feeling. It’ll often reach this point if I’ve been ignoring it — trying to push through whatever work or exertion I’m currently trying to do. The signal becomes amplified beyond the origin point (the abdomen) and booms out from throughout my body. It becomes unavoidable.\nThere are some second-order pains that I experience. After several hours of intense pain, I can sometimes feel a kind of tiredness or exhaustion. This is a kind of mental lack of energy. I sometimes get headaches. I sometimes find that the pain saps the energy that allow me to sustain or find my natural (or, at least, in the past it was natural / default position) default mood.\nSometimes the pain will be so strong that I have to lie down, curled up in a ball. This is my not-so-subtle invitation to spend some time with it. Sometimes I can ignore it a little but keep it in mind so that it doesn’t become bigger. Sometimes it’ll be directly related to what I just ate. Sometimes it appears out of nowhere. I fairly long ago abandoned the need always to find a reason for the pain’s appearance each time.\nMost of this is just telling stories, making up a narrative that may or may not relate to reality. And the thing with stories, I came to realise, is that they can change the pain as well. Sometimes for the better, often for the worse.\nThe pain is constantly changing. Never the same. There is almost always some level of pain on a day-to-day basis. There are periods of acute pain flares. This is when I need to cancel everything, give in to whatever experience my body wishes me to have. They can be predicted to a certain extent — certain things, were I to eat them, would certainly provoke such an acute episode — but sometimes come out of nowhere. This unpredictability is what makes it difficult to live or plan with any kind of medium-long term plans.\nFor the most part this pain I experience does not conjure up any kind of fear. These are pains I know. They are almost a kind of friendly presence by now. Old pain becomes part of the furniture. It is new pain, new sensations in new places or in new configurations, that can sometimes provoke fear. The more time that passes, the more I realise that these are all variations on a theme. They are all sensation, unpleasant sensation to be sure, albeit with something dysfunctional about the messages they send.\nAfter a while, you start to get good at feeling the sensations in your body. Sometimes your body literally stops you in your tracks and you have to go lie down. Distraction makes things worse. You just have to be with the unpleasant thing that is happening. The pain, the uncomfortable sensations, the sense of powerlessness and the things you are unable to do. It’s useful to try not to overdo things on a particular day, or part of the day. You can push yourself, but then you won’t have anything left over for the next day or for the evening. If you push yourself too hard, maybe you push yourself into pain. All the observation is a critical part of ensuring things stay manageable, within the limits of what is manageable and controllable.\nYou get to know certain sensations, or certain patterns of sensations. Some of them become familiar, and thus seem less threatening even if they are painful. You start to discount those sensations; they become more of the background. They become your new baseline. You aren’t well, you don’t feel well, you lack your full energy and aren’t living as you fully could, but you just accept everything. You stop asking for help, stop going to doctors, stop considering the situation unacceptable. You accept it. Sometimes this is useful, other times not. I’ve found myself in that place at several points. Sometimes it’s just a break, it’s a space in between treatments, or its a place where you find yourself needing to pull back from engaging with the world, to pull back from hope — the hope that there is a straightforward medical answer, that the medical system can and will solve your problem.\nThe consequences of this ongoing pain experience are manifold and interact in complex ways. Frustration and grief are both good words to encompass these various responses.\nTo start with, a high tolerance for pain makes doctoring hard. After months of pain, you adopt to that pain as your new baseline. After doctors prove their ineffectiveness, you stop going to them because you know that the tools in their kit aren’t able to address the root cause of what’s going on.\nThe more the pain continues, the more I lose my trust in my own ability to interpret my own pain. In the beginning, pain seemed to mean something (“something is wrong! pay attention! fix me!”) but now after so many months of signal, of this alarm bell ringing, I have lost trust in the meaning it is trying to impart. It has become the new normal. I am more used to ignoring it, to being with it and not reacting to it, so it somehow fades into the shadows of normality.\nChronic pain is not just about the pain, as I stated above. It’s also about the second-order effects, the loss of energy and of mental clarity. In periods of acute pain, or even just as a general pattern, I find that I am unable to go as fast as my mind wants me to go. I’m certainly not able to take on or handle the things that I recall being comfortable doing in the past, a few years earlier.\nWhen I read in a book last year that fourteen percent of cases of chronic pain lead to suicide attempts, I could see and feel how that could be the case. I feel lucky that my pain ebbs and flows somewhat, but I’d be lying if I didn’t admit that the thought hadn’t passed through my mind at my worst moments.\nHospital visits mean lost work hours as well as a steady mounting influx of bills to pay.\nThe dietary restrictions that I’ve found myself adopting mean that eating out, or eating with anyone else, is more or less impossible. This narrows my social circle, already somewhat narrow to begin with. Spontaneous decisions to stay out in town, to get food as a takeaway or to attend things like conferences or work trips require intense planning and forethought. Everything needs planning and consideration ahead of time.\nThen there’s what it does to my mind: chronic pain can turn even the nicest person into a far shittier version of themselves.\nThe loss of control and freedom to choose my life’s path and options is a frustration.\nIn periods of acute pain or when my energy is too low to participate in life in any kind of active manner, this is when I have the feeling that life is moving on around me. I am missing. I am still on this planet, joining everyone in this journey around the sun, but it is as if I am absent.\nThere is also an occasional anxiety and the desire to push the pain away. This is usually short-lived since it is not a useful way of relating to the pain, but it does occur and is not pleasant as a sensation.\nThere are some other consequences of the pain that are somehow less obvious.\nI am more isolated and disconnected than ever. There is simply no way to convey the day-to-day experience — nor the energy or even the inclination — to anyone apart from a very small core circle of people. This means I can’t really share the experience with anyone when I meet them or talk or interact with them, since to do so would require too much background explanation and context. So I just don’t bring it up. You feel pretty alone pretty quickly when this is what’s going on.\nIn the beginning, or in occasional moments where I’ve agreed to do something or other, you feel like you’re getting a reputation for being flaky and cancelling appointments and or commitments. I recently had to go back through my calendar to find something from a few years back and I was struck by how many events, appointments and work commitments I’d had to cancel or pull out of at the last minute because my body wasn’t up to it. Not only do other people find this inconvenient and wearing after a while, but you lose a sense in your own ability to stick to things and/or commitments after a while doing this.\nAnother consequence of being ill in any kind of not-so-easy-to-explain way is that you have to suffer through and tolerate other people’s armchair diagnoses and well-intentioned-but-wearing interventions into your health. Everyone has an opinion. After a while you learn not to share things with people since you can’t face explaining the whole story and then you can’t face their explanations of what they think is going on, their presumptions to know your body better than you do, and their questions — as if you haven’t already thought everything through a thousand times as to what might be going on.\nIf you don’t have a single-word medical diagnosis your pain is not taken seriously by others. If doctors haven’t found something that’s wrong, then that somehow means — though people rarely say this out loud — that there is nothing wrong with you. To have a chronic pain condition is to suffer through the shaming, doubting and negation of what you feel by others.\nMy interactions with the medical system have been instructive, if only in a negative sense. In the UK, where I began some of my investigations into this pain, most doctors simply don’t have the time to listen to the whole story, to really delve into the details of what is going on. My general practitioner doesn’t even take face-to-face appointments as the first point of call; you first get screened with a five-minute phone call. Then if your condition is deemed serious enough, you get an appointment to see the doctor which is limited to 10-15 minutes.\nThe body is a system. The medical structures as they currently exist doesn’t treat that system, however. They treat individual parts of that system. It has developed a pretty good sense of acute conditions and things going wrong — if you get shot, stabbed, have a heart attack or a serious allergic reaction, you’re in good hands. These are all things that the medical system does pretty well at fixing. You’d do well to go to a doctor or a hospital to deal with those things.\nWhat it is less good at is fixing or addressing systemic conditions that have multiple causes, or that are caused by complex interactions between different systems and groups of causes. Most kinds of chronic pain conditions — characterised as something ongoing and recurring for a lengthy period of time, sometimes even absent any clear or specific stimulus — are these kinds of complex problems.\nI saw my GP, received referrals to various specialists. I was referred for followup tests. All the different parts to this universe of treatments were characterised by two- to four-month waiting lists. In between treatments there was no followup. The system wasn’t configured to link all the parts together. Once one specialist ruled something out, I’d get sent back to square one with my GP.\nEventually, the default position for unexplained abdominal pain that reoccurs or exists on some kind of chronic time-scale is a diagnosis of ‘irritable bowel syndrome’. As most doctors will willingly describe, this is a diagnosis of exclusion: they haven’t been able to find anything that fits into a specific ‘bucket’ so you end up with ‘this is something else but we don’t have a word to describe it’. You’ll usually receive a lecture on clean eating, perhaps something about gluten or FODMAPs, as well as instructions to try to ‘manage stress’. Any further investigations are usually much harder to initiate at this point because the doctor — by putting you in the IBS bucket — has essentially decided that there’s nothing left to investigate. At this point you’re on your own.\nIf you have an uncharitable doctor, or if you discuss it with (mostly well-intentioned) family or friends) you’ll hear about how it could ‘all be in your head’. You’ll hear a lot about stress, and how you should ‘really try to get a handle on that’. Your doctors will tell you to ‘learn to live with it’ (read: ‘we don’t know what’s going on so we’re giving up searching any further’).\nWhen you go see a new doctor, particularly after a few years of a chronic condition, you have to play a delicate balancing act in terms of the quantity and type of information you reveal about your story. Tell too little and the doctor won’t understand what’s going on and you won’t get any kind of solution. Tell too much and your doctor will quickly put you into a hypochondriac bracket in their minds.\nSometimes I’ll be in a lot of pain and I’ll know — in an acute episode, for example — that this kind of pain will probably dissipate in a day or two but that I need something to get through it for right now. I’ve tried enough of the pain medications out there by now to know which ones are good for which manifestations of acute intense pain. The problem is that if you go in requesting a certain kind of pain medication — particularly anything of any kind of strength, and especially if it is any kind of opiate — you’ll be labelled ‘drug-seeking’ and you may never get what you came in for.\nFor non-acute episodes, doctors will ask you to rate the pain on the 1-10 scale. Because of my high tolerance, my instinct is to rate the pain fairly low because it is all — on some level — tolerable (quote unquote). (Just unpick that word ’tolerable’ for a second in your mind). Or even if it is quite intense, I’ve lived with it long enough that it doesn’t necessarily manifest on my face or my body as if there is any kind of intense pain. Just because I’m not crying out, wincing or bent over doesn’t mean that there is no pain. In fact, in my worst moments of pain I’m simply unable to get to a doctor or hospital and you’ll find me instead in bed or lying on a cool floor waiting for it to pass.\nAbby Norman’s fantastic book Ask Me About My Uterus has this section on how you can grow to some kind of uneasy familiarity with pain:\n\n“Bodily agonies that do not end beget a kind of forced intimacy with pain that, not unlike other intense relationships, can eventually bleed into something tedious and almost unremarkable in its enduring presence. Its place in our lives can become ordinary and even, at times, oddly reassuring. The moment that pain owns us is not when it chokes our breath, when it knocks us down, or when it steals our pleasure. Pain becomes our master when we wake up one day and realize we no longer fear it. When we come to regard it as not something separate from us, but something of us. As much as we have labored to resist this in our minds, our bodies acquiesce. Our hearts beat, our cells divide, our nerves—frayed though they may be—fire, and one day we realize that we no longer remember what it feels like to live without pain. What becomes remarkable is not our body’s distress call, but the silence. Really, it’s the silence that we fear, because it does not mean we have been healed. Silence after pain usually marks our body’s inability, or unwillingness, to adapt again, to heal itself, and to persist as we do for an answer, or a reason, for our suffering.”\n\nThere is also a kind of shame in this familiarity. The dominant narratives that you’ll hear around illness and healing usually have some kind of ‘struggle to beat adversity / struggle to win’ theme to them. Coming to a familiarity, making peace with pain, or surrender of any kind is somehow socially anathema. That may be the case, but I have been finding something compelling in the feeling of reassurance.\nI gave this piece a slightly odd subtitle. ‘A love story’. Amidst all the uncertainty and inability to describe (or understand) what’s going on inside my body, I have grown much closer to all the myriad sensations and energies happening from moment to moment.\nThe experience — an extended period of illness and suffering in general, chronic pain in specific — has taught me a lot along the way. I’m far less disconnected from the everyday somatic experience of my body than I’ve maybe ever been in my life. I’ve learned all sorts of things about stress, trauma, the mind-body connection as well as ways of coming to terms with it all. I’ve learned that the treatment of chronic conditions by the normal healthcare system is truly broken. I’ve learned that placebos are a real and sometimes beautiful thing.\nI’ve been continually surprised at the body and mind’s ability to adapt to a new normal. Shifting baselines are both a blessing and a curse, but the way I’m constantly calibrating and coping in response to changing circumstances is a marvel to observe.\nAmidst this wreckage of what I once thought ‘normal’, there are things that help from time to time. Protective and defensive measures seem to work the best. Awareness and mindfulness help prevent small irks and aches from growing when I try to ‘just push through’. Following a restricted dietary regimen can sometimes help, but not when I’m so far gone that my body reacts even to the good stuff. Stress and time management is almost a truism, but it’s achieved that status for a reason. Sleep is possibly at the top of my list of things that make me feel better; it’s also the first thing I’ll start neglecting the moment I start to feel better. Slow kinds of movement, be it walking or yoga, have been useful in reconnecting me to some kind of energy and reminding me that there is some life or presence in me yet. Acceptance and a reminder about death (not getting too attached to my body and the world) is a surprisingly handy mental model to have when going through the worst moments; in the end, everything passes.\nWhen pain shows up all guns blazing, that’s who’s in control. For someone who generally has a sense of how they’d like things to unfold, this powerlessness took a long time to come to terms with. But there’s some sort of relief in the surrender, even if that’s also somehow layered alongside shame. And since we’re quite far down the road of dealing in metaphor and story by now, maybe it’s just a question of taking care of this scraggly keukegen beast for as long as it takes. One day perhaps I’ll wake up and it’ll have decided to move on to take up residence in someone else’s back yard.\nPain is a profound teacher. If only the experience weren’t so unreasonably unpleasant."
  },
  {
    "objectID": "personal/2018-12-02-linux-essentials-certification.html",
    "href": "personal/2018-12-02-linux-essentials-certification.html",
    "title": "Earning a certificate in Linux Essentials from LPI",
    "section": "",
    "text": "Last week I went to a test centre here in Amman to take the LPI Linux Essentials certification. The qualification doesn’t expire and takes you from zero to dangerous in a variety of basic scenarios in the Linux operating system.\nI’ve been using Linux for a couple of years now. (Started out with Fedora, then went to Manjaro. Currently actually running OpenBSD as my main desktop but mulling a switch back to Arch). I was getting frustrated with only knowing certain areas to some level of competence — usually ones where something went wrong, requiring me to troubleshoot. I wanted more of a sense of the overview / fundamentals.\nLPI is the Linux Professional Institute. They are the gatekeepers to a number of different Linux certifications (and, coming soon, BSD certifications as well!). The Linux Essentials syllabus gave a good grounding in the history around Linux as an operating system, a number of core UNIX tools, and a basic overview of some things going on under the hood. Some of it was easy / familiar, but I’m glad I went through this systematically.\nI’ve been studying this for a few months now, using a mix of Linux Academy videos and good old Anki. The videos were enough on their own to get a passing grade in the exam (they only have pass/fail, though you do get your final ‘mark’ as well), but I really needed the spaced repetition in order to retain all the key commands and UNIX command line options. Retention might be easier for you if you’re already using Linux tools / OSes in work and you have that practical repetition going on, but that wasn’t my situation so I needed something else to make sure I had it all memorised.\nThe exam itself is multiple choice, which I’m not sure is the best way to test if someone knows their stuff. The level of depth expected was sort of opaque, too; you get an outline of what they expect you to know in the syllabus, but the various books and online resources all go into different levels of depth in terms of how many of the command options you should know etc. On the plus side, it forces you to overlearn which isn’t the worst thing in this case.\nI’m now finishing up Linux Academy’s short DevOps Essentials course. Parts are a little basic, but it gives you a fairly decent overview. After that, most likely in the new year, I’ll settle in for the long haul to study for LPIC-1 (AKA the Comptia Linux+) certification. It’ll bring me a whole new level deeper in my understanding of Linux systems, and I’m excited to get started with that."
  },
  {
    "objectID": "personal/2018-11-07-learn-persian-farsi-dari-with-podcasts.html",
    "href": "personal/2018-11-07-learn-persian-farsi-dari-with-podcasts.html",
    "title": "Learn Persian / Farsi / Dari with Podcasts",
    "section": "",
    "text": "Learning Farsi or Dari from podcasts is a bit easier than with Pashto. There are lots of options in a variety of topic areas, so you’re not confined to listening to news broadcasts 24/7. They also cover a broader spectrum of language abilities, so you have more options even if you’re a relative beginner.\n(All of them are available on Overcast and iTunes — just search in the ‘add podcast’ section and you’ll find them using the names/titles given below.)\n\nBBC Persian Radio — This is a BBC news roundup / discussion programme covering whatever is ‘hot’ on any particular day. It’s released every morning.\nBe Ebarat-e Digar — An interview programme from BBC Farsi. They sometimes have non-Farsi speaking guests on the show but everything is dubbed and exists as a complete Farsi experience. Usually pretty interesting. Heavy on the politics, though.\nChronos — Also an active YouTube channel. This podcast has a large backlist of short episodes. Well worth a listen. Varied themes and content and less formal language used.\nDanestegi — 50 episodes available as an audio podcast feed and since then more recent episodes are available on a Youtube channel. The show is described as covering everything from science-fiction to climate change to extraterrestrial life to psychic readings to crop circles. Conversational style. Would make a nice break from news reports.\nFarsi Readings in Gronigen — Audio from a Persian literature meetup in the Netherlands. Mostly readings from literature / poetry. Higher-level required but good for exposure.\nKhoresth-e Tech — This is a weekly show produced by SBS Persian covering technology and electronics. If you’re at all interested in those topics, this might be a good place for you to get some exposure to how it’s discussed in Persian.\nLearn Persian with Chai and Conversation — This is a podcast for beginners and low-intermediate. Significant parts of the audio are in English, but it’s sometimes nice when going through a poem and explaining all the various implications of a word.\nPargar — A weekly roundtable discussion in Farsi from the BBC in which guests discuss big topics of relevance to society at large. A tendency towards politics but also cultural issues are discussed.\nPersian News - NHK World Radio Japan — This is a news broadcast in Farsi with an emphasis on stories that affect East Asia. Released daily and short episodes.\nPersian Night Story — This podcast broadcasts translations of stories into Persian. Sometimes they’re originally written in Farsi but the majority are ‘classics’, so Chekhov / Tolstoy or Roald Dahl or Alice Munro etc.\nPersian Podcast — This podcast is geared towards lower intermediate students of Farsi and the host, Hosein Pouriman, offers interviews and lectures and stories. The material is pretty engaging, though there aren’t too many episodes unfortunately and new episodes are irregularly released.\nRadiography Farsi — A podcast (with a limited number of episodes and no longer broadcasting) in which the host discusses famous photographs from the history of world photography.\nSamaak Audio Mag — This fortnightly podcast offers discussions of literature in Farsi, particularly poetry. Worth subscribing if you’re into Persian literature.\nSBS Dari — A news broadcast in Dari, with an emphasis on stories relating to Afghans in Australia.\nSBS Persian — A news broadcast in Dari, with an emphasis on stories relating to Iranians or Persian-speakers in Australia.\nShir Khat Persian Blockchain Podcast — This is a podcast in which the host discusses and introduces various aspects of blockchain in Persian. It’s very technical and very in depth (and seems to have been discontinued) but if this is your thing, then there’s lots of material to dig into."
  },
  {
    "objectID": "personal/2018-07-08-regex-python-search.html",
    "href": "personal/2018-07-08-regex-python-search.html",
    "title": "Using Regex with Python to Find Strings",
    "section": "",
    "text": "The next step in my data processing project is to find strings matching certain patterns in the PDF data. Today I worked my way through the relevant chapter (#7) of Al Sweigart’s excellent / useful Automate the Boring Stuff with Python.\nI’ve left some sample code above as a reminder (mainly for myself) of the basic pattern / syntax that you can use. I saw a slightly more concise pattern for running the search in Data Wrangling with Python; I may experiment with that in the future. That has you running something like:\nI guess one of them will have a speed advantage, especially when multiplied over hundreds of thousands of pieces of text.\nThe next step with this project will be to connect this regex function with the splitting file. That way when I split the file, I can rename the file at the same time with a string that I’ve extracted using a regex search.\nIf you’ve reached this far and you don’t know what I’m talking about, there’s an interesting article by Cory Doctorow where he argues that regular expressions should probably be taught as a foundational skill to children:\n\nKnowing regexp can mean the difference between solving a problem in three steps and solving it in 3,000 steps. When you’re a nerd, you forget that the problems you solve with a couple keystrokes can take other people days of tedious, error-prone work to slog through."
  },
  {
    "objectID": "personal/2018-06-23-splitting-pdfs-with-python.html",
    "href": "personal/2018-06-23-splitting-pdfs-with-python.html",
    "title": "Splitting PDFs with Python",
    "section": "",
    "text": "A big part of data science and data engineering is simply getting hold of the data you need from various file types and sources. For better or for worse, PDFs are a big part of the information ecosystem. They contain all sorts of charts, texts, images and other data points but are relatively complicated to parse.\nAs a first step in a larger project working with PDFs, I wanted to make sure I could split a series of multi-page PDFs into individual files that contained only one page. I ended up using Python for this since there are some useful libraries that already exist, saving me from reinventing the wheel.\nI’ve found that a lot of the courses and tutorials teaching programming are big on examples that run neatly in a single file or in a web-browser interface. This is good for minimising complexity while you’re learning the syntax. But they miss an opportunity to connect that to everyday problems and interfaces where you’ll often be working. There’s a great gap (and opportunity) for teachers here. One book exists for Python, entitled ‘Automate All the Boring Stuff’. I wish such a thing existed for Go.\nFor my specific problem, it was non-trivial to get even the most basic prototype working. Before even thinking about the PDF part, I wanted to loop over all the files in a particular folder and print out the names. This seems like a simple thing to do, and in the end it was (sort of), but the documentation doesn’t really lend itself to newbies. This is what one of the functions I used looks like in the documentation, for example:\n[caption id=“” align=“alignnone” width=“600”] Not particularly intuitive… [/caption]\nLuckily, this being a fairly ordinary and common task, various people had suggested ways to iterate over some files and list the filenames. StackOverflow proved helpful, as did some friendly voices over on the PythonistaCafe forum. For a simple file list, os.walk seemed to be the easiest option, though someone has since let me know that for Python 3.5 onwards there’s also glob which looks a lot less verbose to use.\nChapter 13 of Automate All the Boring Stuff is all about PDFs, so I started my initial version of a pdf-splitter with a great deal of assistance from Sweigart’s sample code. He recommends PyPDF2 as the best place to start. I looked over the documentation and it was all surprisingly comprehensible.\nYou start by reading the file with PdfFileReader(), then you can call a variety of objects from that file (its contents, the number of page numbers and so on). If you need, you can then use PdfFileWriter() to make a new PDF file. I initially worked with a single test file, then I combined it all into a loop. Then I added the file names loop (that I’d initially coded up) to provide a meta-loop structure.\nAlong the way I had to remind myself how to do formatted string printing using %. Absent Sweigart’s examples, I wouldn’t have remembered to use .close() on the PDF objects, so that was a useful reminder.\nThe code now works mostly as I’d like it. It’s still fairly slow. It’s a non-concurrent process, so it works through the files one at a time. Now that I know about Go, concurrency and parallelism, I’m very curious to know how it’d perform if I coded this all up using Go. (Yes, I know that asyncio exists but it seems too much effort to try to figure that out.)\nThings I’d like to improve:\n\nI want to try glob to see if that speeds it up and/or makes my code more readable.\nIn some future universe I’d like to try the exact same exercise using Go. I’m not sure what packages are available for splitting / creating PDFs, either in the standard library or as external repositories. I may or may not take this on next week.\nI think the root, dirs is unnecessary from the first loop, so I’ll try removing them.\nIt might be useful to set a PATH attribute to ensure the folder structure is clearly defined.\nI want to add comments and to document my code with docstrings.\nI want to add testing to the file, e.g. if there are no files in the folder I’ll get an error. I don’t really know what my options are for Python testing, but I’ll read the relevant parts of this to learn more.\nI’d like to benchmark the process to get a general sense of how long it takes, and also for possible comparison against my Go version.\nI think it might make sense to turn parts of this code into callable functions. I don’t have enough practical experience to know if that’s overkill or if it’s recommended.\nWhen I installed PyPDF2 I just used a pip install but I think it’s better practice to use a virtual environment to preserve the choices I’m making. This article seems like it’ll tell me more about my options for such wrappers / virtual environments in Python.\n\nThe next big step for this mini-project is to add in a regex search loop. I want to extract a particular string pattern from each page and use it to relabel all the filenames."
  },
  {
    "objectID": "personal/2018-06-18-go-table-tests.html",
    "href": "personal/2018-06-18-go-table-tests.html",
    "title": "Table Tests in Go",
    "section": "",
    "text": "Today I wanted to stretch my use of test scenarios with Go. The example I described a couple of days ago basically had me running individual tests for specific values. What I wanted was a way to test a bunch of different values for the same function. Enter: table tests.\nYou can see the code I ended up with partly in the image above but also on Github here. It took a while to get there.\nI started with some notes I’d taken during Todd McLeod’s excellent GreaterCommons Go course. Those notes were enough to get a framework up and running. I understood the principle: you create a struct to store all the different values, loop over them all to check whether the test fails in any particular scenario.\nWhen I ran go fmt at the end to format my code, it gave me an error as it refused to build:\n\nI could see that it wanted two ints and I was giving it a slice of ints. Basically this turned into a hunt for fixing my loop and which values I was spitting out at various iterations of the loop.\nI ended up isolating the part of the code that was causing the problems, putting it up on the Go Playground so as to isolate exactly what was going wrong. Once I’d figured out exactly how to handle the loop, I could then bring that logic back into my main_test.go file.\nNow I know how implement table tests in Go. My next exploration will be around functions that aren’t located in the same file. So far I’ve been mainly using the same main.go file for all the code I’ve written, but a step up in the complexity will be to interact with different files."
  },
  {
    "objectID": "personal/2018-06-17-testing-workflow-golang.html",
    "href": "personal/2018-06-17-testing-workflow-golang.html",
    "title": "Figuring out the Go testing ecosystem",
    "section": "",
    "text": "I’ve been trying to figure out a way to work my way into increasingly complex Go explorations. I thought I’d start at the most simple and expand from there.\nI thought I’d start off with a simple fmt.Println command. I loaded up GoLand only to find that I had 2 days left in my trial for the software. Open-source software is much more my scene, so I downloaded Atom and continued my coding there.\nAt first I wrote a basic draft of my code. I couldn’t figure out how to test it, however, perhaps because my function wasn’t returning anything, so I switched gears. No point getting distracted by these details. I wrote a function that definetely did return something: the sum of two integers.\n[My code is all available here.]\nI wrote examples for all my functions using the ExampleFunctionname syntax as described in the golang spec / documentation but I couldn’t seem to get that to work. I’ll have to return to that in due course.\nBenchmarking worked fine. Go’s benchmarking tools figure out how many times they need to run a particular function in order to properly test it. In my case, each function ran two billion times for an average of 0.29 and 0.3 nanoseconds per iteration. Pretty fast! I have no baseline for knowing how fast that is, but it seems fast to me. Of course, the function isn’t doing anything particularly taxing.\nMy next task will be to figure out table testing so that instead of my current setup where I have the values for each test written out manually, and just one test per function, I want to have many different sets of values to test."
  },
  {
    "objectID": "personal/2018-06-05-new-book-taliban-reader.html",
    "href": "personal/2018-06-05-new-book-taliban-reader.html",
    "title": "My new book: The Taliban Reader",
    "section": "",
    "text": "My new book is out (finally). The Taliban Reader is somehow the culmination of years of work to drive studies of the Taliban back to primary sources. Some of this work was accidental; more recently it was more purposeful. The book I produced (together with Felix Kuehn) is long and detailed.\nComments and feedback prior to publication were extremely positive. It’ll presumably take readers a while to start getting some real independent reviews in, but I look forward to feedback and whatever conversation is generated off the back of it all.\nYou can pick up a copy at any good bookshop or from Amazon here."
  },
  {
    "objectID": "personal/2018-05-06-prisoner345translation.html",
    "href": "personal/2018-05-06-prisoner345translation.html",
    "title": "My First Arabic Book Translation",
    "section": "",
    "text": "I’ve been sitting on this for a while.\nEarly in 2017, I was lucky to get put in touch with Sami Al-Hajj and al-Jazeera by a good friend. They were looking for someone to translate Sami’s memoirs from Arabic into English. I had done similar work in the past, working Zaeef’s memoirs from his Guantánamo time that later expanded into My Life With the Taliban. I had never done any serious translation from Arabic, however, particularly of this length.\nI finished translating a while back. It was both harder and more enjoyable than I had expected. Harder in that it requires an intense focus that can’t really let up while the translation is happening. More enjoyable since I realised the process was something that brought in a lot of creativity to get the language ‘just right’.\nToday, Sami’s memoirs have been published. You can download the PDF version via Al-Jazeera here. It looks like Kindle and iBooks versions will be made available in due course as well.\nI hope you get a chance to give this book a read. Not only is it some work that I spent a good chunk of time working on, but it’s a really useful and moving account of someone who passed through Guantánamo. In particular, Sami was often on a hunger strike so there is a lot of detail on his mental state and coping mechanisms, as well as the way the medical authorities at Guantánamo attempted to ‘deal’ with the problem."
  },
  {
    "objectID": "personal/2018-01-25-tweet-void.html",
    "href": "personal/2018-01-25-tweet-void.html",
    "title": "Tweeting to the Void",
    "section": "",
    "text": "I’ve previously written about how I turned off Facebook’s news feed. I keep an account with Facebook because people occasionally contact me there. It is also an unfortunate truth that many companies in Jordan (where I live) or in the wider Middle East only have representation on Facebook instead of their own website. (Why they insist on doing this baffles me and is perhaps a topic for a future post).\nI have long preferred Twitter as a medium for filtering through or touching – however obliquely – things going on at any particular moment. I have no pretensions to actively follow every single tweet to pass through my feed. Rather, it’s something I dip into every now and then.\nIncreasingly in recent months, I found myself growing dissatisfied with the pull it often has on me. It has become something of a truism to state that ‘twitter isn’t what it once was’, but there’s less and less long-term benefit in following discussions as and when they happen.\nRescueTime tells me that I spent 86 hours and 16 minutes on Twitter in 2017 – just under quarter of an hour each day. That feels like a lot to me.\n\nEnter ‘Tweet to the Void’. This is a Chrome extension. (For Firefox and other browsers, I have to imagine things like this exist.) When I visit twitter.com, the feed is not visible. All I see is somewhere to post a tweet if that’s what I want to do. (There is still some value in posting blogposts and articles there, since I know some people don’t use RSS). Of course, I can always turn off the extension with ease, but adding this extra step has effectively neutralised Twitter for me.\nTry it; see how you feel about having something standing in the way of your social media fix. Let me know how you get on."
  },
  {
    "objectID": "personal/2018-01-23-lists-shuffling-numpy.html",
    "href": "personal/2018-01-23-lists-shuffling-numpy.html",
    "title": "Making and shuffling lists in Python",
    "section": "",
    "text": "I discovered some useful functions the other day while trying to solve one of the Dataquest guided projects. These all relate somehow to lists and use Numpy. I’m listing them here mainly as a note for my future-self.\nimport numpy as np\n\n# this code returns a list of n number of items starting at 0\nnp.arange(3)\n---- returns [0,1,2]\n\n# this code is a variation on the previous one\nnp.arange(3,7)\n---- returns [3,4,5,6]\n\n# this adds the functionality of steps in between values\nnp.arange(2,9,2)\n---- returns [2,4,6,8]\n\n# these are slightly different; they sort lists\n# if you want to make list of numbers randomly sorted:\n\nnp.random.permutation(10)\n---- returns the numbers 1-9 in a list, randomly sorted\n\n# you can also pass non-numeric lists into the `permutation`\nlist = [a,b,c]\nnp.random.permutation(list)\n---- returns something like [b,a,c]"
  },
  {
    "objectID": "personal/2017-10-20-language-learners-journal-homestay-edition.html",
    "href": "personal/2017-10-20-language-learners-journal-homestay-edition.html",
    "title": "Language Learner’s Journal: Homestay Edition",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and* coaching worktogether with me. For other posts in the series,click here.*]\nMy first time around with immersive language learning (studying for a semester in Brazil ten years ago), it was a no-brainer that I would do a homestay. I was a college student, I had no local networks to tap into to find housing, and forgoing a degree of privacy and independence felt not so different from the dorm life I was living the rest of the year. Furthermore, an employee of my school coordinated an apartment for me from the start, and back then I wasn’t as intentional, organized, and proactive about my learning as I am now.\nBy the time I got to Amman to bump up my Arabic studies last December, I only briefly considered doing a homestay. I wanted my own place or to at least live with other young people like me — I’d have guests visiting, I like to do yoga in my home, spend long times in the kitchen making vegetarian food and listening to podcasts, stay up late laughing with my mom on skype, etc. I also work part-time remotely each day, and I needed to make sure I’d have a quiet place appropriate for that.\nAbout six months into studying, however, I started to think about a homestay seriously for the first time. I had a persistent feeling that my ability to read native texts and understand a newscast was beyond my ability to make small talk or say “Excuse me, I need to go brush my teeth.” \nI had a home in Amman and wasn’t going to force a homestay to happen, but I also began laying out feelers for families of all women interested in hosting a 30-year-old journalist. I had a happening Amman social life by half a year into my stay in the city, with morning yoga, friends who’d pick me up for running groups, dinner parties out, and meetings at coffee shops with my language partner. If I was going to stay with a family, it would ideally be one that understood my educational goals in Amman and how those various parts of my daily life fit into it.\nI lucked out. A friend who coaches a track team broached the idea with a student who lives with her three sisters, parents and grandmother. The sisters range from kindergarten to college-aged and are a lively pack that zips around the city for their own activities each day, be it shopping, going to a sports team practice, or all the daughters going to the balad for a sisters meal out. Their home also had an open-door policy for all the neighbourhood youth, and throughout the day I’d have a smiley primary school girl telling me about the barbecue she ate the day before with her family or an Iraqi pharmaceutical student telling me about the farsi words he uses in dialect back home.  \nAlex has emphasized to me the importance of speaking to a variety of people, since it can be deceptive to feel so confident speaking with language teachers and then, say, find groups of old people really challenging. The times when I was speaking with the grandmother of the family and the kindergarten daughter were indeed my hardest moments, but also fun, no-pressure ones Tayta usually wanted to know my whereabouts throughout the day and my travel plans. The little girl generally liked talking about her dog or the puppy show we saw in the circus together.\nMy favourite time of day was bedtime, when myself and the oldest and youngest daughter all went to bed together in the same room, chattering until we drifted off to sleep. I realized I had never needed to learn to say “Shall I turn off the light?” to anyone in all of my classes (بطفي الضوء؟)\nI wish I had worked more with a notebook to write down new words and phrases I was hearing, but that house was full of life, and I floated between rooms and conversations and meals and situations throughout the day. Their conversations amongst themselves were often too fast for me, and sometimes I knew only the main outlines of what they were discussing and missed what specific point was funny or surprising. There was a huge difference between having a one-on-one conversation versus participating in the group chat – the former was a speaking exercise, whereas the latter was largely a listening one. I was once very lost in a conversation about food that involved peals of laughter until I recognized one word I knew (كبد) and realized they were talking about eating the internal organs of animals. \nEven if I hadn’t learned a single new word (thankfully, I did), it was a valuable booster shot to my confidence to be able to know that yes, I can live amongst a family and handle myself in day-to-day situations. I brought up basic things that I wanted to resolve in Amman with them, like explaining I wanted to find a quality and affordable dentist (they took me to a great one) and that I had Brazilian friends looking for an affordable way to send money abroad (hence many discussions about bank accounts and foreign currencies and a final group trip to MoneyGram).\nIn the end, I stayed about two very happy weeks with them. I could always have done more toward a laser-focus goal of learning Arabic – I could have stayed with them from day one, I could have refused to speak with anyone in English all of my time in Amman, I could have lived in more remote parts of the country where I would be a rare foreigner. But one takeaway I have from my Brazil years is that, for me as a journalist, an immersive experience in a new place is a big-tent one: the gringos are part of it, the English-speaking locals with a foot abroad are a part of it, my bilingual running group and morning yoga are parts of it, the other language students working toward the same goals that I am are a part of it. The college-aged daughter who took me under the wing for my weeks in her home was about to study abroad in Germany, which I think is why she got my loosely defined “spend time with a family” goal so well. My next hope is that I’ll be able to open my doors to the sisters and offer them a home abroad someday."
  },
  {
    "objectID": "personal/2017-06-27-srs-without-computers.html",
    "href": "personal/2017-06-27-srs-without-computers.html",
    "title": "Spaced Repetition Without Computers",
    "section": "",
    "text": "I wrote a little something over for the Spaced Repetition Foundation about getting the benefits of spaced repetition without relying on technology. Check it out here. Also give Matt’s recent post on spaced repetition and creativity a read."
  },
  {
    "objectID": "personal/2017-06-22-italian-mini-challenge.html",
    "href": "personal/2017-06-22-italian-mini-challenge.html",
    "title": "Italian Comprehension: Putting Myself On the Hook",
    "section": "",
    "text": "I’m currently scheduled to do some archival work in August that would greatly benefit from being able to understand written Italian. That’s a little under two months away from now, so I’ve been thinking about how best to approach this problem.\nI’m only interested in comprehension, and not in production, of language, so this would seem a great opportunity to get a little bit of grammar and then immerse myself in some comprehensible input. I bought a quick-and-basic grammar overview and have set myself up with Clozemaster. (Clozemaster is a great way to learn vocabulary through context. I wrote about it previously here.)\nI’m posting about this less-than-two-month mini goal here for a little bit of public accountability. I will in no way be fluent by the end of it, but I hope to be able to make my way through an archive of materials in Italian, or at the very least to know what is worth properly translating and what I can skip over.\nThere’s rumoured to be a Beeminder/Clozemaster direct service connection in the works, but until then I’ll use Clozemaster’s own daily reminder tool. A più tardi."
  },
  {
    "objectID": "personal/2017-06-04-optician-lampedusa.html",
    "href": "personal/2017-06-04-optician-lampedusa.html",
    "title": "How We Forget",
    "section": "",
    "text": "“How naive he’d been, thought the optician, how naive. Because there would always be greater sorrow, deeper and more unfathomable than any of us could ever imagine.” (p. 83)\n\nBad things happen all the time. Suffering is a feature of life for many people. When this suffering happens on our doorstep, an initial flurry of interest is followed by a long steady wane as what was the extraordinary becomes routine. So it is with the boatloads of people making their way towards Europe. For a brief moment, Europe seemed to care. The passage of time saw even these tragic stories become absorbed into the fabric of ‘normal life’.\nEmma Jane Kirby’s book, The Optician of Lampedusa hits the pause button on our collective forgetting. This is an Italian optician’s story, a short tale of his coming into contact with the raw human tragedy occurring with regularity on Europe’s southern shores. The optician is sailing with friends when he comes across hundreds of drowning Eritreans (among others). The book chronicles the moments before, during and after their rescue.\nKirby’s strength is to stick to detailed observations, relaying what was going through the Italian optician’s mind, what he was seeing and hearing. It (re)connects the reader with the unvarnished reality of those being smuggled into Europe. It’s a unique account in its directness, and was a sober reminder of something that I had started to forget.\nIf I have one criticism of the book, it is the perspective. I would far rather have read a book by one of the survivors, or at least to hear the story in their words. I understand that European publishers feel like they need a white face to relay the stories of ‘the other’, I just wish it wasn’t the case. Nevertheless, this was a sensitively written account and one I will be recommending to friends and family."
  },
  {
    "objectID": "personal/2017-04-30-spoken-fluency-taylor.html",
    "href": "personal/2017-04-30-spoken-fluency-taylor.html",
    "title": "Language Learner’s Journal: Increasing Spoken Fluency",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and* coaching worktogether with me. For other posts in the series,click here.*]\nAnother cycle is coming to an end – my eight-week Sijal evening course will soon finish, which means I’ll be cobbling together a mixture of private lessons, independent study and activities, and perhaps a language partner again in the upcoming weeks and months. In these recent weeks taking evening courses and private lessons, I’ve been very glad to have guidance from Alex about how to structure my free study time – doing a single textbook, like we did at my previous course at Qasid, gave us a routine and filled our evenings with homework, but switching to a more self-directed study has given me both the freedom and responsibility to use it productively.\nI feel that my weakest area is my ability to chat and that my speaking is trailing my listening and reading comprehension. Two points Alex has driven home to me is to make time to read out loud and to practice “shadowing,” lip syncing to a recording and trying to imitate its intonation. I remember this being a wake-up for me when I was learning Portuguese in Brazil – somehow, not thinking “I am Taylor constructing the best sentences I can after semesters of Portuguese classes” but instead “I am imitating how a Brazilian would emphatically say this” both greased my confidence wheels and led people to pay attention and understand me, because it “sounded right.” When I review my Anki cards over breakfast, I both read my sample sentences out loud and, when I’ve included an audio clip on the back of the card, try to say it in real time along with it. I’ve also worked on “shadowing” with the Colloquial Palestinian Arabic textbook, which includes nice long dialogues (some of which are too fast for my level, or, I can’t lip sync speedily enough to them!)\nI still feel like I have something like a “deer in the headlights” reaction when someone speaks to me and I don’t have a response ready. Practicing when I know I have a certain phrase coming up, even if I just run through it once in my head (what Alex has called “planned spontaneity”), makes all the difference.\nI generally have ants in my pants (hence going on seven years freelancing and fleeing office work), so I’ve been taking advantage of any interesting Arabic events or activities that come my way. For the past few weeks I’ve gone to a Thursday evening Evangelical church service, which has blown me away by how accessible both its sermons and songs are for my Arabic level. There’s so much to be said for knowing one’s context and making educated guesses at the words we’re hearing – that’s how I picked up that the ١٢ تلاميذ must be disciples and the word مجد repeated in our songs seemed to mean glory or glorify. It’s also a nice mix of dialect and fusha, seemingly varying on whether the song leader/pastor is going for a charismatic or reverent tone.\nAlex has also encouraged me to not let reading go to the wayside even as I focus on speaking, and he suggested I work with a play from Tawfiq al-Hakim, since the dialogue structure of a play is nicely accessible to a learner. It makes all the difference to have a lengthy text with a coherent story – there’s many words I would not spell correctly or I’d waffle if asked to produce them on my own (or I wouldn’t recall them at all), but a story full of coherent clues leads me to understand a pleasantly surprising amount. I’ve been reading الأيدي الناعمة, which is nice social commentary with easily recognizable themes and characters.\nAs my current Sijal course comes to a close, I’ve also been thinking about what kind of activities I want to take on next. I’m game for all things athletic, and the pocket-full-of-tricks coordinator at Sijal gave me a nice playlist of workout videos from the program دنيا يا دنيا.\nI’ve also decided I want to give a language partner another shot. It helps that I met someone, a store clerk, whom I thought would be great – for several weeks after we first chatted about each learning Arabic and English, I thought about how dynamic and fluid our conversation was, and decided to return to the store to ask if she’s like to meet up for an exchange. Some lessons I learned from my last attempt at this: choose a very quiet place (some coffee shops are not!), be stickler on dividing English and Arabic speaking times, decide on some topics beforehand, and be patient and resist the urge to finish someone’s sentence for them, because very often we can find another path to express ourselves.\n[To learn more about coaching with Alex, click here. To learn more about ‘Master Arabic’, a guide for intermediate-level Arabic students, click here.]"
  },
  {
    "objectID": "personal/2017-04-17-on-reading-in-arabic-the-evidence.html",
    "href": "personal/2017-04-17-on-reading-in-arabic-the-evidence.html",
    "title": "On Reading in Arabic: The Evidence",
    "section": "",
    "text": "[This is the second post in a series on the importance of reading when studying Arabic (or any other language). Read the* first post *here.]\nIt is notoriously difficult to study and show which are the most efficient methods to study second languages. For starters, everyone is slightly different, so it’s hard to compare between individuals. Learning a language is also such an involved pursuit (taking place over all hours of the day, and in the mind, where microscope or dictaphone can’t usefully reach) that it is impractical to follow the student for all twenty-four hours of the day.\nHaving given the pitch for why I think reading is so important for students of Arabic, today I wanted to summarise a study that was carried out from 1970-1977. This study, by ElSaid Badawi, is entitled “In the quest for the Level 4+ in Arabic: training Level 2–3 learners in independent reading” and can be found as an article in Betty Lou Leaver and Boris Shekhtman’s fascinating (and underrated / underred) edited volume, Developing Professional-Level Language Proficiency. Given its somewhat obscure provenance, it’s unlikely you’d come across this fascinating article in the normal course of your day, hence my interest in summarising it for you here.\nBadawi offers an overview of his experience running the CASA (Center for Arabic Study Abroad) programme between 1970-1977. This programme was originally started in 1967 for advanced-level students and the idea of it was to give a year of intensive study in order to really catapult students into real competency in being able to read, speak and use Arabic in a professional capacity. (Badawi begins his article with a justification for reading, but I’ll skip those details since their is a great deal of overlap with what I’ve already written).\nThe original CASA curriculum in the 1967-era programme was established around a 3000-word vocabulary list, reading of some short passages using those words in context, a grammar book and two long ‘authentic’ texts that would be covered over the course of the year. The students found this dull and unrewarding, however, so CASA’s administrators decided to design a new course based around familiarising students with a ‘language domain of their interests’. In other words: allowing them to read things that were related to their interests and professional trajectory.\nStudents taking part in the programme were assessed (prior to joining) as being at a high level, but their vocabulary was generally limited to political subjects. They had a poor understanding of morphology and little to no facility with semantics. They had, Badawi writes, bad reading habits in Arabic: too much focus on sentence structure, engaged in ‘parsing-based reading’ and with only a minimal grasp of the “semantic role of punctuation”. In that last case, this is the way Arabic uses words, phrasing and sentence constructions to signify the meaning of a sentence, whereas in English a lot of those meaning structures are conveyed through punctuation. Most of all, students suffered from an ‘excessive / crippling’ use of the Arabic-English dictionary, which was identified as an obstacle to spontaneous and contextualised language learning; words were quickly forgotten.\nThe programme sought to encourage a switch in its students: “a change of attitude toward Arabic from that of a language they are being taught to one which they should start learning”. The responsibility, at this level, generally should switch from the teacher to the students.\nThe programme was split up into three semesters / terms:\n\nSemester 1: 8-week summer programme\n\nThis was made up of introductory cultural classes (based around Cairo, Egypt, where students were living. It offered classes to bring students up to a competent level in functional colloquial Arabic. (Students could solve all their problems and interact with Egyptians in a functional way, following the course). There was also a component of media Arabic where students would become familiar with the formalised language used in printed and spoken contexts.\n\nSemester 2: 14-week autumn programme\n\nThis semester was for allowing students to gain a higher competence in MSA. Reading was one of the core elements here (news reading became effortless and there was some inclusion of classical language as well). Colloquial Arabic was encouraged through the reading of plays (which often used colloquial/dialect expressions and language). An intensive reading programme was added alongside this to boost confidence.\n\nSemester 3: 14-week spring programme\n\nThe final semester included three graduate-level courses in subjects of the students’ interest / choice. There was also some training in ‘Educated Spoken Arabic’ (i.e. the discussion of high-culture topics).\nThe Intensive Reading Course\nThe core belief behind the programme was that reading was important to the students’ knowledge of Arabic in a fundamental way. All the other skills would benefit and develop alongside the reading done as part of the programme. There were different kinds of texts available and a selection criteria for what kinds of reading took place:\n\nFinding materials for intensive / analytic reading was easy. The harder issue was finding materials suitable for extensive reading, i.e. the kind of wide-reading that students are able to do with some level of ease. Arabic poses a particular problem in this regard, given its ‘wide range of active vocabulary in use’, and the ‘complexity of the morphs-semantic system’.\nPlays were believed to be the best for extensive reading. They carried a “high degree of word and sentence redundancy”, usually had only a single theme and were of moderate length. (It was found that reading two 200-page books was much more satisfying than reading a single 400-page book). Plays also lend themselves to real-life activities. There is also the possibility of watching the plays being performed (or, now, on YouTube).\nNovels were also considered useful, but the fact that dialogue is used only minimally means that they were kept for later in the semester. Short stories were denser in meaning and language use and thus harder. They were included in the programme, though, for the sake of variety.\nOverall, texts were chosen for the language structures used rather than for their literary value / content.\nReading Texts\nThe course had students reading three items each week. Usually one novel or a play (a long item) and a short story and a 1-act play (i.e. two short items). These were generally from the same author, and difficulty would escalate over time. All texts were authentic and unabridged. Ideally they were selected from leading literary figures and they would all be texts for which no English translations already exist. Selecting these texts was hard at the beginning, but over the years they settled into a broad pattern, escalating in difficulty:\n\nGroup 1 (first three weeks)\n\nPlays by Tawfiq al-Hakim (short and long). These were good because he uses a lot of redundant vocabulary, follow familiar thematic sources to those with which students would have been familiar, used a lively dialogue and generally contained “straightforward language”.\n\nGroup 2 (5 weeks)\n\nThis consisted of works by Ihsan Abdul Quddus, a journalist, novelist and short story writer. These works tackled themes from social phenomena and thus were appropriate to a young audience. They referenced local customs and expressions. They included fewer dialogues in the novels and short stories. They had a lucid structure and controlled range of vocabulary.\n\nGroup 3\n\nThis was works by Yusuf Idris, blending MSA with colloquial idioms, Qur’anic citations and quotations from the hadith literature. These were at a higher difficulty level.\n\nGroup 4\n\nThis was a mix of items chosen for special topical interest or artistic value. For example, in the final week, students read Fathy Ghanem’s 1958 novel Al-Gabal. They also tackled some of the non-famous novels by Nagib Mahfouz.\nMixed in these various groups were shorter items: one-act plays and short stories. There was generally a balance between length of a text and its linguistic difficulty.\nReading Instructions\nI found this section of the article the most interesting / instructive. Students were told the following:\n\nThe beginning of a story / text is always the hardest. You don’t know what’s going on, who the characters are and what the context / scene is. Bear with it. A lot of this will be scene-setting. You can always return back to it later on.\nArabic has a lot of redundancy. Compare what you are stuck on with what follows and check if you can figure out the meaning that way.\nContinue reading as long as you can make out a story or theme for yourself. Don’t worry or second-guess yourself as to whether what you understand from the story is the same thing as what the author intended you to understand.\nIf you find a word or part of the structure you don’t understand and stop, DON’T look the word up in the dictionary unless:\n\nyou have failed to guess the meaning\nthere is nobody around to ask the meaning\n\nMark / highlight the words you were able to guess in the text. Mark the words you were able to do without understanding.\nMake a list of cultural features that you’d like to be addressed in class.\nMark and make a list of any expressions and grammatical features or constructions that you want addressed in class.\n\nClasses\nClass sessions were essentially there to ensure that students were keeping up with the reading volume. Students would narrate their understanding of the texts they had read, and would raise any issues they wanted to learn more about.\nClasses were also a good time to increase students’ semantic understanding – allowing students to identify shared roots and usages in different contexts and forums.\nStudents submitted written responses / follow-ups to the text in the class with the teacher present. A weekly conference with students gathered feedback on the choices of texts, allowing teachers to adapt the programme depending on the ease/difficulty perceived by each individual cohort of students.\nResults\nBy the end of the 14-week programme, students had read an average of 2500 pages of authentic Arabic texts. Graded text levels showed that their language was improving. They were encouraged by managing to review words and structures that had been marked as ‘hard’ earlier on in the semester. (Usually 25-40% of these words had become intelligible to them, despite no vocabulary learning strategy specifically targeted at learning these words.) The graduate-level courses (all taught in Arabic, obviously) of the final semester were also a proving ground for students.\nThis reading programme increase students’ competence and was transferrable to their other skills. (Yes, even their spoken Arabic.) Reading helped with writing. Reading ‘complete texts’ did a lot for the morale of the students at the intermediate-level, too. And the literary focus of the content was useful for students even if their interests didn’t lie in that particular area.\nMy next post about reading Arabic will detail some options that are available to the intermediate-level student of Arabic, and some practical considerations resulting from this article."
  },
  {
    "objectID": "personal/2017-04-09-independent-study.html",
    "href": "personal/2017-04-09-independent-study.html",
    "title": "Language Learner’s Journal: Independent Study",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and* coaching worktogether with me. For other posts in the series,click here.*]\nFor some Arabic students (myself, certainly), when we first start to learn about the diglossia in Arabic, we feel somewhat cheated, like, “I signed up to learn a language, and now you’re telling me that I need to learn some second, shadow language if I’m actually to use it?” It’s like the rug is pulled out from beneath our feet, like we’d be studying Shakespeare and are frustrated to find that real people actually speak Singlish. Colleagues who work in journalism/research repeatedly encouraged me to study dialect, which led me to leave my last fusha course for an ammiya one at Sijal.\nHappily, it doesn’t feel nearly so intimidating as I once imagined when I turned on some ammiya YouTube videos and despaired that a year in MSA classes seemed to do me no good. Indeed, the listening comprehension and vocabulary I learned at Qasid feels like a swiss army knife I now use to pick a new lock. Also, another useful tool from my MSA classes – an extreme comfort with not understanding many of the words I’m hearing but still staying engaged and hanging on for the ones I do.\nOn a related note, Alex has encouraged me to keep up independent reading even as I’m in a course that largely focuses on speaking and listening. That’s another skill that I appreciate from my time at Qasid – the willingness to dive into a text, even when many of the words are ones I don’t recognize, and look for the keys that will give me some clue about it. I’m a pen-and-paper learner, so I’ve been printing out media articles and reading them through twice, no dictionary, then underlining words I don’t know and making my best guess at what they mean.\nFor example, this week I read one in the Huffington Post about scientists questioning whether we need to drink eight cups of water a day. The piece mentioned drinking a sufficient amount of water so that “البول” is “واضح اللون أو خفيف الصفرة.” I didn’t know that first word, but I was delighted to know exactly what it was as soon as I read the rest of the sentence. I’m convinced that process of discovery is a powerful learning tool, more so than having translations readily at hand to answer our doubts as soon as we have them.\nAlso, having a bit more free time, I’ve been able to take advantage of events going on around me to get extra-classroom contact with the language. I sat in on a Sunday morning lecture from an Al Jazeera filmmaker who produced an extraordinary documentary, “The Boy Who Started the Syrian War.” His discussion afterwards was well above my level, and I only got the outlines of what he was saying, which is still far more than I would have gotten just three months ago when I came to Amman. It was still, of course, a very useful experience. For example, he used the word نظام always when I was expecting him to say حكومة, which led me to ask and confirm with my teacher that it seems to be used like we say “regime” in English, or, a disdainful/pejorative word for a government.\nBeing someone who spends plenty of time in the kitchen (because organic vegetarian food doesn’t make itself, at least not anywhere walking distance from me =) I’m a big podcast fan. Alex’s on Jordanian ammiya is great listening for me at my current level – when I tried this just a few months ago, it was beyond my grasp. I also like the BBC Arabic service and DW’s current events discussion panel.\nAnd repeating what I wrote in my last post – an upcoming post will be some reflections on accents and errors and embarrassment and the ways we as language learners judge ourselves (and others? I hope not. I indeed only judge myself when it comes to foreign language ability, which may point to, as Alex says, how much language is a confidence game). I’m on a scale between sheepish and chatterbox depending on what situation I’m in, and I’ve been chewing over what it is about a given situation that makes me feel either of those ways.\n[To learn more about coaching with Alex, click here. To learn more about ‘Master Arabic’, a guide for intermediate-level Arabic students, click here.]"
  },
  {
    "objectID": "personal/2017-03-12-why-are-arabic-fonts-so-small.html",
    "href": "personal/2017-03-12-why-are-arabic-fonts-so-small.html",
    "title": "Why do Arabic fonts appear so small? (and how to fix it)",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“600”] This is what a Wikipedia site looks like in Arabic [/caption]\nAnyone who’s ever studied Arabic and attempted to increase their exposure to the language through the internet will have encountered this problem: Arabic fonts are always two or three sizes smaller than their English/Roman alphabet equivalent. This can make navigating the web a dispiriting experience. Most big websites take a lot of time and effort to get their browsing experience just right, with fonts that are appropriately scaled and optimised for reading. (Get a sense of how much thought goes into typefaces here, for example, at the New York Times.)\nSo why does this happen? At first I thought it was just a case of Arabic fonts being very much a sideshow in the what-doesn’t-everyone-else-speak-english show that encompasses so much of Silicon Valley’s design mentality. The most-used products are generally designed for an English-speaking audience, with people writing from left-to-right. Apple and Android’s operating systems both work and function much better / logically when set to a Roman alphabet / layout. I happen to have my phones and computers set to an Arabic alphabet, and it’s blindingly obvious that less thought went into designing the experience for such Arabic-speaking users. (For a more detailed explanation of some of the deficiencies of Arabic fonts, read this and this.)\nWhat’s worse, the consistency is subject to random change. To give one small example, iOS’s ‘Save to Evernote’ dialogue box allows me to save articles from Instapaper into Evernote. (This is part of my somewhat laborious workflow for getting articles into DevonThink. Read here for more.) For years, I clicked a button in the right-hand corner to ‘save’, but a few months back they switched all the boxes around and now I have to click in the left-hand corner. The muscle memory is such that this is a hard one to fix. It’s not the end of the world, but it still is an indication (along with the many other times this happens, seemingly without plan) of how little thought goes into this design and user interface work.\n[caption id=“” align=“alignnone” width=“600”] This is what the English version of Wikipedia looks like [/caption]\nComing back to fonts, the real reason for why this happens has to do with the amount of vertical space that letters take up. Thomas Phinney of FontLab explained it clearly over on Quora when he wrote:\n“Arabic letters have a smaller body relative to the extenders above and below, so the most common elements are smaller relative to everything else. Because the height of the font needs to more-or-less fit within the body size, Arabic looks smaller than Latin at the same point size.”\nThere are three main approaches to solving this problem:\n\nDeal with it. A lot of internet ‘advice’ tends towards this attitude. It’s probably wise, but not especially practical for those early on in their studies of Arabic, nor even particularly practical for general audiences who want a pleasant reading experience.\nZoom in on the page. CTRL+ or CMD+ will do this on most browsers. Unfortunately, it messes with the rest of the design and functionality of the page, so you’ll usually have a pretty unpleasant experience if you do this.\nInstall something to make the Arabic letters bigger. There are some scripts that will do this for you that you can have Greasemonkey (in Firefox) or Tampermonkey (in Chrome) handle. These two (here and here) seem to be the best known. I have tried both and couldn’t get them to work. Needless to say, this is pretty fiddly and not at all ideal.\n\nFor now, it seems we’re stuck with a poor browser and operating system experience.\nUPDATE: Gerald Drißner (of ‘Arabic for Nerds’) suggested a Chrome extension, Huruf, which I have now installed and seems to work pretty well. Try that if you use Chrome!"
  },
  {
    "objectID": "personal/2017-03-06-guinea-pigs-reading-list.html",
    "href": "personal/2017-03-06-guinea-pigs-reading-list.html",
    "title": "Everything You Ever Wanted to Know About Guinea Pigs",
    "section": "",
    "text": "Followers of my Goodreads account will recently have noticed a flurry of activity: I read six books about guinea pigs over a period of two or three days.\nThe reason for this is that I’m staying in a house that has recently come into possession of a guinea pig and I became curious. What kind of an animal was this? Where did it come from? How smart was it? What were the key points to bear in mind when looking after a guinea pig as a pet?\nWhat follows are some notes on the various books I picked up. I am unaware of a synoptic guides or bibliographies of guinea pig care practices. I looked for them online a few days back, but nothing stood out. I’ve ordered the books here by how complete I found them.\n\nSharon Lynn Vanderlip - “The Guinea Pig Handbook”\n\nProbably the best handbook for guinea pig care (formatted for Kindle) available on the market. I can’t speak to books that are only available in paperback. In any case, this is an extensive guide to food, behaviours, and even a history of the various breeds. Highly recommended as a one-stop shop for anyone who has just brought a guinea-pig into their lives.\n\nGerry Bucsis & Barbara Somerville - “Training Your Guinea Pig”\n\nA strong overview of how to train guinea pigs. First half of the book is a pretty extensive how-to / first-time owner information, so it’s nice to have that all in one place.\n\nHolly Lloyd - “Guinea Pig Care Secrets: Kids Guide to a Happy Guinea Pig”\n\nUseful tips, despite the length. Sensible advice for a first-timer. Written for children to be able to read as well as adults.\n\nKate H. Pellham - “Guinea Pigs: The Essential Guide To Ownership, Care, & Training For Your Pet”\n\nQuick but to the point.\n\nSarah Yee - “Guinea Pigs Owner Handbook: The Complete Beginner’s Guide to Guinea Pig Care and Facts”\n\nUseful. Didn’t blow my mind. Very basic and short.\n\nJulie Rach Mancini - “Guinea Pigs”\n\nShort. Simple. Basic overview. No depth to it. Suitable for children, perhaps."
  },
  {
    "objectID": "personal/2017-02-26-three-podcast-recommendations.html",
    "href": "personal/2017-02-26-three-podcast-recommendations.html",
    "title": "Three Podcast Recommendations",
    "section": "",
    "text": "I’ve been walking around more this past week, and have added some new listening material into my podcast quiver. So if you’re interested in something outside your usual information diet, give these three a try:\n\nSlate’s Stranger Than Fiction. Conversations with science-fiction authors about the intersection of their writing with technology and the contemporary world. This podcast is no longer produced and there are only 6 episodes (dating back to 2013).\nPod Save the World. Conversations with people who were involved in American foreign policy decisions. This is part of a series of new podcasts put out by some former Obama-government staff members (aka Crooked Media). The foreign policy discussions are often interesting and they offer behind-the-scenes glimpses of how certain deals were made. So far we’ve heard about the Cuba and Iran deals made under Obama.\nArms Control Wonk. A discussion podcast about nuclear weapons. It’s a fairly opinionated glance at some pretty down-in-the-weeds topics relating to nuclear security, particularly from the US perspective, but given that North Korean nuclear weapons are likely to be a big deal in 2017 this is a useful one to follow."
  },
  {
    "objectID": "personal/2017-02-18-llj-keeping-pace.html",
    "href": "personal/2017-02-18-llj-keeping-pace.html",
    "title": "Language Learner’s Journal: Keeping Pace",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and coaching work together with me. For other posts in the series, click here.]\nI had a low low and a high high in the same week, in fact, one day after the other. I hope that may underscore that preparation and, after that, confidence/stubbornness are the motors for getting new words and sentences out of my mouth.\nOn Monday, I wrote in my daily study log that I was tongue-tied and could barely get a complete sentence out in class. On Tuesday, I happened to be the only person to show up to class – and thankfully, I was prepared, having read the night’s text and done our homework on making the أمر (command) and using what seems to be a bit obscure but indeed nice use of the أفعَلَ to express admiration or astonishment, like, ما ألذّ هذه الكنافة. That meant for three hours, I was one-one-one a teacher in each class (and, in fact, with an additional instructor, since an observer/trainee came along too). And for those three hours, I was a chatterbox. I talked about a vengeful husband who cast a spell on his wife to turn her into a donkey (we were, if these are a set of familiar references to you, on the “1001 Nights” chapter of Al Kitaab Two), traded commands with my professor (!كلي الحلويات العيد الحب), and described the Seven Wonders of the World with our new قواعد.\nI make many, many mistakes. I routinely use incorrect prepositions, mispronounce words whose consonants I know but whose vowels I flub, and fumble conjugations. Alex has said that speaking a foreign language is 80 percent a confidence game, which felt like what happened between Tuesday and Wednesday, which was not a time when I got 80 percent better at Arabic.\nOn that note, a few helpful tools that helped me hit a high note this week:\nDaily study log: I mentioned this above, and as simple as it is, it is useful to reflect on highs and lows and how I’m spending my time. Alex made an excel sheet that adds up my total times spent reading, in conversation, etc, each week. It also keeps me a little in check because when I use a timer to log how long an exercise takes me makes me a bit more efficient and, say, not give into some mindless cyborg urge to check my social media or Whatsapp because… if I do, then I’ll kick myself when I see I took an hour and a half to do a simple exercise.\nReverso: I had been asking around for dictionary recommendations, and Alex turned me onto Reverso. I’ve seen this site before but haven’t consistently. In addition to a translation, it includes a list of sample sentences of that word used in context. Many of their examples are short and punchy and easy to absorb even when I just need to look up a word while I’m in the middle of a reading assignment.\nTime matters: Even a few extra hours a week need to be taken into consideration. I’ve decided, for the time being, to stop casual meetings I had been arranging with an language partner each weekend. One, I get a lot out of speaking to my teachers after class and with my fellow students, which I do twice a week when I stay for an extra conversational section at Qasid. And two, I was spending a lot of time getting to and from the weekend exchanges, loitering over ordering tea and trying to find cafes where without much smoke or noise. Exchanges for students at our level can be little disorienting without clear rules and intentions for what we will do, and Alex backed me up when I said I felt like we weren’t getting much out of our time, saying that he, too, didn’t usually find language partner arrangement to work well.\nTime matters (2): Time also matters in the long run, and, after consulting with Alex several times, I changed my plans for how I will spend the next several months. I realized I would fear feeling overwhelmed or incomplete if I went to an intensive summer Arabic program in the United States, which is largely in fusha and starts in June. I also decided to leave Qasid after this term to focus exclusively on Ammiya in a new course and in private lessons. Alex’s advice and answering my many questions about how to include and prioritize dialect in my language learning were invaluable to making that decision, even if he also made many arguments in favor of the summer program, which I plan to keep on my radar for a future summer.\nI feel at peace with that choice because I’ve already been studying fusha for a year and a half, and I don’t expect it to slip away from me if I still find ways to practice what is important to me. I’ve been speaking Portuguese for ten years and I still learn new things (the other day, “você parece estar comendo a sua língua,” or, “you look like you’re eating your tongue” – you look like you’re having trouble expressing yourself); Spanish has been with me for half of my life and the same is true. Rather than honing my sights on some sort of “Well now I’m finished” endpoint, which the summer program had been in my mind beforehand, I’m reminding myself that language, like exercise, is useful to us precisely because we don’t are not trying to “finish” with it."
  },
  {
    "objectID": "personal/2017-02-13-master-arabic-out-now.html",
    "href": "personal/2017-02-13-master-arabic-out-now.html",
    "title": "‘Master Arabic’ is Out Today!",
    "section": "",
    "text": "I’m very pleased to announce that ‘Master Arabic’ is (finally) released and available for direct download. I’ve been busy finalising various small details relating to the text throughout January and it’s great to have it finally out and in the world.\nIf you haven’t already had a chance to check it out, visit this link to learn more about what the book contains and what extra materials come alongside the book as part of the premium edition.\nI’m particularly pleased with how the online resource guide turned out (with over 300 resources listed). I continue to add to it each day, too, so this is a ever-growing resource in and of itself.\nAs always, if you have any questions about learning Arabic or about the book, please do get in touch on twitter or via email."
  },
  {
    "objectID": "personal/2017-02-03-actual-fluency.html",
    "href": "personal/2017-02-03-actual-fluency.html",
    "title": "Actual Fluency",
    "section": "",
    "text": "I was interviewed by Kris Broholm on the Actual Fluency podcast last week. This was a real pleasure to do, since I’ve been listening to that podcast more or less since it first started.\nYou can listen to the interview/episode here. Our conversation was fairly wide-ranging, covering topics like whether to start Arabic by learning the spoken or written form first, how to learn languages when there isn’t much in the way of materials available, and what kinds of meta-skills I’ve found useful in my own language learning journey as well as that of the students I coach."
  },
  {
    "objectID": "personal/2017-01-27-how-to-learn-a-page-of-verbs.html",
    "href": "personal/2017-01-27-how-to-learn-a-page-of-verbs.html",
    "title": "How to learn a page of verbs",
    "section": "",
    "text": "Someone sent me a list of verbs that they were hoping to memorise. They wanted some tips and suggestions for different ways to approach the task. You can see part of the image above. Verb, translation and a sample sentence where the word is used in context.\nArabic verbs are unique in that they often share a similar / familiar rhythmic pattern (da-da-da), so it can sometimes feel like they’re all the same.\nI sent him the following suggestions and thought it might be useful for others so I’m reposting it here. For verbs, it helps to attack from multiple angles simultaneously.\n\nMake sure you understand the meanings\nMake sure you know how they sound. (Use Forvo or check with a native-speaker if there’s any confusion)\nGo through and say them out loud. Take time with each word, saying it out loud a number of times. Get a sense of the physicality of the word\n(Ideally, add them in Anki, though that’s more for long-term preservation rather than short-term passing a test etc)\nLook up each word on Reverso Context. The online dictionary will give you a translation and then a bunch of different examples of other sentences using that word or verb. Look to see how it’s used. Study the examples and try to identify what is unique about the word - its meaning may not be exactly translatable into English/French.\nLook up each word on Tatoeba. Repeat the same steps as you did for Reverso Context in the previous suggestion.\nGo to Google News and type in the word in Arabic. Take a look at the examples that come up. Copy them out onto a piece of paper if you feel like the language is interesting.\nTry using them in a sentence out loud. First repeat the sample sentence given in the example. Make sure you understand how the word is used in the context of that example. Then make a sentence of your own that uses the word. Say it out loud. Make another sentence.\nWrite two sentences for each of the words. Try not to go through the list in order. Either do every other word, or pick at random. Point is, don’t get used to going through the list in order, since that’ll only make you good at remembering the list rather than the words on their own. (BONUS: Post these sentences to Lang-8, get corrections and make Cloze Deletion cards on Anki so that you’re not just learning from single word cards, but you’re learning words in context.\nDo Back Translation with the sentences you’ve just had corrected on Lang-8. First translate them into English on a new sheet of paper / file on your computer. Then close the original Arabic sentences and see if you can translate the sentences back into Arabic from the new English sentences. This will really test your command of the vocabulary and phrasal structures you just learned. You can do this in writing or just orally.\nIf you have problems with spelling, use Iverson’s lists: Take a blank piece of paper - A4 is good (or whatever the US equivalent is) - and write a list of the words you have to learn today on the left side of the page. Try not to take up too much space. Maybe it’s 20 words. Write them down on the left side of the page. Then take a ruler or draw a line alongside that list and to the right of the line, (perhaps in a different colour pen), write the translation of that word. Do that for all the words. If you don’t know the word, then the memorisation image/association hasn’t stuck, so you can look up the correct answer and make sure that your association sticks this time. Once you’ve completed this first test, take another piece of paper (or, better still, something thicker like a book so you can’t cheat) and cover up the first column. Now you only have your answers to look at, and you should draw another line and then write the translations. (i.e. translating things back into the original language). Do all the translations for the list, then check whether you got them right. Then you should repeat this until the entire piece of paper (both sides) are covered with translations back and forth. If you write small-ish, you should be able to get a good 6 or 7 rounds of translations/testing in (if not more). Perhaps don’t do all of these sessions at once. Do one side of the page in one go, and then leave other sessions for later in the day (for reasons I’ll explain now). Once you’ve taken the time to make the sound association, write down the list of words that you’re learning that day on the page as described in my blogpost. Do one round of translating them. Then during the day, every so often, do another round, translating the words back and forth etc.\n\nThere are other ways you can practice words, but this is probably enough to get started with. You don’t have to do all of these steps, though if you get through them all I pretty much guarantee you’ll be better off than doing none of them!"
  },
  {
    "objectID": "personal/2017-01-17-trello-markdown-chrome-extension.html",
    "href": "personal/2017-01-17-trello-markdown-chrome-extension.html",
    "title": "Trello to Markdown: a Chrome extension",
    "section": "",
    "text": "Turns out, whenever you need something on the internet, someone else has already made it.\nI was looking around for a way to get several hundred Trello notes (and their descriptions) into a format where I could work on the texts in an offline format. (Trello doesn’t have an offline mode.)\nI found this excellent extension (made by ‘Trapias’) which allows you to get your data out of Trello. Click here for the Chrome extension itself and here for the source code over on Github. You can export to HTML, Markdown, Excel and OPML. This is a great set of options, and there are all sorts of advanced selections you can make.\nTo use:\n\nGo to the board menu -&gt; more -&gt; print and export\na new button will appear called TrelloExport. (Previously there would just have been a JSON export option)."
  },
  {
    "objectID": "personal/2017-01-15-master-arabic-behind-cover.html",
    "href": "personal/2017-01-15-master-arabic-behind-cover.html",
    "title": "Master Arabic: Behind the Cover",
    "section": "",
    "text": "I’ve been pleasantly surprised by the interest and the number of people who’ve already pre-ordered copies of my new book, Master Arabic. I wrote a guest blogpost at the end of last week for Lingualism, a wonderful small publisher that puts out quality materials relating to dialect. If you’re studying Egyptian Arabic in particular, they have lots of materials you’ll want to check out.\nToday, I wanted to say something about the image I chose for the cover of the book. My mother, Liz Strick, is a painter and this is one of her recent works entitled Energy. She wrote the following about the painting:\n“In 2014 I suffered a severe stroke with partial and permanent paralysis as a result. For an artist this is a tremendous, almost insurmountable, setback! Art took up an essential part of my life up until I suffered the stroke. Art always had a central place in my heart and in my head. After years of continued physiotherapy treatments and rehabilitation, it is hard to explain what happened when I was eventually prompted by my physiotherapist to start using my left paralysed hand for what I had alway loved doing best all my life: art. He gave me back my life and the gift of hope! It is a gift more valuable than I could have thought. Energy started flowing through my body and I had found my own way to overcome the ‘impossible’: drawing with my lame left hand! “Energy” was born and is here to stay.” [SOURCE]\nI thought the image was ideal for the cover not only because the lines reminded me of the Arabic script somewhat, but also because the story of the painting - born out of difficulty and determination - should be relatable to any student of the Arabic language. Check out the rest of my mother’s paintings here, and if you’re in the Netherlands, consider visiting her open exhibition, running until the end of the month."
  },
  {
    "objectID": "personal/2017-01-13-developing-android-udacity.html",
    "href": "personal/2017-01-13-developing-android-udacity.html",
    "title": "Developing for Android with Udacity",
    "section": "",
    "text": "I was lucky to be awarded a scholarship to learn to code apps for Android phones. 10,000 such scholarships were awarded (from 70,000 applicants) to citizens of the European Union through a Google-Udacity partnership.\nIf you’d have asked me a few months ago whether I had an interest in learning to code apps for the Android platform (and learning the requisite Java code) I probably would have said it was low on my priority list. This scholarship allows me to learn the basics over the coming three months (with a possible extension of another three months as part of the full Udacity fasttrack nano degree, however, so I will certainly take the time and make use of the opportunity.\nIt isn’t clear to me how far the course will take me, but I’m already thinking that this might be a really great opportunity to develop a version of my CoachBot language-learning tool for Android, hopefully one that doesn’t require users to be online to use it.\nThe course so far has been fun. As usual, Udacity’s platform and teaching style is highly interactive, iterates over problems and gets you solving practice questions from the start. As a language, Java is different from the Python and Javascript that I’ve encountered thus far, though I’m not deep enough into the weeds to have a strong appreciation of exactly how.\nIn any case, watch this space. If you’re an Android user and would be interested in an app version of the CoachBot tool, drop me an email to let me know."
  },
  {
    "objectID": "personal/2017-01-11-resetting-baseline-caffeine.html",
    "href": "personal/2017-01-11-resetting-baseline-caffeine.html",
    "title": "Resetting my base line: caffeine edition",
    "section": "",
    "text": "I’ve blogged enthusiastically about tea in the past. I enjoy discovering new varieties and the winter weather in particular makes an ever-ready cuppa almost obligatory.\nI’ve been busy with work projects in the past month or two, so I was surprised to realise how much my caffeine intake has increased, almost imperceptibly. Prior to my intervention, I was drinking perhaps two or three cups of caffeinated tea per day. That may seem like nothing to those of you with a six-coffees-a-day habit, but my body (and mind) is more sensitive than most to caffeine. Three cups per day left me feeling rattled and edgy.\nCaffeine, for me, can be good in certain very specific circumstances. It’s great when I’m generating ideas – either for a particular work project or just in general – but particularly bad when I need to sit down and write (and focus). Caffeine seems to stimulate the randomly associative part of my brain, the one that has me connecting chocolate to fish sentience or making some other strange link.\nI’ve found that when I need to focus, I tend to prefer drinks that are completely caffeine-free. Even my newly-discovered favourite, kukicha or twig tea, comes with a caffeine punch, albeit one that is greatly reduced.\nSo a few weeks back I instituted a caffeine-only-very-seldom policy. It’s working well so far. I feel much better, more focused, less jittery, and when I do choose to drink a cup of chocolate puerh, for example, as I did this morning, one is more than enough. (I’ve done similar things in the past with salt and sugar and had equally positive experiences.) As always, Beeminder is holding my hand all the way."
  },
  {
    "objectID": "personal/2017-01-09-classical-arabic-sources-methods.html",
    "href": "personal/2017-01-09-classical-arabic-sources-methods.html",
    "title": "Classical Arabic and a Sources and Methods Recap",
    "section": "",
    "text": "The new Arabic podcast I started has three episodes out of the Jordanian dialect series. Today I’m adding to those with a new series exploring classical literature. I’m extremely pleased to be joined by my friend, the erudite Talha Ahsan, someone who has a strong appreciation for the subject matter and who teaches Arabic online and in person in the UK. Read some of the testimonials from his former students here.\nIn our first episode, we discuss Ibn al-Muqafaa’ and Talha’s interest in classical literature. We get into Ibn al-Muqaffa’s relevance to the modern age and explain what we hope this podcast series will do.\nSources and Methods has a new podcast out today, too. This is an ‘inbetweenisode’ in which Matt and I discuss the past year, what we’ve been up to recently and some of our future plans. Let us know if you enjoyed this style of episode. We were thinking of doing more of these conversations between us where we take a deep-dive into a particular topic."
  },
  {
    "objectID": "personal/2017-01-06-2017-1-introducing-coachbot-your-personal-language-taskmaster.html",
    "href": "personal/2017-01-06-2017-1-introducing-coachbot-your-personal-language-taskmaster.html",
    "title": "Introducing CoachBot: Your Personal Language Taskmaster",
    "section": "",
    "text": "For languages that aren’t new, I often feel like I’m stagnating and get bored when I reach the intermediate levels. This can reflect a lack of materials from which to study (as was the case with Pashto when I first started studying it) or — more commonly — a surfeit of materials. This creates a kind of choice paralysis where the number of options means I’m far less likely to sit down and pick one of them. (In a similar way, I’ll sometimes choose not to watch any of the in-flight entertainment because there are too many choices to pick from.)\nStudying a brand new language is (almost) always fun: you’re making quick progress, everything is new so you still have that nice-and-shiny feeling, and you feel like you’re really on your way to success. Continuing that study after two to four years of effort is a little harder. Like with any longer-term project, you start having to find ways to remind yourself of why you’re even working on it in the first place. It can often feel like you’ve lost that original magic somehow, even to the extent that you question whether you actually want to learn the language.\nIt is useful to address some of these issues ahead of time. That way, when you hit a period of less energy or motivation, you have a pre-formulated plan of action (made by you when you weren’t consumed by whatever mood is dominant). For me, this takes the form of making lists of suggestions to my future-self. I have pre-made task lists for:\n\nWhen I’m travelling\nWhen I’m feeling sick\nWhen I have no time to study\nWhen I have oodles of time to study\nWhen I have lots of energy and enthusiasm for learning\nWhen I have no enthusiasm for learning\n\nTry to have at least 10 or 15 tasks in whatever lists you do end up creating. Maybe save a few pages at the back of your language notebook to list these tasks. This way, you always have them handy. It helps to have a good amount of variety in the tasks you pre-assign to yourself.\nI keep lists as described above, but they weren’t as effective as I’d hoped. I’d glance at the tasks, feel only a limited enthusiasm for the options available and then put the list to one side. I needed a different solution.\nI happened to be teaching myself to program/code at around the same time, so I thought this might make an interesting practice problem to try to solve. (I was studying Python and so I found a way to make a web app that uses that to connect to Flask.)\nCoachBot is the free tool I designed to solve the problem of study choice paralysis for language-learners. It’s still only a prototype, but I’m soft-launching it here now since I imagine it might help those reading who are in similar situations.\nCoachBot gives you a task that you can complete within a specific time-frame. If you have only 5 minutes, it’ll pick a random task from the database that I curated and wrote myself. Have an hour? It’ll suggest a different kind of task. If you don’t want to do a particular task that it suggests, just click a button to get a new one.\nThese are the kinds of tasks I suggest when working with students one-on-one. They’re also the kinds of tasks I had written down in my lists. As of writing, there are 386 unique tasks in the database, which means that the suggestions are far more varied and creative than anything I was previously using.\nI’d suggest you use it as follows: if you ever feel like you don’t know what to do to keep going with your language studies, open up CoachBot, pick a time corresponding to your needs and do whatever it tells you to do. When you’re done, make a note of what you did and how long it took in your learning log. Consider doing another session.\nI’ve been using this for a few weeks already and can attest to its value. One of the key benefits I’ve found is just in getting started. Sometimes I’ll only need to do a five-minute task before I realise that there was something else that I wanted to read or study and then I’ll get busy working on that.\nThere are lots of features that I hope to build in for future versions. I want to include user accounts and tracking of how much time you spend on the different tasks. I want to sub-divide by language skill (i.e. which skill is being trained) and eventually to build in some kind of guidance and interactivity to how the tool functions. But for now, use it as it is: get some studying done by outsourcing the choice of what you’ll be studying.\nThere are more details on the website itself. You can click through to the project’s roadmap where you can see an updated version of features coming soon. You can also make suggestions for tasks that you’d like included in the Bot and/or specific features you’d like me to build as part of the project.\n[Special thanks to Alex, Ian, Kevin and Peter for patiently answering my questions while I was building this initial prototype]."
  },
  {
    "objectID": "personal/2017-01-05-2017-1-taskpaper-omnifocus.html",
    "href": "personal/2017-01-05-2017-1-taskpaper-omnifocus.html",
    "title": "Taskpaper –> Omnifocus",
    "section": "",
    "text": "This is a neat trick for anyone who happens to use both Taskpaper and Omnifocus apps. I think this has probably been there for a long time, but I heard Gabe Weatherhead talking about it on the latest Mac Power Users podcast. (Read more of Gabe’s writings on Taskpaper if you’re interested in a deep dive).\nI’ll assume you know something about the Taskpaper syntax. This video explains more in case you’re lost already. The Taskpaper syntax and .tp extension is useful because I can write lists of projects and things to do in Drafts on iOS without having to mess around with apps and ticking boxes and so on.\nThe trick which Gabe explained relates to getting hierarchically sorted task lists from Taskpaper into Omnifocus. It’s as simple as copy–&gt;paste. So I can go from something complicated in Taskpaper like this:\n\nto a set of tasks like this in Omnifocus:\n\nIt’s a great tip, and a great time-saver."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "",
    "text": "I’ve been living in Jordan for a little over four months, and I’m pleased to say that I seem to have converted what Syrian/Lebanese colloquial Arabic I learnt in the past into the Jordanian dialect.\nAlong the way, I’ve used various resources and watched various materials that have really helped. This blog post will be my review of what is available for the student of the Jordanian dialect.\nLots of people say that you can just choose ‘Levantine’ and that’s enough. “You don’t need to specialise any further than that,” I’ve heard on a number of occasions. My experience is that the country-level distinctions between Jordanian, Syrian and Lebanese Arabic actually do matter, particularly once you get past the pure basics and want to have more natural conversations.\nFor various reasons, Jordanian dialect resources are under-represented in the market for books and audio resources. They’re either subsumed under the ‘Levantine’ bracket or books generally tend to offer Syrian or Lebanese flavours. It is for this reason that I think it’s worth taking stock and gathering together all the resources available for students hoping to travel to Jordan or make a study of the specific dialect spoken here.\nBeing the last somewhat-calm country in the non-Gulf non-north-African Arab-speaking world, it’s a preferred destination for year-abroad students mid-university. If that’s you, I think you’ll find this post really useful."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#how-to-use-these-resources",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#how-to-use-these-resources",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "How to Use These Resources",
    "text": "How to Use These Resources\nThere are a lot of books and resources listed below. The expectation is not that you’ll go through them all, but that you should rather be aware of what is out there. There aren’t really many options for textbooks, so you’re basically stuck with Hakini Arabi. (Good thing it’s a well-designed course, with lots of dialogue and practice). I’d recommend the serious student of Jordanian Arabic get hold of both Tiedemann’s 101 Verbs book and Diwan Baladna. Both are filled with material you’ll find nowhere else. (See below for details on these books).\n\nAs for the rest, pick and choose: watch a comedy show; do some listening practice exercises; take an iTalki lesson with someone from Jordan. Those final steps are up to you, and depend a lot more on your goals and how fast you want to go.\nIf you’d like one-on-one mentoring/coaching in your journey to learn Jordanian dialect (and/or standard written Arabic), have a read of what I offer and get in touch with me.\nFor a one-stop-shop guide to improving your Arabic and getting beyond the intermediate level, particularly for students who’ve studied for a year or two already, read my new book, Master Arabic."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#grammar",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#grammar",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "Grammar",
    "text": "Grammar\n\nJordanian Arabic Grammar: A Short Guide for Beginners (Peace Corps)\n\nThis is a useful overview of the main grammatical features of the language. The grammar covered is very basic, but it is nice to have it all in one place.\n\nCGE Jordan’s Videos (YouTube)\n\nCGE Jordan Institute for Arabic Studies have also published an indispensable book (see below), but these are their videos. They teach the kinds of things you’ll never find in a book, and they also practice them in pairs. They also have material for intermediate and advanced students, like this video in which Fred teaches how to intensify your sentences:\nIf you learn best from watching videos and have the time to wade through hours and hours of short lessons, this is probably an excellent place to start. I spoke with the director of CGE, in many of the videos, for the premium edition of ‘Master Arabic’."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#textbooks",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#textbooks",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "Textbooks",
    "text": "Textbooks\n\nHakini Arabi: Palestinian and Jordanian Colloquial for Beginners\n\nThis is your only option for a full textbook covering Jordanian dialect for beginners. It is taught as if you haven’t necessarily studied any other Arabic before, so parts of it move a little slow, but that’s a minor quibble. The book comes with a link to audio files that you download online, and these are essential for working through the dialogues and listening exercises. There are lots of group role-playing exercises suggested in the book, but even if you’re studying on your own this is a valuable one-stop shop to work through.\nDifferent parts of Jordan speak slightly different sub-dialects, and this book offers options for three varieties: ‘Jordanian’, ‘Palestinian-Rural’ and ‘Jordanian and Palestinian-Urban’. For an idea of the kinds of materials contained within, here’s an example of a vocabulary list from early on:\n\nand here’s an example of a context/phrasal explainer from towards the end of the book:\n\nFor a total beginner, I would certainly start with Hakini Arabi. One of the first things I did after moving to Amman was to go through this book.\n\nLiving Arabic: A Comprehensive Introductory Course\n\nLiving Arabic is part of the Munther Younes Arabic-tuition empire. He generally produces solid materials and though I’ve never seen a copy of this book, I’ve been told by a number of students that it contains useful materials for Jordanian Arabic. The text teaches MSA alongside colloquial, so this may not be totally suited to your goals. Perhaps the best scenario for this is if you get a chance to use it in a library. You can probably work your way through the Jordanian Arabic sections in a few days/weeks."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#words-and-sentences",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#words-and-sentences",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "Words and Sentences",
    "text": "Words and Sentences\n\nDiwan Baladna: The Unprecedented Spoken Arabic Dictionary\n\nThis is a thematic dictionary and phrasebook for Jordanian Arabic. I hesitated to buy this for a long time on account of its cover, but I saw it in a bookshop and the content is really excellent. It comes with an audio CD, though that is problematic since few computers come with CD readers; I hope the authors will make the data available for download as well in the near future.\nThe first part of the book is filled with expressions, phrases, phrase + answer combinations and other materials that you don’t usually find, even in books specialising in colloquial readers. Here is an example of phrases that come with certain responses:\n\nand here are some expressions from their very rich selection, also showing the explanations and how the book makes them very easy to use/pick up:\n\nThe rest of the book is filled with thematically-organised vocab lists (sorted into Arabic / translation / Arabic plural columns). It’s useful to have them all in one place, I suppose, and to know that these are what Jordanians use (as opposed to Lebanese or Syrian speakers of Arabic), but the real value of the book is the expressions and phrases contained in the first half.\n\nJordan Dialect (Memrise Course + Audio)\n\nI include this not because the word selection is particularly amazing but simply because it seems to be the only course on Memrise (the vocabulary-learning platform) that teaches Jordanian dialect alongside the audio files. There are lots of other courses offered, including some by students currently in Amman, but none of them offer audio files as well. There are only 736 words available to learn, but it’s all Jordanian dialect. To my mind, this course is low-hanging fruit. Someone’s made a great selection of vocabulary, most of which you’ll use at some point, so I’d recommend just working your way through it relatively quickly.\n\nThe 101 Most Used Verbs in Spoken Arabic: Jordan & Palestine\n\nThis book is more than just a guide to using verbs in Jordanian Arabic. It’s a repository of sentences and their translations that comes with a CD filled with pronunciation. (As with other books, I really wish CGE would offer the files as digital downloads, since I and most others don’t have a CD reader/drive).\nThe title is actually really deceptive, and I held off getting a copy for far too long because I thought it was just lists of verbs. It is, but at the same time it’s a rich trove of sentences, phrases and associated vocabulary. I’d say this book is pretty much essential for anyone who wants to learn Jordanian Arabic. Take a look at this page, for example, to see the kind of thing 101 Verbs contains:\n\nIt’s a fantastic mix of phrases, sentences, vocabulary and literal translation or explanation. There are also useful appendices and introductory materials about the grammar of the Arabic verb, lists of broken plurals and various indexes so you can look things up as well.\nI spoke with the author of the book (and director of CGE) for the premium edition of ‘Master Arabic’.\n\nBasic Phrasebook - Jordanian Arabic (WikiTravel)\n\nThis is nothing more than a list of useful phrases, but as a way to pick up some essentials, this is a free and easy option.\n\nPre-Departure Arabic Materials (PeaceCorps)\n\nThere are also audio files associated with this document. This is a basic overview of some situations that an English-speaker might encounter in Amman, and offers some basic grammar and vocabulary alongside phrases and cultural explanation. Nothing advanced here, but ideal for total beginners or visitors.\n\nConversational Arabic Quick and Easy: Jordanian Dialect\n\nI’d advise you to stay away from this book. It’s sold on Amazon and seems to be frequently suggested as a ‘suggested purchase’ when you buy anything else relating to Jordanian Arabic. Unfortunately, the contents are not very useful. The book also does not use Arabic script; it is all transliterated. Caveat emptor.\n\n\nThe Arabic Student: Jordanian Arabic\n\nThese are all the posts from ’The Arabic Student’s blog relating to the Jordanian dialect. It was last updated in 2013, but as a resource it’s extremely useful, and he covers a numbers of explanations of phrases, grammar and other interesting features of the dialect. Posts often discuss specific videos (or sections of videos) in detail, so it’s a great place to get a taste for the unique features of the dialect alongside some entertainment and comedy."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#listening-video",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#listening-video",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "Listening / Video",
    "text": "Listening / Video\n\nJordanian Dialect Podcast (Soundcloud)\n\nI co-host this podcast together with Lina Obeidat. We discuss a new topic each week. Show notes include translations of difficult words. The idea behind this podcast was to have some materials available for intermediate students that weren’t too difficult, but at the same time didn’t dumb things down by using too much English and so on. Details on how to subscribe are available here, or just follow us on Soundcloud.\n\nTalk In Arabic\n\nTalk In Arabic is a great resource for those who want spoken materials in the various dialects of the Arab-speaking world. The Levantine Dialect section contains a few dozen videos and audio lessons in Jordanian dialect alongside thematically-organised vocabulary lists. Everything is available as audio files (with transcripts and translations) so this is a useful service. Note that the site isn’t updated that often, so you might find you can make use of all the materials in a month or two.\n\nFemale Show (Roya)\n\nThis is a fun comedy starring comedy superstars, Tima Shomali and Raja’ee Qawwas. I’ve watched the three seasons of this show through from start to finish and learnt a lot in the process. The level is suitable for an intermediate-level speaker. I’d recommend starting at with season 1, episode 1, to give it a try:\n\nUsfuriyya (Roya)\n\nThis is a medical comedy set in Jordan. Think Scrubs, or think someone trying to imitate Scrubs in the Middle East. The Arabic is also at an appropriate level for someone who has a year or two under their belt.\n\nRoya TV\n\nRoya is Jordan’s preeminent contemporary TV channel. They commission most of the new dramas, comedies and other shows coming out of the country. There’s always something interesting to watch here. To watch only their news click here. For their comedy shows, click here. For drama shows, click here. Highly recommended as a way to get exposure to the sound of the dialect.\n\nJordan TV\n\nThis is the YouTube channel for Jordan’s state-run TV station. You can view news broadcasts and more traditional dramas. The content is much more staid in comparison to Roya, but it exposes you to a different style of speech and local culture, so perhaps find a way to intersperse this in with your Roya comedies and dramas.\n\nLangMedia Arabic\n\nThere are a number of different links on this page (to CultureTalk or to their page of Jordanian/Palestinian dialect resources). This is a real treasure trove of materials, much of which is provided with audio, video and full transcripts/translations.\nThere are also some exercises to go through to test your comprehension. Here’s two Jordanians talking about the education system using dialect. Note the transcripts and translations below.\n\nN20 Comedy\n\nMore Jordanian comedy shows. If you watch enough locally-produced TV and comedy you will notice that the same faces show up in many different programmes. Note that N20 has branched out to include other dialects, but it should be fairly easy to distinguish which is which. Lots of material here.\n\nAccents Library (DLIFLC)\n\nSelect Arabic. You’ll be presented with a number of stories read by people from different countries. Jordan is one of them. Compare and contrast with the other dialects. This is more a resource to sharpen your ear. Mother-tongue Arabic speakers grew up among this diversity and so are attuned to the tonal and phrasal differences between dialects. This is a resource that can help you improve this skill on your own.\n\nPhone Conversations\n\nThis is a great database and resource of authentic spoken materials in the form of recorded (and transcribed) phone conversations.\nSelect Arabic and then Levantine. It’ll (mostly) be clear when items relate to Jordan. Each conversation comes with a transcription, translation and study plan. The recordings usually last around ten minutes, so it’s perfect for a quick study session. Use this to get exposure to authentic natural language.\n\nGLOSS (DLIFLC)\n\nGLOSS is a resource developed by the US military. You can select the dialect for which you see materials (Levantine, in this case) and it’ll show you different exercises to complete. There were over 240 unique items as of January 2017.\nOnce you go into the individual items, it states which country the dialect is from. I really like that you can choose whether you want the questions and instructions in Arabic, though the interface itself is really horrible. Here’s one of the exercises where you pair off words to two columns, depending on the meaning:\n\nDespite the issues with the interface, there’s loads of really useful material to work your way through here. This is a great resource.\n\nAamiya Arabic (YouTube)\n\nThis YouTube channel covers a number of dialects, but you can click through to the playlists to choose Jordan-specific materials. These videos aren’t specifically designed for students, but they provide lots of authentic dialogue and dialect appropriate for intermediate students. In this video, for example, sisters Hiba and Lynn talk between themselves about what they’re selling at a market stall:\nTHere’s lots to explore in this YouTube channel, and lots of phrases and unique Jordanian expressions to file away in your personal language notebook. Wade in!\n\nKharabish Comedy\n\nA mix of dialects here, but Kharabish are originally a Jordanian outfit, so lots of their older material uses that. As with earlier comedy channels, you’ll recognise many of the actors and some of the skit premises.\n\nEx in the City\n\nThis is mostly in English, so probably too basic for intermediate students, but the series covers a number of different expressions and culturally-relevant context. The host explains a new expression each episode."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#teachers-italki",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#teachers-italki",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "Teachers / iTalki",
    "text": "Teachers / iTalki\nThere are currently, as of my writing in January 2017, only three tutors from Jordan on iTalki. I’ve only studied with one, so I’ll recommend her highly. You’ll recognise her from the Jordanian dialect podcast that we recently started together. Lina Obeidat works with beginners and intermediate students alike, and makes Skype lessons fun by combining free-flow conversation with a variety of other exercises and challenges along the way. I credit Lina with the lion’s share of improving my spoken Arabic, so I’d highly recommend you work with her to blast through any difficulties you may have."
  },
  {
    "objectID": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#institutes-study-abroad",
    "href": "personal/2017-01-03-everything-you-need-to-study-jordanian-arabic.html#institutes-study-abroad",
    "title": "Everything You Need to Study Jordanian Arabic",
    "section": "Institutes / Study Abroad",
    "text": "Institutes / Study Abroad\nI haven’t studied at either of these institutes, so I can’t attest to the quality of the programmes they offer etc, but I do know that many people in Amman study at these two schools. CGE is founded by Fred Tiedemann, who wrote the essential 101 Verbs I mentioned above and produced many of the videos on their indispensable YouTube channel. They (seem to) take a colloquial-first approach.\nQasid is the other main alternative, and many universities in Europe and the US send their year-abroad language students there for tuition. Both institutes come highly recommended by people who’ve studied there, but as always with study-abroad programmes, a lot depends on what you put into them.\n[NOTE: This list of materials is adapted and drawn from the premium materials available alongside my new book, ‘Master Arabic’. For the full resource guide covering other dialects as well as tried-and-tested advice from experts and others who’ve made it to an advanced level, get the premium edition.]"
  },
  {
    "objectID": "personal/2017-01-02-lynne-kelly-podcast.html",
    "href": "personal/2017-01-02-lynne-kelly-podcast.html",
    "title": "The ways memory skills augment your life",
    "section": "",
    "text": "I released a new episode of Sources and Methods today. This week I spoke with Lynne Kelly, author of ‘The Memory Code: The Secrets of Stonehenge, Easter Island and Other Ancient Monuments’, a fascinating exploration of the intersection between history, archaeological sleuthing and memory techniques. We delved into the contents of her book as well as the practical applications she found for these ancient skills.\nI’ve been fascinated with the work that Lynne has been doing on the history of memory and her own personal memory ‘experiments’. If you need some inspiration as to things you might learn (or ways you might learn them), head over to her blog and the page entitled ‘My 33 Memory Experiments’. She generously includes photos and explanations of how the various systems work. Her latest groups places them in even further context. As someone who’s been following her memory experiments for a while, I found the book really useful to understand why she was picking a certain technique over another.\nShe’s about to start a new project involving memory skills, education and schools (she discusses that in the podcast around the 34-minute mark) and I look forward to reading about the outcome."
  },
  {
    "objectID": "personal/2016-12-30-master-arabic-preorder.html",
    "href": "personal/2016-12-30-master-arabic-preorder.html",
    "title": "Pre-Order Now: ‘Master Arabic’",
    "section": "",
    "text": "We’re getting closer to finalising the materials for our new book and resource guide, ‘Master Arabic’. The full description of the book reads as follows:\n\nThis will be my thirteenth year studying the Arabic language. I’ve tried everything: university programmes, intensive courses, self-study and more.\n\n\nMy Arabic level remained at the intermediate level for years, a learning plateau, and I felt like I wasn’t progressing. I spent the last three years getting to the advanced level and I’m now (finally) able to communicate comfortably in a dialect and tackling contemporary fiction.\n\n\nI’ve decided to write up some of the techniques, attitudes and resources that are available to the intermediate student of Arabic. This is the condensed product of years of experimentation. This book contains advice on how to improve your reading, writing, speaking and listening skills and also offers suggestions for resources you can use to tackle each particular skill.\n\n\nThe premium package includes over a dozen interviews with leading educators, Arabists and students who impart their secrets on how to pass through to advanced materials. It also includes life-time access to an online resource database of materials to study Arabic.\n\nAs a way of saying thank you to all those who subscribed to the mailing list and to ring in the new year, ‘Master Arabic’ is now available for pre-order. A discount code is provided below that will allow you to purchase the book for 10% off.\nThere are two editions:\n\nBasic Edition – You’ll get a copy of the e-book in PDF format. You will also receive access to an exclusive curated guide to materials, content and study aids for students of Arabic.\nPremium Edition – You’ll get everything in the basic edition, plus multiple formats of the ebook to suit various e-readers. You will also get access to over a dozen interviews with students, teachers and experts from Egypt, Jordan and the United States who have unique insights into learning Arabic.\n\nFeel free to email if you have any questions about the book, the materials or have a topic that you really feel we should cover in the book.\nUSE DISCOUNT CODE ‘preorderthankyou’ for 10% off. This one-off discount will no longer be offered once the book is released at the end of January."
  },
  {
    "objectID": "personal/2016-12-28-knot4-empathy-tech-climate.html",
    "href": "personal/2016-12-28-knot4-empathy-tech-climate.html",
    "title": "Knot 4: Empathy, Tech Scepticism and Climate Change",
    "section": "",
    "text": "I completed a major stage in a big project this week. Emerging from the haze, I had a bit more time to read for pleasure / curiosity. This helps explain the older selections of article that follow, and the number of books I managed to get through. I made steady progress with my language-learning coding side-project (about which more soon), and recorded three new podcast episodes to be released in the next few weeks.\nArticles\n\nNick Romeo - “The Chatbot Will See You Now” (New Yorker)\n\nThis is an article about how technology might help provide some kind of therapeutic intervention at scale. A specific case explored in the piece is that of Syrian refugees who use the app to help deal with things they experienced as a result of the ongoing conflict (or as a result of refugee life). Results on efficacy and so on are still unknown, but it’s at least an intriguing idea.\n\nMark A. Heberle - “Must Everyone Write English?” (Claremont Review of Books)\n\nHeberle explores how translation into English is both a blessing and a curse. It brings about this kind of ‘global literature’ and awareness of other cultures for English-reading audiences, but it also changes the kinds of expression that takes place. It’s a challenge to the English-first trend that will leave us so much poorer over time.\n\nLisa Miller - “An Experiment in Empathy” (New York Magazine)\n\nThis piece is accompanied by a video (here). The video is worth watching, but both show what happens when various parties from ‘opposing sides’ of a contentious issue (gun laws in the USA) come together to try to understand and/or empathise with each other. I’ve long talked about the need for finding a place for empathy in how we understand other people, other places, other contexts, but this piece pushes back a little against that logic. Paul Bloom has also recently published a book (‘Against Empathy’) which argues against the use of empathy as a primary means of making decisions. I will be reading that soon to see how it meshes with my current understanding.\n\nMichael Knox Beran - “In Defense of Memorization” (City Journal)\n\nA common charge levelled at those who study ways to improve their memories is that there is no need to learn facts since everything is available online. This article shows how memory fits into different kinds of education and learning and how it enriches your mental landscape.\n\nCal Newport - “Some Thoughts on Transitioning to Digital Minimalism” (CalNewport.com)\n\nSensible advice on how to start thinking about reshaping your digital consumption habits. Newport’s last book, ‘Deep Work’, gets into all of this in much more detail, but it’s a useful entry point if you’re considering making a change.\n\nTristan Harris - “How Technology Hijacks People’s Minds — from a Magician and Google’s Design Ethicist” (The Startup)\n\nA useful reminder of the ways that our oft-used tech-enabled gateways and services are highly addictive, and designed to be so. This, to my mind, should make us think and pause and consider ways we can be less dependent.\n\nJonathan Stray - “The Dark Clouds of Financial Cryptography” (JonathanStray.com)\n\nI’m simultaneously fascinated by the various new digital currencies that have emerged in the last year or two but also overwhelmed whenever I try to understand how they work. This article by Jonathan Stray brought me a little closer to learning about the ‘how’, but also some of the ‘why’ and reasons for possible uncertainty and concern.\n\nTara Duggan - “Seafood’s New Normal” (San Francisco Chronicle)\n\nA richly reported story on how climate change and fishing practices are changing how seafood reaches our plates as consumers. It’s told through a very specific lens (that of California and the American west coast) which helps give the piece its power.\n\nMark Schapiro - “The Unique Burden of Covering Climate Change in the Middle East” (Pacific Standard)\n\nA really nice overview of the challenges that local journalists face in the Middle East when trying to cover climate change. Schapiro tells his story by way of a workshop in Amman hosted by the Arab Reporters for Investigative Journalism. We get to meet reporters from different countries. This was a great way to get at the story, and I wish there were more pieces like this.\n\nDavid Schenker - “Jordan’s Economy Was Always Shaky. The Refugee Crisis Has Only Made Things Worse.” (The Tower)\n\nI understand next to nothing about how economies and financial systems work, but this article on Jordan’s economy seemed to make sense. Things were always difficult, but now is especially so.\n\nDonna Abu-Nasr - “Saudis Are Trying to Figure Out How the Post-Oil Era Works” (Bloomberg)\n\nLots has been written on the Saudi pivot, how they want to be less dependent on oil and reinvent the Kingdom’s role in the Middle East and the world at large. This was the first piece I’ve read that laid out some of the confusion and contradiction as to what exactly that means.\nBooks\nI read Irving’s ‘Guide to the Good Life’, a useful and practical guide to stoicism, a Greco-Roman philosophy that is undergoing a revival in certain circles these days. Harrison’s ‘A Powerful Mind: The Self-Education of George Washington’ was a slightly dry but compelling account of the role of reading in the life and career of George Washington. I read Deci’s ‘Why We Do What We Do’ on the strength of Deb Chachra’s recommendation during her Sources and Methods podcast interview. Deci argues that internal motivation beats external motivation every time; I’ve started employing some of the suggestions in my language coaching work as a result. I also finished Yassin-Kassab and Al-Shami’s excellent ‘Burning Country: Syrians in Revolution and War’ (my longer review here).\nRequest\nIf you’re still reading, please drop me a note if you have a spare moment. I see the stats on how many people open these ‘Knots’ link collections but I’m unsure as to whether they’re useful. Let me know if you find them useful, and/or what else you’d like me to write about."
  },
  {
    "objectID": "personal/2016-12-25-syria-revolution-first-draft.html",
    "href": "personal/2016-12-25-syria-revolution-first-draft.html",
    "title": "Syria’s Revolution: The First Draft",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“351”] A photo taken back when I lived in Damascus [/caption]\nI haven’t been following events in Syria that closely. ‘Burning Country: Syrians in Revolution and War’ (by Robin Yassin-Kassab, Leila Al-Shami) is only one among several books that have been written in the midst of the ongoing conflict, but I’d heard consistently good things about it since its release at the beginning of 2016. It doesn’t disappoint. I started it yesterday, lulled by the conclusion of a large project and a thousand pictures of Christmas dinner preparations on twitter. I woke up this morning and finished the rest.\nBurning Country is compellingly written, not only on a structural level but also on account of its interspersed interview and other oral history-type testimony. This is the book’s primary strength, I felt. It’s hard enough to write accurate and compelling history after the fact. Doing so while events are still unfolding makes the job even harder. Thankfully, Burning Country delivers the goods.\nAfter introducing Syria and Syrians as if to set the stage, Yassin-Kassab and al-Shami launch into an account of the Syrian uprising’s early days, told through a diversity of sources from a variety of backgrounds. They make a strong case for the initial moments of the revolution as being less co-opted by one group or another, even as later chapters show how a variety of forces pushed and pulled that initial impulse in unintended directions. I learnt a lot from the accounts of initial organisational strategies, the way different groups responded to similar kinds of threats from the state and how the logic of violent escalation started to take on more prominence. The chapter on cultural shifts (or expressions) brought about by the revolution (chapter eight) was also excellent. The end of the book falls a little short, if only because the conflict is ongoing and you had the sense that it is an unfinished project.\nAs a one-stop shop introduction and opening-out to some important parts of Syrian society, history and current affairs, Burning Country is well worth your time. The book ends with a note on thinking about and understanding Syria:\n\n“In order to truly think globally – rather than to hide from thought behind clumsy globalising paradigms – it is necessary to act locally. We ask the reader, rather than applying the usual grand narratives, to attend to voices from the ground.”\n\nI hope we get to read other histories of Syria (and other countries) that employ the same logic."
  },
  {
    "objectID": "personal/2016-12-21-two-charities-support.html",
    "href": "personal/2016-12-21-two-charities-support.html",
    "title": "Two Charities Needing Your Support",
    "section": "",
    "text": "If you do any kind of end-of-year charitable donations, now would be a good time to support one or both of these organisations.\nCAGE works in the UK to advocate on behalf of communities and individuals affected by the so-called War on Terror. They occupy a niche space among UK-based NGOs and the work they do is unique and important. Contribute to their campaign here.\nThe Kabul Mobile Mini-Circus for Children is a group that I’ve fundraised for in the past. I visited them a few years back in Kabul and it was one of the happiest places I’ve been to. The children seemed to love being there, exploring what they could do through learning new skills, working with each other and so on. They’re building a new training / social / educational centre, and you can support their campaign here."
  },
  {
    "objectID": "personal/2016-12-21-best-books-2016.html",
    "href": "personal/2016-12-21-best-books-2016.html",
    "title": "The Best Books I Read in 2016",
    "section": "",
    "text": "Compared to 2015, this year was a relatively slow year. I decided to reduce the number of books that I wanted to read after it became clear to me just how much work I’d have to put in to finish my PhD dissertation.\nLast year I wrote that I wanted half of the books I read in 2016 to be fiction. I also said that I didn’t want to buy any new books. I failed firmly and thoroughly on both goals. I only read nine works of fiction this year, and I bought many books. Part of the reason for less fiction was due to a realignment of my broader goals (less writing of fiction in my daily life and more writing of computer/programming code). Nevertheless, I want to be reading more fiction as a regular habit. I’ll probably set a Beeminder goal for 2017 to facilitate this.\nTwo works of fiction did stand out among the 76 (as of today) books I did manage to read. Hanya Yanagihara’s ‘A Little Life’ was an unrelentingly miserable book. At 734 pages, it was also very long. It was a significant time investment, but the craft of the writing kept me reading. I avoided reading reviews when this came out, but was aware that many people were saying positive things. The events that take place in this book are traumatic, to say the least, and the author has been criticised for writing ‘torture porn’ by a number of readers. So take this or leave it. It’s incredibly powerful and beautifully written.\n2016 was also my year to jump on the Elena Ferrante bandwagon. I read the first two volumes in a series of four. I enjoyed the first, ‘My Brilliant Friend’, more than the second, though that was perhaps on account of the pleasure on discovering a new writer (to me) who told engaging and emotionally satisfying stories. This book didn’t send me on a nostalgia-infused journey as it seems to have done with others, but it was certainly one of the books I savoured reading most this year. “Deliciously compelling,” I wrote immediately after finishing.\nCal Newport’s Deep Work was what spurred me to finish my PhD in an intense three-month period at the beginning of the year. I think anyone doing some kind of freelance work, or remote work, or work where you aren’t immediately / directly under the supervision of someone else would benefit from reading this book. Newport is occasionally a bit preachy, but don’t let that distract you from his important thoughts on quality of work and production. (Some related thoughts in an old blogpost here.)\nAustin Kleon’s ‘Show Your Work’ (original blog here) is responsible for the uptick in posts on this blog. (I even have a Beeminder goal to keep me honest.) This was a quick but rich overview of ways in which sharing ideas, work and other moments where you learn something can be useful for both the author and his audience. Lots of practical ideas, most of which could be boiled down to what happens when you keep a blog regularly updated. Highly recommended to anyone who thinks for a living.\n‘The Inner Game of Tennis’ by W. Timothy Gallwey is my final pick, if only because I really need to absorb some of its lessons. Gallwey makes the point that conflict between competing ‘voices’ in our minds is often the source of self-sabotage. I wrote a longer summary here in case you want to learn more.\nThe year is 97% finished, and arbitrary as the passing of time is, I remain mindful of the fact that there aren’t that many days. I also have been trying to hold the paradox of things mattering (but at the same time not mattering) in my head.\nSo 2017: more fiction, (probably) more blogging, (certainly) more coding and maybe a return to Afghanistan-related topics."
  },
  {
    "objectID": "personal/2016-12-19-devonthink-resurgent.html",
    "href": "personal/2016-12-19-devonthink-resurgent.html",
    "title": "DevonThink Resurgent",
    "section": "",
    "text": "There has never been a better time to get into DevonThink and Tinderbox. Winterfest 2016 is on, and you can get 25% reductions on both those apps, as well as a number of other really useful pieces of software like Scrivener, TaskPaper, Bookends, Scapple and PDFPen.\nIf you’re unsure if DevonThink is something you’d be interested in, they have a 150-hours-of-use free trial for all their different apps. MacPowerUsers podcast just released a useful overview of the current state of the app — an interview with Stuart Ingram. ScreenCastsOnline also published the first part of a trilogy of video learning materials on DevonThink.\nIf you’re a Mac user who is perhaps uncomfortable with Evernote’s privacy policies or just seeking to get more out of the data you’ve stored on your hard drive, give DevonThink a try."
  },
  {
    "objectID": "personal/2016-12-17-knot3-encryption-race-tunnels.html",
    "href": "personal/2016-12-17-knot3-encryption-race-tunnels.html",
    "title": "Knot 3: Encryption, Race and Tunnels",
    "section": "",
    "text": "More focus this week on finishing a big project, so less time for books or outside reading, but nevertheless some articles to recommend.\nArticles\n\nUrsula Lindsey - “Funds Cut for Prominent Arabic Language Program” (Al-Fanar Media)\n\nAn extremely short-sighted decision. The CASA programme has been a really important source of support over its forty-nine years. This is a decision that affects American students of Arabic, but we shouldn’t forget that it’s symptomatic of how language studies are treated elsewhere.\n\nRosalind Adams - “Intake: Locked On The Psych Ward” (Buzzfeed)\n\nAnyone who still says Buzzfeed doesn’t do important reporting just isn’t reading. This piece looks at the way a for-profit system works to the disadvantage of patients’ welfare and the healthcare system in general. A must-read for an American audience, and for Europeans / Brits as a portent of what probably is headed our way.\n\nBron Gondwana - “Why we don’t offer PGP” (Fastmail Blog)\n\nPGP has been much discussed (more than usual, at any rate) this past week. This post offers a useful overview of some of the things that PGP can help protect against, versus some of the places where trust in what it can do are misplaced. Pair this article with Bjarni Rúnar’s “Too Cool for PGP” to cover your bases.\n\nAnnalee Newitz - “Evernote apologizes for its new privacy policy” (Ars Technica)\n\nA useful corrective from Ars Technica (one of the best places to go for informed reporting on science and technology) about Evernote’s recent policy changes relating to privacy. The important point to take away from all this is that Evernote probably isn’t where you want to be storing your notes if you care about your privacy. Try DevonThink instead for a far saner approach to encryption and storage.\n\nAmal el-Mohtar - “‘Iraq + 100’ Is Painful, But Don’t Look Away” (NPR)\n\nThis review of a new collection of stories seems (from the review, at any rate) like it is worth reading. Imaginative and creative responses to events on this scale are, I think, the kinds of things that will ultimately survive far longer than the political analysis and commentary that so far has dominated the discussion of what happened in Iraq. I look forward to reading this book.\n\nRyan Holliday - “Want to Really Make America Great Again? Stop Reading the News.” (Observer)\n\nYou can ignore the focus on US politics and still take away the broader argument, that following the day-to-day news cycle is (for the broad majority of us) unnecessary and probably actually detrimental to our health and ability to do work that really matters (to us or others). A useful reminder.\n\nLaura Reiley - “Farm to Table: At Tampa Bay farm-to-table restaurants, you’re being fed fiction” (Tampa Bay Times)\n\nA great piece of investigative work, Reiley shows how a lot of the promises of the farm-to-table movement in California are false claims. This piece is the first in a series that is worth reading in its entirety.\n\nRoger Cohen - “Broken Men in Paradise” (New York Times)\n\nThis is an important reminder of the realities of life for those refugees refused entry to Australia following harrowing journeys across the seas. Authorities in the places they are sent to operate with seemingly little restraint or care for their wellbeing.\n\nTa-Nehisi Coates — “My President Was Black: A history of the first African American White House—and of what came next” (The Atlantic)\n\nThis piece seems to capture a moment. Based on several interviews with Obama as well as his usual command of the context that explains this moment, this is a (long) must-read.\n\nGideon Lewis-Kraus - “The Great A.I. Awakening” (New York Times)\n\nImportant from perhaps a different angle, this is a long overview of the ways so-called artificial intelligence is poised to change our world. It views the transformation through the lens of developments in Google’s online translation service. It offers context and explanation to two different approaches to machine intelligence and the different algorithmic decisions being made. If you’re interested in technology and the ways that computers will continue to be embedded in our daily lives, you won’t want to miss this one.\nBooks\nThis week I read Lynne Kelly’s “The Memory Code: The Secrets of Stonehenge, Easter Island and Other Ancient Monuments” in preparation for an upcoming podcast episode interview. Dr Kelly makes a convincing case for how old archeological sites (like Stonehenge) were probably used as forms of memory aids to recall information important for local communities. The book is best accompanied by a glance at a truly fascinating series of memory experiments that she’s been working on. I also read Rebecca Solnit’s “Men Explain Things to Me”, a very useful and sobering reminder of the ways that society works against women, whether directly through violence or other perhaps less visible means.\nFilms / TV\nI wrote yesterday about “Into Eternity” (trailer here), one of my favourite documentaries of the past year. It’s filmed with an idiosyncratic approach to detail and atmosphere. I am ignorant about where Finnish director Michael Madsen gets his influences from, but I certainly look forward to seeing more. Just today I saw another documentary, “13th”, which focuses on the ways that racial inequality has manifested itself in the United States, specifically in the prison system. This is a powerful statement, particularly for American viewers, I imagine, and it made me think about the ways that similar trends are to be found back in Europe."
  },
  {
    "objectID": "personal/2016-12-15-broken-pots.html",
    "href": "personal/2016-12-15-broken-pots.html",
    "title": "Broken Pots",
    "section": "",
    "text": "Kintsugi or kintsukuroi is the Japanese art of rebuilding a broken pot or other ceramic in a way so as to make it more beautiful than it was before. This is sometimes done with gold or silver, or other materials to join the broken pieces together.\nThere are some great images of this if you search Google Images.\nAnab Jan also has written something on the way that repairing bikes with a kintsugi approach means the history of repair remains visible in the bike even after the repair is finished."
  },
  {
    "objectID": "personal/2016-12-11-daddybot.html",
    "href": "personal/2016-12-11-daddybot.html",
    "title": "Daddybot",
    "section": "",
    "text": "I just finished listening to a fascinating episode of Nerds on Draft podcast. Gabe and Jeff, the usual cohosts, spoke with Erik Hess about how he set up a telepresence robot (as pictured) in his house while he was deployed halfway around the world.\nIt’s worth listening to the whole podcast to get a sense of how this worked out. Erik talks of the initial weirdness of being present in his house in the US while actually being in Qatar. It was fascinating to learn how quickly that turned into the new baseline — i.e. to his children, it became as if he was actually there.\nFor families or situations where parents have to be away from children on any kind of regular basis — or whatever particular configuration your life takes — it seems that this is a really intriguing idea. It certainly isn’t cheap, but it’s also impossible to get back that time with your children either."
  },
  {
    "objectID": "personal/2016-12-09-knot2-translation-as-trauma.html",
    "href": "personal/2016-12-09-knot2-translation-as-trauma.html",
    "title": "Knot 2: Translation as Trauma, Taxis and Artificial Intelligence",
    "section": "",
    "text": "This week I fell back in love with a piece of software (DevonThink Pro Office) while reorganising and refreshing two dozen archival databases. I’ve been busy putting the final touches on The Taliban Reader and I’ve made new strides of progress on the slackline."
  },
  {
    "objectID": "personal/2016-12-09-knot2-translation-as-trauma.html#articles",
    "href": "personal/2016-12-09-knot2-translation-as-trauma.html#articles",
    "title": "Knot 2: Translation as Trauma, Taxis and Artificial Intelligence",
    "section": "Articles",
    "text": "Articles\n\nLina Mounzer - “War in Translation: Giving Voice to\nthe Women of Syria” (Lithub)\n\nA deeply affecting account of the author’s work as a translator of stories from Syria, how language mediates and obfuscates the terrifying realities of daily life, and how even translation and re-communication gives rise to secondary trauma of a kind. If you read nothing else this week, make it this article.\n\nHamzah Nassif - “Understanding Amman’s Yellow Taxi Dilemma: a Different Take on the Government’s Ongoing Battle With Ride-Hailing Apps” (The Black Iris)\n\nI wrote about Amman’s taxis in my Arabic-language newsletter, and this (slightly old) piece helps answer some of my questions as to why taxi drivers in Jordan’s capital behave as they do. It also makes a strong case for changing the status quo. Taxi drivers seem barely able to make a living under current circumstances, and passengers aren’t getting much of a service.\n\nDavid Meyer - “The dark side of .io: How the U.K. is making web domain profits from a shady Cold War land deal” (Gigaom)\n\nAs someone who has just bought a couple of new .io domains, I was unsettled to learn the backstory, how something dating back to the early nineteenth century’s colonial expansion (in this case, to the Chagos islands) has a direct descendent in the modern age. I’m most likely not going to return the domains I bought, but I probably won’t buy anything new from the .io set.\n\nAndrew Ng - “What Artificial Intelligence Can and Can’t Do Right Now” (Harvard Business Review)\n\nThere’s nothing especially new in this overview piece by one of the leaders in the field of artificial intelligence, both in terms of innovation and education. But it’s nevertheless a useful summary of some of the unique features of AI and machine learning, as well as some of the things it is less good at.\n\nRichard Benton - “Language hacking ≠ language love” (Loving Languages)\n\nRichard’s posts are excellent at returning us to the reasons why it’s worth learning languages — to connect, to be useful perhaps, and not to simply chalk up notches on one’s belt. This recent posts is a challenge to the thankfully-less-omnipresent online polyglot community. Recommended if you’ve ever studied more than one second-language."
  },
  {
    "objectID": "personal/2016-12-09-knot2-translation-as-trauma.html#books",
    "href": "personal/2016-12-09-knot2-translation-as-trauma.html#books",
    "title": "Knot 2: Translation as Trauma, Taxis and Artificial Intelligence",
    "section": "Books",
    "text": "Books\nThis week, I reread Rework by Jason Fried and David Heinemeier Hansson. Looking into my Goodreads list, I noted that I gave it a two-star rating when I first read it a few years back. Now, perhaps I was in a better place to absorb some of the lessons, but I gave it four stars. I’ll be writing a longer review for the blog here of some of the more contrarian or unexpected lessons. I’m currently reading Rebecca Solnit’s Men Explain Things to Me alongside Mark Manson’s The Subtle Art of Not Giving a Fck* (as audiobook)."
  },
  {
    "objectID": "personal/2016-12-09-knot2-translation-as-trauma.html#films-tv",
    "href": "personal/2016-12-09-knot2-translation-as-trauma.html#films-tv",
    "title": "Knot 2: Translation as Trauma, Taxis and Artificial Intelligence",
    "section": "Films / TV",
    "text": "Films / TV\nI’m working my way through Channel 4’s Humans, albeit painfully slowly. It was recommended on a recent podcast as a possible vision for what a future with humanoid AI machines might look like: not as different as we might imagine, but still different enough to provoke some real societal challenges."
  },
  {
    "objectID": "personal/2016-12-08-nutritional-density.html",
    "href": "personal/2016-12-08-nutritional-density.html",
    "title": "Nutritional Density",
    "section": "",
    "text": "I wrote a few weeks back about how fundamentals are often superior to hacks. It seems it’s a useful mental model to employ as I’ve encountered various other expressions of more or less the same thing.\nIn the context of food and nutrition science, the call for nutritional density has become commonplace. This either takes the form of warnings against eating ‘empty’ food — i.e. foods with little broad-spectrum nutritional value even if they pack a caloric punch — or at the other end it is the Kale brigade.\nI think there are a lot worse things we could do than strive for nutritional density in dietary choices, and I’ve also been thinking more about how it probably applies to the kinds of information we consume. This could mean choosing books that are perhaps older, but are less ephemeral or have stood the test of time. For language-learning, perhaps it means really diving deep in one text to really suck the marrow from each and every sentence (if you’ll excuse the carnivorous image). I’ve been slowly weaning myself off the output of social and non-social media for this very reason, though perhaps now I’ve only started to gain the vocabulary to describe what I was doing. In any case, food for thought for a Thursday evening."
  },
  {
    "objectID": "personal/2016-12-02-memory-course-positive-feedback.html",
    "href": "personal/2016-12-02-memory-course-positive-feedback.html",
    "title": "Positive feedback on my Memory Skills course",
    "section": "",
    "text": "I haven’t written much on this blog about the memory course I wrote and launched a few months back. Its target audience is Muslims hoping to learn the non-canonical Asmaa al-Husnaa or 99 Names of God.\nThe point of the course isn’t so much to learn the 99 Names. They have their uses, to be sure, but I chose them mainly as a proxy to teach some basic memory techniques and skills that will have relevance and use throughout your life.\nThe course of emails lasts for a week, but as part of the package you get:\n\ndaily emails containing the course’s core lessons\n13 custom worksheets and handouts to practice the daily email lessons\na summary sheet at the end of the week with an overview of the main principles of the course\nrecommendations for ways to apply the principles in your daily lives and in other studies\nsupport through the Incremental Elephant community on the forum\n\nI haven’t done that much marketing or outreach except what has been easily to hand, but I’ve been pleasantly surprised by the numbers of users trickling in bit by bit. Some recent feedback from users:\n\n“Thank you for setting up this helpful course! It’s been fascinating to learn the memory techniques and tips you’ve offered.”\n\nD.N., Singapore\n\n“Overall, I’m quite pleased that this course gave me the incentive to memorise the names properly and I also learnt other interesting things (can see now how this could easily become an obsession!). The emails were readable and contained about the right amount of information to make it doable in one week. Your didn’t make any grandiose claims and the price was very reasonable. Jazakallahu kheyra. I’ll recommend this to others.”R.H., United Kingdom\n\nR.H., United Kingdom\nI’ve also been pleased how those who finish the course have gone on to develop interests in more advanced memory techniques beyond the foundations contained in the email lessons.\nIf you’ve ever been interested in having a better memory, being able to learn longer lists and complex ideas for school or daily life, head on over to Incremental Elephant and give it a try!"
  },
  {
    "objectID": "personal/2016-12-02-himal-article-2014.html",
    "href": "personal/2016-12-02-himal-article-2014.html",
    "title": "Reading the Taliban: Himal Magazine article",
    "section": "",
    "text": "An article I wrote for Himal Magazine back in 2014 has just been released on their website in a digital format. It’s a bit dated, and there have since been a number of significant developments in terms of the availability of primary sources, but I wouldn’t change the overall shape of the argument. The nub of what I tried to say was this:\n\n“These sources allow for a far fuller and rounded portrayal of the Taliban, both as individuals and as a movement. It is only through understanding the viewpoints of those in senior leadership positions – as expressed in statements, poems or internal memos – that one can arrive at an understanding of why certain decisions were taken, and get more of a sense of the Taliban’s social identity. It is not enough to view the Taliban through an Islamist lens, or a Pashtun lens for that matter. From the 1980s onwards, the Taliban’s identity was subject to change, with different groups and individuals following different paths in the ever-changing environment the movement found itself in. These processes continued after 9/11, and a nuanced and detailed understanding of the evolution of the Taliban and their identity will prove to be a valuable and instructive exercise.”\n\nThis also happens to be the justification for my current work finishing up The Taliban Reader, a sourcebook and reference tool which will hit bookshelves in the spring. For the full Himal article, go here."
  },
  {
    "objectID": "personal/2016-11-29-arrival-faces.html",
    "href": "personal/2016-11-29-arrival-faces.html",
    "title": "Looking the Wrong Way: Faces in ‘Arrival’",
    "section": "",
    "text": "In Denis Villeneuve’s excellent new film, Arrival, suspense is handled more eloquently than is the norm. I won’t spoil the plot for those who still plan on catching this in the cinema, but it isn’t revealing much to say that this is a film with contact with extraterrestrial beings and spaceships. That much you can even figure out from the poster.\nWe’ve all seen films where this is handled bluntly with brute force: masses of computer-generated effects, amp up the soundtrack and throw in a few explosions for good measure. (Exhibit A: Independence Day).\nArrival chooses to reveal its cards not through directly witnessing the spectacle itself, but rather by focusing on the faces of those who bear witness to what is going on. To be more specific, for much of the significant moments in the film, the camera focuses on Amy Adams’ face. It probably only works because she’s a great actor, but it just goes to show that sometimes subtlety and doing the opposite of what convention dictates can go a long way in making an impression, even in a big-budget Hollywood film.\n(An unrelated note: if you enjoy learning or thinking about language(s), you’ll probably want to check this film out, and/or possibly the short story on which it is based, Story of Your Life by Ted Chiang.)"
  },
  {
    "objectID": "personal/2016-11-22-learning-a-language-try-a-new-approach.html",
    "href": "personal/2016-11-22-learning-a-language-try-a-new-approach.html",
    "title": "Learning a Language? Try a New Approach",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“700”] Image credit: Alina Daniker [/caption]\nAside from my mother tongue (English, sort of), I’ve been studying and thinking about studying languages for twenty-five years. Back at school, I studied Ancient Greek, Latin, French and German. Since leaving, I’ve tackled (to varying degrees) Arabic, Dutch, Farsi / Dari, Pashto, Russian, Toki Pona, Turkish, Urdu and Japanese. If there’s one thread that connects it all, it is this: variety.\nEach language brings its own challenges, and each language is learnt amidst a unique set of circumstances. The way I studied Latin — in a classroom, two or three times a week, with tests and homework over a period of several years — is probably irreproducible for me. Each language that I’ve studied came, therefore, with its own context.\nIgnore your own particular context at your peril.\nHere are some of the approaches I’ve tried:\n\nvocabulary-heavy approach — learning lots of words first, usually from a list of most frequent terms\ngrammar-heavy approach — skipping the vocabulary and focusing on adding words later on once the main structures are present and solid\nspeak from day one — I tried this with Japanese, where the emphasis is on talking to people (in my case, various iTalki teachers) even though it’s still too early to be having long conversations\nuniversity-degree — I have a BA in Arabic and Farsi from the School of Oriental and African Studies in London (UK)\nintensive immersion-heavy language schools — I’ve done this for Arabic and German, both summer programmes\nlearning from textbooks\nlearning without any textbooks — when I started Pashto, there were no decent learning materials available, so I mostly learned through speaking to Afghan friends in Kandahar\nlearning online via a computer-driven course — I’ve studied French, German and Turkish via Duolingo\nwriting lots — I use Lang-8 to get corrections on materials\nspeaking lots — iTalki is the best option for this (cheap lessons, usually great teachers / conversation partners)\nlistening to lots / watching lots — I’ve used Beeminder to set myself input / consumption goals for most of my recent languages\nlearning lists of key phrases — this helped a lot with Japanese at the beginning\nlearning using listen-and-repeat audio courses — Michel Thomas and Pimsleur are the best known and gold-standard for this kind of thing. If the language you’re learning is available for either of those, it’s probably worth just doing the entire course before you start your formal study.\nlearning from friends — just talking to people with whom you share interests or activities is often a great way to pick things up along the way. You’ll usually find that (especially with obscure languages) people are enthusiastic to help when they learn you are interested in their language.\nlearning through music — in a previous life, I was heavily invested in a career in music(ology) and I mostly learned German as a way of being able to read more about Wagner.\ntravel — there’s nothing like the motivation that comes from knowing you’re travelling to a new country. For many years I would even try not to travel to countries where I didn’t at least speak the basics of their language.\nmeta-study — I’ve studied language learning techniques for a little over ten years. I wish I’d come to read more about these things much earlier. It can seem like a waste of time, especially if you’re only studying one language, but taking a bit of time to read about the most efficient techniques and, increasingly, technologies is time well spent.\n\nI’m sure I’m forgetting some. (I’ll update as and when I remember more). And note that not all of these were successes. The more I’ve tried, the more I’ve realised what does and doesn’t work for me. That last bit is important. There is no one method that works for everyone. You have to figure out what works for you.\nI’ve had the opportunity to try so many things out because I’ve been studying languages for a decent number of years, but also because my circumstances have been changing as I’ve grown older, left school and so on.\nIf you’re learning a language at the moment, make sure you don’t allow yourself to stagnate. Sticking to things that worked in the past may mean you’re missing out. So try something new. (Let me know how it works out for you.)"
  },
  {
    "objectID": "personal/2016-11-17-encrypt-your-dropbox-files.html",
    "href": "personal/2016-11-17-encrypt-your-dropbox-files.html",
    "title": "Encrypt Your Dropbox Files",
    "section": "",
    "text": "Much has been written about securing your digital life since the conclusion of the US election. (This was one of the better posts, I thought.) It’s worth doing this regardless of whatever is going on in the politics of country x or y. If the country is big enough, it’s going to want to suck your digital life up into its archives one way or another. So why make it easy for them?\nEncrypting your Dropbox is one of the low-hanging fruit when it comes to securing your digital life. Most of us store various files in the Dropbox cloud, and they’re just sitting there unencrypted unless you take specific action. Moreover, Dropbox has been hacked before (more than once).\nFortunately, Boxcryptor exists to package everything up in an encrypted form. It’s a paid service, and worth every cent. Sign up by clicking here."
  },
  {
    "objectID": "personal/2016-11-12-music-sound-technology.html",
    "href": "personal/2016-11-12-music-sound-technology.html",
    "title": "Music, Sound & Technology",
    "section": "",
    "text": "Steven Johnson, the author of the fantastic book Where Good Ideas Come From, has been releasing podcasts episodes in advance of the publication of a new book. Wonderland: How Play Made the Modern World seems to cover the relationship between innovation and ‘play’, ideas he has worked to explore in previous books. The podcast series offers a preview of these ideas, engagingly produced and narrated through a series of vignettes and smaller stories.\nEpisode 3 (“Strange Loops and Circuit Benders (Or, How New Music Comes from Broken Machines)”) was an exploration of sound, how our sound universe has been changing alongside technological developments and cultural evolution. Johnson talks to Alex Ross (of The Rest is Noise fame) about how new sounds started to enter into the sonic vocabulary of musicians and creators. The other episodes are interesting, too. If you’re interested in music and/or technology, you might want to check this one out."
  },
  {
    "objectID": "personal/2016-11-05-python-side-project-approach.html",
    "href": "personal/2016-11-05-python-side-project-approach.html",
    "title": "Python Side-Project: Approach",
    "section": "",
    "text": "Since finishing the Udacity IPND I have been drawing up plans to start practical work on a specific side-project. I won’t go into too many of the details for now — it’ll be more fun to release it as a surprise — but I wanted to take a note of some of my evolving thinking on my approach.\nI had wanted a project that combines the various skills I’d learned on the Udacity course. Thus: HTML, CSS, Python, SQL & databases. My idea is a website that will have a python/database back-end. I want to code it all from scratch. Much of my thinking about how to break it down into smaller chunks so far was focused on coming up with some sort of basic prototype that showcased the interactions between these various structures.\nI’ve recently realised, however, that it makes a lot more sense to just code the entire project in Python as a standalone programme and to think about linking this functionality into a website once the Python prototype is done.\nThis simplifies my life a lot. Instead of looking around for frameworks or wrappers (I don’t even know if these are the right words to describe what I’m looking for) like Django and Flask, I can focus on the core functionality of the service / programme.\nOnce that’s complete, I’ll figure out how to hook it up to a front-facing web interface, and then will work on the presentation as per my specifications.\nI’m quite excited about this project. As a final clue I will add that it’s going to be a service that helps people learning languages."
  },
  {
    "objectID": "personal/2016-11-05-finishing-elements.html",
    "href": "personal/2016-11-05-finishing-elements.html",
    "title": "Finishing GMB’s Elements",
    "section": "",
    "text": "Today marks the final day in my eight-week work on GMB’s Elements training programme. I’ve made allusions to GMB on this blog in the past, but the short story is that they’re an amazing community of trainers who put together fun, sensible and rewarding programmes that increase your ability to move your body. I’ve been working — one way or another — with them and their programmes for the past three years, and they’re consistently supportive and constructive in their approach.\nMy previous training with their methods was working with a one-on-one trainer to get a handstand. I was starting to get close to the point where I was able to do that when I got ill. I’ve been working my way back from that since then (the past year or so).\nElements is sort of the baseline entry point into GMB’s world. It teaches you three basic movements (and a number of variations on each) and then encourages you to find ways to self-express in a ‘flow’ in which you combine the different movements. The programme takes seven weeks to work your way through. I took an extra week since I had some shoulder pain caused by an adventurous climbing move at some point half-way through.\nElements is really perfect for working through alongside weekly climbing sessions. Climbing at the wall is much more about pulling movements, and Elements does a lot of pushing. Both work to strengthen the upper body and shoulders, but as mutually reinforcing sides of the coin. If you just do one and not the other, those imbalances will start working against you before long.\nI am pleased to have made it to the end of the programme. I’m still at the beginning of what I consider to be a 2–3 year long programme of rehabilitation and reconditioning of my body. I now have a useful baseline which supports my climbing during the week, and from which I can now start doing more interesting training. I’m excited to get back to handstand work and I’m thinking I’ll start one of the other GMB programmes next up on the slate: either Vitamins or Integral Strength. Flexibility work is another pretty important part of the work with my body that I really ought to do every day. I’m getting better with this, but the more it’s integrated as part of a programme, the more likely I will be to follow through."
  },
  {
    "objectID": "personal/2016-10-28-greengeeks-ecological-footprint.html",
    "href": "personal/2016-10-28-greengeeks-ecological-footprint.html",
    "title": "A Greener Technological Footprint",
    "section": "",
    "text": "I’ve been trying to reduce my ecological impact on the planet for several months. Some of this has meant not taking flights, while other parts have involved me re-evaluating more habitual choices I make such as my diet.\nA small, but not insignificant, service I rely on is web servers, domain hosting and the like. I had been a loyal Dreamhost customer for many years and have had no reason to complain about their services. Cheap, fast, and their web panel does everything you’d want from it.\nSo why should you care about your internet server’s ecological footprint? Because small things matter and because the internet is no small thing. In fact, some reports project the infrastructure of the internet to be as big a polluter as the airline industry by 2020.\nI started looking into Dreamhost’s green credentials and at first glance there’s no real reason to make any changes. From their website:\n\n“When we learned that running DreamHost generated as much carbon dioxide as 545 average-size homes we realized we had to do something to neutralize our emissions. With a bit of research we found the most effective approach begins with resource conservation: turning off the lights, reducing travel, printing on both sides of the page. Efforts are being ramped up here daily to do what we do with less. The next step is to use clean, renewable energy. Without the option to put up solar panels or connect with a green power utility for us this means investing in renewable energy projects taking place right here in the United States through the purchase of carbon offsets. Finally, to neutralize those unavoidable emissions from our day-to-day operations (e.g. purchases of office products, commuting, and planned travel) we’ve invested in carbon offsets from international renewable energy and energy efficiency projects, effectively erasing our remaining greenhouse gas emissions.”\n\nThere’s a little more to their pitch, but the bottom-line is that they claim to be carbon neutral.\nI won’t go into the various ways in which carbon offsetting schemes are essentially scams, but it’s worth knowing that there seems to be a pretty high risk that things aren’t what they seem.\nThat’s when I discovered GreenGeeks. There are a bunch of different domain / hosting / server providers that cater to those who want to leave less of a footprint, and GreenGeeks seem to be one of the larger of these. They have a long history, which is useful given that many of the services I visited were already defunct.\nAbove all, I needed a service that could do pretty much everything that Dreamhost offered. It was a tall order, but GreenGeeks fits the bill.\nMoreover, GreekGeeks are a whole lot more environmentally-friendly than Dreamhost, it turns out. For every unit of energy consumed by their servers/customers, they invest in green companies and green energy sources three times over:\n\n”At GreenGeeks we work with the Bonneville Environmental Foundation out of Portland, Oregon. They are an EPA recognized and approved Green Power Partner as is GreenGeeks. Through the Bonneville foundation GreenGeeks purchases energy powered by wind to be put back into the grid and we purchase 3 times the amount of energy we consume to be put back into the grid making GreenGeeks 300%. We do this for all of our servers and all of our employee work spaces and computers.”\n\nIn an email from their support team, they explained this further:\n\n“When we say 300% green, we mean that we contribute such a large amount of funding to green energy companies that we are actually 200% carbon negative, and our servers’ carbon emissions are 100% offset.”\n\nSo I’ve started work to transfer my domains over from Dreamhost to GreenGeeks. I have only been using them for a few weeks, and while I’ve had no problems so far, it’s too early to offer an unqualified endorsement so I’ll make a note to write more here in six months from now.\nIf you’re currently using web hosting of some sort, I’d encourage you to check out GreenGeeks or some other eco-friendly option."
  },
  {
    "objectID": "personal/2016-10-24-udacity-ipnd.html",
    "href": "personal/2016-10-24-udacity-ipnd.html",
    "title": "On Completing Udacity’s Intro to Programming Nanodegree",
    "section": "",
    "text": "Add this one to the drawer of semi-meaningless certificates!\nThis took me several months longer than I had expected, but it’s done now: I graduated from Udacity’s Introduction to Programming Nano-degree or IPND. I’ve always liked Udacity’s free courses ever since I did their very first offering, “Intro to Computer Science” or CS101. They make sure to intersperse lots (and lots) of practice with the fun bite-size videos explaining concepts or techniques.\nThe IPND (estimated to take around 190 hours to complete) takes you on a tour of a variety of languages, programming tools as well as ways of thinking about coding. You learn basic HTML and CSS (for making standalone websites), Python to do more tricky computational things, and some ways to use Python and HTML/CSS combined together. Each unit is divided into a learning stage and then a practical project that you have to build on your own. This is where you prove you’ve mastered the skills that were taught, and you get lots of useful feedback from the examiners / reviewers of the projects.\nAt the end of the project you get to choose an ‘elective’, a specific area of programming that takes your interest. Current options include:\n\nFront-end developer\nBack-end developer\niOS developer\nAndroid developer\nData analyst\n\nI wrote earlier that I had chosen the data analyst elective, but this turned out to be far harder than I’d expected so I changed to the back-end developer part. As part of this elective, I learnt about databases, SQL and how to code so that my databases were interacting with Python code.\nThe course as a whole was — with the exception of the data analysis elective — pitched at just the right level: not too hard but also not too easy. I enjoyed pretty much the entirety of the materials and I really felt like I was growing my skill and capacity as I went through the course.\nOf course, spending a few months doing a nano-degree for an hour or two every day does not make a functional programmer. I have a small project I want to code with what I learnt so far, so I’m taking time to do that. I’m also trying to keep my Python fresh by tackling a challenge problem each day over on Codewars.\nWhen learning a skill like coding, there’s a strong temptation to just keep doing more courses. It’s a little like riding a bike with training wheels. It feels safe, protected from the ‘real world’ somehow. But ultimately that’s not going to help me grow, so I’m forcing myself to actually do something practical with my skill before I take another course.\nUdacity’s IPND is not cheap at $200 / month, though if you finish the course within 12 months they give you 50% of the tuition money back. That means that you’re actually paying $100 / month for the course, and it’s a strong incentive towards completion. I took a little longer than three months to complete, but you could probably get it all done in two months if you didn’t have a lot of other things going on at the same time. You can do all of the courses they offer for free, though you miss out on the practical side of the tuition, the interactions with one-on-one mentors and the forum support. I used all of that stuff a lot, so all in all it was worth it for me.\nThere are lots of ‘intro-to-programming’-type courses and materials available, so you should do your research first depending on what you want to do with it later. I really want to do the data analysis nano-degree; this is a full degree requiring an average/estimated 380 hours of study time. It made sense for me to do my initial studies within the Udacity ecosystem, though in this interim period I’m making sure that I’m getting exposure to other resources:\n\nDataCamp has a great set of Python and R tutorials which I’m looking forward to exploring\nCodewars (mentioned earlier) offers many challenges which test my command of code as well as of maths, logic and reasoning.\nFreeCodeCamp’s excellent (free) course will help me become more comfortable with coding web-facing projects, with the added bonus of connecting coders to non-profits along the way who could benefit from my skills.\n\nFor the moment, that’s enough to be getting on with before I start the next nano-degree…"
  },
  {
    "objectID": "personal/2016-10-18-spaced-repetition-uses.html",
    "href": "personal/2016-10-18-spaced-repetition-uses.html",
    "title": "Different Uses for Spaced Repetition",
    "section": "",
    "text": "[This is a cross-post via the Spaced Repetition Foundation blog, where it was originally published]\nMaybe Spaced Repetition isn’t the best word. It’s only descriptive after the fact, once someone has explained to you about Ebbinghaus and his experiments learning sheets of random numbers during the 1870s. For a general audience, we probably need a better term to help people understand what you can do with Spaced Repetition. We also need better examples and models of people who use it in a variety of domains of their life.\nThe first use case or scenario in which many encounter spaced repetition software or uses is language learning. And yes, it is great for learning vocabulary in an efficient manner. You can see how people have found ways to blast through large numbers of words by using things like Anki or Memrise.\nBut it would be a waste to limit our understanding of what Anki or spaced repetition can do to just learning vocabulary lists. It can be used for so much more. Indeed, expanding people’s awareness of the various possibilities is an important reason why Matt, Natalie and I decided to set up this foundation.\nWith that in mind, I wanted to take a few moments to write down some thoughts on how you can use Anki and Spaced Repetition in other areas of your life. This list isn’t in any particular order. You’ll find that most things involving information or learning can be somehow coupled with the use of spaced repetition software, and the extent of your use is basically only as limited as your creativity.\n\nBirthdays – learn all the birth dates of your friends and family. This probably works best in conjunction with some other system like the major system for remembering numbers, but you might be able to brute force this through.\n- Kindle Clippings – using Anki’s in-built cloze deletion tool, you can blank out key parts of your kindle clippings to remind you of things you read in previous months or years. This way, books that you’ve read needn’t be confined to vague memories. You can be more deliberate about what you’re learning. Even things like book titles could be something you could learn with Anki. Consider a card which tests me on the author’s name and the title (based on a short description). This way I avoid having those moments where I forget the title of the book but remember the contents pretty well.\nMagazine articles / things online as you’re reading\nI do this a lot. Whenever I’m reading online articles – in Instapaper, let’s say, which is the place where I read most things – I’ll save the highlights of passages I want to keep in my reference library. At a later date, I’ll go through those highlights and see what I want to remember for the longer-term. I’ll then transfer the clipping or highlight into my Anki library.\nNew English words – I’ll occasionally come across technical vocabulary that is new to me while readying. If it’s a word that I find myself looking up often, I’ll save that entire sentence into Anki and use the cloze deletion method to set up an appropriate test.\nPeople / Faces\nMatt seems to use this far more than me, though I’m starting to do it more often. The idea is that you find a picture of people you meet in meetings or in work contexts (Google Images or Facebook works well for this) and test yourself on their names and maybe one or two key facts about them – that they’re a vegetarian, perhaps, or that they just had a baby, or whatever it is. You can even do this pre-emptively. I once did this prior to a conference/event on Afghanistan. The conference home page had everyone’s name and photo (those who were speaking / attending) so I just downloaded the lot, made cards for everyone as well as what their job/specialism was. By the time I arrived at the conference I knew everyone’s name and who they worked for and what they worked on. It was a far more relaxed and personable experience than previous events of that kind. I don’t get invited to speak at such events any more really, so I have no use for this, but I imagine many readers would find this useful.\nReference materials – You probably have things in your life that you’re perpetually looking up. The time in-between looking-up probably is a matter of months (or maybe years). Just long enough to forget, but not long enough to forget that you once knew this thing. With Anki, those tip-of-the-tongue moments can become a feature of the past. No more remembering that you once new something.\nSome examples: perhaps the precise proportions of a recipe you make every  month or two. Yet every time you make the recipe you have to look up whether it is 1 part flour to 2 parts water, or 1:3. Anki can keep your memory fresh.\nSimilarly with important medical data, or phone numbers, or points of contact for certain work projects or people.\nCoding language – I used this while studying coding this past summer, and have found it great for making sure that the conventions I learnt for one language didn’t get forgotten while I worked on the next language / scenario.\nRevising things you should probably already know but maybe have forgotten\nThis, for me, is the subject of mathematics. I studied it at school, but since then have had less need for it. This past year I’ve returned to mathematics with the somewhat embarrassing realisation that even things like my times-tables aren’t as solidly in my brain as I’d like. I can tell you what 6x7 produces, but I’ll probably need to think about it for a few seconds. Enter, Anki. I’ve been drilling myself on times tables for a few months and have found a real uptick in the speed at which I can recall those answers.\nGeography\nYou probably did a certain amount of this at school, but since then haven’t added to the knowledge (or some maybe has been forgotten). Learning all the capitals of all the countries of the world (and being able to locate them on a map) isn’t just an abstract skill. Things like this enhance your ability to understand and interact with the world. Instead, when you read a news article referencing a number of countries, you can place them on a map and realise that Ouagadougou is the capital of Burkina Faso and that it’s included in the article for that reason.\n\nOther niche things I’ve used Anki to help me with:\n- brewing times for different varieties of tea, and also the ideal temperature of the water\n- Ayas for the Qur’an (i.e. learning to recite these by heart). I know many use Anki for learning Bible verses, too.\n- Learning the periodic table (alas, many years ago, in a deck now since deleted from my phone)\n- The Japanese Kanji\nOthers use Anki for learning med-school data.\nAs I wrote above, the limit to what you can use Anki for is basically limited only by your creativity and imagination."
  },
  {
    "objectID": "personal/2016-10-16-fundamentals-versus-hacks.html",
    "href": "personal/2016-10-16-fundamentals-versus-hacks.html",
    "title": "Fundamentals Versus Hacks",
    "section": "",
    "text": "I was talking to someone a couple of days ago who pushed back a little against some of my recent posts. In particular, she was resistant to the idea that there were special techniques or skills that could be learnt that might help with writing, studying and learning.\nI obviously disagree with the premise that we can’t get better at the things we do in a domain like learning, but it encouraged me to define the core principles as something separate from the less important lessons.\nI’m writing something on the specific question of software — i.e. do we really need to buy all this expensive software to be able to do great work — so I’ll set that part of the problem to one side for now. For the rest, I think it’s useful to make a distinction between fundamentals and ‘hacks’.\nThe idea of a hack or ‘lifehack’ has garnered a decent amount of criticism over the years, often for good reason. Whole sites exist to tell you small things you can change — little tweaks to the way you’re already doing things — that will supposedly free you up to do the important work. My experience is that these are mostly cosmetic changes, and that they don’t deliver 10x (or more) improvements. The only time when you should be looking for tweaks of that kind is if you’re already in the 99.9% bracket for what you do.\nFor whatever reason, people seem to prefer the idea of hacks to fundamentals. Perhaps because fundamentals are harder to change, more ossified and because to change a fundamental belief or practice is to question who you are on some level.\nA preference for hacks is why people think that drinking some new type of tea or a particular kind of food is better for their ability to make decisions in the long run than simply making sure to get enough sleep.\nI have read a lot in and around the productivity / health domain, and there’s a strong tendency towards these small ‘hack’-like tweaks. But what are the fundamentals? I think often a lot of that comes down to things we already know:\n\nget more sleep\neat more vegetables\nmove your body more\nprioritise the long-term over the short-term\nFind a way to do the ‘hard thing’ earlier in your day\n\nSome other principles that I’ve found useful:\n\nSubtract first, then add — if you can find a way to simplify your current process (or whatever it is) by taking something away, I’ve found that that really helps. This can apply to big things (like the conflict in Afghanistan and the international involvement there — see this for more) as well as smaller things (your daily work schedule).\nPerfect the process; think less about results — I’ve noticed this most profoundly in my exercise and movement work over the past few years. Whenever I’ve focused too much on the hoped-for results of a particular routine or programme, I’ve had things go wrong (either injuries or setbacks of other kinds). When I stay focused on the process, trying to make each movement as good as I can, trying to stay mindful of where I am on a particular day vs where I want to be, then I find this really beneficial.\n\nThe fundamentals are low-hanging fruit that offer big returns, albeit in a slightly less sexy packaging. Hacks are shortcuts that offer minute incremental improvements, at the expense of your time and energy.\nThere is a time and place for tweaks and fine-tuning and smaller productivity wins, but unless you’re finding a way to sort out your fundamentals, you’re missing a great opportunity."
  },
  {
    "objectID": "personal/2016-10-13-keep-moving.html",
    "href": "personal/2016-10-13-keep-moving.html",
    "title": "Keep Moving",
    "section": "",
    "text": "There was a moment, three-quarters of the way up the wall yesterday, where I started to think. I was starting to slip off the wall, and the holds were starting to become slippy with sweat from my palms. My arms were both tense, holding the weight of my body in a way that was tiring them out. I was doing everything wrong – holding myself onto the wall instead of finding a way to keep my arms long and straight, the rest position that climbers are encouraged to take.\nAt this point, my mind moves in different directions simultaneously. I think about the next move – something I’ve tried to reach for or figure out just beforehand – and come up with nothing. I think about quitting, “because you’ve come so far” (!). I try to ignore that voice, though sometimes when your arms are screaming at you it’s a fairly seductive call. Then I think about how I’m starting to slip off. I feel the initial tentacles of panic. Panic is an exponentially increasing phenomenon, I’ve found. You start with just letting a little in, and before you know it (in less than a second) you’re sharing the wall with this monster who seeks to throw you off the wall. I shut the door on panic just in time, though it’s only a matter of minutes before he shows up again.\nAnd then I think to myself that maybe it’s better I just keep climbing. Keep moving. One of the reasons why my arms are tired is because I’m on the wall for so long. The route is tougher than what I’m used to and it’s designed to make it harder to take moments of rest, so I’m less worried about needing to use my arms so much. Sometimes that’s just what the route calls for.\nAlso, the less time I spend on a particular step or move, the less likely my sweaty hands are to find a chance to slip off a hold.\nI try to keep that in mind and find a rhythm of movement that aligns with my breath. Breathe, move hand, move foot, step up, move hand, move foot. Repeat. It doesn’t last forever; at some point I’ll be stymied by some tricky part of the climb, and then I’ll have to go through the whole learning cycle again.\nBut this seemed to me one of those lessons that the wall teaches you in climbing that are applicable to life outside: sometimes it’s better just to keep moving. Focus and reflection on your route or your goals is useful, but perhaps not best done while you’re mid-route. There are usually inflection points at staggered intervals (when you finish a shorter project, when you do a weekly or monthly review of your activities and so on) when it’s good to stop and reflect. But when you’re mid-flow, exerting the effort and starting to slip off the wall, this isn’t the time for reflection. In fact, reflection will sap your ability to keep moving forward. You just need to keep pushing on, keep delivering, keep pushing out the words, or whatever it is that you’re doing."
  },
  {
    "objectID": "personal/2016-10-11-exist.html",
    "href": "personal/2016-10-11-exist.html",
    "title": "Seeing The Forest But For The Trees: On Exist.io",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“1146”] Sometimes Exist helps me feel better about myself :) [/caption]\nSo many services, so much data. How can I make sense of it all? You’ve probably had this thought yourself on occasion. You have your Fitbit data, your calendar data, your email data, your task management system, your last.fm music data and so on. All of these things exist in their own silos.\nIf you’re halfway intrepid, you’ve maybe even made efforts to liberate your data from these prisons. Best case scenario: you’re left with a half-complete .CSV file that is incompatible with all the other .csv files you’ve downloaded from other services. Now, if you want to hook all these files up together, you’ll have to clean them up, massage the various tables and forms so that they match up. And then you’ll have to perform a whole host of calculations in order to figure out how you’re doing, or what the data has to tell you.\nCollating all this data and making sense of it can be a full-time job in and of itself. I admire those who give talks at institutions like the Quantified Self, telling tales derived from the past 20 years of financial tracking or weight data.\nPersonally, I don’t have time to do all of that. I need a service to help me out, something that will serve up platters of charts and correlations that address the main broad questions that I might address of these various data streams, notably:\n\nhow am I feeling?\nhow am I sleeping?\nam I moving enough?\nam I getting enough work done?\n\nNow you might say that it’s easy to look at each of the services individually and you’d be right. I can look at a chart over at Fitbit.com and I’ll see something that shows me all the data for the past 12 months:\n\nThis is nice, but it doesn’t tell me a great deal, aside from the fact that I seem to be walking less in Amman than when I lived next to a huge forest in a provincial part of Holland. No surprises there. Also, a whole year is a bit too much for me to process. It doesn’t really help me calibrate my current actions and my plan going forward.\nEnter, Exist. I’ve been using Exist for a little over a year now, checking in on the site pretty much every day. Their own explanation for what they do is:\n“We turn numbers into insights. We collect data from the services you already use and find trends and correlations in the results.”\nYou hook up all your web services, and your Exist dashboard will be populated with pretty graphs, useful correlations and various other goodies. These are the services currently supported:\n\nYour dashboard page has a bunch of different panels which aggregate data from different parts of your life. For instance, my activity panel for today looks like this:\n\nI usually go for a walk in the evenings, so I’ll most likely end today with a higher count, but you get the idea. There are panels for activity, productivity, sleep, mood, workouts, health, location, social media, music and weather.\nSo the dashboard is where you can come to get a snapshot of the reality of your current short-term. Exist also offers correlations and suggestions based on its combination and number-crunching of all the various parts of your data stream. This is often most interesting when it comes to your mood data. Check out these recent correlations that Exist crunched from my self-reported mood data alongside the other automated streams:\n\nThere are so many different types of charts and graphs; it’s difficult to pick my favourite ones. Here you can see my slow but steady battle to increase the amount of time I sleep every day, for example:\n\nThe mood data is self-reported. Exist sends you an email every day at a time of your choosing and you respond with a number from 1–5 alongside some comments about your subjective experience of the day. These comments are also then processed into useful charts and prompts. Weird things I’ve learned from Exist’s number crunching:\n\nthe less I sleep, the more likely I am to have a ‘perfect’ day (i.e. 5 on the 1–5 mood scale)\nthe more it rains or snows, the happier I am\n\nYou can use Exist in a bunch of different ways, depending on what you hope to get out of it. For my own situation, I love how it encourages awareness of my current patterns on the short- to medium-term timescale (i.e. one week to three months). For things like sleep, where it’s easy to forget whether you’re managing to do what you seek to do (i.e. sleep more, or get up earlier etc), Exist is really excellent.\nThere’s a sizeable userbase who actively contribute to shaping the future of the company / service through feedback. There are a bunch of things I’d love to see happen with Exist – changing the 1-5 scale for mood feedback to 1-10 as a start – but the site is actively developed, and I’m really happy with all the new features that have been added in the year or so since I signed up.\nYou can find out the future / expansion plans that Belle and Josh have for Exist here. There’ll be more services and correlations and integrations will be more usefully presented. All of this is as you’d expect. Belle and Josh are good people (together, they make up HelloCode which is the Australia-based parent entity that produces Exist, among other things). I spoke to Belle for a Sources and Methods podcast episode; that’ll be out soon, I hope, so keep your eyes out for that.\nIn my interactions with Belle / Josh I’ve been really impressed with the service that they offer through Exist and their other products. I put them in my personal category of ‘nice, kind people seeking to do good things in the world through offering really useful services’. Like the team who run Beeminder and GMB Fitness, Belle and Josh offer something that saves me time and adds meaning on a daily basis to my life. Exist is a paid service. If you already run a bunch of the services listed above in the diagram, you’ll probably benefit from Exist’s charts and correlations. I hope you’ll consider signing up for their free one-month trial."
  },
  {
    "objectID": "personal/2016-09-29-learning-without-seams.html",
    "href": "personal/2016-09-29-learning-without-seams.html",
    "title": "Learning Without Seams",
    "section": "",
    "text": "We’re often told to learn from others, to take (role) models and to emulate either the path they took, or some of the key lessons they learnt along the way. Sometimes, though, it’s hard to figure out how someone reached a particular goal, or they actively work to prevent you from seeing the struggle that took them there.\nThis is as much the case for sedentary pursuits like fiction writing as it is for active things like climbing. You can watch a video of someone like Alex Honnold, someone known for his lack of self-aggrandisement and who always seems to downplay the stakes of what he’s just done, and find it hard to see the work, the sweat, or the basic realities of what he did to reach this point. (In his case, years of practice/experience possibly combined with a disposition towards risk-taking.) His book was frustrating for exactly that reason: you had only a minimal sense of the stakes of what he did.\nFor fiction or many other kinds of rewriting, the whole work of editing, rewriting is more or less deliberately geared towards removing the friction, removing the sense that someone toiled to put this story together. As readers or consumers of stories, we don’t want to sense the seams, don’t want to be aware of how something was constructed. But as practitioners or artists, we need to see those seams to be able to learn our own craft.\nSo there’s a tension, and that tension instructive in helping you find good models. I’ve been trying to write more about my own failures on this blog so as to balance things out. Whenever I’m learning a new skill, I try to find different types of models. Firstly, people who have reached the heavens, who are trailblazers and who break new ground in their particular skill / discipline. Secondly, those who are a few steps behind, and who really have had to work to get there. It’s this second kind of model where you’re more likely to see the seams of their journey, and you’ll most likely be able to learn from them."
  },
  {
    "objectID": "personal/2016-09-27-lessons-learnt-language-study-habits.html",
    "href": "personal/2016-09-27-lessons-learnt-language-study-habits.html",
    "title": "Lessons Learnt: Language Study Habits That Work",
    "section": "",
    "text": "I’ve been coaching a dozen or so students for a few months now, and I wanted to reflect a little on the experience. (For more information on what language coaching is, click here). Since I mainly work on meta-skills around habit formation, scheduling and planning, and accountability, I even work with those who are studying languages I don’t speak. (I also work and consult on specific productivity-related issues, such as with PhD students to figure out work plans, idea structures and schedule reconfigurations).\nToday, however, I wanted to specifically take stock of the habits and characteristics of the students who have made the most solid progress. (Small caveat / usage warning: All of this is flexible. Most students will do all of these things at some point; the challenge is to make these habits the norm.)\nKeep Regular\nThe key habit that really determines whether you’ll be able to learn a language is studying every day. Students who engage with their chosen target language on a regular basis, even if that is sometimes only for 5 minutes, will do better on average than those who try to wait for the perfect moment to study or who bunch study time into one day of the week. Of course, sometimes your circumstances will make regularity a challenge, but I challenge anyone to tell me that they really don’t have 5 minutes in their day to write some practice sentences. (Even astronauts take breaks.).\nHave a Plan\nUnless you’ve been learning languages before, this is usually where I come in. You need a macro plan (your goals for the coming year or two) and a micro plan (what you’ll be doing this week or today). There’s often space for a plan for in-between those levels — a few three-month challenges to be accomplished over the course of a year. These goals should be specific and they should be measurable. The smaller short-term goals need to match up with the longer-term goals, and these longer-term goals need to be realistic. (There’s no point thinking you’re going to go from zero to reading modern Arabic literature in 6 months.) There’s a lot more to say about this (a lot of which is best learnt through experience), but for now suffice it to say that you need to know what you’re doing and where you’re going.\nBe Flexible\nYour goals need to adapt to your life. Life throws up all sorts of surprises and changes, and you need to be able to have the flexibility to change your plans if need be. This doesn’t mean changing your long-term goals every time the wind changes direction or based on momentary enthusiasms and/or distractions. Flexibility means that sometimes the world changes around you (or sometimes you change on the course of your journey) and that you should make sure to take this into account.\nTrack Your Progress\nHaving a goal like “become fluent in x” is not a particularly useful goal, mainly because it’s not really concrete enough to be able to monitor your progress. There are many different ways of making your goals concrete, but generally if you can track it in a spreadsheet then you’re probably on the right track. I have all my students track the number of minutes they’re spending working on various skills. They also write qualitative notes on what worked or what didn’t work in a particular session. Anki, the pre-eminent SRS application, tracks a whole bunch of vocal-related numbers as well, which is useful at the beginning since that’s the bulk of the work. Then you can use reviews to see how your day-to-day activities are matching up with your short- and long-term goals.\nPlan for the Unexpected\nA large number of my students live in Kabul or other countries that face instability, coup attempts and the like. It’s not unusual to get a message like, “I had to move house today because of the kidnapping threat so we’ll have to reschedule today’s lesson.” Or sometimes it’s something mundane like, “There’s no electricity in my part of the city this week.” If you live somewhere like that, you need to plan for the unexpected and for emergency-type situations. This usually means coming up with a list of easy exercises and/or tasks that you can accomplish in 5 minutes or less, or that can be done without any need for electricity or internet. This way, even when you have a bad day and everything is failing around you, you’ve made a plan for such a situation and you can make sure to stay regular.\nBreak Tasks Down\nIf you’re living in a chaotic environment, it often helps to break tasks down into smaller components. Even though it’s much better to set aside time (and schedule into your calendar and inform others around you that this is time to be respected), sometimes this isn’t possible. In that case, you can’t just wait until you have an uninterrupted stretch of 60 minutes to study. If you do that, you’ll never study and then you’ll be putting your regularity in jeopardy. So you have to come up with smaller task groupings (that you can accomplish in 10-minute dashes, for example) and you need to be ok with that. A student will sometimes be fine coming up with suggestions for shorter activities, but will strongly resist the idea of studying in shorter bursts. It’s not ideal, but if your life is that chaotic then it might be your only change.\nTime\nAt the lower end of the spectrum, the more time you spend, the more progress you’ll make. This applies to everything from zero to sixty minutes per day. Once you’re studying for over an hour every day without too many exceptions, then it’s time to talk about speeding up the process and making efficiency improvements. But until then, you just need to keep finding time in your day to be doing short small bursts, making sure that it adds up.\nLanguage learning isn’t rocket science. Millions do it every day, the majority without a great song and dance. If you keep in mind these basic principles, you’ll go far.\nIf you want to work with me on your language-learning goals, please read this overview and get in touch with me using the link specified."
  },
  {
    "objectID": "personal/2016-09-19-phd-tools-tea.html",
    "href": "personal/2016-09-19-phd-tools-tea.html",
    "title": "PhD Tools: Tea",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n[caption id=“” align=“alignnone” width=“1306”] I got a bit carried away with my tea experiments… [/caption]\nThis will be the last post in my PhD Tools series. I thought I’d end with something a little less serious, though still potentially of use.\nAround the time I started working intensely on my PhD, I became a little obsessed with tea. Looking back, I can see the traces of procrastination around this ‘learn-about-tea’ project. I put together a Trello board to track the different types of leaves I was trying. I read books about the cultivation of tea. I corresponded with various companies about how they source their products. (Sidenote, I settled on Rishi Tea as the best company selling tea online. Hopefully I’ll be able to get them to record a podcast on Sources & Methods soon).\nMy Perfect Four Hours, for the record, were fuelled by two cups of Oolong tea. I’ve discovered over the years that I’m particularly sensitive to green tea, (which gives you a dose of theine rather than the better-known caffeine), such that one too many cups will have my hands shaking and my body unable to think or work in any useful way.\nYou’ll need to figure out your maximum sensitivity point, but for most people I’d suggest it probably is one cup less than whatever you’re currently drinking. There’s a tendency (especially with coffee drinkers) to think that more is better. More coffee = better focus, more awake, etc. In reality, as I think many would admit, you reach a point of diminishing returns. I don’t drink coffee, though I did in the past and I remember that feeling.\nThat said, some kind of stimulation in the form of green tea or coffee can be really useful when starting your core work sessions. It takes 20-30 minutes for the chemical components of tea or coffee to have their effect on your brain, so it can even make sense to have your first cup before you leave your house. That way you’re hitting your first session at your peak.\n\nI hope that this series has been useful for some of you. If there’s a particular topic or problem that you feel it would be useful for me to write more about (or cover afresh), let me know over on twitter. I also offer (paid) consultancy on these productivity issues, so if you feel you’d like to discuss your particular situation in more detail, drop me a line."
  },
  {
    "objectID": "personal/2016-09-16-2016-9-phd-tools-sleep-movement.html",
    "href": "personal/2016-09-16-2016-9-phd-tools-sleep-movement.html",
    "title": "PhD Tools: Sleep and Movement to Nourish the Body",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nIntense focus on a particular mental challenge, problem or project has the tendency – at least in my experience – to become an all-or-nothing proposition. Any non-PhD-related activities are considered unimportant or irrelevant, and you end up sitting in front of your chair for hours on end.\nI’ve already written about the importance of periodic breaks in your work routine. These breaks were short breaks that I was referring to, but you also need to find a way to include – your own situation permitting, of course – ample opportunity for recharging your physical body and needs.\nThis is common sense. We all know that we should probably sleep more and move more. Most of us aren’t getting enough of either, and we feel its effects on our concentration or we feel the physical aches and pains in your body that come after a few hours sitting hunched over in a chair in front of a laptop.\nIf you’re doing intense work thinking about particular problems, getting more sleep and movement will really invigorate your ability to keep doing that. Your body will thank you and you will feel the difference in your work and attention.\nMovement doesn’t need to be something as structured as going to work out, or a specific activity, even. The mental and physical benefits of long walks (or multiple shorter walks over a single day) are pretty well established in the scientific record, I think, and I know that when I make sure to include lots of walking in my day I generally feel better. (I actually have a bunch of quantitative data to back that up from various tracking projects that I maintain, but that’s a topic for another day).\nAll of this is not about being prescriptive, but I think you’ll find that if you can find a way to sleep a little more and move a little more each day, your body and mind (and your PhD) will thank you. This is all about realigning your own sense of what you want for yourself with the reality of how you go about your day."
  },
  {
    "objectID": "personal/2016-09-14-phd-tools-primary-sources.html",
    "href": "personal/2016-09-14-phd-tools-primary-sources.html",
    "title": "PhD Tools: ‘Always return to your primary sources’",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nThis phrase became a kind of mantra for me during the final write-up of my PhD. Friends and colleagues have since become accustomed to my frequent invocation of this phrase. I wrote up a longish blogpost which stemmed from my frustration at the takeup of primary sources and their use by fellow researchers and analysts in the Afghan context.\nWith regards to my PhD, I often felt that when I reached a point where I was stuck, the thing that would unstick me was a return to the primary sources. For my specific project, I was lucky to have a rich variety of sources on which to rely. Some may not have this luxury, but for all but the most stalwart of abstract theorists, there is going to be some kind of primary data on which you are basing your research work and writeup.\nThus, whenever you get stuck or you feel your writing starts becoming too self-referential and circular in its logic, go back to the primary sources. I think you’ll find this helpful, and you’ll return to your writing reinvigorated with new ideas and approaches."
  },
  {
    "objectID": "personal/2016-09-13-phd-tools-freewriting.html",
    "href": "personal/2016-09-13-phd-tools-freewriting.html",
    "title": "PhD Tools: Freewriting and Journalling to Think Through your Work",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nA few years back, I read a book with the (intentionally) provocative title, Write Your Dissertation in 15 Minutes A Day. I was travelling back to Afghanistan from a short stay in Europe, and I was sat in Istanbul airport, waiting for my connecting flight. I remember the moment quite clearly, because a long wait time plus a delay didn’t phase me. I was sucked into the book and the idea that the author presented. (There’s also another good one along a similar theme: How to Write a Lot: A Practical Guide to Productive Academic Writing by Paul Silvia.\nBasically, she explained how writing for a very short amount of time each day, taking the time to think through whatever was going on with your research, but on paper instead of your head – was a trick that would really help your work. It’s not a new idea, this technique of freewriting. When you take this time, these 15 or 20 minutes, you aren’t writing a section of your thesis itself, you’re writing almost a note to yourself about how it is going, what you think are important things you  need to consider, whether this is a useful line of inquiry and so on.\nSince that day, I’ve incorporated this kind of writing much more often as a general practice. There’s a great service run by all-round make-useful-things-for-everyone-to-benefit-from person Buster Benson called 750Words. It sends you a friendly reminder every day to write 750 words on its site. There’s all sorts of gamification and encouragement of writing streaks etc, and while writing the middle sections of my PhD, I would check in to 750words.com every day at the start of the morning to journal out my current research position and think through whatever problems I was about to face in my work that coming day.\nIt may feel a bit redundant at times, but I’ve found the practice really useful. Give it a try. You might find that it works for you."
  },
  {
    "objectID": "personal/2016-09-09-phd-tools-the-secret-to-finishing-your-phd.html",
    "href": "personal/2016-09-09-phd-tools-the-secret-to-finishing-your-phd.html",
    "title": "PhD Tools: The Secret to Finishing Your PhD",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nFour Perfect Hours. Repeat every day for two and a half months, and you have your PhD.\nAt least, that’s how it worked for me. I was in Holland, living near a little town called Delft, full of picturesque waterways, bridges and historical buildings, and I had a deadline to finish my PhD. The previous months hadn’t been too great in terms of sitting down to write. There had been quite a few distractions outside my control, some entirely within my control, and I hadn’t managed to sit down and focus. Worse still, the times when I did focus would be offset by days (sometimes weeks) where nothing happened. So each time I rededicated myself to the PhD writeup I had to refamiliarise myself with sources and even the structure of what I was writing itself.\nCal Newport’s book Deep Work happened to be released around that time, and while none of the ideas he was suggesting were wholly original, the combination and explanation hit me just in the right place and at the right time. (The book is excellent, by the way, and I’d recommend giving it a read, particularly if you have some sort of big project that you’ve been finding hard to make progress on, though I will try to outline some of the key principles below).\nNewport’s argument runs as follows: the ability to focus, to apply yourself to a particular creative task in a deep way, is a valuable skill. If you can manage to do this, you’ll deliver better work, become more valuable a commodity to employers or to your community, and you’ll feel more fulfilled.\nThe ways he suggests to reach that point are a collection of ‘golden oldies’ from the productivity world. Block off time specifically for your core work. Turn off the internet and anything that will distract you. Maybe try for a change of scenery. Don’t think you have an unlimited capacity for work – four hours is probably the limit. Take breaks, and take them regularly. (And so on).\nI was glad Newport takes time in his book to tackle the ‘always-on’ expectation that many employers and people in general have. If you’re not careful, you spend all your time replying to emails (and each email generates another email in response etc) or instant messages or pings on Slack or Facebook and so on. It’s not hard to imagine this scenario, particularly if you’re juggling a number of different projects or responsibilities. An increasingly connected world brings benefits, but you need to take efforts so that you don’t find yourself becoming hostage to the demands of the system. And I don’t use the word hostage lightly. Sometimes it really does feel like there is this responsibility to be continually ‘on’, to be responsive to all the requests coming your way – requests on your time, requests on your emotional energy, requests on your skills.\nThe solution Newport is prescribing – sometimes in a slightly preachy / curmudgeonly fashion, I’ll admit – is to abstract yourself out of this world. Not forever, and not as a non-negotiable proposition, but at certain times, for certain activities and to serve certain goals.\nHardly a week goes by without the announcement of some new study assessing the damage that comes from switching attention. I wouldn’t stake anything particularly valuable on the accuracy of the specific numbers they propose (i.e. the 23 minutes proposed as the cost of disruptions before you return to a state of focus), but my personal experience and anecdotal evidence does strongly suggest that there is a cost to switching your attention from one thing to another. If you’re writing about a complex issue, you’re going to find it much harder if your phone is ringing or pinging messages the whole time. (For this reason, I’ve turned off almost all notifications on my phones. If someone messages me on Signal or Whatsapp, I want to see that message when I’m ready to see it. I usually don’t want to be disturbed while I’m in the middle of something else. Obviously, this level of disconnection may be impossible for people with responsibilities, families etc, but consider employing it for specific times..)\nEveryone tells you this when you start your PhD, and probably at every moment during, but it is true: the business of doing a PhD is connected with putting words down on a page. The more you do that, the closer (usually) you’ll get towards that end point. Even if those words are just initial thoughts or reflections to yourself, they will still be useful. You’ll be thinking on paper. And ‘writing’ doesn’t need to be paragraphs of prose. I’m a big outliner – though I concede that this may not fit with everyone’s work style – so I consider sitting down on a couch and drawing some diagrams of how ideas connect together, or doing a more structured ordered outline, to be useful work. With all these things, it’s possible to spend too much time on them, but in general they are useful and contribute to the overall work at hand.\nAs a corollary to this, anything which takes you away from writing is something which is detracting from your ability to complete your project. You need data, of course, and you need to read and fill your head with ideas, and to become familiar with a number of different domains etc. But it’s easy to get sidetracked into this ‘work’, since it is often exciting and interesting, even though it may not directly contribute to you finishing your thesis. Most of the time, the internet functions as a sort of crutch (emotional or otherwise) in many people’s writing workflow. Every time you need to look something up, you go to Google (or, better yet, DuckDuckGo!) and then perhaps you get lost in a 30-minute black hole of discovery and cat gifs.\nI’m a firm believer in separating out these processes. Writing is when you write. If you find there’s some fact you don’t know, you write a little comment to yourself in square brackets ([like this – look something up about x or y]) and then you know to look that up later when your internet comes back. But in the meanwhile, you can continue your train of thought, you can keep writing, keep thinking on the page. Your internet excursions may be useful in some sense, but most of the time they are highly inefficient. If you separate the two processes (writing and internet research) you’ll make much more progress with the former and streamline how much time you’re taking on the latter.\nThe routine I settled on after reading Deep Work was as follows:\n\nI’d arrive at the coffee shop at 8am on my bike, just as the doors opened. I’d order a tea, settle downstairs and unpack whatever notebooks or papers I’d brought with me.\nAt 8:15 on the dot, I’d begin writing.\nFreedom app has turned off the internet from 8am-12pm in any case, so there’s no way I can use it, though the cafe does offer free wifi. I have no way out. I just have to write.\nI write for 45 minutes until my alarm tells me the session is over, and I have 15 minutes of break. In my break, I try to do things that refresh my body and my mind. I’ll get up, perhaps walk around a bit. I’ll do some light stretching to undo the fixed chair position I’ve been stuck in. Maybe I’ll chat briefly with the cafe staff upstairs. But I’ll be keeping an eye on the clock because at 9:15…\nI start another session. (Note that some people might prefer a 25 mins writing : 5 mins break setup to their work. I prefer to write for longer and take a slightly longer break. But each to his/her own). I work until 10am, then take another break.\nAt this point I get another tea. (I’ll be writing more about tea later, but for now know that I’ve noticed I have certain tolerances to how much caffeine or theine I can drink and still think usefully, so this is ideally calibrated to my body. YMMV.) At 10:15 I begin another session until 11.\nBy this point, I’m nearing the limits of what I can usefully do in a day. Newport suggests the same in his book, that most people have around 3-5 hours of ‘deep work’ capacity per day. 4 was a stretch for me, but I just about managed. In the break before the final session, I make sure to be conscious of how my body and mind are feeling, take stock of where my energy level is at, and perhaps calibrate the task or section to be tackled in that final session accordingly. Thus, I’m more likely to save a difficult new topic for the start of a new day, when I’m fresh, rather than try to start it at the end of the morning.\nAt 12pm, my work day is over. I sit back, feel good about what I’ve achieved (a little gratitude is often a good thing) and know that I don’t have anything else to do for the rest of the day.\n\nThe deal I made with myself was this: if I sit in the chair and have my “Four Perfect Hours” in the morning, then the afternoons are mine to do with as I wish. Of course, these were often taken up with things like shopping, paying bills, washing clothes etc, though just as often I’d take my bike and ride around town in the sunshine, listening to podcasts. Or I’d go climbing in Delft’s bouldering centre. Or I’d go see a film at the cinema.\nThe one work-related task I’d allow myself to do in the afternoon was reading or anything relating to outlining. I usually found that any work I do to prepare myself for the writing process the next day was useful. This could be as simple as making a list of the key sources I’d need to refer to, or a brief list of points that I’d do well to write about, or it could be something like selecting quotes to highlight from a certain source. Usually it was something that that wouldn’t take more than 30-60 minutes, and it was never really ‘difficult’ mental work. But if I did that kind of work the day before, then I’d always find it valuable. (The problem was that I didn’t always do it, or find the time, or have the energy, so this wasn’t something I always managed to do, by any means).\nI’d make sure to sleep at a reasonable time – since if I didn’t get enough sleep, the knock-on effect for my ability to write the next day would be significant. So sleep (as I’ll write about in a later post) was key. This meant setting an alarm to start getting ready for bed at around 9pm. By 9.30 or 10 I was usually in bed and I’d either listen to a bit of a podcast or an audiobook for 15 or 20 minutes, or perhaps read in a novel or something completely unrelated to work. More often than not, I’d skip that entirely and just sleep. Deep work is tiring, and demands a lot of your inner resources. Luckily, it is extremely rewarding, too.\nI hope I’ve made the case for having fixed times where you are writing. This applies in particular to those who are in the write-up phase of their PhD or writing project. The idea that you should wait to be somehow ‘inspired’ is less common in technical disciplines like non-fiction, but nevertheless it’s worth a reminder that writing is work of some kind. The more you can do to preserve that time as a core protected space, the more you’ll produce.\nAs a final side-note, post-PhD I am not in such a regimented routine. The six-days-per-week schedule of four perfect hours (Sunday was always completely ‘off’; rest is important) helped me complete my draft and make the final corrections to get the text ready to submit to the examination board. Nowadays, I’m still writing, though less as part of a structured single project. I know, though, that when I take 2 or 3 core hours and I spend them in a focused way, I usually find this time to be of value. For this reason, I started the Amman chapter of the Shut Up and Write meetup group (it’s sort of part of a wider, though unconnected, network of ‘Shut Up and Write’ groups around the world), where I get three hours to work on producing thoughts on paper. I think no matter what I end up working on, whether it’s research or blog posts or fiction or even my language studies, writing is always something that will bring value to my life and make it richer, so anything that keeps me regularly doing that is useful for me.\n[UPDATE: I currently do part-time work as a coach to help people working through big projects like learning new skills, large writing projects like PhDs and learning new languages. Please feel free to get in touch with me for more information.]"
  },
  {
    "objectID": "personal/2016-09-07-phd-tools-omnifocus.html",
    "href": "personal/2016-09-07-phd-tools-omnifocus.html",
    "title": "PhD Tools: Omnifocus for Managing your Non-PhD Life",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nDiscussions of task management systems have a tendency to devolve into disagreement and discord, so I’ll state upfront that the choice of how you manage the various projects and goals in your life is a very personal one. There is no one single ‘best’ software for task management; there is only the best one for you.\nOne problem when writing a PhD is that it is impossible to completely isolate yourself to the extent that might be optimal for the wiriting of the PhD. Even if you’re lucky enough not to have to work while you’re doing your research/writeup (i.e. you have funding), you still have things to plan outside the work of your dissertation: you have to shop for food, you have to pay your taxes, you have to workout, and so on.\nIf you’re anything like me, it’s easy to ignore these tasks and let them pile up in the absence of a system or a specific place where you’re storing these different commitments. There are many different approaches to both the storage of tasks as well as the precise way of implementing those tasks (the order in which you do them, for example), but the one that I’ve found most useful in my work and personal life is the Getting Things Done (GTD) system proposed by David Allen.\nYou could lose yourself on the internet reading about approaches to GTD and advocates have the reputation of being a little intense in their zeal to convert you to its glories. Suffice it to say for now that the basic idea is pretty simple: split all your tasks down into the smallest possible component and assign each task an overarching project and a context (i.e. ‘working on my laptop’ could be a context, so could your phone, or a specific shop in town etc). Everything else is just icing on the cake.\nThere has been a lot of debate as to how suitable a GTD approach is for creative professions (such as PhD writing) and I’ve changed my mind on this a number of times over the years since first reading Allen’s original classic book when I lived in Kandahar. My current position is a blend: I think GTD itself isn’t probably the best single system for the kind of complicated ‘knowledge work’ that creative pursuits demands. In particular, there is a certain encouragement to reduce all tasks down to little ‘widgets’ that doesn’t quite gel with how I write. (See the recent post by Kouroush Dini which examines some of this). That said, I do think that GTD is pretty excellent as a system to contain and support everything else that goes alongside your creative pursuits. Again, I don’t have that much sense of the variety of everyone’s approaches to task management, but I have enough going on (and I suspect you do, too) that I need a system that is more flexible than a big long list. In particular, I need something that can ping me about things that will happen in the future (or that I have to do in the future). I don’t want to see those things in the interim period, mind you, so the system is already somewhat complex.\nFor me, all these tools are not important or useful in and of themselves. They are means to an end, or means to a series of interwoven goals. The whole point of having a task management system should, I believe, be to reduce friction and to give you back as much time as possible to do the important work to which you are committed. This means a system that is flexible and light-weight in terms of maintenance. It means something I can carry everywhere with me (from my laptop to my phone). And it means something that won’t get in my way.\nWith regards to the various tasks that formed my PhD process, I moved most of those over to a Trello board (as I’ve explained in a separate blogpost here). I generally have a specific place for creative work – either Trello, or Tinderbox, or perhaps just a specific notebook. Everything else goes in Omnifocus.\nOmnifocus itself is a Mac-and-iOS-only programme. There’s a popular competitor, Things, which others swear by and I used to use. For a more cloud-based approach, some love Todoist. All three offer are based around a GTD philosophy. I like how Omnifocus works, but it may just be because I’ve been using it for a long time and it’s what I’m used to. All have a free trial period, but you’ll only figure out whether they work for you over the longer term. I would not advise constantly changing task management systems. It takes a lot of time (relatively speaking) to get comfortable with how the software works (and how you fit it into your life and workflows). Moreover, these systems aren’t cheap, especially once you shoot for the mobile versions / licenses alongside the desktop version. For affordability, I think Todoist is probably your friend. For power and if you really find you click with it, Omnifocus might be more suited.\nWith all of these approaches, having a broader sense of what you want the software to do for you really comes in handy. I don’t think it is essential to read Allen’s Getting Things Done prior to working with one of these systems, but I know my own use of them wouldn’t have been the same without a sense of the guiding principles. It’s a quick and easy read.\nThree things that I found really essential and stimulating from his book:\n1) The idea of splitting things down to smaller chunks and the ‘next action’:\nLet’s say you want to host a dinner party on the weekend. You could just write a line on a piece of paper, “prepare for dinner party” and be done with it. But, as Allen pretty convincingly shows in his book, without defining exactly what that means (using sentences that have verbs in them, in Merlin Mann’s useful phrasing) then you’re likely to procrastinate about that particular task. You’re also likely to forget things, and you’ll probably feel like you’re juggling a thousand separate small balls prior to getting ready for the party.\nSimilarly with something in the knowledge work field. If you’ve ever had a task like “write article” or “write chapter” or (even worse) “work on PhD” in your task list, you’re pretty sure to have avoided that at some point, probably often. It is the lack of specificity that really causes problems. So Allen encourages you to figure out what is the smallest single-action next step in order to move forward with your particular project. Maybe you need to read something before you can work on your next PhD chapter. But then you realise that you don’t have a copy of the book at home, so you’ll have to go to the library. But then you realise your library card needs renewing before you can take it out. Finding and specifying these chains of dependencies is a really great way, therefore, to get and keep moving with your work.\n2) Regular reviews:\nGTD encourages weekly reviews of your tasks and projects. Since reading Allen’s book, I’ve sometimes neglected my reviews, but I am certain that when I do block out an hour or so to make sure I am on track (or figure out what went wrong) each week, I feel much more in control of what’s going on. (I even have used Beeminder to make sure I keep doing my weekly reviews in the past).\nReviews keep you aligned with the various levels of goals that you have. Allen talks about the runway level (the individual specific tasks you have to do), then the 10,000, 20,000 ft all the way up to the 50,000 ft perspective. At 50,000 ft, you’re starting to talk about your purpose as a human being on the planet. At 10,000 ft, this is your list of ongoing projects. And so on. At the beginning, I was much more focused on the day-to-day actionable side of GTD, but as the years have passed I’ve become more convinced of the use of having these higher-level goals and perspectives.\n3) ‘Capture’, or an inbox to store random things during the day:\nThis was half from David Allen, half from Merlin Mann (who in turn was inspired by Allen). The idea here is that whenever you think of something that needs to be done, or an issue that you have to handle/tackle, make a note of it. If you just try to keep it in your head, you’ll either forget it, or you’ll lose energy and mental bandwidth because you have too many such items hanging around.\nI have a digital inbox in Omnifocus where I’ll make notes of tasks or things I need to handle as they occur to me. Adding it into my inbox is easy, and I know it won’t be forgotten because I’m reviewing everything at least once a week. In reality, I’m sorting through my task inbox once every day or two as well, so as to stay on top of these tasks.\nI also have a paper notebook, which I’ll use to jot notes down when I’m out and about, or when I don’t want to be using digital technology etc. I’ll transfer any tasks or notes from this notebook into Omnifocus usually at the end of every day, but if I’m particularly busy then I’ll just do it during my weekly review on Sundays.\nThis turned out to be a longer post than expected. I barely scratched the surface of my workflow around Omnifocus but I think you’ll have to develop your own if it is really to stick. Let me know if you find these concepts useful, or if you end up having some success with a task management system like Omnifocus."
  },
  {
    "objectID": "personal/2016-09-05-phd-tools-freedom.html",
    "href": "personal/2016-09-05-phd-tools-freedom.html",
    "title": "PhD Tools: Turn Off the Internet with Freedom",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nFreedom does one thing and it does it well: turning off the internet (or parts of it). It removes temptation by giving you a time slot where the internet is turned off (and no way to turn it back on) on both your laptop and your phone. [Note: at the current moment there is no Android version of Freedom, but it’s been a long time coming so I imagine that will be released in the near-term future – a recent twitter query suggested “end of the summer”].\nYou can run it on an ad hoc basis – i.e. you decide that you want 30 minutes of ‘freedom’ starting now, click, and then you’ve turned off the internet. OR you can pre-schedule those times (my preference) such that you can say Every Monday-Friday, I want to turn the internet off from 5am-12 noon every day. That time will thus be core time for writing, reading or using in some other kind of productive manner, free from distractions and interruptions.\nYou can tweak the settings so that you’re not turning off the entire internet. You can make your own custom blacklist of sites that you know are kryptonite for you. (RescueTime is a great way of coming up with that list of which sites you’re sinking too many hours into, especially when you have a few months of data). I don’t particularly like this selective blocking because there’s always going to be a new site of some kind or other that I haven’t preemptively added to my blacklist. I don’t need any access to the internet for my work, actually, so it’s easiest to just turn it off completely.\nIn short, Freedom is great for aligning your goals (i.e. write words for my PhD every day) with a reality in which there are many shiny sites and videos and social media streams to follow. If you can find a way to turn that all off (or down to as minimal a level as possible) you’ll get a lot more done and feel better at the same time."
  },
  {
    "objectID": "personal/2016-09-02-existential-battles-climbing-amman.html",
    "href": "personal/2016-09-02-existential-battles-climbing-amman.html",
    "title": "Existential Battles: Climbing in Amman",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“960”] Me, climbing at Fuheis last week [/caption]\nOver the past few months, since submitting my PhD (and then successfully defending it) I’ve been engaged in a number of activities that push me outside my comfort zone. From swing dancing to starting a new small business (the 99 Names Challenge) to learning how to code, I’ve tried to push the envelope of what I know. Like many of us, I’m a creature of habit and routine. I like my routine and my habits. But I also know that those habits and routines – the same ones that delivered results and even a PhD in the past – can grow stale. If I’m to grow – professionally or personally – I have to get more comfortable with change and with discomfort. The best way to start figuring this out, I have found, is to expose myself to newness and that discomfort as often as possible.\nOne of the things I’ve chosen to pursue is climbing. I’d done some bouldering in Holland (at the Delftsebleu centre) a few months back, and a good friend of mine here in Amman mentioned that she does top-rope climbing. The centre here happens to be the biggest climbing facility in the whole Middle East (take that, Dubai!) and there are knowledgeable staff and challenging walls etc so I started going twice a week.\nI used to be a runner, but some bone/muscle issues in my foot meant that I haven’t actually been running for a year or so. I know and have a strong appreciation for the way exercise and moving my body in general makes me feel better and work better, so I’ve been looking for a sport or activity to replace running in the meanwhile. (The running was probably a reason why I’ve neglected any kind of muscular strength training of my upper body. Runners like to be as lean as possible.)\nNow is probably also a good time to mention that I have a fairly intense fear of heights. I’m not exactly sure when or where it started, but some key experiences in my mid-childhood certainly contributed to it becoming what it is now. I went to a boarding school in the north of England, near York, so there were lots of outside activities. During the summer, and at ‘holiday camp’-type experiences, we were taken to do various adventure challenges on the weekends.\nMy ‘adventures’ included abseiling off high bridges, potholing in claustrophobia-inducing narrow passageways (and having my foot get stuck half-way), as well as various obstacle courses positioned in trees and so on. I resented the fact that we had no choice in the matter and I resented the fact that it was less about training or learning a skill than simply having an experience. I remember being pushed off the bridge by the instructor, clipped into a harness but unsure whether I’d survive the descent.\nSince that time, I’ve avoided activities or experiences that necessitated me visiting high-up places. Confession: I even never made it all the way up Kandahar’s Forty Steps (chilzina) for this reason. Halfway was my limit.\n[caption id=“” align=“alignnone” width=“590”] Amman’s ClimbAt centre [/caption]\nCut to the present day: I’m 10 or 15 metres up a wall at Amman’s Climbat centre. This seems to be the point where things shift. My existential battle begins. I use those words only partially in jest.\nThe first time I tried rope climbing, I only made it to that half-way point, not knowing to trust the rope, not knowing to trust the knot I’d tied or the harness or a million other things.\nNow, I can make it half-way up without too much angst, but then it begins. Rivers of sweat open up all over my body. The most distracting ones are the unceasing flow on my hands. Climbers use magnesium chalk to deal with this problem, though mostly it’s just everyday sweaty palms. I dip my hands into the bag, holding on to the wall with my other hand, feeling my grip slip as the waterworks go to town. I see that my palm is sufficiently white with chalk. I swap hands, repeating the process with the other, only to find that in the meanwhile my first hand has sweated through the first application of chalk.\nMy inner dialogue kicks up. I wonder why I’m here, on the wall, trying to climb up. I look at the rope and the knot, wondering if I would even be able to tell if there was something wrong with it, I look around me at the other climbers, each breezing up their respective paths on the wall with seeming ease. Sometimes I look below me.\nI try to talk myself down. It’s a different kind of anxiety from that I’ve experienced before public speaking, that social anxiety that makes your heart race, your stomach churn and the adrenaline pump. Up on the wall, it isn’t that adrenaline rush I feel. In fact, aside from the sweating, it’s more symptomatically benign, expressing itself in the form of a puzzle or a predicament that I can sometimes remain detached from.\nUsually the thing that works best is to try to focus on the physical experience of the moment, on my breath and what that feels like in my body, on the sensations of my fingers on the wall, on the feeling of gravity pulling me back down towards the earth. That sometimes manages to carve enough space that I can then try to think about the problem more analytically – the problem of which step to take next. If I’m stuck in my existential loop it’s hard to make those decisions and I end up wasting energy trying and retrying the same holds and foot movements, to no avail. This tires me out on a muscular level and the problem is compounded.\nA week ago, I set my mental discomfort to one side and went outdoor climbing with some friends to a wall or crag near the city of Fuheis (see the photo at the top of this post). Climbing outside felt like even more of a proposition than the indoor wall. More possibility of failure, perhaps. I’m not precisely sure. It’s sometimes hard to put my finger on the precise configuration of my fear. But it ended up going well. I ascended the wall, nothing went wrong and I even enjoyed the experience. It took me an age and a half to get up, but the getting up there was all me. I’m less likely to do regular outdoor climbing, since it’s more of a hassle to arrange, but it’s not going to be something I say no to in the future.\nNeedless to say, my ongoing climbing practice is exactly that: a work in progress. I’m working on both the physical and the mental blocks simultaneously and while I’m fairly confident that I’ll be in a more confident and stronger place in a month or two from now, I’m also frustrated by the slow pace of progress.\nTo cherry-pick signs of improvement, I’m no longer quitting half-way up the wall. Most times – as long as the route isn’t too difficult – I generally reach the top, even if it sometimes means multiple iterations of sweaty-hand-mind and multiple recommitments to completing the route. Even though climbing isn’t necessarily a sport where you use your arms much – it’s much more about your legs and how you balance and position your body – you do need at least *some* upper body strength and this is starting to come. I get a pleasurable sense of satisfaction when I return home after half a day spent at the climbing wall, and I’m wearing my muscle soreness as a badge of achievement.\nI’m pretty sure that the solution to my mid-wall fears rests in being more conscious of what’s going on and what I’m feeling earlier in the climb, and there are a bunch of mental exercises and training that seem to work for many climbers. I’m keen to put some of those into practice.\nWhen it comes down to it, everything is in your mind, especially with rope climbing where the dangers associated with falling or losing your grip on the wall are pretty minimal. Worst case, you bash up against the wall and get a bruise or two. (I have a bunch of those on my knees already.) But the rest, that’s all something I can work on and improve at (I hope). Watch this space for an update in a few weeks once I’ve been doing battle for a bit longer."
  },
  {
    "objectID": "personal/2016-09-01-phd-tools-mellel.html",
    "href": "personal/2016-09-01-phd-tools-mellel.html",
    "title": "PhD Tools: Mellel for Layout and Final Presentation",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nMellel is what I use for the final formatting of documents. It might be overkill for some, but in the case of my PhD, the extra features really saved me some time and headaches.\nAt first glance, there isn’t much to distinguish it from something like Pages.app or Microsoft Word. Mellel is a word processor. It allows you to format how the text is presented on the page. The level of control over those decisions is what distinguishes Mellel over the free/default alternatives.\nFor example, styling formatting for certain types of text is easy in Mellel. Want to change the way all headings of a certain level are formatted? Mellel can do this. Want to manage the formatting of Arabic, Pashto and Dari text without worrying that things will come out the wrong direction? Mellel is designed to handle these right-to-left languages and scripts. Want to do things with bibliography formatting and scanning? Mellel plays well with Bookends and the other reference managers. Similarly with things like your Table of Contents: Mellel handles it all with style (literally!).\nAn alternative to Mellel is Nisus Writer Pro. As far as I can make out there isn’t that much difference between the two. Mellel also has a version for iPad so you can work on documents on the go as well."
  },
  {
    "objectID": "personal/2016-08-30-2016-8-phd-tools-scrivener.html",
    "href": "personal/2016-08-30-2016-8-phd-tools-scrivener.html",
    "title": "PhD Tools: Scrivener for Writing Long Things",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n[caption id=“” align=“alignnone” width=“700”] I spent several years with this particular file… [/caption]\nScrivener is the go-to tool for anyone working on longer structured pieces of fiction or non-fiction. It’s great for structuring your work as well as the writing itself.\nWhen you write a PhD, it’s important to keep word counts in mind from the beginning, otherwise you’ll be left with hundreds of thousands of words and only 80,000 permitted to submit to the university and your examining committee. Scrivener allows you to manage the word counts of individual sections and their sub-sections (see the image above). It offers a variety of ways of displaying these word counts, setting goals and generally staying on top of this important metric. Of course, PhDs are more than just the number of words you manage to type, but I’ve met enough people who wrote too much to know that this is a common problem.\nScrivener also excels at structuring texts. You have 80,000 or maybe 100,000 words to write, so you split it up into chapters, but then those chapters must be split into chunks of roughly 500-1000 words as well. You can do this structuring using a corkboard-style visual interface (that I never use much and don’t particularly like, but am fully willing to concede that some people do like and use it) or a more standard outline tool.\n(Note, too, that there are 1001 other bells and whistles that come along with these core functions. It’s highly customisable and adaptable to your specific needs. You can tag, show selective views of your text etc etc to your heart’s content. There is also an iOS version for your iPhone / iPad that some people who are more mobile might find useful).\nAnother thing that PhDs seem to involve is references and footnotes. Scrivener works beautifully together with the major bibliographical reference managers (Bookends, Sente etc) so you can rest assured that you won’t have any trouble there.\nFinally, it’s easy to get things out of Scrivener, when the time comes. Sometimes you just want a copy of a single chapter to show to your supervisor, minus incomplete footnotes and in-text notes or annotations to yourself. Such a custom export is easy to set up. Similarly, when you’re finished with the drafting and want to work on the presentation (more on Mellel in a separate post) somewhere else, it’s easy to export exactly as you want."
  },
  {
    "objectID": "personal/2016-08-26-phd-tools-devonthink.html",
    "href": "personal/2016-08-26-phd-tools-devonthink.html",
    "title": "PhD Tools: DevonThink for File Storage and Discovery",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n[caption id=“” align=“alignnone” width=“700”] Discovering similar notes in one of my DevonThink databases [/caption]\nI first heard about DevonThink in the same breath as Tinderbox. They go together, though they serve different purposes. Some people want to make an either/or decision about which to use. I see them as sufficiently different to assess them on their own merits and as per your usage scenario.\nAs with all tools, you should come to the decision table with a set of features that you’re looking for. Don’t just shop around for new things for the sake of newness or for the sake of having a really great set of tools. These programmes are not cheap. Luckily almost all of them come with generous trial versions or periods, but I don’t recommend ‘newness’ as a feature of any particular merit.\nDevonthink (I use the Pro Office version) is a place to store your files and notes. It can, I think, take any file you can throw at it. It comes with software for processing PDFs into fully-searchable documents (OCR software, in other words) which is part of the reason why the license for the Pro Office version of the programme is so expensive.\nIf you’re anything like me, you’re drowning in PDF documents. They all come with helpful names like “afghanistan_final_report_02_16.pdf” and unless you have a rigorous file hierarchy and sorting system, you’ll probably be unable to find one file or the other. And using the basic file hierarchy system for storage doesn’t help you with situations like when you want to store the same file in multiple folders (i.e. what if a report is about Afghanistan and Tunisia). (DevonThink has a feature which allows you to store the files in multiple locations, but without saving two copies of the file. Any changes or annotations you make in one file will automatically be transferred to the other).\nYou might ask yourself why you would need DevonThink and Tinderbox (see this post for more). The short answer is that they store different kinds of files/data, and that DevonThink is less about thinking than about storage (to a certain extent) and discovery.\nOne of the key features of DevonThink Pro Office is its smart searching algorithms, its ability to suggest similar texts based on the contents of what you are looking at, etc. It does this by means of a proprietary algorithm, so I can’t really tell you how it works, but just know that it does. It works best on smaller chunks of text. In this way, I was reading through a particular source from the 3 million-word-strong Taliban Sources Project database and then I clicked the “See also” button and it had found a source I would never otherwise have read on the same topic, even though it didn’t even use one of the keywords I would have used to search for it. It uses semantic webs of words to figure this stuff out. Anyway, beyond a certain database size, this power becomes really useful. It can also archive websites, store anything including text, do in-text searches on e-books etc etc. (Read more on how I use DevonThink for research in general here.)\nI also used it a little as an archive for substantive drafts / iterations of the writeup process. That’s another important part of the process: making backups of many different kinds. I never found any use for them, but at least they were there (just in case).\nIf you’re a data and document hoarder at heart, like me, you’ll soon have a Devonthink database (or several databases, split up by topic) that is bigger than you can fully comprehend it, or remember what was inside the files. At that point, search becomes really important. Not just a straightforward search, but the ability to input ‘fuzzy’ terms (i.e. if you search for “Afghanistan” it’ll also find instances where it’s incorrectly spelt “Afgahistan”), and boolean language, into your query is really powerful/useful. DevonThink is an amazing search tool. The company that developed the database software also make something called DevonAgent, which is basically a power-user search tool for the internet. Google on steroids, if you will. Fully customisable, scriptable… you can really go crazy with this stuff. I use it, but my PhD wasn’t really about searching things on the internet, so I didn’t use it much for my research or writeup. But it’s a great tool, too.\nIn short, DevonThink is a research database tool that will help you store and find the documents that relate to your research, and do smart things to help you find sources and texts that maybe you’d forgotten you’d saved. Highly recommended for anyone working with large numbers of documents."
  },
  {
    "objectID": "personal/2016-08-24-2016-8-phd-tools-trello.html",
    "href": "personal/2016-08-24-2016-8-phd-tools-trello.html",
    "title": "PhD Tools: Visualise Structure and Kanban Flow with Trello",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n[caption id=“” align=“alignnone” width=“700”] What was leftover by the end of my PhD submission process… [/caption]\nI used Trello for structuring my PhD argument and for tracking my progress during the drafting and redrafting of the final text.\nTrello is primarily associated with the Kanban workflow / movement and as such it offers a fast and easy-on-the-eyes way to visualise structure, the passage of tasks through a particular workflow and so on.\nIt only works with an internet connection, however, which makes this a somewhat qualified recommendation. The mobile apps associated with Trello also lack an offline mode.\nTasks are split up into lists, and these are organised in a sequence. Thus for me, my lists at one point were my different chapters. It’s easy to email things (links, notes to yourself, or anything else text-based) into your lists from outside Trello, so it can function as a useful ‘bucket’ where you can deposit things you want to research in the future, or just tasks that need to be performed for a certain chapter.\nIt’s a way of seeing what needs to be done, or what you want to add to a particular chapter, at a single glance. Not essential, but I found it useful at certain junctions of the editing process."
  },
  {
    "objectID": "personal/2016-08-22-2016-8-phd-tools-beeminder.html",
    "href": "personal/2016-08-22-2016-8-phd-tools-beeminder.html",
    "title": "PhD Tools: Beeminder",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\nI feel like I’ve mentioned the end of the PhD several times in recent posts (PHD IS OVER!). It occurred to me that it might be useful to go through some of the tools and principles that I found most useful in completing the doctoral thesis, the research and the work in general. Part of this is by way of giving thanks to the application or methodological creators, and the other part is me thinking that others (future / current PhD students?) might find this useful.\nIt took me many years to finally settle on these tools. It would probably be unwise to adopt my entire writing style and process for yourself, because everyone’s unique. I read a lot of books, blogposts and discussed things in forums and at meetings with others. This is all the product of a lot of procrastination (some active, some just resulting from hanging out on twitter or subscribing to a bunch of productivity-related blogs in my RSS reader).\nEach post will vary in size. For some I’ll go into a bit more detail because the principle will be somewhat unknown. Others are mega-players in the tech world so I’ll just tip my hat in their direction."
  },
  {
    "objectID": "personal/2016-08-22-2016-8-phd-tools-beeminder.html#minding-the-bees",
    "href": "personal/2016-08-22-2016-8-phd-tools-beeminder.html#minding-the-bees",
    "title": "PhD Tools: Beeminder",
    "section": "Minding the Bees",
    "text": "Minding the Bees\nMy first pick is, of course, Beeminder. (I’ve written about Beeminder before here.) The principle behind this service is pretty simple: you commit to doing a certain thing (or things) by a certain date (or regularly each day etc) and if you don’t do them, you’re penalised with money taken from your credit/debit card. The amount of money taken depends on whether you’re a first-time offender (free, or $5), but then it increases exponentially. Pretty soon you’ll be facing $270 or even higher fines.\nNeedless to say, this is a pretty strong motivator. You can hear about some of the nitty-gritty details in a podcast interview I did with Matt Trevithick and the founders of Beeminder, Bethany Soule and Daniel Reeves.\nI have used Beeminder for a really wide variety of things – not just for my work but for my personal life, too – but in terms of my PhD, I had three main goals it supported:\n\ntracking the amount of time I spent writing. You can hook up RescueTime (a passive activity tracker on your laptop) to feed into Beeminder. I can then say that I want to make sure I do a minimum of 1 hour of writing in Scrivener each day (for example), and Beeminder keeps track of the rest. This is a good thing to track because, ultimately, the PhD is all about keeping writing. You can get lost in the research, but after a certain point you just have to deliver it and ship the damn thing. This keeps you honest about the writing part, the sitting down in the chair and putting words on the page.\nwords drafted – this one’s a bit more delicate, since often when you’re starting out, drafting a new section or chapter, the words that come out are useless drivel (or replace with a far less charitable way of describing their quality, and say hi to my inner voice while you’re there!). At the beginning, doing basic drafting, it’s hard to get started because you feel everything has to be perfect. The best antidote to this is to work on a ‘shitty first draft’. Here, the idea is simply to churn out enough thoughts to fill the blank space in the outline, or book, or chapter or wherever.\nA specific example: I flew to Karachi in late 2012 to hammer out the first draft of my dissertation. I setup a Beeminder goal of having 100,000 words of text (approx the maximum word count allowed for submission to the university) and a date 6 weeks in the future, and I got writing. Beeminder calculates and tells you how many words you have to get done each day in order to stay ahead of the curve. (There are graphs. They are awesome). As long as you keep writing, you’re ok. And I did it. Most was horrible, and some of it was inner conversations between myself and myself about the subject under consideration, almost all of which I had to rewrite in some shape or form later on. But… it was words on the page, and it was me thinking through the issues. It was essential.\nSources Read – this might be unique to me, but at some point I had to return to the newly-gathered sources of the Taliban Sources Project. I looked in my DevonThink database (about which, more to come in a future post) and saw I’d flagged 1000+ articles to reread, catalogue/tag and integrate into the main thesis argument. So I plugged those numbers into Beeminder, gave myself a workable daily rate (50 or 100, I think) and then it calculated the rest and kept me honest.\n\nSo, to sum up:\n\nBeeminder forces you think backwards from your goal if you have a specific endpoint in mind. This is extremely valuable as it makes sure you’re not being overambitious.\nBeeminder gives you accountability. It keeps you honest. This is what I initially found was most valuable, but later on I needed this less. YMMV.\nThe community of Beeminder users is wonderful. The forum is a great place to get ideas, discuss approaches / failures etc.\nIt works! Many people have had great results using Beeminder.\n\nI’m not going to say I couldn’t have written my PhD without Beeminder, but I’m almost saying it. Go check it out!"
  },
  {
    "objectID": "personal/2016-08-19-2016-8-walking-amman.html",
    "href": "personal/2016-08-19-2016-8-walking-amman.html",
    "title": "Walking Amman",
    "section": "",
    "text": "I’ve been walking around Amman a little in the past couple of days. My poor sense of direction with the city’s somewhat haphazard street layout mean I make use of digital GPS maps on a regular basis. In Europe or North America, Google Maps is my service of choice, with due acknowledgement of their general creepiness.\nBut I discovered yesterday that Google Maps is pretty atrocious when walking around Amman. Either their data is old and of poor quality, or the algorithm for calculating time/distance between two points is not properly calibrated for a city with many hills. If you look on Google Maps’ display, you’ll see what looks like a flat terrain. Everything can seem very close. If you look out of the window, or walk on the streets, you’ll see that hills and a highly variable topography are very much a part of the experience of the city. (This gives some idea of it).\nGoogle Maps knows how to deal with hills or variable terrain. After all, San Francisco, close to their centre of operations, is a pretty hilly city and I found the maps and the estimated timings worked pretty well when I was there last year. Which suggests to me that the problem isn’t that Google forgot to take into account topography but rather that the data is poor.\nI’m studying data science-y things these days, so I thought a bit about how they might improve this data. Some possible solutions:\n\nThey’re already monitoring all the data coming from app usage etc, so why not track whether its estimations match up with how long people actually take to walk certain streets/routes. Mix that in with the topography data, and average it all out.\nThey could send out more cars. I don’t know how accurate the map data for driving in Amman is, but some anecdotal accounts suggest that it suffers from similar problems. This is probably too expensive, and I’m assuming it’d be preferable to find a solution that doesn’t require custom data collecting of this kind. Maybe something for when the world has millions of driverless cars all powered by Google’s software, but for now it’s impractical as a solution.\nFind some abstract solution based on satellite-acquired topographic data which takes better account of gradients of roads etc.\n\nFor the moment, Google Maps is pretty poor user experience as a pedestrian. Yesterday evening I was walking back home from the centre of town. The walk would, Google told me, take only 12 minutes. 40+ minutes later I arrived home.\nOthers have noted this same problem and suggested an alternative: OpenStreetMap data. The data is unattached to a particular app, but I downloaded one alongside the offline mapping data for Jordan/Amman. It seems pretty good at first glance, and I’ll be testing it out in the coming days. I’m interested o learn why it seems to perform better. My initial hypothesis is that its data is just better than that which Google Maps is using."
  },
  {
    "objectID": "personal/2016-08-18-2016-8-jordan-diaries-start-with-geography.html",
    "href": "personal/2016-08-18-2016-8-jordan-diaries-start-with-geography.html",
    "title": "Jordan Diaries: Start with Geography",
    "section": "",
    "text": "I arrived in Jordan this morning and am beginning my attempts to get to know the lay of the land. First thing is always geography. If I don’t have those mental hooks to know that someone is talking about a particular muhafadha  / governorate, I can’t properly engage in the conversation / information, so the only way to do this at the beginning is to learn a bunch of those initial hooks in a more deliberate fashion. Later on, you can learn things naturally, incorporating new pieces of information as you hear it, but at the beginning you need that initial scaffolding.\nTo that end, I’m starting off with Memrise’s Governorates of Jordan course, which teaches you all the names, plus their administrative capital town. I’ll be doing the same with Amman’s geography over the coming days, too, though no pre-written course exists so I will have to make one myself. For Amman, I can combine the more abstract map-work with travelling to see the various places with my own eyes. Then I can combine the two together for a solid grasp of the geography and different neighborhoods. In due course I will also be travelling around the various parts of Jordan for myself, but for now I have to content myself with learning all their names."
  },
  {
    "objectID": "personal/2016-07-24-2016-7-chondrichthyan-fun-a-review-of-shark-mooc.html",
    "href": "personal/2016-07-24-2016-7-chondrichthyan-fun-a-review-of-shark-mooc.html",
    "title": "Chondrichthyan Fun: a review of Shark MOOC",
    "section": "",
    "text": "Everyone needs a good distraction. For the past four weeks, mine has been an online edX course run by the University of Cornell and the University of Queensland (Australia) entitled, Sharks! Global Diversity, Biology and Conservation. With my PhD newly submitted to external examiners and nothing to do but to wait until the oral viva examination, this was my reward to myself. The course would be a way to engage with the world in a different way, through the medium of biology, and maybe open myself to new possibilities.\nI’ve been interested in ecology, environmental science and the hard sciences in general for a while, but as an outsider to the discipline, it’s hard to find a way in. This course (the “Shark MOOC”, as it became known) was a way to engage with biology, ecology and start to expose myself more to scientists doing their work.\nI’ll spare you my raw enthusiasm for the course and tell you every fact I learnt about Chondrichthyes — yes, that’s the label that refers to sharks, rays, skates and a weird group of mainly deep-sea fishes called chimaeras. The course wasn’t really a deep dive into any aspect of shark biology, but surveyed the whole territory with moments of detail and depth. I learnt about:\n\nShark Anatomy — this was presented in a decent amount of detail. I found it a bit overwhelming at the beginning, having to draw (and then test my recall again by redrawing the same diagrams without reference to books) shark brains, the different types of tails and so on.\nShark Senses — did you know that sharks have two extra senses?!\nThe Diversity of Shark Habitats and Behaviour — getting to know all of the different places where sharks hang out and what they do there, and the similarities and differences between all the different species.\nPalaeontology — we examined old fossils of sharks and rays, connecting us to this group of ancient fishes that predates (and mostly outlived the extinction of) the big dinosaurs.\nEcology — the final week of the course was wise to connect the particularities of what we had just been studying (i.e. sharks and rays and their anatomy and such) to the bigger picture of ecosystems, conservation, ethical questions around shark ‘ecotourism’, the realities of what industrial fishing has done to shark populations and so on. This way, the detail of previous weeks brought us to a deeper understanding of topics and issues that otherwise might have only been covered superficially.\nMythology — the course blended in a lot of ‘mythbusting’, since media and entertainment have shaped the way most of us (including yours truly, prior to taking this course) think about sharks. It was fascinating to explore how this demonisation had played out (since the release of Jaws in 1975/6) in public debate and understanding.\n\nThe course was mainly taught by William E. Bemis, Professor of Ecology and Evolutionary Biology at Cornell University, and Joshua Moyer, a research technician at Cornell University. Both transmitted great enthusiasm for their subject to their students and seemed to have an endless list of friends and colleagues on speed-dial from whom students got to learn. This was one of the best things about the course, the fact that we got to (virtually) hang out in so many different countries, with so many different scientists, both under the water and back in the lab.\nI knew nothing about nothing when it came to sharks before the course, and so I was surprised to learn that there is a very active enthusiasm for the study of Chondrichthyes in the USA, especially among children. There were primary-school-age students participating in course discussion forums, and this was great to see. I learnt about the activities of The Gills Club, a “STEM-based education initiative dedicated to connecting girls with female scientists from around the world, sharing knowledge, and inspiring shark and ocean conservation”. What a great initiative!\nMost of the course was presented through videos and/or text with diagrams and photos. Having taken a number of other MOOCs, I know that two of the challenges in teaching people through this medium are (a) keeping it interesting and (b) making sure that learning is active where possible. There were some multiple choice questions which formed the entirety of the course assessment, but they covered only a tiny fraction of the material being taught and were sometimes oddly worded.\nNevertheless, it came as no surprise to me that the course authors took two years (and probably a decent amount of money) to develop the MOOC, film all the different interviews and lessons and so on. It was well worth it, and I hope whoever funded the project will consider this a really worthwhile investment as something that can inspire a new generation of scientists.\nIf you study a course like this on your own, you have to find ways to make the learning process active. For me, this included:\n\nadding important concepts, anatomy diagrams and new words into Anki in order to test myself throughout. (If you haven’t heard of Anki before, it is software that uses a spaced repetition algorithm to super-charge your learning and retention of information into the long-term. I consider it an essential piece in the toolkit of anyone studying anything in the twenty-first century. Check out more here.) Anki’s useful stats and charts tells me that I added 289 cards relating to the course, most of which are now headed into my long-term memory courtesy of the spaced-repetition algorithm.\ndrawing and redrawing diagrams of anatomy or timelines or whatever it was that I was studying. Where possible I tried to find a way to keep it visual. All of my notes were taken by hand, something I’ve been doing more and more recently after studies have shown it can help recall.\ntalking to other people about Chondrichthyes. Explaining the lessons I’d been studying on a particular day was fun to do and it meant my brain got another chance to synthesise the material. Teaching newly-learned concepts to others is a great way to do this.\nfinding other ways to deepen my exposure to the material and concepts being studied. I immersed myself in documentaries and fictional representations of sharks and rays. Luckily, there is a real richness available here, from the semi-numinous expience of the BBC’s 2015 three-parter Sharkto the no-less entertaining Sharknado quadrilogy.\n\nI learnt on the first day of the course that there are sharks with twitter accounts. OCEARCH is one organisation (profiled in the New Yorker back in 2013) that attaches real-time satellite trackers to sharks. You can go to their website to follow the movements of a bunch of different sharks, most of whom are given cutesy names like Carolina or Johnny.\nWe were instructed to pick a shark and follow her throughout the weeks that followed. I selected a female white shark called Katharine (watch her being tagged in this video) and saw as she swam up the Eastern coast of the USA. Last time she surfaced she was headed out into the Atlantic ocean. (As a reminder of the realities of industrial fishing, some of the sharks chosen by fellow students ended up ‘pinging’ in their signals from fishing docks in west Africa or east Asia after being caught and killed, probably for their fins but possibly just as so-called ‘bycatch’).\nIt was fun to get to know a new community of enthusiasts for the study of a particular topic. I had deleted my @strickvl twitter account a few months back, frustrated by the noise:signal ratio and generally finding less and less reason to justify ‘keeping up with the news’. This shark course, though, saw me a bit more active on my (previously entirely passively-tweeted) @stricklinks account. I discovered fellow Chondrichthyes scientists, photographers and institutions this way and I was reminded of what Twitter felt like back in 2008 when I first joined. It was a place to meet other people, to find people who shared the same interests (back then I was all-Kandahar-all-day-all-night) around the world, to connect and cross-pollinate in a way whose speed was impossible even decades before. I think this is one of the best uses for twitter: in the beginnings of an interest in a topic, you can use it to generate a community round you, to find others who can inspire and drive you forward into learning more, people who can challenge your assumptions and model best practices in their fields.\nAs you can probably see, I found the course incredibly stimulating. I’m not sure my future is in marine biology (who knows?), but I’ve taken away a far greater appreciation for the ocean, the creatures in it and the beginnings of an understanding of how all of this starts to fit together: the history of how species grow and develop, the interactions between species, the threats and challenges to this whole system…\nFor now, though, I’d strongly recommend anyone with even a passing interest in science or in the ocean or in sharks to enroll in this course. It’s free, immensely entertaining and enriching and who knows where it will take you."
  },
  {
    "objectID": "personal/2015-12-26-reading-the-afghan-taliban-67-sources-you-should-be-studying.html",
    "href": "personal/2015-12-26-reading-the-afghan-taliban-67-sources-you-should-be-studying.html",
    "title": "Reading the Afghan Taliban: 67 Sources You Should Be Studying",
    "section": "",
    "text": "I’ll be taking a break from regular blogging for the next few months while I focus on finishing the writeup of my PhD dissertation. Most of what I’m interested in about the Afghan Taliban is their pre-2001 history, so I recently put together a list of ‘value-added’ sources that offer useful and/or unique information. As I write things up for my research, I’ll often come across someone’s name or some minutiae I know very little about. These are the sources to which I turn when I reach those moments. They’re often biased, but they’re rich in detail and in first-person observations. I’ve long grown frustrated by long analyses of the Taliban that don’t contain the results of time spent in Afghanistan and that haven’t bothered to engage with the ever-growing list of useful primary sources on the movement.\nWhat follows is an annotated version of my go-to list of sources. The first part consists of books or reports, ordered by year of publication. The second part is a list of institutions or collections of multiple sources. If you think of anything I’m missing, drop me a line and I’ll update the list. Note, too, that this list is mainly focused on pre-2001 history (often neglected by scholars of the Taliban), so I realise I’m missing various things on the post-2001 period.\nUPDATE: I added a few things as of August 2021 below. Notably, The Taliban Reader, a one-stop shop collection of primary source material, and the website of the Taliban Sources Project.\n\nBooks & Reports\n\nJere Van Dyk, “In Afghanistan” (1983)\n\nJere van Dyk’s book contains recollections of time spent in south-eastern Afghanistan (including conversations with Jalaluddin Haqqani) and Kandahar during the early 1980s. Lots of atmospheric description and snippets of discussions. Not definitive, by any means, but useful nonetheless.\n\n\nWilliam Maley, “Fundamentalism Reborn?” (1998)\n\nThis was the first mainstream book published about the Taliban movement in English. It should come as no surprise that Hurst Publishers (in London; also my publisher) were the ones to put it out. This is a fairly variable book in terms of the criteria specified above. Most essays are synoptic in nature rather than based on fieldwork or reporting from Afghanistan itself. Anthony Davis’ essay on the Taliban’s military strategy and tactics is based on time spent on the ground during the early years of the movement’s expansion, though, and offers a lot that isn’t available elsewhere.\n\nAnna M. Pont, “Blind Chickens & Social Animals: Creating Spaces for Afghan Women’s Narratives Under the Taliban” (2000)\n\n\nThis report was initially commissioned/published by Mercy Corps in 2000. The text was republished as a book in 2001. Pont was investigating the situation for women in Helmand and she managed to gather (together with a team of Afghan researchers) interviews with women in Lashkar Gah, Darwishan, Nad Ali, Gereshk and Naw Zad. She also did some interviews in Baluchistan (Pakistan). These interviews were carried out in 1998 and, as such, offer a fascinating window into the lives of women in an area where the Taliban had strong support and presence. The report/book includes lots of quotations of the testimony. It is also of interest since a lot of what emerges offers a counterfactual to what is often written about the experiences of women under Taliban rule. Download the original report here.\n\nM.J. Gohari, “Taliban: Ascent to Power” (2000)\n\nThis was another early account of the Taliban, published a year before 2001 saw a glut of books on the movement. Gohari includes some details quoted from Taliban publications which are unavailable elsewhere, but this isn’t otherwise particularly useful as a source.\n\nWilliam T. Vollmann, “Across the Divide” (2000)\n\nThis story for the New Yorker contains a lot of unique interview material and closely observed profiles of individuals in the Taliban leadership. I don’t see it referenced much, but there are some useful details in here.\n\n\nAhmed Rashid, “Taliban” (2001)\n\nOften held up as the grandfather of Taliban studies, Rashid’s 2001 work is a compilation of magazine articles originally published during the late 1990s. The book is highly variable in its contents and the sourcing is minimal. That said, it contains a lot of original reporting and quotes from various figures within the Taliban. The book was revised and reissued in 2010. Essential reading for anyone interested in the Afghan Taliban pre-2001, but treat its contents with caution.\n\nIslamic Emirate of Afghanistan, “The Official Gazette” (2001)\n\nThis book was published in an impeccable English translation in the mid-2000s by Dr Khaled Abou El Fadl at talibanbook.com. It is a translation of one of the legal compendium’s released by the Islamic Emirate of Afghanistan on September 4, 2001. The book itself is part of the Taliban Sources Project (TSP) collection, but we didn’t translate it as the quality of this version is excellent.\n\nMohammad Salah, “Narratives of the Jihad Years: The Journey of the Afghan Arabs” (2001) (2001)\n\nMohammad Salah is a journalist who wrote in Arabic for al-Hayat and various other publications. This book is a recollection of some memories and interviews he carried out with Arabs (particularly those from Egypt) who were involved in Afghanistan during the 1980s and 1990s. There are interesting details on how the Arabs interacted with Afghans during this time period. Parts of this book are quoted in my book, An Enemy We Created.\n\n\nRobert D. Kaplan, “Soldiers of God” (2001)\n\nKaplan travelled inside Afghanistan during the 1980s and this book contains memories of that time. He was in southern Afghanistan as well (unusual among foreign journalists, most of whom went with Massoud or Haqqani’s groups in the north and/or east), spending time with Hajji Latif and as such, this is useful for the richly described account of those travels.\n\nAyman Sabri Faraj, “Dhikrayat Arabi Afghani (Abu Jafar al-Misri al-Qandahari) [Memoirs of an Arab Afghan]” (2002)\n\nThis is essential reading for anyone who wants to understand how the fronts functioned during the 1980s in southern Afghanistan. The author, an Egyptian who came to Afghanistan to fight during the mid-1980s, spent time in and around Kandahar, participating in a number of important battles towards the end of the war. He includes a wealth of names and places, all of which can be used to triangulate information about the location of fronts, commanders and fighters in the south.\n\n\nDavid Edwards, “Before Taliban” (2002)\n\nEdwards’ book only explicitly mentions the Afghan Taliban movement towards the end. There are some interesting interview quotations in that final chapter. The book is also interesting for its portrait of a couple of the key religious schools and groups that operated during the 1908s war. This includes Harakat-e Enqelab-e Islami and Hekmatyar’s Hizb-e Islami with extensive primary source quotation (in the form of interview transcripts).\n\nJon Lee Anderson, “The Lion’s Grave” (2002)\n\nThis is a collection of Jon Lee Anderson’s dispatches for the New Yorker, published post-2001 but including recollections of the pre-2001 Taliban government as well. There are recollected stories about Mullah Mohammad Omar, for instance, that you won’t read anywhere else.\n\nPeter Bergen, “Holy War, Inc.: Inside the Secret World of Osama bin Laden” (2002)\n\nThis was the writeup of Peter Bergen’s earliest investigations into bin Laden’s activities pre-2001. It includes time spent in Afghanistan during their 1990s rule of the country (including conversations with senior figures during that time). There isn’t that much of value relating to the Afghan Taliban, however, in this book, and you’d be better off reading his later edited volume, The Osama Bin Laden I Know.\n\n\nWayne Madsen, “Forbidden Truth” (2002)\n\nI hesitate a bit about including this book in among the list of sources. This is a somewhat overblown account of some of the negotiations between the Taliban and foreign governments over oil pipelines and the like during the time of the Taliban’s government. It includes original documents, however, so check those out, albeit with a forewarning that the narrative/analysis in this book strains credulity at times.\n\nMuzhda, Wahid, “Afghanistan va panj sal-i sultah-i taliban [Afghanistan Under Five Years of Taliban Sovereignty]” (2003)\n\nWahid Muzhda was an official working in the Taliban’s Foreign Ministry during their rule over Afghanistan during the 1990s. This short memoir includes a number of recollections from that time, for example of meetings between senior Taliban leaders and foreign delegations. It’s unclear how accurate all of the book is, but a lot has been confirmed through other interviews so, to my mind, this is a useful atmospheric source for insight into the inner workings of a Taliban ministry from a somewhat dispassionate observer.\n\n\nMuhammad Amir Rana, “A to Z of Jehadi Organizations in Pakistan” (2004)\n\nMuch of the information in this book is of dubious provenance, and it’s very difficult to verify or check any of it. Nevertheless, there is a lot of ‘stuff’ here. It covers “Jehadi Organisations” in Pakistan, so the intersection with the Afghan Taliban is relatively limited. There are some interesting clippings from the late 1990s, however, and accounts derived from interviews that are unavailable elsewhere. Handle with care, but perhaps of use in some limited sense.\n\nMustafa Hamid, “The Cross in the Sky of Kandahar” (2004)\n\nMustafa Hamid is a prolific writer and this is just one of over a dozen titles published online since 2001. This volume specifically covers the details of the relationship between the Taliban and bin Laden (and his associates) as relayed through a series of stories from the pre-2001 period. This book isn’t available in English apart from liberal quotations used in my own book, An Enemy We Created. The Arabic version appears online from time to time, though currently I can’t seem to find a stable link to share with you.\n\n\nHusayn Ibn Mahmud, “Al-Rajul al-’Amlaaq: The Giant Man” (2005)\n\nThis is an account of Mullah Mohammad Omar by an Arab who spent time in Afghanistan during the late 1990s. The translation was made by At-Tibyaan Publications and is mostly sound, apart from Arabic-derived spellings of Afghan place names and people (“Bashtoon” for Pashtun and so on). The account is extremely hagiographic, but there are some useful details here and there. The account also includes a transcription of a Taliban-era radio broadcast in which Mullah Mohammad Omar speaks and describes the early days of the movement in his own words. Worth a read for those pages alone. An archived version of the document (in English translation) is available here.\n\nIslamic Emirate of Afghanistan, “De Afghanistan Islami Emarat de Dustoor [The Constitution of the Islamic Emirate of Afghanistan]” (2005)\n\nThis document was the product of the Taliban’s internal constitutional review process that took place from 1999-2000. IEA authorities never ratified the document while they were in power, but it was published in 2005 on the Taliban’s website along with dozens of signatures. It was scrubbed from the movement’s website a few years later and is no longer available for download online. Unchanged since the late 1990s, it offers a semi-official vision of the movement’s conception of the state and how they thought governance should work.\n\nNasir al-Bahri, “Al Qaeda From Within” (2005)\n\nExcerpts of this book are included in Bergen’s The Osama bin Laden I Know, but the full account is worth a read. Only some of it is germane to the Afghan Taliban, but there are some interesting gems in those sections relating to the relationship between the Taliban and the foreign fighters in Afghanistan during the era of the Islamic Emirate of Afghanistan (late-1990s).\n\n\nRobert Fisk, “The Great War for Civilisation: The Conquest of the Middle East” (2005)\n\nFisk reported from Afghanistan from the Soviet War through the Taliban era (albeit infrequently). He was famously one of the last to receive a visa from the Taliban to enter Afghanistan prior to the beginning of Operation Enduring Freedom on October 7, 2001. There are some recollections of the Taliban’s government and character portraits of certain officials in this book.\n\nIftikhar Murshed, “Afghanistan: The Taliban Years” (2006)\n\nMurshed was Pakistan’s Special Envoy to Afghanistan during the late 1990s and as such, this book is filled with interesting stories about the author’s time spent interacting with the Afghan Taliban. Lots of this is unavailable elsewhere, so it’s worth reading for exposure to the stories, even if it’s unclear how much of it can be relied upon as an accurate account.\n\n\nPeter Bergen (ed), “The Osama bin Laden I Know” (2006)\n\nThis is a treasure-trove of oral history accounts by individuals involved in bin Laden’s story. As such, the focus of this book is mostly concerned with things that haven nothing to do with the Taliban, but for the periods when bin Laden spent time in Afghanistan (the 1980s and then again post-1996) there is a lot of useful testimony. Much of it is unavailable elsewhere.\n\nIslamic Emirate of Afghanistan, “Layeha” (2006, 2009, 2010, 2011)\n\nThese so-called ‘rulebooks’ for the Afghan Taliban started publication in 2006 and there have been several versions issued since then. Initially a semi-random list of injunctions, these grew in complexity as they were revised by various individuals within the senior leadership — notably Mullah Baradar in 2008-9. They offer a glimpse of the kinds of problems that the Taliban movement faced post-2001, especially with regard to command-and-control of subordinate commanders and groups. The documents are publicly available at the following addresses:\n2006 Layeha (Pashto / English)\n2009 Layeha (English)\n2010 Layeha (English)\n2011 Layeha (Unavailable online)\n\n\nKathy Gannon, “I is for Infidel” (2007)\n\nKathy Gannon is a long-standing reporter based out of Pakistan and this book contains lots of first-hand interviews that the author made with Taliban figures from the early days of the movement onwards. Towards the end, the tone of the book becomes a little ‘preachy’, but otherwise this is an extremely useful collection of perspectives and conversations.\n\nSarah Chayes, “The Punishment of Virtue” (2007)\n\nSarah Chayes lived in Kandahar for several years post-2001 and her book was one of the first to really explore how southern Afghanistan worked. She delves into the history of greater Kandahar and the south and explores various biographies of key figures. As such, there’s a good deal of second-hand observations and stories.\n\nWakil Ahmad Mutawakil, “Afghanistan aw Taliban [Afghanistan and the Taliban]” (2007)\n\nOf all the Taliban memoirs, Mutawakil’s is perhaps the least interesting. This is unfortunate, given how much he must have been exposed to as Foreign Minister, but perhaps unsurprising that he’s chosen to retain a lot of stories for possible narration in the future. Nevertheless, this is part of the ‘canon’ of memoirs by senior leadership figures and as such is essential reading.\n\n\nOmar Nasiri, “Inside the Jihad: My Life with Al Qaeda” (2008)\n\nA lyrical look at life for foreign fighters in Afghanistan during the late 1990s by someone who sought to infiltrate al-Qaeda for international intelligence agencies. Nasiri encounters the Taliban during his time in Afghanistan and as such, this book offers insights and anecdotes that are unavailable elsewhere.\n\nAntonio Giustozzi, “Decoding the New Taliban” (2009)\n\nI haven’t mentioned Giustozzi’s earlier work, Koran, Kalashnikov and Laptop (2007), because it is mainly a synthesis of various secondary source materials. This edited volume, however, offers various encounters with primary sources, whether this is Joanna Nathan’s account of Taliban propaganda, Graeme Smith’s writeup of his ‘Talking to the Taliban’ interview series, or Tom Coghlan’s oral history of the Taliban in Helmand. Lots of gems quoted in this book.\n\n\nCrews/Tarzi, “The Taliban and the Crisis of Afghanistan” (2009)\n\nThis is an edited volume of essays, some of which include materials that are otherwise unavailable. A good example is that written by Lutz Rzehak in which he details the Taliban’s rule in Nimroz province through various interviews conducted with residents.\n\nDavid Loyn, “Butcher and Bolt / In Afghanistan” (2009)\n\nLoyn spent time in Afghanistan during the mid-late 1990s with the Taliban, including trips to Kandahar. As such, parts of this book offer recollections and stories about senior figures within the movement that are unavailable elsewhere.\n\nSami Yousafzai, “The Taliban’s Oral History of the Afghanistan War” (2009)\n\nThis article was published by Newsweek in 2009 and it consists of raw testimony from a variety of figures within the Afghan Taliban about their post-2001 history. The sources aren’t identified so it’s hard to know who is reliable or not, but I trust Yousafzai’s ability to turn up the kind of people would would have real knowledge of these events. In any case, treat with caution, but make use of the detail in this long set of accounts.\n\n\nAbdul Salam Zaeef, “My Life With the Taliban” (2010)\n\nI helped edit this book together with Felix Kuehn. It is the memoir of a participant in the Taliban movement, and tells the story of his life from childhood till the present day. Zaeef was present and involved in events from the 1980s onwards and tells many stories relating to the early jihad days as well as the 1990s Emirate and his role in that government. It’s essential reading, though obviously treat the recollections with a decent amount of caution as with any primary source.\n\nGretchen Peters, “Seeds of Terror: How Drugs, Thugs, and Crime Are Reshaping the Afghan War” (2010)\n\nGretchen Peters sources a lot of this book to interviews with “Western officials” (some named, many unnamed) but she was on the ground in Afghanistan during parts of the late 1990s and she did extra interviews while she was preparing the book. Because some parts are unclearly sourced it’s difficult to know how to assess some of the anecdotes, but there are a number of stories that are unavailable elsewhere.\n\nJere van Dyk, “Captive” (2010)\n\nJere van Dyk’s second book on Afghanistan details the time he spent as a Taliban prisoner in Pakistan. He relates many conversations he had with his captors and thus, like David Rohde’s account, this is a useful, albeit biased, source.\n\n\nNajwa bin Laden, “Growing Up bin Laden” (2010)\n\nThis is the co-authored account of one of bin Laden’s wives and his sons. It details time spent in Afghanistan during the 1990s as part of the book, and as such it’s interesting for the stories told about bin Laden’s interactions and relationship with Taliban leader Mullah Mohammad Omar. Treat with care, but it’s worth a read nonetheless.\n\nRohde/Mulvihill, “A Rope and a Prayer” (2010)\n\nPublished upon Rohde’s release/escape from Taliban captivity, this book contains long recollections of the author’s contact and conversations with his Taliban captors. As such, it’s a useful encounter with those perspectives, albeit from a biased observer.\n\nCamille Tawil, “Brothers In Arms: The Story of al-Qa’ida and the Arab Jihadists” (2011)\n\nTawil gathers together a number of new materials and interviews with those who knew bin Laden, were involved with Afghanistan and who interacted with the Taliban. There are unique anecdotes in this book about the relationship between the Taliban and bin Laden. To sample some of the anecdotes that relate to the Afghan Taliban, check out Tawil’s report entitled The Other Face of Al-Qaeda, published in November 2010.\n\nJames Fergusson, “Taliban” (2011)\n\nFergusson spent time in Afghanistan during the late 1990s. He also returned to Afghanistan post-2001 to report parts of this book and as such, it includes a number of interviewees and accounts of the Taliban that are unavailable elsewhere. This is a somewhat partisan account, but the interview materials make it a worthwhile read.\n\n\nKabir Mohabbat, “Delivering Osama” (2011)\n\nIt remains unclear to me why more people haven’t discovered this quirky memoir of an Afghan who gets involved with the Afghan Taliban government in order to try to limit the influence of bin Laden and others during the late 1990s. The text is available for purchase from Google Books. Lots of interesting anecdotes relating to diplomacy, oil politics and international intelligence agencies’ activities during the Taliban’s rule of Afghanistan. There are also a number of photos and scans of original documents published as part of this book.\n\nKlaits/Gulmamadova-Klaits, “Love and War in Afghanistan” (2011)\n\n\nThis book is a collection of oral history accounts from Kunduz and Takhar provinces. It covers several decades, but parts venture into the Taliban’s time in northern Afghanistan and as such this is a fascinating book. Moreover, these are not the recollections of senior leaders (with axes to grind) but rather those with little or no power. As such, this book has limited but focused value.\n\nISAF, “State of the Taliban” (2012)\n\nThis internal report on the Taliban, based on the testimonies of dozens of Taliban prisoners was leaked to the media in 2012. It offers an interesting snapshot of the war post-2001 as international involvement started to wind up.\n\nStrick/Kuehn, “An Enemy We Created” (2012)\n\nMy own book, written together with Felix Kuehn. Most of this is a summary of the research of others, though we supplemented that research with interviews with various individuals associated with the Taliban and al-Qaeda. Mindful of the fact that the original interview transcripts are often left out of research writeup, I pushed to include the raw texts as much as possible, so you’ll find lots of lengthy quotations peppered throughout the book.\n\n\nStrick/Kuehn, “Poetry of the Taliban” (2012)\n\nThis is an edited collection of poetry published on the Taliban’s website. We made sure to include examples from the pre-2001 period as well as more recent texts. Two years after this book was published, the Taliban released their own anthology/book online. That ‘official’ anthology is available in Pashto only and can be found among the collection of the Taliban Sources Project.\n\nBrown/Rassler, “Fountainhead of Jihad: The Haqqani Nexus, 1973-2012” (2013)\n\nThis is the fruit of several years’ research by Don Rassler and Vahid Brown. They use a huge trove of documents relating to the Haqqanis available as part of the Combating Terrorism Center’s (CTC) Harmony database. It’s very unfortunate that much of this isn’t publicly available, but items quoted in the book (I think) are there, in translation and in the original. The book quotes these primary source materials pretty liberally and even if you (like me) find the overall argument in parts of the book doesn’t hold up, this is still essential reading for contact with those primary sources.\n\nCarter Malkasian, “War Comes to Garmser” (2013)\n\nMalkasian spent time in Helmand with the US State Department and this book is the product of interviews he made during that time. Much like Mike Martin’s Helmand book, this explains local histories through the stories of individuals, often with extensive quotation from his interview subjects. Again, there isn’t as much on the 1980s and 1990s as I would have liked, but there is a lot here that isn’t available anywhere else and as such it’s a useful source.\n\n\nAkbar Agha, “I Am Akbar Agha” (2014)\n\nI started First Draft Publishing together with Felix Kuehn in part so that accounts like that of Akbar Agha would find a public outlet. The first volume of his memoirs, I Am Akbar Agha (another is forthcoming in Pashto), contains a lot of detail about the networks and individuals fighting together during the 1980s as part of the Taliban fronts in southern Afghanistan. As a memoir, parts are inevitably self-serving, but nevertheless there is a lot to learn from this book about the early years of the movement. Read more here.\n\nGumnam, “Kandahar Assassins” (2014)\n\nThis is the first of two books dealing with the 1980s jihad in the greater Kandahar area. Gumnam was a Afghan doctor in Quetta treating patients arriving from the war’s front lines across the border. He gathered these stories from interviews with various fighters. This first volume, Kandahar Assassins, details the lives of the assassins who worked inside Kandahar City. The second volume, Kandahar Heroes (forthcoming from First Draft Publishing), details what it was like to fight in the trenches against Soviet and Afghan government forces. Both books are rich in detail and filled with names and mini-biographical portraits of a variety of figures, many of whom would take on roles during the Taliban’s rule over Afghanistan. Read more here.\n\nMike Martin, “An Intimate War” (2014)\n\nMike Martin’s research includes a lot of interview material which he gathered in Helmand. This book is extremely valuable if you are interested in hyper-local histories of the Taliban. There isn’t as much about the 1980s or 1990s as I would have liked, but this book contains lots of information that you won’t find anywhere else.\n\n\nMufti Rasheed Ludhianvi, “Obedience to the Amir” (2015)\n\nOriginally published in the late 1990s, Mufti Rasheed’s text explains a lot about how the Taliban movement functioned and how religious clerics situated their claims to power. The text is available for the first time in a translation by Yameema Mitha. Michael Semple has written introductory materials that give the document context. I recently wrote on my blog that this was one of the most interesting primary source texts I’ve read relating to the Afghan Taliban in recent years. Highly recommended.\n\n\nFarrall/Hamid, “The Arabs at War in Afghanistan” (2015)\n\nThis dialogue between two individuals contains a lot of material and discussion of the Taliban as far as it impacted the presence of foreign fighters inside Afghanistan, particularly pre-2001. There’s lots of new details raised here, and the book is a goldmine of little stories.\n\nMullah Mohammad Omar, “Eid Statements” (Twice yearly)\n\nYou’ll have to dig around online (and offline) to find these, but he gave speeches and/or issued statements twice every year from 1995 until he died. Post-2001, there is much to be doubtful as to whether he was writing the statements himself (or as to what parts of the statements were written by him), but nevertheless they were put out under his name and that indicates something: i.e. this is what the Taliban movement wanted you to think he was writing, even if it wasn’t him doing the writing. It’s possible to do interesting compare-and-contrast exercises with all the texts of these statements from the mid-1990s up to the present day.\n\nal-Suri / al-Uyayri, “Are the Taliban from Ahl as-Sunnah?” (Unknown (pre-2001)) (Unknown (pre-2001)) (Unknown (pre-2001))\n\nThis is a compilation of two texts written by prominent Arab Islamist writers. The original texts were published during the late 1990s in response to growing unrest among so-called ‘Afghan Arabs’ about the Afghan Taliban. Al-Suri and al-Uyayri wrote in defence of the Taliban along ideological lines and this compilation/translation (again from at-Tibyaan Publications) offers various interesting details that aren’t available elsewhere. Get a PDF copy here.\n\nThe Taliban Reader: War, Islam and Politics in their Own Words (eds Alex Strick van Linschoten and Felix Kuehn)\n\nThis is a collection built on everything listed above. It’s a great starting point / one-stop shop for primary sources relating to the Afghan Taliban movement. I edited it together with my colleague, Felix Kuehn.\n\n\nDatabases & Institutions\n\nAfghan Islamic Press (AIP)\n\nAIP is a Pakistan-based news service. They had good access to the Taliban during the 1990s so their archive from that time (available for a subscription fee online) contains nuggets of information unavailable elsewhere. Post-2001 their access was less unique.\n\nAfghan-bios.info\n\nThis is an odd site, of uncertain provenance. Yet it’s undeniable that there’s a lot of information available. Profiles of individuals are often highly partisan or partial to unproven gossip, yet it’s often worth checking against the names of those you are interested in.\n\nAfghanistan Analysts Network (AAN)\n\nAAN has long been the go-to place for analysis and commentary on Afghan politics. There is an abundance of riches available in its back-catalogue of reports and dispatches. Reports are impeccably sourced and pretty much anything you read on a particular topic will be essential reading. Dive in.\n\nAfghanistan Center at Kabul University (ACKU)\n\nIf you’re in Kabul, make sure to visit the ACKU library. It has a large collection of old documents, reports, newspapers and magazines. A lot (if not most) of that is digitised and available through a partnership with the University of Arizona. There is lots available here, particularly on pre-2001/historical aspects of the Taliban.\n\nAfghanistan Research and Evaluation Unit (AREU)\n\nAREU has information in its research papers as well as in its physical library collection, maintained for many years by go-to librarian Royce Wiles. A lot of the valuable material in the library’s collection are documents that are unavailable elsewhere. Make sure to visit if you are in Kabul, but gather your wishlist of titles beforehand by using their online search tool.\n\nAfghanistanNewsCenter.com\n\nThis is the passion-project of Fawad Afghan Muslim, a sometime employee of the Afghan Foreign Ministry. He has collected wire (and other) news reports from Afghanistan and made them searchable and indexed them all by date. Best of all, his collection dates back to 1998 so anyone researching the years of the Islamic Emirate of Afghanistan has a good place to search most (English-language) wire reports from that time. It’s a bit difficult to navigate, but if you use an ‘in-site’ Google search, there is a way to search by year.\n\nBBC Monitoring\n\nThis is hard to access without an expensive subscription (or via your university or research institution) but there are real gems in this collection of reports from the early-late 1990s. It’s particularly valuable for summaries or transcripts of radio reports, many of which are now lost/unavailable.\n\nCTC Harmony Documents\n\nLots of primary source documents in this collection, most available in the original and in translation. There are more available behind the scenes, so enquire with the CTC for access to that larger collection. Not all of it has relevance to the Afghan Taliban, and what does is often tangential, but this is still an important and unique source for researchers.\n\nGraeme Smith, “Talking to the Taliban”\n\nGraeme Smith won an Emmy award for this project, and it’s not hard to see why. Interviews with over thirty Talibs in southern Afghanistan are presented in the raw, alongside extensive explanation that offers relevant context. Essential watching to understand the post-2001 Taliban. [Note, I was involved in this project in a very limited fashion, helping out with some of the subtitling of these interviews].\n\nJamestown Terrorism Monitor\n\nThis newsletter/publication has been running since 2003. The quality of reports is variable — of late they have been less-than-essential — but a few years back they were running important analyses based on fieldwork and interviews with key players.\n\nNational Security Archive\n\nThis is a goldmine for anyone interested in the Afghan Taliban, albeit from a certain perspective, that of the US government. The National Security Archive presents and collates recently-declassified documents relating to a variety of issues. The collections that contain new and interesting materials relating to the Afghan Taliban include the following, ordered by date: (each link contains a summary and links to multiple original primary source documents)\n\n\n“Afghanistan: Lessons from the Last War” (October 2001)\n\n\n\n\n“The Once and Future King” (October 2001)\n\n\n\n\n“The Hunt for Bin Laden” (December 2001)\n\n\n\n\n“The Taliban File” (September 2003)\n\n\n\n\n“The Taliban File Part III” (January 2004)\n\n\n\n\n“The Taliban File Part III” (March 2004)\n\n\n\n\n“The Taliban File Part IV” (September 2004)\n\n\n\n\n“Update: The Taliban File Part IV” (August 2005)\n\n\n\n\n“Pakistan:”The Taliban’s Godfather”?” (August 2007)\n\n\n\n\n“1998 Missile Strikes on Bin Laden May Have Backfired” (August 2008)\n\n\n\n\n“The Taliban Biography: The Structure and Leadership of the Taliban 1996-2002” (November 2009)\n\n\n\n\n““No-Go” Tribal Areas Became Basis for Afghan Insurgency Documents Show” (September 2010)\n\n\n\n\n“Secret U.S. Message to Mullah Omar:”Every Pillar of the Taliban Regime Will Be Destroyed”” (September 2011)\n\n\n\n\n“The Central Intelligence Agency’s 9/11 File” (June 2012)\n\n\n\n\n“The Haqqani History: Bin Ladin’s Advocate Inside the Taliban” (September 2012)\n\n\n\nODNI, “Bin Laden’s Bookshelf”\n\nReleased in May 2015, this is a collection of documents found in the raid on bin Laden’s house. It includes a number of letters relating to the Afghan and/or Pakistani Taliban, or sometimes details conversations with affiliates. As such, there are interesting details available here (in the originals and in translation).\n\nTaliban Websites\n\nObviously, go to the ur-source. The list of Taliban-affiliated websites is constantly changing, either as new ones are created or as they are taken offline. There are sites for each language, and for certain themes or topics (i.e. one for films, one for poetry, one for Islamic matters, another for news, another for certain magazines etc). Make use of archive.org to access old or extinct sites. Most of what you’ll find currently available dates back a few years only, so you have to be creative about finding the old stuff.\n\nThe Taliban Sources Project (TSP)\n\nRead more about this collection here. This is the largest (to my knowledge) publicly-accessible archive of materials relating to the Afghan Taliban. It’s not online yet, but we’re working hard to make it available. It consists of digitisations of Dari, Pashto and Arabic-language primary sources, but a lot of it has been translated into English as well.\nUPDATE: this collection is (mostly) available online here.\n\nWikileaks, “Gitmo Files”\n\nThis is a collection of profiles (“assessment briefs”) posted by Wikileaks relating to prisoners held at Guantánamo Bay, Cuba. A good number of those are Afghans. The quality of the information is often dubious, but information it is nonetheless. It shines a light on the US government’s conduct alongside that of the subjects it is attempting to describe. Proceed with caution."
  },
  {
    "objectID": "personal/2015-11-29-obedience-to-the-amir-or-how-to-understand-how-the-taliban-functions.html",
    "href": "personal/2015-11-29-obedience-to-the-amir-or-how-to-understand-how-the-taliban-functions.html",
    "title": "‘Obedience to the Amir’, or how the Afghan Taliban govern",
    "section": "",
    "text": "It’s finally out. I’m really glad that other researchers, journalists and anyone else with a bit of curiosity can read this translated volume.\nIn the last year of the Taliban’s government in Afghanistan, visitors to Mullah Omar’s office in Kandahar received a parting gift. As they left, the movement’s supreme leader asked them to take a slim volume from a pile beside the door. He told them that if they wanted to know how the Taliban were meant to behave, they should read the book. The books which Mullah Omar handed out were Pashto and Farsi translation of Eta’t Amir, or ‘Obedience to the Leader’. Mufti Rasheed published the original in Urdu after having toured Taliban-run Afghanistan. Mullah Omar’s endorsement indicates that he believed that Rasheed had captured the essence of the Taliban Movement. Michael Semple and Yameema Mitha have translated this important primary source and added a commentary and appraisal.\nLong-time Afghan scholar and analyst Barney Rubin had this to say upon reading the manuscript:\n\n“In war, and especially guerrilla war, the best organised party is likely to win. While numbers of fighters and weapons count, organisation determines whether the leader can use them. This book is the guide the Afghan Taliban used to organise themselves differently from other Afghan groups. Anyone who wants to defeat them or negotiate with them should understand the organisational principles that guide them.”\n\nMichael Semple has written a useful introduction in which he outlines the context of the document, and he worked on the translation together with Yameema Mitha.\nThis is one of the most interesting documents coming out of the Afghan Taliban that I’ve read in terms of helping explain how power works within the movement and, accordingly, how they govern. If you’re interested in the history or the present state of the Afghan Taliban, give this book a read."
  },
  {
    "objectID": "personal/2015-11-15-upcoming-maniac-week.html",
    "href": "personal/2015-11-15-upcoming-maniac-week.html",
    "title": "Upcoming Maniac Week",
    "section": "",
    "text": "I hereby commit to doing a maniac week. This is inspired by Nick Winter and the good people at Beeminder, namely Bethany Soule and Daniel Reeves. The idea is as follows (borrowing heavily from a format over here):\n\nI will begin at 6am on Sunday December 6.\nI will continue until 6pm on Saturday December 13th.\nI will not be checking my email at all during the week. I will also be turning off and/or disabling all chat programmes and my phone.\nI will not use any social media websites or check RSS news. (This block will be handled by the StayFocused plugin and RescueTime’s Get Focused mode.\nI will ensure I am in bed for 7 hours every night. This will be tracked via Fitbit.\nI am allowed 3.5 hours every day for things which aren’t work (showers, preparing meals, eating, rest, meditation and walks outdoors). This will be tracked using TagTime using the tag “notwork”.\nThe remaining time will be for my work. This will be tracked using TagTime and RescueTime, and my main focus during this week will be my PhD dissertation.\nAs with others’ maniac weeks, I’ll be recording the whole week using time-lapse photography, though I’ll see how much hassle it is to assemble a video after the week is finished. Also, part of my work will involve me away from the computer, writing and outlining things by hand, and anything involving interview transcripts etc will obviously have to be blurred out or blacked out. Thus, I’m not committing to posting a video, but I will publish a post-maniac-week blogpost during the week that follows.\nI reserve the right to tweak these rules (by editing this post) up until the evening of December 5. After that point it’s time to work, and I cannot change the rules any more.\n\nNo, I am not crazy. Yes, you can do one too."
  },
  {
    "objectID": "personal/2015-11-01-afghanistan-fades-away.html",
    "href": "personal/2015-11-01-afghanistan-fades-away.html",
    "title": "Vanishing Interest in Afghanistan",
    "section": "",
    "text": "Afghanistan has been fading from the international media map for several years. This chart (courtesy of Google Trends) – illustrating search interest and media publication – shows how the peak of late 2009 has been followed by a slow decline, one set to continue as international media outlets continue their pullout in favour of newer, flashier conflicts."
  },
  {
    "objectID": "personal/2015-09-24-2015-9-taliban-public-punishments-19962001.html",
    "href": "personal/2015-09-24-2015-9-taliban-public-punishments-19962001.html",
    "title": "Taliban public punishments, 1996–2001",
    "section": "",
    "text": "Executions are a recurrent motif in how historians, journalists and analysts have chosen to write about the Afghan Taliban. See the opening to Dexter Filkins’ The Forever War as one example, or this Reuters piece from May 1999. I wanted to study the role of executions and public punishments in the Taliban’s government for a while, but lacked data to place the anecdotes into some sort of context.\nThis short overview is a compilation of sources relating to the Taliban’s public punishments, 1996–2001. It is compiled from publicly available sources as well as from the materials gathered as part of the Taliban Sources Project. I think it is as complete an overview as is possible to get from these public sources, given that the Taliban weren’t shy about publicising their ‘public justice project’ – indeed, for them, the publicity was the point – and that we have multiple complete newspaper runs for the time they were in power. This was collated and triangulated with sources from Associated Press, Agence France Presse, BBC Monitoring and the Afghan Islamic Press news agency.\nAs a brief summary, I was able to find 101 incidents in total that chronicled the deaths of 119 individuals. I included some instances of public punishment not resulting in death, but this wasn’t really the focus of my search so their numbers may be underrepresented in the list. As another caveat, I was of course only looking at public executions, not anything that went on in secret as part of intelligence or domestic security operations and so on. Kabul, Kandahar and Herat were the most prominent locations for incidents and executions, with over half the total numbers coming from those three provinces alone. (Note that this may reflect a bias in whether incidents were reported from the provinces or not).\nIn any case, I wanted to present the raw data here alongside a timeline and another chart or two in case this is useful for other researchers/analysts. If you find I’ve missed an event, please drop me a line via email or on twitter and I’ll be sure to add it to the database.\nNow head over here for an interactive timeline, charts and the raw data…"
  },
  {
    "objectID": "personal/2015-09-09-2015-9-sources-and-methods-back-for-season-2.html",
    "href": "personal/2015-09-09-2015-9-sources-and-methods-back-for-season-2.html",
    "title": "Sources and Methods: Back for Season 2",
    "section": "",
    "text": "I’m very glad to be able to announce the resumption of normal services over on the Sources & Methods podcast. Matt and I took a break over the summer while I was away at Middlebury but we’re now back and excited to share a new set of interviews with interesting people doing interesting things.\nFor our first episode, we catch up with Will McCants whose book, The ISIS Apocalypse: The History, Strategy, and Doomsday Vision of the Islamic State, is about to hit the shelves. We start of with a discussion of the policy world and how it intersects with academia, moving on to ISIS, the study of Arabic as well as (small question) what keeps the cogs of history turning.\nI really enjoyed chatting with Will for this episode and I’m really excited about the lineup we have for coming episodes. We’re recording a bunch of episodes ahead of time for logistical reasons but we’ll be releasing a new one every couple of weeks so as not to overload our regular listeners.\nAs always, you can subscribe to the show through iTunes and your preferred podcast client on a mobile/cellphone. For new listeners, I’d recommend checking out our back catalogue. My four favourite episodes (in chronological order:\n\nErin Cunningham (#2): on reporting in the Middle East and Erin’s work in Gaza\nMark Bernstein (#5): on the practicalities (and abstractions) of note-taking and working with information\nRohini Mohan (#9): on writing non-fiction and the difficulties of covering Sri Lanka as a journalist and researcher\nAndrew Abbott (#15): on working with information in the twenty-first century and the use of libraries\n\nFor new listeners, I hope you can take the time to check out some of our old episodes. There’s a lot of useful information and thinking-through of difficult issues that repay (re-)listening. If you’re already subscribed, thank you and please help us by letting your friends and colleagues know about our work. Thanks!"
  },
  {
    "objectID": "personal/2015-08-28-2015-8-afp-covers-the-taliban-sources-project.html",
    "href": "personal/2015-08-28-2015-8-afp-covers-the-taliban-sources-project.html",
    "title": "AFP covers the Taliban Sources Project",
    "section": "",
    "text": "A few years back I put out a call (together with Felix Kuehn and Anand Gopal) for translators to work on a new project I was trying to get off the ground. Thankfully, that project is coming to a close, but as you can read in this article, we’ve had some bumps along the way.\n\nAcademics have criticised the British government for creating a “climate of fear” after the national library declined to store the world’s biggest collection of Taliban-related documents over concerns it could be prosecuted under terrorism laws.\nA group of international researchers spent years putting together a trove of documents related to the Afghan Taliban, including official newspapers from their time in power, poems, maps, radio broadcasts, and several volumes of laws and edicts – digitising the estimated two-three million words and translating everything into English.\nIt was hoped the project, which was launched in 2012 and included members of the British Library on its advisory board, would prove an unprecedented resource for academics and officials trying to understand the movement and the ongoing insurgency in Afghanistan.\nBut despite hopes that the library would host a master copy of the digital collection, it got cold feet at the last minute, telling the project’s organisers that they feared they could be in breach of Britain’s increasingly stringent counter-terrorism laws. (LINK)\n\n(Read the rest of the article by clicking the link above)\nThe project has been a digitisation and translation of the world’s largest archive of (Afghan) Taliban documents (dating back to the 1980s). We hope to present this in the coming months to researchers and the general public alike.\nThe AFP’s article on the British Library’s refusal to host the project has been met with incredulity by other scholars and researchers whose work often sees them dealing with primary sources:\nThomas Hegghammer (Director of terrorism research at the Norwegian Defence Research Establishment (FFI)):\n   \nAaron Zelin (Richard Borow Fellow @WashInstitute, Rena and Sami David Fellow @ICSR_Centre, PhD candidate @KingsCollegeLon, Founder of @Jihadology_Net and @JihadPod):\n\nChris Woods (journalist / researcher):\n\nAymenn Jawad Al-Tamimi (working with primary sources in the Middle East and a Shillman-Ginsburg Fellow at the Middle East Forum):\n\nGraeme Smith (Senior Analyst for the International Crisis Group in Afghanistan):\n\nThe Guardian newspaper (UK) has a story out as well covering the reasoning behind our disappointment with the decision.\nThe New York Times (USA) also published a story with some really interesting comments on the legal aspects of the case:\n\n“David Anderson, the independent reviewer for Britain’s antiterrorism laws, said Friday that the Terrorism Act was a broad law that could be even more broadly interpreted “by police and lawyers who want to give cautious advice.” Such interpretations could easily impinge on academic freedom, he warned.\n\n\n“If this law were interpreted to prevent researchers from accessing Taliban-related material that would impact their academic work, it would be very regrettable,” he said. “That’s not how academics work.”\n\nAl-Jazeera have followed up with a story including comments from Dr Rizwaan Sabir, an academic at Liverpool John Moores University:\n\n“The decision of the British Library may seem far-fetched to some but the law is clear…it says that sharing information that encourages or is useful for terrorism is a criminal offence,” Sabir told Al Jazeera.\n\n\n“Simply holding or sharing the information is a criminal offence that can carry a prison sentence…such laws have a deeply damaging effect on the freedom of scholars to research.\n\n\n“Where such offences exist, a climate of fear and self-censorship becomes inevitable, and free scholarly inquiry becomes next to impossible.”\n\n\nSabir was himself arrested in 2008 while conducting research on terrorism for downloading an al-Qaeda training manual from the US Department of Justice website. In 2011, he won compensation and an apology from the British police for false impirsonment.\n\nAnd two pieces in Arabic. Click here for al-Sharq al-Awsat’s writeup, and click here for an article over on BBC Arabic.\nUPDATE: Was just on the BBC World Service’s Newshour programme talking about all things TSP/British Library. Listen here:\nUPDATE: Two analysis/comment pieces have also been released:\n\n“Self-Censorship in Action: The British Library Rejects Taliban Archive” by Shaheed Fatima – offer the legal case that probably supported / lead to the British Library’s decision\n“British Library Won’t Hold Taliban Documents for Researchers Due to Anti-Terror Laws” by Peter van Buren – summarises some of the broad issues\n\nUPDATE: Two further commentary pieces in NYU’s School of Law web journal and forum Just Security:\n\n“The British Library Did Not Need to Self-Censor” by Clive Walker\n“The British Library and the Taliban Sources Project: A Short Reply to Professor Walker” by Shaheed Fatima\n\nUPDATE: A long article summarising much of the above in conjunction with new interviews with Felix and Mike:\n“British Library Declines Taliban Archive, New Hosts Step Up” by Lisa Peet\nUPDATE: A reply from Clive Walker to Shaheed Fatima’s post:\n“A Short (Yet Still Forlorn) Reply in the Taliban Sources Project Debate” by Clive Walker"
  },
  {
    "objectID": "personal/2015-03-21-2015-3-how-i-use-goodreads-to-pick-what-i-read.html",
    "href": "personal/2015-03-21-2015-3-how-i-use-goodreads-to-pick-what-i-read.html",
    "title": "How I use Goodreads to pick what I read",
    "section": "",
    "text": "So far this year, I have read 35 books. I’m trying something new for 2015 so I thought I’d write up the outline in case someone else finds it useful. As I wrote at the end of last year, I’ll be reading 150 books over the course of 2015. That’s fifty books more than I read in 2014. The point of it is to expose myself to lots of different ideas, different styles, different perspectives. I’ve found that 150 probably isn’t an impossible amount to be reading (less than three a week) and I really relish brushing up against interesting authors and ideas.\nI’ve used Goodreads as a way of tracking what I read for a long time now. I’m lucky enough to have an interesting group of ‘friends’ who also use it (more or less regularly) so there’s usually a decent amount of new or niche books that I discover that way. I also use it as a way of noting down the books I want to read in the future. (Incidentally, I’ve never really had a problem in finding something new to read. The list of books I want to read will always be larger than the time I have to read them. That’s just life.)\nGoodreads offers a ‘list’ function whereby you can not only state that you ‘want to read’ a book, but where you can categorise things to your heart’s content. Each year I set up a list (“2015toread” and so on) so I can see which books I think I’m more motivated to read that year. I’ll usually take 5 or 10 minutes each weak checking over the list to make sure the things I added to the list are actually things I still want to read (versus things I added in the heat of a moment, after reading a particularly persuasive review, for example, but which I probably don’t need to spend my time on).\nPreviously, I was generally following my gut with what I wanted to read next. Unfortunately, this often meant I went with the easiest option, or the path of least resistance. Long books (weighty histories, or more abstruse theoretical texts) would be passed up for the latest *it* novel or someone’s entirely forgettable memoir about their time in Afghanistan that I’ll feel obliged to read.\nThis year I’ve been trying a different approach. Goodreads allows you to sort lists by various bits of metadata attached to each book (author name, date added etc) but you can also sort by “average rating”. This is the average rating given to a particular book by the entire Goodreads user base (20+ million users). You can see how this pans out in my current set of ‘up next’ books:\n\nThis “average rating” isn’t in any way a guarantee of anything resembling quality. It’s not that hard for authors to game the system, and books with few reviews (common for niche subjects like Afghanistan or Islam) have either really high or low ratings. But I’m finding this approach brings me to read far more books outside my path-of-least-resistance choices and often brings me into contact with some real gems.\nNeedless to say, this method of discovery is only a little better than putting all the names in a hat and picking one at random, but I am still finding some benefit. It does mess with my desire to read fewer male authors (you’ll note in the picture above that only book number seven is by a woman; the rest are men) but everything in life is a tradeoff of some sort, I suppose.\nLet me know if you find some use to this or if you have any other ways you pick what books to read next."
  },
  {
    "objectID": "personal/2014-12-31-north-waziristan-a-reading-list.html",
    "href": "personal/2014-12-31-north-waziristan-a-reading-list.html",
    "title": "North Waziristan: A Reading List",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“1596”][](https://www.flickr.com/photos/drregor/4302412283) Technically, this is South Waziristan… Photo credit: Drregor (via Flickr ) [/caption]\nI’ve been doing a bit of reading about North Waziristan in the English-language sources that are available outside Pakistan. It took a bit of time to put together a decent collection that gave real information. By ‘real information’, I mean things that speak of names, dates, places and events. I wasn’t really interested in analysis, though that forms part of what follows. I was interested in the basic factual building blocks that must precede any analysis or understanding of a place. (That, and actually going there yourself). Most of these sources have are filled with stories and little details, all of which need triangulating with one another and with interviews on the ground.\nI can’t vouch for the veracity of any of it – my experience in Afghanistan has given me an innate distrust for anything I read in a report, particularly if it was assembled outside the country – yet this is what we have. There are, of course, thousands if not hundreds of thousands of news articles in the databases of Pakistan’s media outlets, but I didn’t trawl those yet. Needless to say, this is a work in progress and I will continue to update as and when I read more. It seems the area is also missing a well-sourced chronology akin to something like what I did for Kandahar or for the Taliban/Al-Qaeda relationship. I don’t have the time at the moment to do this myself, but perhaps someone will be inspired to work on it. If you have any suggestions for additions to this list, please let me know."
  },
  {
    "objectID": "personal/2014-12-31-north-waziristan-a-reading-list.html#books-core",
    "href": "personal/2014-12-31-north-waziristan-a-reading-list.html#books-core",
    "title": "North Waziristan: A Reading List",
    "section": "Books (Core)",
    "text": "Books (Core)\n\nHassan Abbas (2014), The Taliban Revival: Violence and Extremism on the Pakistan-Afghanistan Frontier\nShahzad Bashir and Robert D. Crews (2012), Under the Drones: Modern Lives in the Afghanistan-Pakistan Borderlands\nVahid Brown & Don Rassler (2013), Fountainhead of Jihad: The Haqqani Nexus, 1973-2012\nImtiaz Gul (2010), The Most Dangerous Place: Pakistan’s Lawless Frontier\nSana Haroon (2011), Frontier of Faith: Islam in the Indo-Afghan Borderland\nKhan Idris (2013), The Pakistan-Afghan Borderland: Pashtun Tribes Descending into Extremism\nBasil Muhammad (1991), al-Ansar al-Arab fi Afghanistan (in Arabic)\nMuhammad Amir Rana (2004), A-Z of Jehadi Organizations in Pakistan\nAhmed Rashid (2009), Descent into Chaos: The U.S. and the Disaster in Pakistan, Afghanistan, and Central Asia.\nSaleem Shahzad (2011), Inside Al-Qaeda and the Taliban (along with his whole series of articles for Asia Times)"
  },
  {
    "objectID": "personal/2014-12-31-north-waziristan-a-reading-list.html#books-supplementary-tangential",
    "href": "personal/2014-12-31-north-waziristan-a-reading-list.html#books-supplementary-tangential",
    "title": "North Waziristan: A Reading List",
    "section": "Books (Supplementary / Tangential)",
    "text": "Books (Supplementary / Tangential)\n\nMariam Abou-Zahab and Olivier Roy (2006), Islamist Networks: The Afghan-Pakistan Connection\nAkbar Ahmed (2013), The Thistle and the Drone\nPeter Bergen & Katherine Tiedemann (eds.) (2012), Talibanistan: Negotiating the Borders Between Terror, Politics, and Religion\nClaudio Franco (2009), “The Tehrik-e-Taliban Pakistan,” in ed. Antonio Giustozzi, Decoding the New Taliban\nBenedicte Grima (2004), Secrets from the Field: An Ethnographer’s Notes from North Western Pakistan\nArif Jamal (2009), Shadow War: The Untold Story of Jihad in Kashmir\nThomas Ruttig (2009), “Loya Paktiya’s Insurgency: The Haqqani Network as an Autonomous Entity” in ed. Antonio Giustozzi, Decoding the New Taliban\nDavid Rohde (2010), A Rope and a Prayer: The Story of a Kidnapping\nAbubakar Siddique (2014), The Pashtun Question: The Unresolved Key to the Future of Pakistan and Afghanistan\nStephen Tankel (2011), Storming the World Stage: The Story of Lashkar-e-Taiba\nVarious (2010), Granta 112: Pakistan"
  },
  {
    "objectID": "personal/2014-12-31-north-waziristan-a-reading-list.html#reports",
    "href": "personal/2014-12-31-north-waziristan-a-reading-list.html#reports",
    "title": "North Waziristan: A Reading List",
    "section": "Reports",
    "text": "Reports\n\nAmnesty International (2012), The Hands of Cruelty: Abuses by Armed Forces and Taliban in Pakistan’s Tribal Areas\nLaila Bokhari (2006), Waziristan: Impact on the Taliban Insurgency and the Stability of Pakistan\nCAMP, Understanding FATA\n\nvolume 1\nvolume 2\nvolume 3\nvolume 4 (not sure where it is)\nvolume 5\n\nCSIS (2013), Trends in Militancy across South Asia\nChristine Fair (2008), Who are Pakistan’s Militants and their families?\nFOI / Magnus Norell (2010), Militancy in the Pakistani Federally Administered Tribal Areas (FATA) and Afghanistan\nAnand Gopal, Mansur Khan Mahsud and Brian Fishman (2010), Militancy and Conflict in North Waziristan (New America Foundation)\nInternational Crisis Group (2006), Pakistan’s Tribal Areas: Appeasing the Militants\nSana Jamal (2014), Tehrik-e-Taliban Pakistan – Analyzing the Network of Terror\nJinnah Institute (2011), Extremism Watch\nSabina Khan (2012), North Waziristan: The Ground Realities(Journal on Terrorism and Security Analysis)\nShuja Nawaz (2009), FATA - A Most Dangerous Place\nGretchen Peters (2010), Crime and Insurgency in the Tribal Areas of Afghanistan and Pakistan (CTC)\nGretchen Peters (2012), Haqqani Network Financing: The Evolution of an Industry (CTC)\nQandeel Siddique (2008), The Red Mosque Operation and its Impact on the Growth of the Pakistani Taliban(FFI)\nQandeel Siddique (2010), Tehrik-e-Taliban Pakistan: An Attempt to Deconstruct the Umbrella Organization and the Reasons for Its Growth in Pakistan’s North-West (DSIS)\nAnne Stenersen (2010), Al-Qaeda’s Allies: Explaining the Relationship Between Al-Qaeda and Various Factions of the Taliban After 2001 (New America Foundation)"
  },
  {
    "objectID": "personal/2014-12-31-north-waziristan-a-reading-list.html#articles",
    "href": "personal/2014-12-31-north-waziristan-a-reading-list.html#articles",
    "title": "North Waziristan: A Reading List",
    "section": "Articles",
    "text": "Articles\n\nHassan Abbas (2008), A Profile of Tehrik-e-Taliban Pakistan\nAll the Jamestown Terrorism Monitor articles\nSadia Sulaiman (2009), Hafiz Gul Bahadur: A Profile of the Leader of the North Waziristan Taliban\nVarious (2013), Pakistan’s Tribal Militants: A Militant Leadership Monitor Special Report (Jamestown)\nRahimullah Yusufzai (2008), Who’s Who of NWFP - part 1 and part 2\nRahimullah Yusufzai (2008), The Impact of Pashtun Tribal Differences on the Pakistani Taliban"
  },
  {
    "objectID": "personal/2014-12-31-north-waziristan-a-reading-list.html#websites",
    "href": "personal/2014-12-31-north-waziristan-a-reading-list.html#websites",
    "title": "North Waziristan: A Reading List",
    "section": "Websites",
    "text": "Websites\n\nBrookings Pakistan Index (2009-2011)\n\nUPDATE: This continues to be added to as recommendations come in from various places here and there. (Last Update: January 3, 2015)"
  },
  {
    "objectID": "personal/2014-12-10-new-book-an-educators-tale.html",
    "href": "personal/2014-12-10-new-book-an-educators-tale.html",
    "title": "New book: An Educator’s Tale",
    "section": "",
    "text": "The publishing house that Felix Kuehn and I set up has two new books out. The first of these is called [An Undesirable Element](http://www.firstdraft-publishing.com/publications.html#an_undesirable_element): An Afghan Memoir and it tells the story of Sharif Fayez, the man responsible for much of the progress seen in Afghan higher education since 2001. The book also includes a lot from the 1980s jihad and pre-Taliban periods where the author was forced to leave the country – fleeing to Iran before heading for the United States. This is an extremely readable book, and the story has a fast pace to it.\nIt’s important to keep these Afghan voices and Afghan narratives in mind whenever thinking about the country. Amidst the plethora of commentary on Afghanistan written by foreigners it is easy to forget that Afghans understand their country best. Multiple ‘understandings’ exist, to be sure, but a failure to privilege the lived experiences makes useful intervention and hamkari much harder.\nBut don’t take my word for it. Here are some clever people writing about what they learned from Fayez’s book:\n\n“An Undesirable Element is a fascinating tour through the tumultuous years that helped create modern Afghanistan. Fayez survived Soviet Afghanistan and revolutionary Iran, only to find himself watching from exile as his country devoured itself. Improbably, he returns after 2001 to help resurrect Afghanistan’s devastated higher education system, giving an insider account of the challenges of building education in a land dominated by warlords and fundamentalism. The result is a poignant reminder of how much Afghanistan has endured, and the flicker of hope that remains despite it all.”\n\n– Anand Gopal, author of No Good Men Among The Living\n\n“A compelling read, An Undesirable Element recounts an Afghanistan many have forgotten. It serves as a rallying cry to once again imagine all that country might be. It’s a tale as extraordinary as the land from which it comes.”\n\n– Elliot Ackerman, author of Green On Blue\n\n“An Undesirable Element moves fast as flames and offers a luminous account of the last half century of Afghan conflicts and redevelopment. Trevithick’s oral history of Sharif Fayez’s story is a trove: from a kiss on the head by the Afghan former King Zahir Shah, Fayez’s life intersected with the future leaders and quiet supporters of his country–both heroic and tyrannical–from Columbia University to a Post-revolutionary University in Mashad, Iran. Fayez is a modest but robust storyteller whose eventual position as Afghanistan’s first Minister of Education after the Taliban is only one of the strange twists and turns his story offers. His deft handling in the rebuilding of Afghanistan should be read by anyone interested in how one can use patience and determination to bring hope to a country reduced to rubble.”\n\n– Adam Klein, editor, The Gifts of The State: New Afghan Writing\n\n“The term visionary tends to be misapplied to those who are merely headstrong. But it is a perfectly apt description for Sharif Fayez, the most important figure in education in 21st-century Afghanistan, yet one that history may have neglected without his memoir. Such an omission would have deprived future generations of Afghans from understanding how Fayez, perhaps more than any single person, created hope for the country’s young minds at the turn of the millennium and, in so doing, altered a nation’s destiny.”\n\n– Martin Kuz, freelance journalist\nLinks to purchase paperback and electronic copies here…"
  },
  {
    "objectID": "personal/2014-10-23-fa8eri6cf9zjmigzx2hasdgg8pek57.html",
    "href": "personal/2014-10-23-fa8eri6cf9zjmigzx2hasdgg8pek57.html",
    "title": "Sources & Methods: Podcast Follow-Up",
    "section": "",
    "text": "We’re five episodes old! The small podcast I started together with Matt Trevithick is coming along nicely. In our most recent episode, we talk with programmer and note-taker Mark Bernstein. Mark is the force behind the notetaking and outlining software, Tinderbox, much beloved by knowledge workers. This episode is about note-taking, its uses and why people need to think reflexively about the work they’re doing.\nIt seems to have struck a chord with listeners: we’ve had three times as many as usual. That could also have been helped by a mention over at The Atlantic from James Fallows:\n\nMark Bernstein is the creator of intriguing idea-organizing Mac software called Tinderbox, which I’ve mentioned over the years. I have never met him but have often corresponded; three years ago, he was a guest blogger here. This week, in a podcast interview for the Sources and Methods site, he talks not so much about his software but about the larger question of how thinking interacts with the tools of the electronic age. If you find the podcast provocative, you might well be interested in the book The Tinderbox Way, which is equal parts guide to Bernstein’s Tinderbox program and meditation on the right and wrong approach to “information farming.” As you’ll gather from the podcast and see in the book, the kind of farming he has in mind is nothing like mega-scale factory farming and very much like an artisanal plot.\n\nThe Director of Teaching and Learning, for the Bedford/St. Martin’s imprint of Macmillan Education publishing house wrote a blogpost about the episode in which he recommended educators give it a listen:\n\nThere’s a lot in the discussion that maps on to teaching writing, teaching research, teaching thinking, and faculty development for those professors who want to help students get better at writing, research, and thinking. The interview can be assigned in time points for students, or one might scroll to to a point and play a snippet as a way to launch a discussion. For students especially, this discussion focuses on the role of noting, of seeing and recording, and in the act of doing so, of thinking, organizing, and find order. In a way, it’s about slowing down, of taking the time to start a system that will serve a learner as a writer, and over time, as they change as writers, learn more, know more, and will find it more and more useful to be able to go back into their reading and writing history to recall, reorganize, and rethink, note taking as a key element for revision.\n\nI’ve added a number of podcasts to my regular queue in Overcast and in case you could use some recommendations, the following are almost always worth a listen, especially if you like the kinds of things you hear on Sources and Methods:\n\nNew Tech City\nMiddle East Weekly\nThe Tim Ferriss Show\nArms Control Wonk Podcast\nTechnical Difficulties"
  },
  {
    "objectID": "personal/2014-09-07-sources-methods-or-why-i-started-a-podcast.html",
    "href": "personal/2014-09-07-sources-methods-or-why-i-started-a-podcast.html",
    "title": "Sources & Methods, or why I started a podcast",
    "section": "",
    "text": "logo-blackjpg\n\n\nIf you go over to sourcesandmethods.com you’ll find the inaugural episode of a new podcast I’m doing together with Matt Trevithick. Seriously, go there now and have a listen. You’ll be able to subscribe from the iTunes directory etc in about a week or so, but for now if you want to hook it up to your preferred podcast client, use the RSS feed provided.\nMatt and I wanted to do this partly as an excuse to talk to interesting people, but also out of a sense that there is a certain type of person who gets interviewed less than they ought (if at all). So here’s hoping that we keep interviewing interesting people, esp those who aren’t already saturating the media.\nHead on over and give it a try. Anand talks about things he’s never talked about in a public forum (as far as I’m aware).\nUPDATE: You can subscribe here."
  },
  {
    "objectID": "personal/2014-08-09-two-new-co-authored-reports-on-afghanistan.html",
    "href": "personal/2014-08-09-two-new-co-authored-reports-on-afghanistan.html",
    "title": "Two new co-authored reports on Afghanistan",
    "section": "",
    "text": "Just a short post. Two reports that I co-authored have just been published. They were both finished a few months back, but they’re not so time-sensitive that this will make much of a difference.\nThe first is for Chatham House, written together with Felix Kuehn. You can read the executive summary here, and download the full report here. The central point we were trying to get across is that a political settlement in Afghanistan must be about more than just ‘talks with the Taliban’. That ship has sailed, and new realities mean it’s important to bring all parties into a discussion about the future. I remain skeptical as to internal and external parties’ ability to make this happen, but here’s hoping…\nThe second report, much longer, was mainly an effort of Felix Kuehn and Leah Farrall but I contributed some things on the sidelines. This was expert witness testimony in the case of US vs. Talha Ahsan and US vs. Babar Ahmad. You can read some of the background to the case here and here. The report we were tasked with writing related to Talha and Babar’s activities in Afghanistan during the 1990s, and the extent to which this equated with support for or ‘membership’ in al-Qaeda. Felix and I have already written a decent amount on the topic, but it was great to team up with Leah to dive into the foreign fighters’ side a great deal more.\nYou can read our report here, starting on page 148. It’s a long report, but there’s a lot of new material in there which has never been published (as far as I’m aware).\nI’d also recommend reading through the judge’s statement Talha’s sentencing memo. I don’t quite understand why there hasn’t been more media coverage of this trial, and how the government were more or less told their case was extremely rickety. Perhaps it’s because of all the other things going on at the moment.\nUPDATE: Edited on August 11 to reflect error in identifying the sentencing memo."
  },
  {
    "objectID": "personal/2013-12-27-the-best-books-i-read-in-2013.html",
    "href": "personal/2013-12-27-the-best-books-i-read-in-2013.html",
    "title": "The Best Books I Read in 2013",
    "section": "",
    "text": "This year was a big one. 79 books in total, and still a few days to go. I only have one Afghanistan-related pick this year, and that’s Vanessa M. Gezari’s The Tender Soldier.Gezari’s tale of incompetence, misunderstanding and tragedy packs a punch. Some of the middle sections of the book – profiling those involved higher up in the Human Terrain Teams’ management – could probably have been ditched. Speed through them, though, and you have a story whose complexity and strangeness has the eerie ring of reality. Definetely the best book I’ve read on Afghanistan in recent years.\nThe rest of my favourites are a bit all over the place:\nDonna Tartt’s The Goldfinch needs no selling by me, but it’s a real return to true form. As always with Tartt, be careful when you start as it is incredibly gripping and you’ll be neglecting work, friends and family in order to keep reading. Also, be careful: this one’s a bit of an emotional kick-in-the-guts.\nJon Moallem wrote Wild Ones: A Sometimes Dismaying, Weirdly Reassuring Story About Looking at People Looking at Animals in America as a way of reconnecting with the natural world while his daughter grew up, and it was one of the most unexpectedly enjoyable books I read this year. Stories about conservation, humans and how we interact with all the other species on the planet. The book is broadly structured in three parts, covering polar bears, butterflies (Lange’s Metalmark) and the whooping crane. Mooallem looks at the ways in which people are involved in efforts to save these three species, often ending up telling us more about humans than the animals that are nominally his subject. He tells a hopeful tale, for the most part, and although his slightly saccharine ending wasn’t really to my taste, it probably is correctly pitched for an American audience. We don’t understand the effects of our actions in the world, or when we think we do, we don’t anticipate second and third-order consequences. It’s not a new idea, but it was memorably told in this well-written book.\nLion Kimbro’s How to Make a Complete Map of Every Thought You Think (available for free here) isn’t new, and it definitely wins the strangest-book-of-2013 prize. Kimbro decides he wants to map out his thoughts 24-7 for a few months and the book is a description of the process he used to map it all out. It’s written in a stream-of-consciousness style, and there’s an awful lot about pens and paper and binders and organisational systems and information hygiene (for want of a better term). There’s a lot worth taking away from the experiment, though, and if you can make it through the book you’ll have learnt a few tricks on the way.\nI LOVED Matt Potter’s Outlaws Inc., the first book I’ve read where a piece of technology (in this case the IL-76 cargo plane) is a lead character, but WHAT a story. Potter takes you through the world of air cargo transport and the post-Soviet airmen who fly the planes. It’s perhaps a bit long, but that’s probably me being unfair. I was up all night reading this book. Definitely recommend it for shining a light on something I hadn’t really thought about but that is very much a part of trade and international aid.\nGuy Deutscher’s The Unfolding of Languagewasn’t an easy read, but it’s the smartest (and easy-to-understand) book on linguistics I’ve read. It offers an overview of how languages work, and how they change over time (fragmenting and joining together in a myriad of strange ways). I’m looking forward to reading Deutscher’s other book in 2014.\nI’ve been a big fan of Marie (aka puredoxyk) for about a year now. I did a 2-month experiment switching to a polyphasic sleep cycle earlier this year – that’s 2-4 hours of sleep only per day, depending on various things – and I couldn’t have done it without her book Ubersleep: Nap-Based Sleep Schedules and the Polyphasic Lifestyle. (She’s also very nice/helpful online). This book offers clear, useful advice on shifting to a polyphasic sleep cycle. If you’re interested in sleep modification (and being able to sleep only 2 or 4 hours per day without medium-long-term issues), give this book a read. If nothing else, it’ll expand your sense of what is possible, and that’s always a good thing.\nFinally, two health-related books. The past couple of years have been filled with various health issues (some mine, and some of others close to me) and I read more about health, fitness and diet than usual. Anti-Cancer by David Servan-Schreiber was one of the more helpful books on cancer that I read. It could probably use a bit of updating, but it is clear, offers evidence with references to follow up with further reading, along with useful lists of the basic ingredients of the so-called ‘anti-cancer’ diet. Was also glad to see that emphasis is placed on the mind-body connection. Could easily have been left out with all the focus on diet and nutrition. Full Catastrophe Living: Using the Wisdom of Your Body and Mind to Face Stress, Pain, and Illness by Jon Kabat-Zinn has a bit of a hippy title, but it’s full of really important and powerful techniques. I’d strongly recommend it to anyone with ongoing chronic pain/illness issues.\nThose were the best. Drop your book recommendations for 2014 in the comments below. I’m going to try for 100…"
  },
  {
    "objectID": "personal/2013-07-20-book-of-the-week-al-shabab-in-somalia.html",
    "href": "personal/2013-07-20-book-of-the-week-al-shabab-in-somalia.html",
    "title": "Book of the Week: ‘Al-Shabab in Somalia’",
    "section": "",
    "text": "An excellent overview of the history of the Somali al-Shabab group, one with many lessons or reminders of Afghanistan (at least for this reader).\nThis is a short book, based on some reports written for FFI and others, and in that it has the virtue of concision. Hansen covers al-Shabab’s history starting with early proto-Islamist movements and groups started several decades ago. It was the best explanation of where the networks that make up al-Shabab come from that I’ve read, although it may just be that I haven’t been following this too closely since the last time I was last in Mogadishu a couple of years ago.\nIt was packed with stories and trends that reminded me of Afghanistan, both in the way the international actors chose to respond and intervene, and also in the development of al-Shabab itself. For this reason I’d strongly recommend this book to those working in and on Afghanistan. You’ll find a rich vein of material that you can bring back to enrich your understanding of the Taliban and/or the past decade or three. (Needless to say, I’m the last person to suggest that everything is the same in every country, and that there aren’t hundreds of reasons why comparisons aren’t useful in a this-happened-in-somalia-so-it-must-be-the-same-in-afghanistan.)\nThere are lots of names and places mentioned, and if you’re not familiar with at least the bare outlines of the plot so far as well as some of the key players, you might find it confusing. I wish there was some sort of reference in the back to allow you to keep track of all the different people mentioned.\nAs always, I wasn’t really sure I got a sense of the leaders of al-Shabab (or their fighters) as people in this book, but maybe that’s one step too far and one in which it’s harder to offer anything that isn’t highly subjective or just unrepresentative. Perhaps we can look forward to a book of al-Shabab’s songs and poems from Hurst in the future?\nOverall, though, an impressive collation of information. Hansen has done us all a service in spending time in Somalia doing fieldwork and in taking the time to put this book together.\nBuy it here."
  },
  {
    "objectID": "personal/2013-02-04-learning-to-code.html",
    "href": "personal/2013-02-04-learning-to-code.html",
    "title": "Learning to Code",
    "section": "",
    "text": "Previous posts have been about languages and how to learn them. Not all languages are for communication with other people, though. It is a truism that more and more of our lives are lived through various technologies – be it computers, ‘smart’ phones or other appliances – but we often aren’t too good at understanding how those things work. I’ve been trying to remedy this by getting a better understanding of the back end through programming languages. Not only has it been an interesting intellectual exercise, but I have found practical applications for the skills I have learnt. Recently, for example, I wrote a piece of code that crawled through webpages, saving only the parts of text that I needed to a separate database.\nThere is a huge variety of things that you can try out here, so I’ll just offer some suggestions for things that I’ve found useful along the way. Most of this is aimed at complete beginners. I’ll assume that’s where you are as well.\nPython is considered by many (if not most) to be the best place to start as a novice programmer. It teaches lots of transferrable skills that can be applied to other languages that you might want to pick up.\nMy top recommendation would be to enrol in Udacity’s CS101 course. Udacity is a relative newcomer to the scene, but I’ve found the parts of this course that I’ve done (I’m still working my way through) to be excellent. It has LOTS of practice, frequent testing of your ability to solve problems along the way, and is not dull to watch at all (as some of these courses are). What’s more, by the end of the course you will have built your own search engine using the skills you’ve learnt. It’s free, so you have no excuse. Go sign up.\n(If that’s not for you, although I’m not sure why that would be the case, then visit Codeacademy or Learn Python The Hard Way for alternatives.)\nIn case you’re interested in learning Ruby, you can try the following, many of which are designed for young children to be able to use and follow along, so, again, no excuses…\nRubyMonk\nTryRuby\nHackety Hack\nRails for Zombies\nCodeAcademy Ruby\nLearnStreet.com Ruby\nNow go try some of those out…"
  },
  {
    "objectID": "personal/2013-01-15-isafs-first-fifteen-days.html",
    "href": "personal/2013-01-15-isafs-first-fifteen-days.html",
    "title": "ISAF’s First Fifteen Days",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“839”][](http://static.squarespace.com/static/544a27a7e4b042df02dea00c/544a2abfe4b017ad0e2a476a/544a2ac0e4b017ad0e2a47ca/1358261154000/chart1.png?format=original) chart1[/caption]\nI have been working on the ISAF press releases data again. Just as a little teaser, here’s a chart comparing the first fifteen days of January in 2011, 2012 and 2013. It shows the number of ISAF operations in which someone was captured or killed (as reported in their press releases).\n[caption id=“” align=“alignnone” width=“842”][](http://static.squarespace.com/static/544a27a7e4b042df02dea00c/544a2abfe4b017ad0e2a476a/544a2ac0e4b017ad0e2a47d3/1358261535000/chart21.png?format=original) chart2[/caption]\nThe second chart compares the numbers of operations carried out over the course of the first fifteen days of the month, as well as the numbers of people who were killed or detained during the course of those operations.\n[caption id=“” align=“alignnone” width=“817”][](http://static.squarespace.com/static/544a27a7e4b042df02dea00c/544a2abfe4b017ad0e2a476a/544a2ac0e4b017ad0e2a47d9/1358261732000/chart32.png?format=original) chart3[/caption]\nThe third chart compares the numbers of leaders and facilitators who were removed from the fight in some way (either killed or detained).\nNote that all the data for the above charts comes from ISAF’s press release reporting. ISAF under-report incidents, so there will have been more operations and deaths and detentions etc than are mentioned here."
  },
  {
    "objectID": "personal/2013-01-07-5-things-i-wish-someone-had-told-me-about-learning-languages.html",
    "href": "personal/2013-01-07-5-things-i-wish-someone-had-told-me-about-learning-languages.html",
    "title": "Five Things I Wish Someone Had Told Me About Learning Languages",
    "section": "",
    "text": "1195995_44850378\n\n\n[UPDATE: I now offer one-on-one language coaching. Read more about what it involves and what kinds of problems it’s best suited to addressing.]\nI often get emails and requests on twitter about language study methods and tips. I’m no great expert on this, but I have read and experimented a good deal with various techniques while learning languages. I’m often surprised that many people don’t know the sheer variety of resources that are available to them, or just the basics of how to go about studying a new language.\nI’ll be returning to some of these points in a bit more detail in the coming weeks, but these are some thoughts to start off the discussion.\n\n1. Learning a language is the easiest way to put yourself ahead of the competition\nThis applies for everything from research to journalism to people simply planning a holiday. For various reasons, it’s pretty unusual for people living in English-speaking countries to study a second foreign language to anything approaching fluency. Part of this is the social expectation and a feeling that it isn’t necessary, and part is a pedagogical deficit in schools and universities that gives people the impression that studying languages is boring.\nIn reality, studying Arabic, Farsi and Pashto was the single best thing I did in terms of advancing my ability to understand the people and places where I have been working over the past few years. The fact that hardly anyone else bothers means that you are automatically in a better position than they are and more able to be able to engage with the country where you’re living working. There’s the added bonus that you save money that you would have to spend on translators and fixers otherwise.\n(Note that this applies less to non-English-speaking European countries, especially France, where you aren’t taken seriously as an area studies ‘expert’ if you don’t speak the language of the country you’re studying. It’s actually a good way of assessing someone’s committment to studying a particular area: if they’ve put the hours in and can speak the local language, you know they’re serious.)\n\n\n2. Studying languages needn’t be expensive\nWe’re living in a golden age of language learning. There are free online resources for more or less all the major languages, so much so that you almost have no excuse for moving forward with that dream you once had of being able to speak a foreign language.\nIf you want to learn Spanish, French, German, Italian or Portuguese, start with duolingo.com. If you’re thinking about learning Mandarin Chinese, visit HackingChinese which also happens to be an excellent place to read about study methods. If you’re thinking about studying Arabic – about which more below – you could do worse than starting here and getting to grips with the alphabet. If you want to study Russian, check out RussianForFree. I could keep going. There are vocabulary study tools available at Anki, Memrise and Keewords. For practicing your writing, set up an account at Lang-8 and have your work corrected by native speakers. Need some practice of your spoken skills? Log on to Verbling.\nEVERYTHING IN THE PREVIOUS PARAGRAPH IS FREE. There are other resources which you can supplement these free courses with if you get serious about your studies, but you can go a LONG way with free materials available online.\n\n\n3. Learning some basic techniques and reading about language study methodology at the outset helps a lot\nI’m guessing you haven’t spent much time reading about the science of learning, or the science of learning languages. If you’re going to teach yourself a language – and I’m mainly talking about self-study here, not learning as part of a class – it really helps to have some idea of the basic dos and don’ts.\nA few suggestions for things to read. All of these are available as ebooks on the kindle store, so no excuses…\nDaniel Coyle’s The Little Book of Talent: This short book has 52 practical suggestions for how to learn skills, how to practice, how to make the most of your time. I refer to this book a couple of times a week for suggestions on how to improve my skills. Highly recommended.\nGary Marcus’s Guitar Zero: Again, another short book about the science of learning. It’s a story of the author’s attempts to learn the guitar after turning forty, but really it’s a book about different ways to approach learning. A good complement to The Little Book of Talent.\nMichael Erard’s Babel No More: You could do much worse than reading through this book to get a sense of the different approaches people have to learning languages. Erard’s subject is people who are studying five or ten languages (or more) at the same time, so it’s at the extreme edge of things, but it’s an easy read and extremely interesting.\nI haven’t dipped too much or exhaustively into the huge number of ‘how-to’ books on language study, but here are five that I read and found useful:\nAmorey Gethin’s The Art and Science of Learning Languages: advocates a text-heavy approach with lots of reading to cement vocabulary in the context of ‘real language’.\nBoris Shekhtman’s How to Improve Your Foreign Language Immediately: This is a very practical book with suggestions on improving spoken fluency and your ability to converse with other people. After you’ve done a few months of study, give this super-short book a read and try out his suggested exercises.\nBarry Farber’s How to Learn Any Language: Nothing monumentally new here. Mostly common sense, but it’s worth reading if you haven’t thought about this stuff before.\nA.G. Hawke’s The Quick and Dirty Guide to Learning Languages Fast: If you need to be able to function in a language very quickly, take a look at this book. Written by a former US Army Green Beret officer (?), it advocates a very practical approach that gets you speaking and mastering the basics in no time. It’s worth getting hold of a paper/hard copy of this book since there are boxes and things to fill in once you’ve decided what language you want to learn.\nGregg A. Miller’s The Pocket Linguist: Again, another common sense overview of the kinds of things you should be doing to study languages.\n\n\n4. It takes a lot less than you think to get to a basic level of usefulness\nHawke’s Quick and Dirty Guide advocates diving head first into a language and there is a whole school of language learning that argues you need to be speaking from day one. Benny Lewis of the website FluentIn3Months has some comments on that here and he’s one of the more prominent advocates of this approach.\nThe trick – as with so many things – is to do a little bit every day rather than by hoping to cram things in to just a few mega-sessions lasting 5 hours each spaced out over a year. Want to learn Arabic, or Pashto? Do half an hour of deliberate practice every day for a year and I guarantee you’ll be able to have a conversation using what you learnt during that year.\n\n\n5. Learning the Arabic alphabet is your biggest barrier to studying Arabic (or Pashto or Farsi/Dari) – and it isn’t actually difficult\nThis one’s just a bonus, since I’ve come across this particular hump in a lot of people wanting to study languages that use the Arabic alphabet or variants thereof. Lots of people think that Arabic is hard to learn because of the alphabet, and this seems to be a really common reason for people not bothering to start.\nThis is really unfortunate, because the alphabet is actually really easy. There are good online courses that teach you how to write and read Arabic letters, and once you’ve overcome that obstacle your confidence will already be so much higher that the rest that follows will seem easy.\nSeriously, if your main reason for not starting to study Arabic (or Pashto, or Farsi/Dari) is the alphabet, you don’t have a leg to stand on.\n*\nNote that I also write about languages elsewhere online:\nMy tumblr where I jot down things from time to time relating to how my language studies are going.\nMy lang-8 account where I practice my writing in Arabic, Pashto, Farsi and Urdu.\nMy profile on how-to-learn-any-language.com, a really excellent site for discussing learning methods, finding self-study materials and in general discussing the study of languages."
  },
  {
    "objectID": "personal/2012-12-29-some-things-i-read.html",
    "href": "personal/2012-12-29-some-things-i-read.html",
    "title": "Some Things I Read",
    "section": "",
    "text": "I read 43 books this year. 44, if you count the book I’m about to finish. For some reason, 2012 doesn’t feel like it was a particularly great year in terms of the books and longform articles that were published.\nJeevan Deol and Zaheer Kazmi edited a fantastic collection of essays entitled Contextualising Jihadi Thought. Anyone interested in that kind of thing should read it.\nFor a fun read about the Taliban pre-2001, check out Mohammad Kabir Mohabbat and L.R. McInnis’ Delivering Osama. As you can tell from the title, it covers some of the Taliban’s internal policy over bin Laden before the September 11 attacks. This is something I have written about together with Felix Kuehn, but this manuscript wasn’t published when we wrote our book. If you’re the kind of person who enjoys downloading old archive documents relating to Afghanistan till the wee hours of the morning, give this book a read.\nI loved this profile of a complex character from Pakistan’s Sindh by Saba Imtiaz. Go read it now if you haven’t done so already.\nFinally, I wrote about this one two years ago, but I have to mention it again since I’ve been revisiting its arguments for a new research project I’m working on: Noah Feldman’s Fall and Rise of the Islamic State. It remains one of the most lucidly-written books I’ve read on the aspirational statebuilding of Islamists and what happens when they start thinking about constitutional law.\nDrop me your book recommendations in the comments below and I’ll see if I can get through 52 books in 2013…\nUPDATE: I’ve now finished book #44 and think I’ll get to #45 before the year is over…"
  },
  {
    "objectID": "personal/2012-12-16-catching-up-an-enemy-we-created.html",
    "href": "personal/2012-12-16-catching-up-an-enemy-we-created.html",
    "title": "Catching Up: An Enemy We Created",
    "section": "",
    "text": "It’s been a while now that An Enemy We Created has been out now. You can get it from all decent bookshops, as well as on kindle (finally), both in the UK and in the USA.\nThe reviews that have come in have been almost entirely positive. You can read clippings and mentions and browse through the full list over here.\nI was invited to talk at the University of Oklahoma (Norman, Oklahoma) and at Creighton University (Omaha, Nebraska) where An Enemy We Created is being assigned as required reading for some courses. The students had many (many) questions and it was great to be able to interact with an audience that had read the book.\nIf you haven’t had a chance, perhaps take a look next time you’re in a bookshop. It’s about a lot more than just the Taliban-Al-Qaeda relationship. You can read some excerpts from the reviews over here."
  },
  {
    "objectID": "personal/2012-11-26-writing-the-history-of-the-taliban-movement.html",
    "href": "personal/2012-11-26-writing-the-history-of-the-taliban-movement.html",
    "title": "Writing the history of the Taliban movement",
    "section": "",
    "text": "Conversations in corridors, in queues for food, or whispered at the back of the auditorium are the lifeblood of conferences. A recent event hosted by the Forum for Arab and International Relations was a good chance to meet some friends and fellow researchers. One discussion ended with the realisation that the definitive history of the Taliban would probably only be written 15 or 20 years from now. It’s a sobering thought, although the fact large multi-volume histories of the Second World War continue to be written should temper our surprise.\nThe problem is one of sourcing. Almost everything so far written about the Taliban – and I am talking primarily about the pre-2001 period – has been done on the basis of ad hoc oral history interviews conducted with practitioners within the movement. I cannot think of a historical work that has taken the large written corpus of materials from this time as a source. Similarly, we are still waiting for the institutional history of the movement to be written, one that can explain how the mechanisms of government (such as they existed) functioned during the late 1990s.\nIn many ways, it is exciting as a researcher to be confronted with such a landscape: almost everything you unearth and touch is of some significance. Certainly, there are many PhDs worth of material still to be written up. A joint translation project (with myself, Felix Kuehn and Anand Gopal) of Taliban primary source material will start work soon, and we hope the provision of originals texts and translations can stimulate some new research and analysis.\nThere is much work to be done. The history of the Taliban movement in Afghanistan is still to be written."
  },
  {
    "objectID": "personal/2011-12-07-entropy-and-insurgent-radicalisation-an-isaf-goal.html",
    "href": "personal/2011-12-07-entropy-and-insurgent-radicalisation-an-isaf-goal.html",
    "title": "Entropy and insurgent radicalisation: an ISAF goal?",
    "section": "",
    "text": "Attacks in Kabul on Tuesday are believed to have been carried out by those from or affiliated with the Pakistani group Lashkar-e Jhangvi al-Alami. Much of the commentary so far has looked at the extent of past precedent for sectarian tensions and violence in Afghanistan. A piece by Ghaith Abdul-Ahad asks whether the attack reflects a Sunni-Shi’i dynamic or rather ethnic issues. And Anand Gopal usefully notes that “a unilateral LeJ-Alami attack would mark a significant erosion of the Taliban’s control over the battlefield.” The Afghan insurgency has long been – to a greater or lesser degree – a hodgepodge of different groups and actors. This is a situation the Taliban’s central leadership have previously been willing to tolerate; more affiliate fighters means more violence directed at the foreign forces, even if these groups often turn their guns on other insurgent fighters or the general population at large. Periodically, the central leadership will attempt to clamp down on some of those who claim to fight under the Taliban banner. Several mass-casualty incidents in Kandahar involving large numbers of civilians dead and wounded last year saw an effort by the leadership to reinforce command-and-control structures. Similarly, the layeha or rulebook issued by the leadership every year or two has increasingly concerned itself with these issues of power retention.\nThe usual confusions of a messy conflict fought among the people mean it is difficult to penetrate all the inner machinations behind these events, however one thing is clear: the Taliban’s central leadership (based, for the most part, in Karachi) have been steadily losing control over the violence in Afghanistan. This is not to say that they are a spent force, nor do I meant to imply that there aren’t insurgency command structures that function more or less as intended.\nFor a variety of reasons – best explored elsewhere for reasons of brevity – there has been a steady erosion of the ability of the old-generation leadership based in Pakistan to control the use of violence by the fighters that nominally pledge allegiance to the Taliban or ‘Islamic Emirate of Afghanistan’. This is no great secret; the layeha itself implicitly acknowledges this diagnosis. In fact, the imagined breakup of the Afghan insurgency was one of the reason many outside parties and figures have sought an acceleration of ‘political solutions’ instead of continuing to rely on military options.\nIn case it needs repeating, the conflict in Afghanistan is a political conflict, one with a strong military dimension, to be sure, but also one whose seeming intractability reflects a yawning political entropy that grows with each day.\nBut do the current means of addressing this conflict really address the fundamental political issues or are they actually accelerating the very entropy they seek to avoid? I have argued elsewhere (together with my colleague, Felix Kuehn) that one of the things bearing significant responsiblity for this fragmentation of the insurgency are capture-or-kill raids carried out by ISAF and Afghan security forces. Quite apart from the question of whether they are effective or not – a report written for the Afghanistan Analysts Network raised some of those issues – they have played a significant role in removing mid- and lower-tier Taliban leaders from the battlefield.\nThe capture-and-kill raids have been a quantifiable tool in the hands of ISAF to target the insurgency, but have they ended up radicalising the Taliban movement as an unwanted side-effect? There are numerous indications that this is the case. The insurgent commanders who replace those removed from the battlefield in ISAF operations are, for the most part, younger and often of a different ideological bent than their older predecessors.\nThe extent to which this is a goal of the ISAF campaign or just a side-effect remains a significant question, however. Off-record briefings with American military officials or reports of conversations with special forces in the field relatively frequently elicit admissions that it is an explicit goal of the capture-or-kill raids to “radicalise the insurgency.” The idea seems to have come over from the US military experience in Iraq. Sidestepping the extent of US agency in radicalising actors in that conflict, a more radical Taliban would supposedly carry out more atrocities and, in so doing, would themselves drive a wedge between the insurgency and the people. In effect, the idea is a hangover from the golden days of counterinsurgency rhetoric.\nInternational political and military actors didn’t come to Afghanistan with malign intentions, but the unintended consequences of their actions constantly threaten to overturn the very few unambiguously positive effects of their presence. Foreign money – in all its different forms – has arguably had more of a corrosive effect than the war itself.\nYesterday’s attack on an explicitly sectarian target may turn out to be yet another unintended consequence. The more radicalised the younger commanders become, the more they’re willing to tolerate people from Pakistan coming in to ‘help out’; just take a look at Kunar and Nuristan today. By the same token, the less control the Afghan Taliban’s central leadership has over things inside Afghanistan, the more chance we have for violence on the ground to be hijacked by external groups with their own agendas: witness the Rabbani assassination.\nA radicalised mid-level leadership that claims less and less allegiance to a senior leadership may be what the ISAF campaign intended to promote, but it can only harm the Afghan civilian population."
  },
  {
    "objectID": "personal/2011-09-13-those-40-al-qaeda-insurgents.html",
    "href": "personal/2011-09-13-those-40-al-qaeda-insurgents.html",
    "title": "Those ‘40 al-Qaeda insurgents’…",
    "section": "",
    "text": "I never really thought I’d be writing something in semi-solidarity with the Long War Journal, but this jogged my mind. 10 days ago, ISAF put out a press release following the killing of Sabir Lal, someone who was said to be a ‘key affiliate’ of al-Qaeda in Afghanistan. You can read more about him here and here (watch how those headlines start to resemble chinese whispers).\nI’ve been working on a report for the Afghanistan Analysts Network relating to ISAF’s press releases these past few months, as those of you following me on twitter will already know. A pleasant byproduct of that research is the database of press releases that I’ve put together, one that allows me to isolate, for example, all the instances where ISAF went after someone they thought was associated or affiliated with al-Qaeda in some way.\nFor 2011, operations against al-Qaeda in eastern Afghanistan amount to the following:*\n\nJanuary 7, 2011 [2011-01-S-091] - ‘Several’ (minimum 3) suspected insurgents killed in an airstrike in Pech Valley, Kunar, while pursuing an “al-Qaida-associated Taliban leader”. Later ‘confirmed’ that he was ‘Qari Baryal, an al-Qaida-associated Taliban leader’. [Total: 1 ‘associated leader’ killed]\nJanuary 8, 2011 [2011-01-S-099] - One suspected insurgent detained in Chaprahar, Nangarhar while in pursuit of an ‘al Qaida-associated Taliban leader’.\nApril 11, 2011 [2011-04-S-039] - Taliban leader detained in Behsud district, Nangarhar province. “The leader operated for al-Qaida and the Taliban.” [Total: 1 leader captured].\nApril 11, 2011 [2011-04-S-042; 2011-04-S-047; 2011-04-S-079] - Several ‘al-Qaida insurgents, including the suspected al-Qaida leader in Kunar province’ killed in Dangam district, Kunar province, in an airstrike. Editorial comment notes 25 leaders and fighters killed between March 14-April 13 [Total: approx 3 killed].\nApril 19, 2011 [2011-04-S-060] - 17 insurgents killed ‘including foreign fighters’ and one detained while searching for a senior al-Qaida leader in Dangam district, Kunar province. [Total: 17 killed, no indication who is AQ or not]\nJune 23, 2011 [2011-06-S-079] - 5 detained in Gailan district, Ghazni province, with suspected ties to al-Qaida. [Total: 5 detained on suspicion of having ties to AQ]\nSeptember 2, 2011 [2011-09-S-002] - Key AQ affiliate killed in Jalalabad district, Nangarhar province. [Total: 1 killed and several (minimum 3) suspected insurgents captured]\n\nTOTAL 22 killed and 10 captured from the above numbers\nNote first of all, that the numbers don’t even reach the minimum ‘40’ claimed in the press release, EVEN if we assume that all those listed were ‘al-Qaeda insurgents’. As Bill rightly notes in his post, “what is the intelligence community and the military’s definition of al Qaeda?” A lot of the people in the list above will have been Afghans, and many are simply noted as having been ‘suspected ties’.\nI hope that all the discussion about this magical number ‘40’ ISAF put out has made it back to whoever wrote the press release on Sabir Lal’s killing. I suspect not. It goes to something deeper about the things we read about the war in Afghanistan, namely that the ‘metrics’ used to define and claim success – remember when the war was all about metrics? – are, in various ways, false metrics, or at the very least highly misleading.  I’ll have more on this in my report.\n*[Note that for all of Afghanistan there were 13 operations mentioned, several of which captured only ‘Afghan insurgents’ and 3 of which took place in either Zabul or Balkh. I didn’t really feel Ghazni (see #6 above) qualified as ‘eastern Afghanistan’, but I gave ISAF the benefit of the doubt on that one since technically Ghazni is part of RC-East.]\nPhoto Credit: © Philip Poupin"
  },
  {
    "objectID": "personal/2011-05-17-isaf-press-release-word-clouds.html",
    "href": "personal/2011-05-17-isaf-press-release-word-clouds.html",
    "title": "ISAF Press Release Word Clouds",
    "section": "",
    "text": "…and we’re back here again. I know I said I’d hold off on posting, but these charts will never make their way into the final report so I’ll just put them up here. These are word clouds of the common terms used in sets of ISAF press releases.  As with all word clouds, the larger the word, the more times it occurs in the press releases for the particular period.  This first one covers the entire set (November 2009-May 1st, 2011): \nThe following images I split up the data into chunks. The first four months: (Nov 2009-Feb 2010 inclusive)\n\nThis covers March-June 2010 (inclusive:\n\nThis covers July-October 2010 (inclusive):\n\nThis covers November 2010-February 2011 (inclusive):\n\nAnd this covers the last two months (March and April 2011):"
  },
  {
    "objectID": "personal/2011-05-15-kandahar-prison-escape-the-talibans-tale.html",
    "href": "personal/2011-05-15-kandahar-prison-escape-the-talibans-tale.html",
    "title": "Kandahar Prison Escape: the Taliban’s Tale",
    "section": "",
    "text": "The Taliban issued a new edition of their Arabic-language Al-Somood magazine a few days ago.  You can view the original on Aaron Zelin’s excellent Jihadology website. The most interesting articles, I found, were the two relating to the Taliban’s recent prison escape from Sarpoza jail. There has been a lot written about the escape, but we’ve heard relatively little from escapees. These two articles offer a fair amount of new detail. Of course, al-Somood needs to be taken with a pinch of salt, but the accounts are interesting nonetheless. Accordingly, full translations of the two articles are presented below, exclusively to this website.\n{Part 1: pp12-14 of al-Šumūd (Steadfastness), 5th year, volume 60, Jumada al-Thaniya 1432AH/May-June 2011}\nThe Prison Break Story\nHow Fiction Became Reality\nBy Abd al-Ra’uf Hikmat\nKandahar Prison\nKandahar’s main prison lies in the Sarposa area, north of Kandahar-Herat highway. It is considered the largest state prison in south Afghanistan with its capacity to hold thousands of prisoners. It comprises a main gate and multiple segments; it is also surrounded by high and impenetrable walls.\nThis prison has essentially been built professionally, with the establishment of high surveillance and watch towers in its four corners. It is also surrounded by a number of underground walls, further to its high [overground] walls, to prevent digging tunnels to the outside.\nNotwithstanding this impervious building and tight security measures, this prison has become the scene of a fascinating story not only in the Afghan domain but internationally as well. Over the past eight years, political prisoners have been able to escape on three occasions. The first time saw in June 2003 inmate mujahideen of the political ward digging a tunnel from within the prison to the outside, thus freeing all of this ward’s prisoners who totalled 45 via the tunnel. Then in June 2008, the Islamic Emirate launched militant martyrdom attacks on this prison that caused the death of all the prison’s guards and released close to 1,200 inmate mujahideen. Subsequently, Americans and Kandahar officials took care of maintaining this prison; Canadian forces trained special policemen to guard the prison; watch towers were increased and monitoring cameras installed; all the prison was encircled by a deep and wide trench. Despite all these measures, the mujahideen were able for the third time to release 541 prisoners following a long planning on the 25th of April of this year, 2011.\nPure Fantasy\nOne of the surprising mujahideen squad in the city of Kandahar, who by his connections gained full knowledge of the inside and outside of the prison, pondered one day whether it could be possible to dig a tunnel from the inside of a house on the other side of the street to the prison as a means to releasing the prisoners. This fantasy and imagination seemed laughable at first even to its owner; he dared not share his opinion with others. But, after more time and continued thinking, he reached a conclusion. On one of these days, while he was riding a motorcycle with two of his comrades, he shared that view with them. They thought it impossible initially and deemed it a fruitless, dangerous attempt. Finally, they placed their trust on God and shared their opinion with the mujahideen high command in Kandahar. With guidelines from the command, the aforementioned four revealed [to] their trusted comrades their decision to implement this plan regardless of its risks and even if it looked impossible.\nConcrete Workshop\nSix months ago these committed mujahideen rented a house opposite the south corner of Kandahar prison. The old house rooms were in disrepair. Initially they built a new room. Then they brought in all necessary [equipment] and machines to make concrete, hiring a number of workers who worked during the day. But in the afternoon, when the workers left, the mujahideen stayed under the pretence of guarding. It was during that time that they proceeded to dig the tunnel from within the room they had just built.\nHard Labour for Four Months\nAt first, four mujahideen were plodding through this operation. Their work method was the following: one was to hit with the pickaxe, digging the tunnel, while the other three were to move the soil. The tunnel was narrow, the soil could not be moved out by wheelbarrows, so some operation planners went to the market and bought a number of children bicycles, removing their small wheels and fixing barrows on them. They were able to prepare wheelbarrows that suited their task. Now they filled these barrows with soil, pulling them by a rope to the tunnel opening and collecting the soil there before moving it to the lorry. In the morning, when the soil lorries headed to the city for its sale, mujahideen would bring in their soil-filled lorry and sell it, thus getting rid of it.\nFor two months, four mujahideen were working in the tunnel digging. Then their number increased to eight mujahideen. Now they were digging four meters every night. When their continued work reached 100m, they faced the issue of ventilation and lack of oxygen; nevertheless they carried on until cutting a distance of 150m. At this point it was terminally difficult to continue working, due to oxygen lack, and work carried on no further. In the beginning they tried a ground fan; it resolved the ventilation issue but it was winter and the cold weather caused headache. Then they made an air-pumping machine, delivering air by a pipe from the outside to the inside of the tunnel. This was the best method to resolve the ventilation and lack of oxygen – the machine worked quietly by a charged battery. But they then realised the risk of their digging a tunnel under the road that carried the heavy enemy vehicles to the inside of the prison: There was a possibility of a tunnel collapse under intense vehicle pressure. The question was how deep was the tunnel to be dug to exclude that possibility. As an experiment, they parked a lorry atop the tunnel; it suffered no damage, assuring them that it would not suffer because of enemy vehicles. The tunnel was 2.5m deep between the house and the public road, but as a precaution they deepened it further. Four months passed and the tunnel went 220m, a well iron pipe surprised them before realising it was not a prison pipe but a pipe to a village south of the prison. In fact, the tunnel diggers, having no map, deviated from the correct path to the right, crossing the road and reaching a village close to the prison. Here they recognised that the target could only be reached with the prison map and distance measuring tools.\nOne and a Half Months of Efficient Work\nThe tunnel diggers who lost their way and made an extra 120m now downloaded the prison map off the internet and by which were able to pinpoint the prison location. Using earth measurement tools they re-dug at the distance of 100m of the tunnel directly towards the prison. However, with the passing of the winter, night was shorter. Consequently they increased the number of labour mujahideen until they reached twenty-one. Furthermore, earth evaporates less in summer, so ventilation was a lesser issue, work faster and more effective. By digging 166m they reached the middle of the prison (it must be added that the Islamic Emirate’s site mentioned the distance dug by mujahideen including the distance dug by error, giving a total of 360m, when the precise distance, excluding additional distance, was 266m. Noteworthy to add the tunnel’s 70cm height and 60cm width).\nThe prisoner mujahideen were in two separate locations within the prison: Most were in the political ward, where they numbered 530, but a small number was in a room called ‘Tawqif Khanah’ [arrest room]. The tunnel was dug first towards Tawqif Khanah room, as it held a linked mujahid aware of the case. He used to hit the ground for a reason and no reason in order for the tunnel diggers to recognise whether they were ahead or behind or at the target. Thus they were able to pinpoint the place, but for verification they raised a blade to the room, until the prisoner assured them of hitting the target. They moved on to the political ward. Five days produced further 23m, reaching the political ward where its room 7 held two linked prisoners aware of the case. The aim was to take the tunnel to room 7. Here again the tunnel diggers wanted to raise a blade to ascertain and avoid any error when opening the tunnel to the prison. The mujahideen were hesitant: were they under room 6 or 7? To keep the matter secret when the blade would be raised, the two prisoners held a Qur’an completion [session]; all the rooms were vacant, the two aforementioned prisoners left, one to room 6 and the other to room 7. When the blade was raised they realised they were under room 6, contrary to their expectation. Then two further meters were dug until reaching room 7. Now they could not vacate the rooms with the Qur’an completion excuse again, so the mujahideen used the afternoon time when prisoners would go out to washing rooms and get prepared for the noon prayer; the blade was raised and it made it successfully to room 7. The opening place was specified for the escape operation day. It should be added that the blade raising operation was made subsequent to a wise and interesting plan: The prison ground was about 2.5m above the tunnel, with the tunnel’s height of 70cm, how a long blade could fit through this tunnel to reach the prison ground? The mujahideen cut iron blades of 50cm length and joined them together; when they were raising a blade 50cm by a car lift, they would fix to it another blade and raise it by the lift. Thus they prepared for the dismantling of blades a machine that would be attached to each blade then hit by a hammer downwards. This was how they were able to pinpoint the location precisely.\nPrison Release Plan\nFollowing the tunnel digging to the desired target, the persons responsible for digging finished their work. They requested from the Islamic Emirate’s high command guidelines concerning the prison release planning. The Kandahar Province’s and high ranking Islamic Emirate’s officials held continuous consultations for the secure and successful delivery of the release operation. Subsequent to consultation, the following plan was adopted.\nThe mind behind this operation, who on his own hit the pickaxe laboriously to dig 300m of the tunnel, would himself be the commander of the prison release operation as well. He would adopt during the operation ad-hoc plans as needed. The high command would tell him about whatever might happen. The operation would be kept secret until the last moment of execution. Links would be established with the linked brothers within the prison; they would be prepared inside the prison to take the responsibility of organising and moving out the prisoners according to the plan. Similarly all decisions were taken, delegating the operation command to the aforementioned person.\nRelease Operation\nFor the best execution of the operation, precautionary measures were checked and preparatory processes were taken again to solve the ventilation problem inside the tunnel. A powerful machine to pump air was operated while the pipe laid inside the tunnel was holed in ten places to deliver air to all parts of the tunnel. Forty-five lamps were also switched on for illumination of the tunnel. As a precautionary measure, a team of to-be-martyrs were sent to the prison neighbouring areas to launch a militant attack if necessary.\nFor the operation concealment and fear of being exposed, the operation-tasked person chose five mujahideen, out of the 21-mujahid team as stated before, on the operation day, so he would not lose all his friends, God forbid, if some bad thing were to happen. Subsequently, the release operation team was six persons. These six told the three linked mujahideen within the prison at 9am, one of them was in the Tawqif Khanah room and two in the political ward, that the coming night would be the date of executing the operation, God willing, in order to be prepared. The two linked persons in room 7 of the political ward, for the purpose of telling the rest of prisoners about the case at an appropriate time, prepared some hospitality in their room and invited one or two persons from each room.\nThe operation commander planed as follows: four brothers of the six would enter the tunnel, two would start working to open the tunnel to the Tawqif Khanah room and two would work to open a tunnel to the political ward; the remaining two would be outside the tunnel. The mujahideen would extend a telephone wire inside the tunnel, establishing a connection between brothers outside and brothers inside and allowing exchange of information e.g. where the work reached and what need be done, etc.\nThe four brothers entered the tunnel with car lifts and] solid iron [poles]. They started opening the tunnel to the “Tawqif Khanah” room and the political ward. At about 10 o’clock they easily opened the “Tawqif Khanah” room floor with the lift. The prisoners exited. But as they had among them two spies from the prison administration disguised as prisoners, one was made unconscious by the mujahideen and the other taken out via the tunnel handcuffed to prevent him from causing noise.\nAs for the ground of the political ward, its construction was heavy-duty and it took the mujahideen long to make a hole through. The lift was raising the (cement) concrete ground but due to blocked air in the tunnel it was difficult to hole. After many trials the mujahideen were able to smash the ground. After cutting a huge hole to the ward’s room 7, the brothers down the tunnel gave four pistols and four daggers to the linked brothers for use in the operation. They also gave them a telephone handset to establish a connection with the brothers out of the tunnel. Thus the prisoners went on to exit until 1.30am (April 25th, 2011); approximately 250 prisoners exited this way. But the work team realised that if exiting would carry on as such it would last until 2 o’clock, while the plan was for the prisoners not to wait long [outside] as waiting till dawn would be dangerous leading possibly to a botched operation. Therefore, the team postponed the exiting of prisoners for half an hour. They started letting prisoners out again at 2am. By 3am no prisoner was in this ward.\nWe would like to add that all prisoners were being inspected at the entrance and exit of the tunnel. When entering, their luggage boxes would be taken as carrying them would have caused their delay and risked their re-arrest. When exiting, any money surplus to 3,000 rupees would be withheld and granted to those with no money.\nWhile leaving, it was properly organised. The tasked brothers would wake up the prisoners of each room in turn and guide them to the tunnel. At the exit they would ride the lorries parked at the house; each lorry would carry 36 persons. It was 3:10am when [all] the prisoners left and lorries were allowed to depart. The lorries left from the yard but some brothers headed to the town suburbs on foot – they were instructed to cut a distance before returning to the Kandahar-Herat highway after daybreak and to leave the yard using taxis.\nIt must also be added that two of these lorries that were transporting the prisoners made two journeys to transport them. By 3:30 or 4:00am no prisoner was in the prison neighbouring areas. It is noteworthy to say that by God’s favour and then the mujahideen carefulness and sagacity, the enemy felt nothing throughout concerning what was going on next to it – the house used in the operation was about 20m from the enemy’s watch tower that oversaw easily the middle of the house. A surveillance camera was also installed facing the house door. Nevertheless and thank God, it noticed nothing.\nThe Operation’s Expenses\nWe must add that there were no body losses and the mujahideen shot no bullet. Furthermore, the financial expenses were much lower than expected. According the person in charge of the operation and its planner, the expenses during the operation’s five months reached about 900,000 Afghanis (i.e. US$20,000). These included the house fees, mujahideen food, lorry charges and other equipment the mujahideen left in the house after the operation.\nOn the last day of the operation, the person in charge who built the concrete workshop for the operation execution stated: we sold during the five months 150 concrete blocks, making much profit. He added: After the operation and the final exit, when the house gate was locked, we left the air pumping machine, 45 lamps, 10 concrete blocks, a pole valued at 50,000 Afghanis, 2 power generators, 2 wheelbarrows, 2 car lifts and some building material; but this historic house benefited us much that these expenses seemed nothing.\n{Part 2: pp24-25 of al-Sumud, 5th year, volume 60, Jumada al-Thaniya 1432AH/May-June 2011}\nStory narrator: Muhammad Idris\nEditor: Habib Mujahid\nI was the Second Person to Exit\n(A Prisoner Tells his Story)\nMuhammad Idris, a 23-year old Kandahar resident, had for many years been launching surprise operations in Kandahar city under the Islamic Emirate’s command. He was caught 7 months ago by the enemy in Kandahar city and sent to Kandahar prison. He said he had yet to be tried. Since his captivity, he lived in room 9 of the huge prison with other 15 mujahideen. He was the second, out of hundreds, to exit the prison via the tunnel dug from the known house to the prison.\nLet us allow Muhammad Idris to tell us his story himself:\nThe Kandahar prison is thus established: in the middle of the political ward there is a vast yard. All room doors open to this yard; so the ward’s main gate is always locked while the internal room doors are always open. Therefore, prisoners are able to enter other rooms with no difficulty; they gather for the communal prayers as well.\nThe prisoners of the room that the tunnel reached hosted on Monday night their friends and invited from each room one or two persons. To the hospitality this ward’s prayer imam, a scholar prisoner, was also invited, while I represented my room. So to the supper we went.\nWe had our supper. Then Mawlawi, the prayer imam, started talking. After some beneficial advice, the sheikh [imam] started telling the persons present about the release operation plan. None of us knew anything about the subject until then. During his talk he told the prisoners sitting in the room: “Tonight an operation to free and release us will be made; we better be ready for it”. “Anyone of you exiting is ordered not to operate his mobile until tomorrow afternoon. If he talks on his mobile he must be careful not to mention how he got out,” he added. Following these guidelines he told us: “Keep mentioned God sincerely so he brings this operation to success.” We all started mentioning God. Within half an hour all brothers were busy with praises and prayers. Then the brothers who were aware of the plan came to the mentioned room. They cleared the items and mat in a particular part of the room. Moments later the cleared area was knocked at, and the brothers in the tunnel under the area put a car lift underneath. They continued the pressure until breaking the (cement) concrete ground. As this place was low, they brought with them many robust poles in order for the lift power to reach the concrete [ground]: they would place the poles on the lift then raise it. They repeated this two or three times until a huge hole was opened in the middle of the room.\nThereafter, the brothers inside the tunnel gave the linked prisoners a number of pistols, daggers and knives. They also gave them a box containing a telephone headset, video camera and other devices that I did not recognise. I looked to the tunnel and saw two mujahids: one who gave the box and another. Both retreated and headed to the other opening. At this point the prisoner release operation was delegated to the prisoner mujahideen who were aware of the operation. They linked the telephone headset with the wire, establishing a connection with the mujahideen on either side of the tunnel: inside and outside of the prison.\nThese operation-aware prisoners distributed then the arms among themselves, adding a number of trusted mujahideen to them. This ward was holding 2 rooms of criminal prisoners [as well]. There were also a number of state spies among the prisoners. So the decision was made if such spies were to cause trouble or attempt telling the prison guards we would kill them by these arms and knives. They said that such an operation would be difficult to comprehend; if any brothers would not trust it and refuse leaving, we would force them with these arms to exit.\nMeanwhile, the operation-aware prisoners said to persons present prior to going down the tunnel: “When you go out on the other side of the tunnel, you will meet a number of mujahideen. They will take surplus money from you, mobiles and other items. They will allow you neither to talk nor to leave; listen to them in whatever they say to you.” Alright, will do, we said. Now I was the second in turn in the group sitting in the room to the tunnel, the first prisoner went down and I followed. The tunnel was wide, but not very much, I mean we could walk kneeling or crawl easily. But the mujahideen dug it fantastically: Every 15m there was a lamp, it was very bright. Similarly the mujahideen laid a 6″ diameter plastic pipeline along the tunnel for ventilation. At its start they operated a device for pumping air in, and made small holes in the tunnel. We thus felt no lack of air in the tunnel. Additionally two wires were laid in the tunnel: One was for the telephone and I did not know whether the other was for electricity or otherwise. We went about 15 minutes inside the tunnel until reaching the other side: One went in front of me and a large number were behind me. Upon reaching the tunnel exit there were 15 armed mujahideen. They meticulously inspected all people exiting from the tunnel, taking from all their mobiles and sim cards. If one had money they would leave 3,000 rupees to him and take the rest. There was a coat where they would put the money they took from us in. As for prisoners with no money or less than 3,000 rupees, they would give them from the collection in order to have 3,000 rupees. This was the best method for everyone to have money that would help with errands until reaching their destination. After exiting I saw three mujahideen I knew, so they joined me to them in the operation execution. In the house were six transport cars. The brothers told us to make anyone exiting from the tunnel get in the cars after inspection and ordered them to say nothing but mention God discreetly, as close to the [house] yard was a prison tower; if disorder was to occur the enemy could notice. This way the prisoners exited from the tunnel and we made them ride the cars. Whenever a car would be full we would cover it. When all brothers finished, some friend said: “Not much space is left in the cars. Brothers who know the area and town alleys should go on their feet to the town suburbs”. The house gate was facing the prison while its back faced a residential area. We made a hole in the back wall and through which the brothers who could not be taken by cars walked to the town suburbs.\nBut I and four of my friends who were of the city residents discussed it between us and agreed to go to the city. At this point the cars left the house and the five of us left minutes later to the street. We waited a little while on the pavement until a taxi came that was heading to the city. We rode it. It was 4am. Upon heading to the city the police stopped our car at “Dand” roundabout then told us by his hand to go. The same inspection process was repeated at “Madad” roundabout; the policeman said nothing. We reached the city and we had our rescue; salvation.\nI need add that the prisoners in the political ward of Kandahar prison are all mujahideen. Nearby there was another ward called Tawqif Khanah where a prison room held mujahideen. From the main tunnel the mujahideen dug a secondary one to that room, allowing 10-15 prisoners of mujahideen to exit that room, thank God.\nIn the morning, when I walked about the town and was following the news, in my view until 8am the enemy felt nothing concerning the runaway mujahideen from the prison, as I saw no mandatory checks being carried out in the city. After 8am, the enemy started action. The prison guards used to count us twice: at 8am and in the afternoon. I think that when they arrived to the political ward at 8am to count the prisoners, they found none. Then they started searching and looking for them.\nTo my knowledge, not a single mujahid remained in the prison’s political ward, but there was a room for the mentally ill and they remained inside the prison. As for the other wounded and sick prisoners, all were freed. There was even a wounded prisoner with two iron bars in his legs, during his walk inside the tunnel the two bars were broken and he fainted of extreme pain. Nevertheless, the mujahideen carried him in this condition, got him out of the tunnel and transported him by car."
  },
  {
    "objectID": "personal/2011-05-03-the-afghan-taliban-react-to-bin-ladens-death.html",
    "href": "personal/2011-05-03-the-afghan-taliban-react-to-bin-ladens-death.html",
    "title": "The Afghan Taliban react to bin Laden’s death",
    "section": "",
    "text": "As expected, an extremely lukewarm and non-committal statement from the Afghan Taliban. Initial translation here: Replaced initial translation with the official IEA translation: &gt; From one point of view, the Americans did not present sufficient evidence to prove their claim, and from the other point of view, the sources close to Sheikh Usama bin Laden have not announced their position - confirming or denying - what Obama announced about the above-mentioned martyrdom, and therefore, the Islamic Emirate of Afghanistan considers discussion about the subject, before the release of an official statement from sources close to the Sheikh, premature.\nI’m going to write up some longer thoughts later this week probably. Note that given the issues with communicating between the different senior figures (especially since they’ll likely be far more cagey at the moment) it’ll take a fair amount of time before we see a formal statement that processes the news in any significant way. Maybe we’ll even have to wait until Mullah Mohammad Omar’s eid letter later this year. More to follow."
  },
  {
    "objectID": "personal/2011-04-03-the-kill-team.html",
    "href": "personal/2011-04-03-the-kill-team.html",
    "title": "‘The Kill Team’",
    "section": "",
    "text": "I’ve been thinking about the so-called ‘Kill Team’ over the past few days, prompted by a disturbing article in Rolling Stone magazine. I’ll try to write something later this week, but for the moment, I’d strongly recommend two books (both out of print, I think, so use bookfinder.com to locate a copy) to help put it all in some sort of context. They’re both oral history sources for the experiences of Soviet soldiers who fought in Afghanistan (and one also offers the additional comparison point of US soldiers in Vietnam). At any rate, give these two books a read:\n\nParallels: The Soldiers’ Knowledge and the Oral History of Contemporary Warfare - J.T. Hansen, A. Susan Owen and Michael Patrick Madden (especially chapter 4)\nZinky Boys: Soviet Voices from a Forgotten War - Svetlana Alexievich (a must-read, in any case)\n\nI’ve just finished another round of edits of An Enemy We Created, so have a bit more time to take my head out of the sand and blog here and at Current Intelligence over the next few days hopefully."
  },
  {
    "objectID": "personal/2011-01-21-helmand-refugee-appeal-update-distribution-day.html",
    "href": "personal/2011-01-21-helmand-refugee-appeal-update-distribution-day.html",
    "title": "Helmand Refugee Appeal Update - Distribution Day",
    "section": "",
    "text": "_POU1490_bd.jpg\n\n\nPhoto: © Philip Poupin\nThanks to the many kind donations of over a hundred readers of this blog, twitter followers and others, we completed the distribution of materials to those living in two camps in Kabul on January 18th and 20th. By the time we stopped taking donations, we’d raised $9118 from 124 individuals (see at the bottom of the post for a full list).\nIt’s easiest if I simply give the floor to Orzala, who organised the distribution with help from her brother, Sohrab:\n\n“The story begins with me sitting at the library busy with my studies, while part of me is still thinking about home, people, news and everything else that is happening back at home. Alex sends me a report with some pictures published in a newspaper about the life of internally displaced people coming from southern Afghanistan and living in Kabul …\nStaying in the UK for the last 4-5 months, Helmand has become a place I hear the most about, from politicians. I hear most often, success stories and how wonderfully everything works out there. From journalists I hear about war and their pictures of the UK forces in the field, yet the least can be found about the voices of people…. This story struck me in two ways: 1) I found a voice which I could identify with, given my own life experiences as a refugee running away from violence and war, so I couldn’t be passive about them. 2) I thought, I am in danger of being ‘spoiled’, living a comfortable life, warm heated, good shoes, good living conditions and everything I want available for me here, while I just read news about others in challenge and that I have become a typical ‘consumer’ of the news and information.\nSo I got back to Alex and told him, I am ready to help, if he is to support too, because of all deadlines and too many other priorities on my list. So the idea develops, I start contacting with various friends and organizations dealing with refugees etc. to get a picture about their numbers and also on what organizations are involved for assistance, as usual, the official response is bureaucratic, while a personal response recommends the best option as simply going there and doing it on our own.\nI visited Kabul for a short time (even shorter because I was delayed in London for three days on account of the snow!) and most of you will have read my account of a visit to the first camp at Char Rahi Qanbar.\nJust a day before leaving Kabul, I learnt that there is another camp, with refugees from (mainly) Helmand and also other southern provinces, around 300 families living in a far more desperate situation, as they get much less attention than the bigger camp in Charai Qanbar. Perhaps so far no report or anything is being published about this other part. I found the situation in some ways harder for them. With conversations I’ve had with around 10-15 men from village, some young, middle aged and some elderly, they all shared their sufferings and why/how they made it to Kabul…\nI had to explain who I was and what brought me to see them. I’ve told them about the newspaper and the fact that some of the people who came visited them, read about their situation on the newspapers, decided to provide an urgent assistance, which is only for one time and is meant to at least keep them warm for part of winter if not for whole winter. They were very friendly and said thanks to anyone who is ready to support them in this hard time. One of them told me, we are not used to beg, if people come and help us, we’re grateful, otherwise, we just sit here. Another shared what I exactly also hear in camp one. ‘They are not giving us jobs here [he meant job as in daily labor], they say we look like Taliban’ he smiles. ‘It is not my fault to have the same clothes as the Taliban’!\nThe wakeel [elder] of the camp provided us with a list, he says there were 250 families before, now another 50 have joined very recently. They are all coming from areas where the military operations are going on. We write up the names of the additional 50 families, the names are given by 5 men who represent the families.\nChoosing what to distribute:\nThe original appeal asked for charcoal, because some of the residents of the first camp mentioned this, and I too was thinking this is the best option working to keep tents warm. However, we faced some challenges: the challenge was the much larger number of refugees in camp one: 870 families in camp one and 300 in camp two. This meant our only charcoal option could not work for mainly financial reasons. We’d have needed at least $2000 to provide them with 50 kgs. On the other hand, we also realised that camp one is already receiving this package of winter assistance from a German NGO which also includes wood as part of that support. Residents and elders of camp two said they had not received this help so far. On the day I visited them, I saw bags of wheat being off-loaded in the camp; we could not 100% verify who brought it (some said it was the Afghan Ministry of refugees, others said it was Aschiana), but I observed they did have wheat for all families.\nTaking this situation into account, we decided to provide the families of camp one with 5 kgs of cooking oil, which is one of the top two priorities for food right after wheat, and provide the residents of camp two with 25 kg of charcoal each. The goal for us was to contribute to every family at both camps. The whole decision on what to distribute is result of discussions among the camp residents, men and women whom I visited and spoke with.\nOrganizing the distribution:\nMy absence from Kabul made it bit challenging to organize the distribution, but nevertheless, thanks to my brother (the co-founder of Kabul Dreams, Afghanistan’s first indie rock band) who agreed to take the responsibility, it all went smoothly.\nOur photographer friend, Philip Poupin, had kindly agreed to be present there as witness and also take photographs of the whole process so that it is documented. Also, Abaceen Nasimi, Alex’s friend, was there to observe the distribution.\nAlthough initially I was thinking to put together a team of volunteers who could help with the distribution (as I had done in previous distributions of this sort), in the process, we found that it would be easier to involve the people themselves. One day before the distribution, Siddique visited both camps, verified the lists for final and identified all representatives for distribution. In camp one they have been pretty well-organized in this way: for the entire camp there were two wakeels. Each wakeel, then, has 10-15 representatives who each represents 5-10 families. The fact that in both camps refugees themselves volunteered to help with distribution confirmed my old message once again: people in difficult situations should not be seen as subjects, or helpless victims; if you give them hope and an opportunity to act, they can be active agents for a better life. This is far too small an example, but it can perhaps lead into a big one, some day!\nSimilarly, in camp two, people were far more organized than camp one. There were literate men among these refugees who had prepared a much better organized list of all families in the camp. Each 5-10 families (mostly related) were represented by one slightly elderly man. These representatives all gathered and identified how many families they represented. Based on this list the charcoal sacks were distributed among the representatives who would immediately carry them to the tents. If we (I mean myself here) were to organize this distribution, it would take us weeks and we would still miss some families. In a lucky coincidence, some people from the Ministry of Finance had gathered money and bought rice, grains and cooking oil that was being distributed among the families in camp two on the same day after our charcoal distribution was done.\nOne challenge as it seems was the fact that among distributors no woman was involved, so there may not be pictures of women from the camp. It is simply something we are not supposed to push for, given that most of the refugees are coming from areas where women are still covered and it is a sensitive matter. And women themselves prefer not to be in pictures either.\nWe would like to thank you all for being so generous in supporting this very important cause. This assistance maybe be too little to be sustainable or help the refugees, but at least a message is conveyed there that not all outsiders agree with the bombings and killings of civilians; not all outsiders are passive readers of the news and looking at their situations and… so, THANK YOU ALL who contributed to this little mission to be a success. We tried our best to make sure that each can of oil and each bag of charcoal entered into each family tent, and that’s best we could do. Our hope is to see them one day back into their fields, orchards and their own little houses.”\n\nAnd I would like to add my own heartfelt appreciation for all those who donated and helped out in various ways with this appeal. I must admit I was initially skeptical that we would be able to raise the money; I had assumed others were as weary with the progress of the war as I am and that such a small effort, reaching relatively few people, would find little traction. As it turns out, we raised considerably more than our original target and were thus able to contribute in a more extensive way than originally imagined. We all need hope in these difficult times; the generosity of strangers demonstrated here fulfils our need.\n[Update/Edit: It occurred to me (and I had some emails) that people might not want their names displayed here, so I took the list of donors down.]"
  },
  {
    "objectID": "personal/2010-12-28-an-appeal.html",
    "href": "personal/2010-12-28-an-appeal.html",
    "title": "UPDATED: An Appeal",
    "section": "",
    "text": "It’s easy to talk in the abstract about war. The dead become numbers, the displaced are statistics, and slowly we begin to forget about the people who live through it all. Afghanistan is a case in point. Tens of thousands of words of commentary are written every day, but very few of these seem to accurately bring these day-to-day particulars across. Earlier this month, I read an article by Josh Partlow in the Washington Post on the situation for those who have fled the conflict in Helmand – U.N.-speak = IDPs – for an area near Kabul City. It was a detailed, movingly-described account of some of these ‘particulars’ of their lives: &gt; “For those who have escaped Afghanistan’s worst violence, some things are hard to forget: the sight of a woman’s hair entangled in the mulberry branches, her legs strewn far away in the dirt. Or the sounds they heard as they hid in an underground hole, counting the bombs to pass the time, praying the American troops would leave. Some of those Afghans have tiptoed in the footsteps of neighbors to avoid the mines. They’ve been hit with shrapnel and tied with flex cuffs, threatened by the Taliban and frightened by the coalition, seen relatives shot and homes destroyed. And so they left Helmand province and made their way to this dirt lot on the outskirts of Kabul, where month by month the settlement expands with those who have come to wait out the war. “In a situation like this,” said Sayid Mohammad, a Helmand native who has spent the past year at the refugee camp, “how could I ever go home?”” [Read the full article here]\nThere’s nothing new or particularly special about this group of refugees from Helmand, but for some reason this piece said something to me. It’s easy to become passive consumers of the news coming out of Afghanistan, particularly when it’s often so frustrating to read. I first read the article in London, a place where everything is taken for granted: warmth, walking on the snow, heating in the house, electricity, water, you name it. But if you allow your imagination to drift, imagine living away from home, in a place far colder than what you’re used to, in tents and makeshift huts on account of a war taking place in the villages, one that you have seen sweep through with random but seeming deathly certainty and claim your friends and family. For another account of life in the camp, watch the documentary account made by Alberto Arce here.\nSo I decided together with a long-standing Afghan friend and respected NGO-practitioner – she used to run HAWCA – to try to find some way to contribute to bettering the lives of these refugees at the camp. Orzala explains more:\n\n“We contacted the UNHCR office to find out about the numbers of refugees and how we can make sure that our possible help is going to reach the neediest. Their formal response was, it can happen through government or NGOs working with refugees. A good friend who also is part of an international organisation involved in the field advised small scale donations and funds to go through private initiatives rather than the formal ones. Additionally with my experience in the past, I believe the winter will be over if we follow the lengthy procedures. I visited the site itself a couple of days ago to talk with those living there and also to get a realistic sense of how many people were living there. A representative stated that there were 870 families living there at the moment, and we got an idea of what other organisations were working there as well (Aschiana, the World Health Organization, the Afghan Ministry of Public Health along with Welt Hunger Hilfe). It seems, however, that there is a shortfall in terms of the amount of assistance being provided, as well as the speed that this is happening.”\n\nSo in the short-term what we want to do is – at the suggestion of those from the camp, but also an idea Orzala had had beforehand – to raise some money to provide charcoal. People are accustomed to using this in the winter; and it’s neither heavy nor particularly expensive. 50 kilograms of charcoal costs about $20 and so to be able to provide around 20 kg of charcoal to everyone will cost just under $7000. I know it’s easy to just close this page and move on to something different, but I hope you’ll be able to donate something – perhaps $10 or $15 – via the paypal button below so that we can try to ensure that this group of people have at least some warmth to rely on when the snows come in Kabul.\n[THE APPEAL HAS NOW CLOSED]\nSince the donate button doesn’t display a running total, I’ll do that myself here on the blog, and will of course keep you all updated with how things go once we have raised our target amount.\nFinal total raised: $9,118 from 124 people."
  },
  {
    "objectID": "personal/2010-12-22-deedee-derksen-picks-her-2010-books.html",
    "href": "personal/2010-12-22-deedee-derksen-picks-her-2010-books.html",
    "title": "Deedee Derksen picks her 2010 books",
    "section": "",
    "text": "This is a guest-post by Deedee Derksen, a Dutch journalist just out with a good book on Afghanistan that helps deflate many stereotypes commonly believed.  It’s only out in Dutch at the moment, but I’ll bet an English version will come out before not too long… &gt; I love reading autobiographies and biographies.  A few I’ve read this year convey profound belief, be it in:\na.    creating the best rock band in the world (Keith Richards) b.    establishing the best Islamic Emirate in the world (Mullah Abdul Salam Zaeef) c.    writing the best books in the world (Somerset Maugham, Patricia Highsmith) d.    great reporting (Martha Gellhorn, Hugh Pope) e.    himself (Tony Blair)\nKeith Richards and Mullah Zaeef share more than their belief. They’re both icons of their time, or at least wingmen to icons. They were both part of a band that made headlines the world over. They both know a thing or two about the dangers of drugs and loose women. And they were both once wanted men – though the hordes of semi-naked girls and English bobbies after Richards probably weren’t quite as menacing as the war on terror justice unleashed on Mullah Zaeef.\nBoth excellent autobiographies offer rare insights to lives otherwise closed off, and often misrepresented.  Anyone doing anything Afghanistan related should read Mullah Abdul Salam Zaeef’s memoir (My Life With The Taliban, ed. Alex Strick van Linschoten, Felix Kuehn), which provides a unique insider’s view on the Taliban movement. Keith Richard’s book (Life, co-author: James Fox) may not be as vital to world peace as Mullah Zaeef’s, but it’s nonetheless a lot of fun to read. For all the sex-drugs-and-rock-and-roll stories (and there are many), what struck me most was that Richards is, above all, an ambitious, hardworking guy.\nTwo biographies of writers that appeared in 2009, which I read in 2010 and which are unlikely to be bettered, are The Secret Lives of Somerset Maugham, by Selina Hastings, and Beautiful Shadow – A life of Patricia Highsmith, by Andrew Wilson.  Somerset Maugham’s short stories are among my favourites (as are those of Alice Munro, mentioned elsewhere on this blog), and this biography gives an account of the tortured, and often quite unpleasant, genius behind them. Like Maugham, Patricia Highsmith was a loner, according to the beautiful biography by Andrew Wilson.  I read her series of Tom Ripley thrillers again after reading about the author. They’re amoral, and gripping from the first page. Terrific.\nMartha Gellhorn’s reporting on the Second World War is some of the most interesting. As a woman, she wasn’t permitted to embed with the American troops. So while reporters like Ernie Pyle and Gellhorn’s husband Ernest Hemingway were embedded, and thus subject to official censorship, Gellhorn wrote freely about the horrors in Europe (Gellhorn: a Twentieth-Century Life by Caroline Moorehead).  Now women reporters can go embedded, many consciously choose to work independently, like Minka Nijhuis from the Netherlands in Iraq and Afghanistan. Her beautifully written and very moving book on Burma – Birma. Land van Geheimen (2009) – won a well-deserved, and prestigious, Dutch award in 2010.\nMany foreign correspondents find it difficult to convey to their editors images or impressions that contradict stereotypes at home. This is especially so in the Muslim world, as Hugh Pope explains in Dining with al-Qaeda.  Pope, after thirty (!) years reporting across the Middle East, has some tremendous stories to tell – and he does so with much empathy and wit.  A great read. I also liked People Like Us by the Dutch former-correspondent Joris Luyendijk, a Dutch book on Middle East reporting which was published in 2006 but translated into English in 2009. Luyendijk rightly shatters any lingering belief in objective coverage of the Middle East.\nI haven’t yet started Tony Blair’s eulogy to himself. Judging from the bits I’ve read here and there, I don’t have high expectations. Not only does it beg for a good edit (I assume he made the fatal mistake of writing it himself as it reads like a column in Good Housekeeping). But Blair’s take on civil liberties would make Dostum blush (read Dave Eggers’s wonderful Zeitoun, also out this year, for the sharpest antidote to Blair’s call for the suspension of Habeas Corpus). Perhaps just as offensively, Blair expresses no remorse over Iraq, and lumps all Islamists together, conflating Hamas and Hizbollah with al-Qaeda, and portraying them as an existential threat, the gravest ever faced by mankind, perhaps with the exception of Gordon Brown. And all this from one of most successful politicians of our times and the man currently entrusted to bringing peace to the Middle East. Now that’s a great piece of fiction."
  },
  {
    "objectID": "personal/2010-12-14-open-letter-the-response.html",
    "href": "personal/2010-12-14-open-letter-the-response.html",
    "title": "Open Letter: The Response (UPDATED)",
    "section": "",
    "text": "After a slow beginning, the open letter to President Obama that I co-signed has finally started to get some media coverage and blogger/commentator reaction. I’m listing here all the different places it’s shown up so far, and I’ll try to keep it as up-to-date as possible.\nNote, too, that the list of those who have signed continues to grow as word about the letter spreads. We are now over 50 names.\nReprints of the Letter\nMain/Official Site\nWar is a Crime.org\nThe Guardian @ Comment is Free\nThe Daily Telegraph\nJean Guisnel translates the letter into French\n‘War in Context’ blog republishes part of the letter\nThe official Afghanistan Operation blog of the British Ministry of Defence reprints part of the letter and links to the Daily Telegraph reprint\nThe UK’s Stop the War Coalition reprints the full letter\nAnthony Loewenstein reprints the full letter\nE-Ariana (a news wire service) reprints the full letter\nComment and Explanation from those who signed\nGerard Russell explains why he signed over on his personal blog\nFour of those who signed answer some questions posed to us by a blogger/journalist\nGilles Dorronsoro was on BBC World Service Radio (no link available)\nDaniel Korski explains why he signed (on his blog at The Spectator)\nJoshua Foust explains why he signed (and why he’s changed his mind on negotiations) over at registan.net\nI explain on BBC World Service a bit about the context of the open letter (44:23mins in)\nNews/Wires\n“No decisive victory one year into Afghan surge” - Associated Press (republished elsewhere, including at NPR.org)\n“US surge in Afghanistan ‘not working’” - The Daily Telegraph (UK)\n“Afghan insurgents kill six foreign soldiers” - AFP posted on Khaleej Times\n“Obama”Must Talk to Afghan Taliban”” - Asharq al-Awsat (reposting AFP)\n“Des experts internationaux appellent Obama à négocier avec les talibans” - AFP posted on Le Monde website (in French)\n“Obama must talk to Afghan Taliban, experts say” - AFP published on Emirates 24/7\n“Academics, experts appeal to Obama to back Taliban talks” - Myra MacDonald writes a piece for Reuters about the letter, quoting extensively.\n“6 Nato soldiers killed” - The Morning Star Online\n“Letter to Obama calls for change in Afghan strategy” - Daily Times (Pakistan)\n“Pak intelligentsia urges Obama to change Afghan strategy” - AfghanistanNews.net (needless to say, we are not the ‘Pak intelligentsia’)\nAllvoices runs a news piece on the letter\nPakistan Today, a newspaper, outlines the main points of the letter\nThe Century Foundation feature Praveen’s critique of the letter on their Afghanistan page\nDawn newspaper (Pakistan) features the letter\nFrance 24 cover the letter on their website news wire\nBlogging and Analysis\nMalou Innocent mentions the letter and part-quotes it in a piece entitled “Spinning Us to Death” - The National Interest\nTim Mathews disagrees (reposted here), but finds some common ground here and there\nMax Boot strongly disagrees\n“Top Analysts Blame American Intransigence In Not Talking To Taliban” - Steve Hynd agrees over at NewsHoggers.com (reposted at Rethink Afghanistan\n“Commentary: Vietnam syndrome?” - Arnaud de Borchgrave comments (mostly sympathetically) for UPI.com\nChristian Bleuer mentions the letter, but declines to comment\nAnn Marlowe sees an opportunity for satire in the open letter\nPaul Pillar cites the open letter in the context of the strategic review and wonders why there hasn’t been more criticism\nTea and Politics cites the letter and equates talks in Afghanistan to ‘negotiation with the Nazis’\nRobert Naiman suggests the ‘progress’ cited in the strategic review may not be all it seems, citing the open letter (@ the Huffington Post)\nThe ‘Obama Blog’ suggests the US president is ignoring the ‘Afghanistan-Pakistan reality’\n‘The Lift’ blog on ‘legal issues in the fight against terrorism’ cites the letter in a post\nJason Ditz of antiwar.com cites the letter in a post about ‘bleak metrics’\nHugh Pope updates a post about Deedee Derksen’s new book ‘Tea with the Taliban’ and cites the letter\nThe Council on Foreign Relations cite the letter in an analysis brief looking at the post-Holbrooke strategy\nCompatible Creatures blog cites the letter in a discussion of Holbrooke’s alleged last words\nJayshree Bajoria (Council on Foreign Relations) cites the letter in a post on her Huffington Post blog\nSmall Wars Journal’s forum (Small Wars Council) mentions the letter and kicks off a very frank discussion\nPraveen Swami (The Daily Telegraph) disagrees with the suggestions contained the letter\nColumbia University Press’ blog cites the letter\n‘American Everyman’ cites the letter\nDr Mohammad Taqi (Daily Times, Pakistan) cites the letter and suggests both it and the strategic review misconceive the environment\n‘Rehmat’s World’ cites and quotes part of the letter\nAfghan Reactions\nKabulPress.org disagrees with the letter (in Dari, and interesting as one of the few Afghan reactions so far – aside from those Afghans who have already signed the letter)\n8am or Hasht-e Sobh daily newspaper also disagrees with the premise of the article (also in Dari)"
  },
  {
    "objectID": "personal/2010-11-23-foreign-fighters-down-south.html",
    "href": "personal/2010-11-23-foreign-fighters-down-south.html",
    "title": "Foreign fighters down south?",
    "section": "",
    "text": "Not so much. Last weekend’s Sunday Times carried an article by Miles Amoore headlined, “Love drives repentant Taliban chief to defect.” It also includes the following:\n\nMany fighters are thought to have mixed feelings about leaving [the Taliban]. “If I stop fighting, maybe the government will still persecute me as a Talib while the Taliban try to kill me,” said Rahman. “I am stuck in the middle.” The greater number of foreign fighters in the south and southeast – mainly from Pakistan and the Middle East – will make it even harder for foot soldiers there to defect.\n\nI’d like to see some numbers on how many ‘foreign fighters’ – “greater” in number – there supposedly are down in southeastern Afghanistan, let alone in the south. Needless to say this is an exaggerated claim. Watch this space for more detail."
  },
  {
    "objectID": "personal/2010-11-13-fly-freely-afghan-womens-poetry.html",
    "href": "personal/2010-11-13-fly-freely-afghan-womens-poetry.html",
    "title": "‘Fly Freely’ - Afghan Women’s Poetry",
    "section": "",
    "text": "I’m going through our selection of poems written by young and old Talibs and remembered a different set of poems that I translated from the Dari a few years ago, those of Nadia Anjuman. I’ll be republishing these poems online soon – since the old website has lapsed and doesn’t work any more – but you can order the full printed version on the HAWCA website. It includes an essay written by Christina Lamb, the complete side-by-side English-Dari translation of Nadia Anjuman’s book of poems as well as four stories written by victims of violence against women. This is one of my favourites among the collection: Fly Freely (2001)\nOn a day when my thoughts bring me firewood\nas a gift instead of cold feelings\nOn a day when my eyes are wide open\nAs if\nBy seeing a withered leaf, oceans would flow\nOn a day when my hands are inspired\nto weave clothes full of wheat and roses\nfor the body of this creation\nOn a day when my lullaby can\ngrant sleep to the eyes of the sick and street-bound children\nOn a day when with soaring melodies\npray\nto the fire spirits\nOn that day,\nI will write a poem, a great romance\nsweet as a palm tree and as enchanting as the moon."
  },
  {
    "objectID": "personal/2010-07-07-irish-parallels.html",
    "href": "personal/2010-07-07-irish-parallels.html",
    "title": "Irish Parallels",
    "section": "",
    "text": "I’m finally getting round to finishing a book I blogged about a while back, Talking to Terrorists. In the conclusion, I keep getting struck with a sense of deja vu. No, Afghanistan is not Northern Ireland, nor are the Taliban the IRA. But there’s definitely something to be learned here:\n\n“It was this absence of a long-term strategy which was to be one of the key contributory factors to the sharp increase in violence from 1969 to 1975-6. The rapid oscillation of policy in these years proved particularly damaging: from an ‘ostrich-like’ policy of neglect as the province spiralled towards collapse, to full-blown intervention and ‘Direct Rule’, to negotiations with the IRA in 1972, to an abortive attempt at power-sharing with moderate parties in 1973-4, only to return to more exploratory talks with terrorists in 1975. What characterised this era was the inability of the state to recognise how its own behaviour could exacerbate the situation. The lack of a consistent approach or over-arching vision – not to mention periodic flirtations with the possibility of a complete withdrawal from Northern Ireland – heightened suspicion of British intentions and undermined those moderate voices who were the most likely partners for peace (including the Irish government). […]\n“From the mid-1970s, as violence spiralled out of control, the British government – with some reluctance – came to the decision that it needed to establish a ‘long haul’ commitment to Northern Ireland, in order to end the instability upon which the terrorist campaigns (both loyalist and republican), had thrived. By focusing their energies on ‘normalising’ the security situation and prioritising economic regeneration over constitutional experiments, the British effectively abandoned the hope that they might reach a peaceful settlement in the near future. Yet in taking this new path, they also wrested the initiative away from those violent groups that were prepared to use spectacular attacks to influence political events at important junctures. It was this change of tactics that forced the IRA to adopt its own ‘long war’ strategy – effectively an admission of weakness on the part of the republicans and a marked departure from the ‘one last push’ philosophy which had prevailed in their ranks until that point.” (p.243)"
  },
  {
    "objectID": "personal/2010-06-21-kandahar-timeline-1979-2010.html",
    "href": "personal/2010-06-21-kandahar-timeline-1979-2010.html",
    "title": "Kandahar Timeline 1979-2010",
    "section": "",
    "text": "Many of you have already downloaded and visited my previous post which contained a PDF version of a chronology of events in Kandahar from September 2001 up to the present day. For various other projects in the past (most of all, for work in connection with Mullah Zaeef’s My Life With the Taliban) I have found it useful to put together event data of varying levels of granularity.\nVarious projects made it difficult for me to work on compiling these various chronologies and event lists, but I finally found time to finish it off this week. Accordingly, please visit http://www.alexstrick.com/timeline/ for a more or less complete listing of events that took place in or relating to Kandahar from 1979-2010. Some years are less thoroughly presented than others, but this will change as I incrementally update the timeline over the next few months as I simultaneously go through the final stages of editing (together with Felix Kuehn) Mullah Zaeef’s second and forthcoming book.\nI hope, also, to be able to find time to explain how I put the raw data together and was able to present it in this format. In short, I used an extremely nifty piece of software called Tinderbox (Mac only, apologies…) and was given a lot of help by some people who understand its ins and outs far better than I currently do. So special thanks to Mark Anderson for that, and to Mark Bernstein for writing the software in the first place. I use Tinderbox for almost all of my work these days (data gathering, data sorting, data organisation… the list goes on) and strongly recommend others with high-volume complex data projects to give it a try.\nAnyway, find the timeline here and please don’t hesitate to get in touch with comments/corrections."
  },
  {
    "objectID": "personal/2010-05-14-kandahar-portraits.html",
    "href": "personal/2010-05-14-kandahar-portraits.html",
    "title": "Kandahar Portraits",
    "section": "",
    "text": "I’d been talking about this piece so much over the past 5 months that I almost believed I’d never finish writing it, but anyway, it finally got printed in ‘The National’ newspaper’s weekend supplement.\nREAD IT HERE"
  },
  {
    "objectID": "personal/2010-04-29-kandahar-chronology-september-2001-october-2009.html",
    "href": "personal/2010-04-29-kandahar-chronology-september-2001-october-2009.html",
    "title": "Kandahar Chronology (September 2001-October 2009)",
    "section": "",
    "text": "I compiled this chronology of significant events relating to Kandahar province last year. The primary source for these dates/events was the New York Times’ archive, but then (almost) everything has been double-sourced. Everything from about 2008 onwards was while I was here in Kandahar so that then is my own observations and event listings. Perhaps someone will find it useful and it will save someone somewhere some time.\nHere is the file:\nLINK"
  },
  {
    "objectID": "personal/2010-04-25-newish-kandahar-blogs.html",
    "href": "personal/2010-04-25-newish-kandahar-blogs.html",
    "title": "New(ish) Kandahar Blogs",
    "section": "",
    "text": "Just a short shout-out to three blogs also posting from Kandahar for those who don’t already follow them. Not everything is always interesting, but given the dearth of information they’re worth keeping up with.\nKandahar Diary - PSC Contractor based down in Kandahar, managing operations all over the south it seems.\nKnights of Afghanistan - Observations from an Expat Country Manager for an Afghan PSC, based down in Kandahar\nFree Range International - Most of you will already read these guys, but now they’re increasingly posting from Kandahar rather than just from Nangarhar."
  },
  {
    "objectID": "personal/2010-04-23-kandahars-electricity-problems.html",
    "href": "personal/2010-04-23-kandahars-electricity-problems.html",
    "title": "Kandahar’s Electricity Problems",
    "section": "",
    "text": "I’m with the short-termers on this one:\n\nConvinced that expanding the electricity supply will build popular support for the Afghan government and sap the Taliban’s influence, some officers want to spend $200 million over the next few months to buy more generators and millions of gallons of diesel fuel. Although they acknowledge that the project will be costly and inefficient, they say President Obama’s pledge to begin withdrawing troops by July 2011 has increased pressure to demonstrate rapid results in their counterinsurgency efforts, even if it means embracing less-than-ideal solutions to provide basic public services.\n…\nU.S. diplomats and reconstruction specialists, who do not face the same looming drawdown, have opposed the military’s plan because of concerns that the Afghan government will not be able to afford the fuel to sustain the generators. Mindful of several troubled development programs over the past eight years, they want the United States to focus on initiatives that Afghans can maintain over the long term. (excerpted from The Washington Post)\n\nI wrote about this a few weeks back, suggesting that it would probably be better just to pay for fuel and generators so as to deliver something tangible and real for people in Kandahar City. Martine van Bijlert (one of the co-founders of AAN) just posted a must-read commentary from her recent trip down to Kandahar in which she notes that:\n\nI have returned from Kandahar shaken. Not because of the blasts and the warnings and the feelings of apprehension, but because of how dark the future looks when I listen to what people have to say. I fear that all the shiny plans will do very little to change that.\n\nElectricity would, at the very least, be something that the government and foreigners could point to as having improved – only, that is, if it can be maintained past just a few months. The last two times we had regular and reliable electricity – just after Governor Torialai Weesa was appointed to the post for a month or so, and in the run-up to the Presidential and Provincial Council Elections – nobody benefited from the provision of the service because (a) there was very little follow-up in terms of publicising and trying to advertise and remind people it was there and (b) because it soon stopped and people went back to moaning about how useless the government and foreigners are."
  },
  {
    "objectID": "personal/2010-04-18-kandahar-survey.html",
    "href": "personal/2010-04-18-kandahar-survey.html",
    "title": "Kandahar Survey",
    "section": "",
    "text": "This is a pretty useful survey to read through. I have my usual concerns about how it was conducted, who they spoke to, who did the interviews, where people were interviewed, how they managed to get through all these long lists of questions etc etc, but there are some general trends here which reflect things said by people I speak to.\nThe conclusion presents a bleak picture:\n\nThis survey’s findings indicate endemic corruption, along with a lack of security and basic services, in Kandahar Province. Collectively, this sets conditions for a disenfranchised population to respond either by not supporting the government due to its inability to deliver improvements in the quality of life or, worse yet, by supporting the Taliban.\n\nWe should keep in mind, though, that this survey was carried out between December 23-29, 2009, a period that – compared to now – was and felt much safer. The exponential increase of insecurity, particularly in Kandahar City, since then would surely give more pause for thought. Next time they’re doing these surveys I’d be interested to see some data collected on whether people are sending family members outside the province in anticipation of the coming summer; I’ve heard mountains of anecdotal evidence that this is the case, but something concrete would be useful to confirm this.\n[h/t to Nathan Hodge at Wired’s Danger Room Blog for distributing this survey online]"
  },
  {
    "objectID": "personal/2010-04-12-overheardinkandahar.html",
    "href": "personal/2010-04-12-overheardinkandahar.html",
    "title": "#overheardinkandahar",
    "section": "",
    "text": "“The storm is coming. Believe you me. The storm is coming. I try telling people, but it seems they’re all just making themselves busy with fixing the leaky roof or the squeaky door. The storm will destroy their entire house and city, though. The storm is coming. You have two options: get out now, or climb down into your bunker and hope that the storm will pass and that you’re still alive six months from now. The storm is coming.” (Businessman in Kandahar City)."
  },
  {
    "objectID": "personal/2010-04-12-civilian-casualties-from-zheray.html",
    "href": "personal/2010-04-12-civilian-casualties-from-zheray.html",
    "title": "Civilian Casualties from Zheray",
    "section": "",
    "text": "Heard this morning about a bus travelling on the Herat-Kandahar road which was shot up by NATO troops. Above you can hear one of the victims explaining what happened. He seems to be the only one in the Mirwais Hospital in Kandahar City; the rest were taken to the main base at Kandahar Airfield (KAF).\nA couple of pictures follow:"
  },
  {
    "objectID": "personal/2010-02-08-real-people-real-war.html",
    "href": "personal/2010-02-08-real-people-real-war.html",
    "title": "Real People, Real War",
    "section": "",
    "text": "Newspapers, politicians and the military on both sides of the Atlantic are salivating at the prospect of a great clash in Marjah (Helmand) in the coming days – “the most dangerous areas of central Helmand in a series of daring raids — the biggest since the first Gulf war” (Sunday Telegraph, UK) – and you’d be forgiven for forgetting the human cost. Good thing that we have Holly Pickett’s latest blog post to remind us that war affects real people with real lives – strategise all you like, but remember all of this is about people in the end.  Holly’s photos are hard to look at.  That’s the point.\nGo there.  Look at them.  Think.  Then act."
  },
  {
    "objectID": "personal/2010-02-02-john-nagl-and-the-future-of-counterinsurgency.html",
    "href": "personal/2010-02-02-john-nagl-and-the-future-of-counterinsurgency.html",
    "title": "John Nagl and ‘the future of counterinsurgency’",
    "section": "",
    "text": "I just got back from an incredibly depressing lecture by John Nagl at King’s College London entitled “Afghanistan and its lessons for the future of conflict.” Unashamedly addressing the problem from the perspective of the US army, Nagl took us through his conception of counterinsurgency warfare, how the US – in his analysis – have responded and learnt from mistakes made in the past, and what this might mean for Afghanistan at the moment and the wars of the future. There were quite a few points and broad themes where we were in complete agreement: the absolute importance of the information or ‘strategic communications’ element in Afghanistan to any success that might manifest itself, or in terms of any buy-in from Afghans; we agree on the importance of history (“history doesn’t repeat itself, but it rhymes”) and on the need for careful, diligent study in order to prevent repeating the mistakes of the past.\nWe differ, though, primarily on the different basis of our professional and personal experience. John Nagl served many years in the US Army, taking part in Operation Desert Storm in 1991 as well as Operation Enduring Freedom post-2001. He is concerned with the institution that he knows best (the US military), the people who form its staff and worried about its ability to adapt to change from within. These are all valuable pursuits, but it’s a very different world to the one that I inhabit, sharing in the ordinary problems and insecurity that Afghan friends face on a daily basis – with the caveat, of course, that I have a foreign passport and can leave at any point that I choose.\nAlmost entirely absent from tonight’s presentation was the Afghan narrative – the ordinary experiences of people who have to exist at the sharp end of the spear. I’m not even talking about the ‘counter-narrative’ which we’re starting to see more of from the policy community – specifically the kind of thing that Mullah Zaeef’s book seeks to encourage, and that recent talk of negotiations will only promote further (at least in name).\nTo that end, I am incredibly worried about his seemingly wholehearted endorsement of ‘community defence initiatives’. I don’t think I need to go into the reasons why creating and funding tribal militias in southern Afghanistan is to open Pandora’s Box – others have written about it – but the US military’s continued involvement with this idea (with what amounts, by now, to wilful ignorance of the very loud counter-discourse) indicates, to my mind and from where I’m sitting, an emphasis on short-term fixes over long-term strategy and consistent communication of those goals.\nThere’s a whole literature now from scholars, military practitioners, and also from within the US establishment, on how and why the fostering of these tribal or local defence groups is a bad idea, and the only thing to explain it is a reliance on something I like to call ‘hope tactics’. About half a year ago, I received an email from a American soldier about to deploy to Nuristan. He’d read a post I’d written together with Felix on tribal militias and wanted to know more about why I thought it wouldn’t work. In the end we had to agree to disagree, but he had these words in final response:\n\nIt’s not that militias are good or bad for Afghans - rather which militias, in which geographical/political setting, with what mission, under whose supervision/ownership, for what purpose, and with what training. In my view - seconded by quite a few Afghans I have interviewed - a locally sourced, tribally/communally managed, non-militarized, properly trained over the long-run, arbakai force may be the preferable solution in some areas of Afghanistan.\n\n…which is all fine and well, except just to go ahead anyway in the hope that you’ll be the one who can make it work (even if we forget that people are never deployed long enough to see this kind of thing through to the conclusion and in the kind of detail and perspective that an incredibly important decision like this should entail) is just wishful thinking.\nThere was also a lot of talk of ‘enemies’ tonight. Obviously there is a dialectic at the core of counterinsurgency studies – the insurgent vs the counterinsurgent – but to my mind this needs to be complicated by the on-the-ground reality that there are no such clear lines dividing government, people, Taliban and all the myriad of other ‘groups’, particularly in somewhere like southern Afghanistan. While the Q&A session afterwards had him admit more of this detail and ‘messiness’, this didn’t come across in the quite confident presentation that preceded.\nFinally, the most worrying of all was his suggestion that, for the future, maybe “the military needs to become more like the State Department, and the State Department need to become more like the military.” One of the biggest problems – in my analysis – that we suffer from in southern Afghanistan is western political establishments’ almost complete reliance on the military to form policy in the absence of their own more creative and useful alternatives. We see this with the United States in particular, but also in the United Kingdom. What we most certainly DON’T need, is a further creep of political power into the hands of the military who, we must remember, only come with a limited toolbox and set of resources to respond to different kinds of problems, notwithstanding Professor Nagl’s hopes to the contrary."
  },
  {
    "objectID": "personal/2010-01-13-presenting-mullah-zaeef.html",
    "href": "personal/2010-01-13-presenting-mullah-zaeef.html",
    "title": "Presenting Mullah Zaeef",
    "section": "",
    "text": "Felix and I are busy putting together presentations for the UK and USA at the moment. In case any of you are in either of those countries, please see the list of presentations below. I’ll try to keep it updated, but in any case the most up-to-date list will always be on the book’s website itself – here.\nUNITED KINGDOM\nJanuary 21st, 2010 – Talk – School of Oriental and African Studies (SOAS)\n10 Thornhaugh Street, London, WC1H 0XG – 5.30-7pm.\nhttp://www.soas.ac.uk/events/event55661.html\nFebruary 1st, 2010 – Talk – International Institute for Strategic Studies (IISS)\n13–15 Arundel Street, Temple Place, London WC2R 3DX – 12.30-1.30pm.\nhttp://www.iiss.org\nFebruary 3rd, 2010 – Talk – London School of Economics (LSE)\nRoom U8, Tower 1, Clement’s Inn, London WC2A 2AD – 12.30-2.00pm.\nhttp://www2.lse.ac.uk/mapsAndDirections/findingYourWayAroundLSE.aspx\nFebruary 5th, 2010 – Talk – Chatham House\n10 St James’s Square, London SW1Y 4LE – 1.30-2.30pm.\nhttp://www.chathamhouse.org.uk/\nFebruary 9th, 2010 – Book Launch – Frontline Club\n13 Norfolk Place, London W2 1QJ – 7-9pm\nhttp://www.frontlineclub.com/\nUNITED STATES OF AMERICA\nFebruary 18th, 2010 – Discussion Panel - “Talking with the Taliban” – New York University (NYU)\nManhattan, New York, NY 10011 – 6.30-8.30pm\nhttp://journalism.nyu.edu/events/index.html?ev=20100218-taliban\nFebruary 26th, 2010 – Talk – Brookings Institution\n1775 Massachusetts Avenue NW, Washington DC 20036 – 2.30-3.30pm\nhttp://www.brookings.edu\nMarch 2nd, 2010 – Talk – Middle East Institute (MEI)\n1761 N Street Northwest, Washington, DC 20036-2882 – 12-1pm\nhttp://www.mei.edu/\nMarch 11th, 2010 – Talk – Carr Center, Harvard University\nJohn F. Kennedy School of Government, 79 JFK Street, Cambridge MA 02138 – 4-6pm\nhttp://www.hks.harvard.edu/cchrp/index.php\nMarch 11th, 2010 – Talk & Signing – The COOP Bookstore, Harvard\n1400 Massachusetts Avenue, Cambridge MA 02138 – 7-8.30pm\nhttp://harvardcoopbooks.bncollege.com"
  },
  {
    "objectID": "personal/2010-01-01-new-year-new-website.html",
    "href": "personal/2010-01-01-new-year-new-website.html",
    "title": "New Year, New Website",
    "section": "",
    "text": "I’ve been silently feeling quite guilty over the past months for not posting more on my blog at the Frontline Club. This was a manifestation of my own busyness-cum-lazyness in combination with the very slow upload/back-end to the Frontline Club site. So I’ve set myself up here with a simple wordpress blog. For those of you who care, I’m using Ecto to post my entries (or the iPhone posting app), all on my dear and much loved MacBook Pro.\nI’ve resolved to post more this year. I am frequently frustrated with articles in the media about Afghanistan (and other areas that I write on), so I’ll respond to some of those. I’ll post film and book reviews, not because you necessarily care, but because it helps me keep track of the things I’m reading.\nFrom time to time I’ll post on the software (and occasionally hardware) that I use almost every day to process and store information – I’m a researcher/writer by profession – as it’s taken me a long time to come to terms with all the streams of writing that lay a claim to my attention, from RSS feeds, Facebook and Twitter to reports, books and emails.\nIn a few weeks I’ll be posting from the road: Mullah Zaeef’s My Life With The Taliban is finally going on sale (pre-order at Amazon.com here and at Amazon.co.uk here). Felix and I will be travelling round the UK and USA in January, February and March. You can find a list of the public events we’ll be participating in on the book website."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned (TIL)",
    "section": "",
    "text": "Short-form notes on things I’ve learned while working on ML engineering, software, and related topics.\n\n\n\n\n\n\n\n\n\nWhat is the Rust prelude?\n\n\n\nrust\n\nlearning\n\nTIL\n\n\n\nA quick post on what the ‘prelude’ is and why it exists.\n\n\n\n\n\n2024-09-16\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "personal.html",
    "href": "personal.html",
    "title": "Personal Blog",
    "section": "",
    "text": "Writings on Afghanistan, books, languages, travel, and life.\nFor technical content on MLOps, machine learning, and software engineering, see the technical blog.\n\n\n\n\n\n\n\n\n\n\n\nChoice Density: A Better Way to Think About AI and Authenticity\n\n\n\nwriting\n\ntech\n\ngeneral\n\n\n\nReframing the ‘human vs AI’ debate: what matters isn’t whether AI was involved, but how many choices you made along the way. I call this ‘choice density’ - and it applies to…\n\n\n\nJan 21, 2026\n\n\n\n\n\n\n\n\n\n\nFirst stitches: on learning to knit\n\n\n\ncrafting\n\nlearning\n\ncrafts\n\nskills\n\nlessonslearnt\n\nknitting\n\n\n\nStarting my knitting journey: reflections on early practice swatches from the TKGA correspondence course, including garter stitch, stockinette, and rib patterns.\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\nLanguage Learning Crash Course: from slightly more than zero to slightly less than advanced\n\n\n\nlanguage\n\nstudy\n\nlearning\n\neducation\n\nlanguages\n\nlanguagecoaching\n\n\n\nTips on learning a language efficiently, from someone who started with almost nothing to reach conversational ability.\n\n\n\nAug 10, 2023\n\n\n\n\n\n\n\n\n\n\nAll the things I wish I knew about studying at school\n\n\n\nproductivity\n\nmemory\n\nstudy\n\nskills\n\nmathematics\n\nmemorisation\n\neducation\n\n\n\nReflections on studying effectively at school, with advice I wish I’d known earlier—covering how to learn for understanding, study strategies for exams, and specific tips…\n\n\n\nAug 6, 2023\n\n\n\n\n\n\n\n\n\n\nAutomating social media posting for my new blogposts\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\nautomation\n\nsocial-media\n\ntechnology\n\n\n\nHow I set up automation to post my blog updates to social media using Zapier, Buffer, and ChatGPT.\n\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\nVermeer at the Rijksmuseum\n\n\n\nart\n\nreviews\n\nnetherlands\n\n\n\nI visited the Vermeer exhibition at the Rijksmuseum in Amsterdam and was struck by how seeing his works together—especially the small Delft landscapes—created a different…\n\n\n\nFeb 19, 2023\n\n\n\n\n\n\n\n\n\n\n2022 Readings\n\n\n\nbooks\n\nreading\n\n\n\nMy reading year in review: 75 books that ranged from gems to middling, with highlights including Hossenfelder on physics and beauty, Ameisen on ML in practice, Delany’s…\n\n\n\nDec 27, 2022\n\n\n\n\n\n\n\n\n\n\nOn the interpretability of models\n\n\n\nscience\n\ntech\n\ndeep-learning\n\ndata\n\ntechnology\n\ndeeplearning\n\n\n\nSome reflections on the “black box” problem in deep learning and why neural networks can be difficult to interpret.\n\n\n\nMay 28, 2021\n\n\n\n\n\n\n\n\n\n\nFastAI Lesson Zero: video notes\n\n\n\nproductivity\n\ndeep-learning\n\nstudy\n\nfastai\n\ndeeplearning\n\n\n\nMy notes from Jeremy Howard’s FastAI Lesson 0 video on how to approach learning deep learning effectively.\n\n\n\nMay 27, 2021\n\n\n\n\n\n\n\n\n\n\nGetting Out of the Intermediate Language Plateau: Arabic Edition / Principles\n\n\n\nlanguage\n\nuseful-tools\n\nstudy\n\nlearning\n\nskills\n\nlanguages\n\narabicintermediateplateau\n\narabic\n\n\n\nPrinciples for breaking through an intermediate plateau in Arabic language learning, drawn from skill acquisition research and personal experience.\n\n\n\nMay 26, 2021\n\n\n\n\n\n\n\n\n\n\nArthur Samuel and the ‘Frontier of Automation’\n\n\n\nscience\n\ndeep-learning\n\nstatistics\n\nmachinelearning\n\ntechnology\n\ndeeplearning\n\n\n\nHow machine learning as we know it today traces back to Arthur Samuel’s 1962 essay on automating computers to learn from their experience.\n\n\n\nMay 26, 2021\n\n\n\n\n\n\n\n\n\n\nTelling Cats from Dogs\n\n\n\ndeep-learning\n\nsoftware\n\ntechnology\n\ndeeplearning\n\n\n\nHow neural networks learn to classify images differently than traditional programming, illustrated with a cat vs. dog classification example.\n\n\n\nMay 26, 2021\n\n\n\n\n\n\n\n\n\n\nDeep Learning: Best in Show?\n\n\n\ncoding\n\ntech\n\nuseful-tools\n\ndeep-learning\n\ntechnology\n\ndeeplearning\n\n\n\nSpecific areas where deep learning has already surpassed human-level performance, from translation and medical imaging to protein folding and game-playing.\n\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nHeld back by misunderstanding\n\n\n\ntech\n\ndeep-learning\n\nacademia\n\ntechnology\n\ndeeplearning\n\n\n\nHow theoretical misunderstandings about neural networks held back the field of deep learning for decades.\n\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nPDP: a precursor to modern neural networks?\n\n\n\ntech\n\ndeep-learning\n\nhistory\n\ntechnology\n\ndeeplearning\n\n\n\nLooking back at the foundational PDP work from 1968 and how its eight core principles anticipate what we now call deep learning.\n\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nRemoving Barriers: Deep Learning Edition\n\n\n\ntech\n\nuseful-tools\n\ndeep-learning\n\nsoftware\n\neducation\n\ntechnology\n\ndeeplearning\n\n\n\nChallenging common myths about what you need to get started with deep learning, and why domain expertise matters more than you’d think.\n\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nRosenblatt’s Mark I Perceptron\n\n\n\ntech\n\ncoding\n\nuseful-tools\n\ndeep-learning\n\ntechnology\n\ndeeplearning\n\n\n\nI learned about Rosenblatt’s Mark I Perceptron, the 1958 machine that pioneered artificial neural networks and how Minsky and Papert’s critique inadvertently triggered the…\n\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nSmall, unexpectedly powerful boxes\n\n\n\ntech\n\nuseful-tools\n\ndeep-learning\n\ntechnology\n\ndeeplearning\n\n\n\nWhy GPUs are the small, powerful boxes powering everything from your screen to deep learning models—and why they’ve been impossible to find.\n\n\n\nMay 23, 2021\n\n\n\n\n\n\n\n\n\n\nA basic overview of how to use Handlebars\n\n\n\ncoding\n\nuseful-tools\n\njavascript\n\nweb\n\ntechnology\n\nlaunchschool\n\n\n\nA basic overview of the Handlebars templating language and how to use it with JavaScript to separate HTML from your code.\n\n\n\nDec 10, 2020\n\n\n\n\n\n\n\n\n\n\nHow to use jQuery and Handlebars in your website\n\n\n\ncoding\n\nuseful-tools\n\nweb\n\nlaunchschool\n\njavascript\n\n\n\nA quick guide to including jQuery and Handlebars libraries in your HTML with script tags.\n\n\n\nDec 10, 2020\n\n\n\n\n\n\n\n\n\n\nUsing APIs to make things happen on the web\n\n\n\ncoding\n\njavascript\n\ninternet\n\nweb\n\ntechnology\n\nlaunchschool\n\n\n\nHow API calls unlock creative possibilities on the web, and what I learned about the simple text behind so much of our online interaction.\n\n\n\nDec 4, 2020\n\n\n\n\n\n\n\n\n\n\nHow events drive programming for the web\n\n\n\ncoding\n\njavascript\n\ninternet\n\nprivacy\n\nlaunchschool\n\n\n\nHow I learned to use events and the DOM to make web pages interactive without full page reloads, and some thoughts on the privacy implications of JavaScript event tracking.\n\n\n\nDec 3, 2020\n\n\n\n\n\n\n\n\n\n\nThe Four Positions for Inserting Elements into the DOM\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\nHow JavaScript’s four position arguments work for inserting elements into the DOM.\n\n\n\nDec 2, 2020\n\n\n\n\n\n\n\n\n\n\nDifferent ways of accessing the text contents of DOM nodes in JavaScript\n\n\n\ncoding\n\nweb\n\nlaunchschool\n\njavascript\n\n\n\nI spent some time unpacking the different properties available on DOM nodes for accessing their text contents, and here’s what I learned about how .textContent, .data…\n\n\n\nNov 14, 2020\n\n\n\n\n\n\n\n\n\n\nUsing CSS selectors with JavaScript DOM methods\n\n\n\ncoding\n\njavascript\n\nweb\n\ntechnology\n\nlaunchschool\n\n\n\nHow I learned to use CSS selectors with JavaScript DOM methods like querySelector and querySelectorAll.\n\n\n\nNov 12, 2020\n\n\n\n\n\n\n\n\n\n\nTurning array-like objects into arrays with JavaScript\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\nHow I figured out how to convert array-like objects into actual arrays using Array.prototype.slice.call().\n\n\n\nNov 10, 2020\n\n\n\n\n\n\n\n\n\n\nUnderstanding the use cases for Closures in JavaScript\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\nHow I came to understand why closures are actually useful in JavaScript.\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\nWhat is Lexical Scope?\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\nUnderstanding lexical scope and how nested functions access variables in JavaScript.\n\n\n\nOct 13, 2020\n\n\n\n\n\n\n\n\n\n\nWorking with JavaScript’s Object Prototype model\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\nWorking through JavaScript’s prototype-based object model and exploring OLOO patterns for encapsulation and private data.\n\n\n\nOct 12, 2020\n\n\n\n\n\n\n\n\n\n\nSholay\n\n\n\nreviews\n\nbollywood\n\nfilm\n\n\n\nMy thoughts on Sholay after finally watching this classic Indian film—what surprised me about the plot, the songs, and how it holds up today.\n\n\n\nJun 14, 2020\n\n\n\n\n\n\n\n\n\n\nDil Dhadakne Do\n\n\n\nreviews\n\nbollywood\n\nfilm\n\n\n\nMy thoughts on Dil Dhadakne Do—a Bollywood film about a family cruise that aims for drama and romance but lands somewhere between Titanic and Shakespeare.\n\n\n\nJun 11, 2020\n\n\n\n\n\n\n\n\n\n\nKapoor & Sons\n\n\n\nreviews\n\nbollywood\n\nfilm\n\n\n\nMy thoughts on Kapoor & Sons, a family drama that captured an unfamiliar corner of India and delivered despite its narrative shortcomings.\n\n\n\nJun 11, 2020\n\n\n\n\n\n\n\n\n\n\nTwo Ruby patterns around map and reduce\n\n\n\ncoding\n\nruby\n\nlaunchschool\n\n\n\nTwo fundamental patterns for transforming data in Ruby: using map to convert arrays into transformed arrays, and using reduce to combine multiple values into a single result.\n\n\n\nFeb 12, 2020\n\n\n\n\n\n\n\n\n\n\nHow the Internet Works\n\n\n\ntech\n\ncoding\n\ntechnology\n\nlaunchschool\n\ninternet\n\n\n\nDeep dive into the layers and protocols that make the internet function, from physical infrastructure to the application level.\n\n\n\nJan 29, 2020\n\n\n\n\n\n\n\n\n\n\nNew book, new ways to order\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nbooks\n\npublishing\n\ntaliban\n\n\n\nI announced a new book by Abdul Hai Mutma’in, a Taliban insider and former political advisor to Mullah Omar, and explained why firsthand accounts like this matter for…\n\n\n\nSep 10, 2019\n\n\n\n\n\n\n\n\n\n\nMastery-based Learning with Launch School\n\n\n\npodcast\n\ntech\n\ncoding\n\nstudy\n\nlearning\n\neducation\n\ntechnology\n\n\n\nA podcast episode where I spoke with Chris Lee from Launch School about mastery-based learning and the tradeoffs involved in designing an effective curriculum.\n\n\n\nMar 21, 2019\n\n\n\n\n\n\n\n\n\n\nSources and Methods Does Technology\n\n\n\npodcast\n\ntech\n\ncoding\n\n\n\nI spoke with the creators of RightLobeMath about their online abacus teaching program, and shared what I’ve learned from using their course over the past few months.\n\n\n\nMar 12, 2019\n\n\n\n\n\n\n\n\n\n\nUsing Ruby’s .digits method\n\n\n\ncoding\n\nruby\n\nlaunchschool\n\n\n\nI discovered Ruby’s .digits method and explored when it’s actually worth using over simpler alternatives like .to_s.chars.\n\n\n\nFeb 25, 2019\n\n\n\n\n\n\n\n\n\n\nSolid Study Habits for Coders\n\n\n\nproductivity\n\ncoding\n\nstudy\n\nruby\n\nanki\n\nlaunchschool\n\n\n\nReflections on study habits I’ve developed while working through a coding bootcamp curriculum, with emphasis on core principles like mastering fundamentals, focusing on…\n\n\n\nFeb 16, 2019\n\n\n\n\n\n\n\n\n\n\nUsing Ruby’s .zip method to combine arrays\n\n\n\ncoding\n\nruby\n\nlaunchschool\n\n\n\nHow I used Ruby’s .zip method to combine two arrays into paired elements.\n\n\n\nFeb 13, 2019\n\n\n\n\n\n\n\n\n\n\nPain: A Love Story\n\n\n\ngeneral\n\nmovement\n\nbody\n\npain\n\nhealth\n\n\n\nMy experience living with chronic pain for the past five years, and what I’ve learned about living in uncertainty without clear answers.\n\n\n\nJan 31, 2019\n\n\n\n\n\n\n\n\n\n\nRaspberryLPIC: A New Series & Setup Steps\n\n\n\nraspberrylpic\n\ntech\n\ncoding\n\nraspberrypi\n\nlpic\n\nlinux\n\ntechnology\n\n\n\nStarting a new series working through the LPIC-1 exam using a Raspberry Pi as my learning environment, and the setup steps I took to get it running.\n\n\n\nDec 31, 2018\n\n\n\n\n\n\n\n\n\n\nEarning a certificate in Linux Essentials from LPI\n\n\n\ntech\n\nlinux\n\ntechnology\n\n\n\nI earned the LPI Linux Essentials certification and here’s what I learned from studying for and taking the exam.\n\n\n\nDec 2, 2018\n\n\n\n\n\n\n\n\n\n\nIn Afghanistan, watching someone learn to code for the first time\n\n\n\nafghanistan\n\ntech\n\ncoding\n\nlearning\n\n\n\nReflections on watching someone start learning to code in Kabul, and the challenges of self-teaching computer science in Afghanistan.\n\n\n\nNov 13, 2018\n\n\n\n\n\n\n\n\n\n\nLearn Persian / Farsi / Dari with Podcasts\n\n\n\nlanguage\n\nfarsi\n\npodcasts\n\nlanguages\n\npersian\n\ndari\n\n\n\nA guide to learning Persian, Farsi, and Dari through podcasts, with recommendations across different difficulty levels and topic areas.\n\n\n\nNov 7, 2018\n\n\n\n\n\n\n\n\n\n\nLearn Pashto with Podcasts\n\n\n\nafghanistan\n\nlanguage\n\npashto\n\npodcasts\n\nlanguages\n\n\n\nI returned to studying Pashto after a few years away and compiled a list of podcasts for language learners, since options are limited despite millions of Pashto speakers…\n\n\n\nNov 5, 2018\n\n\n\n\n\n\n\n\n\n\nUsing Regex with Python to Find Strings\n\n\n\nproductivity\n\ncoding\n\nuseful-tools\n\npython\n\nregex\n\n\n\nI worked through regex pattern matching in Python to find strings in my PDF data processing project, and documented the basic syntax for future reference.\n\n\n\nJul 8, 2018\n\n\n\n\n\n\n\n\n\n\nPython Virtual Environments, Testing Environments and Markdown Strikethrough\n\n\n\ncoding\n\npython\n\n\n\nSetting up my Python project with virtual environments, testing considerations, and project structure decisions.\n\n\n\nJun 24, 2018\n\n\n\n\n\n\n\n\n\n\nReal-World Go with Referenced Functions\n\n\n\ncoding\n\ngolang\n\n\n\nHow I figured out structuring real-world Go projects with functions split across multiple files.\n\n\n\nJun 23, 2018\n\n\n\n\n\n\n\n\n\n\nSplitting PDFs with Python\n\n\nI built a Python script to split multi-page PDFs into individual single-page files, using PyPDF2 and exploring different approaches to iterate over files in a directory.\n\n\n\nJun 23, 2018\n\n\n\n\n\n\n\n\n\n\nDiagnosing Diabetes with Weka & Machine Learning\n\n\n\ntech\n\ncoding\n\nscience\n\nweka\n\ndata\n\nmachinelearning\n\n\n\nI built a diabetes diagnosis classifier using the Pima Indians dataset and Weka, working through a practical machine learning workflow on a classic binary classification…\n\n\n\nJun 18, 2018\n\n\n\n\n\n\n\n\n\n\nTable Tests in Go\n\n\n\ncoding\n\ntesting\n\ngolang\n\n\n\nHow I learned to use table tests in Go to test multiple scenarios against the same function.\n\n\n\nJun 18, 2018\n\n\n\n\n\n\n\n\n\n\nFiguring out the Go testing ecosystem\n\n\n\ncoding\n\ntesting\n\ngolang\n\n\n\nI worked through Go’s testing tools, learning how to write benchmarks and examples while exploring what works and what doesn’t in the testing ecosystem.\n\n\n\nJun 17, 2018\n\n\n\n\n\n\n\n\n\n\nFrom AI to to Brahms\n\n\n\nart\n\ntech\n\ncoding\n\nmusic\n\nmachinelearning\n\ntechnology\n\n\n\nReflections on attending PyLondinium and CogX conferences, and how an evening of classical music provided perspective on what humans uniquely offer.\n\n\n\nJun 13, 2018\n\n\n\n\n\n\n\n\n\n\nMy new book: The Taliban Reader\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nbooks\n\nproductivity\n\ntaliban\n\ntalibansourcesproject\n\nwriting\n\n\n\nI published The Taliban Reader, a comprehensive book that brings Taliban studies back to primary sources.\n\n\n\nJun 5, 2018\n\n\n\n\n\n\n\n\n\n\nMachine Learning with Weka\n\n\n\ncoding\n\nuseful-tools\n\nweka\n\ngolang\n\nmachinelearning\n\nstatistics\n\n\n\nGetting started with machine learning using Weka’s graphical interface, and working through the Titanic dataset from Kaggle to build a foundation in ML workflows before…\n\n\n\nJun 4, 2018\n\n\n\n\n\n\n\n\n\n\nMy First Arabic Book Translation\n\n\n\nbooks\n\narabic\n\nguantánamo\n\ntranslation\n\nsamielhajbook\n\n\n\nI translated Sami Al-Hajj’s memoirs from Arabic to English—my first major translation work from Arabic, and a project that turned out to be both more challenging and more…\n\n\n\nMay 6, 2018\n\n\n\n\n\n\n\n\n\n\nFuzzy Searching and Foreign Name Recognition\n\n\n\nafghanistan\n\ncoding\n\npython\n\nresearch\n\n\n\nHow I tackled fuzzy searching and name matching across multiple spellings, particularly for transliterated Afghan names, using approaches like Levenshtein distance and the…\n\n\n\nJan 31, 2018\n\n\n\n\n\n\n\n\n\n\nTweeting to the Void\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\nsocial-media\n\ntools\n\ntechnology\n\n\n\nHow I used a Chrome extension to stop mindlessly scrolling Twitter and reclaim some time.\n\n\n\nJan 25, 2018\n\n\n\n\n\n\n\n\n\n\nInstalling PostgreSQL on a Mac\n\n\n\ncoding\n\nuseful-tools\n\npython\n\nsql\n\ndatabase\n\n\n\nA straightforward guide to installing PostgreSQL on Mac using Postgres.app and setting up psycopg2 for Python integration.\n\n\n\nJan 24, 2018\n\n\n\n\n\n\n\n\n\n\nMaking and shuffling lists in Python\n\n\n\ncoding\n\npython\n\nlists\n\nnumpy\n\n\n\nQuick reference for useful Numpy functions I discovered for creating and shuffling lists in Python.\n\n\n\nJan 23, 2018\n\n\n\n\n\n\n\n\n\n\nTabula for extracting table data from PDFs\n\n\n\ncoding\n\nuseful-tools\n\ntech\n\nafghanistan\n\nproductivity\n\ndata\n\nsoftware\n\ntechnology\n\n\n\nI discovered Tabula, a tool that extracts table data from PDFs and exports it to formats like CSV and JSON.\n\n\n\nJan 17, 2018\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Homestay Edition\n\n\n\narabic\n\nlanguage\n\njordan\n\nlanguagelearner'sjournal\n\namman\n\ntaylorjournal\n\nlanguagecoaching\n\n\n\nHow I decided to do a homestay in Amman to push my conversational Arabic skills to the next level.\n\n\n\nOct 20, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Phone Calls and Timed Breaks\n\n\n\narabic\n\nlanguage\n\nlanguagelearner'sjournal\n\njordan\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nHow taking extended breaks and daily small pauses improved my Arabic learning, and what I gained from spontaneous language practice during time off.\n\n\n\nAug 13, 2017\n\n\n\n\n\n\n\n\n\n\nSpaced Repetition Without Computers\n\n\n\nuseful-tools\n\nmemorisation\n\nsrsfoundation\n\nspaced-repetition\n\nanki\n\n\n\nHow to use spaced repetition techniques without needing digital tools or apps.\n\n\n\nJun 27, 2017\n\n\n\n\n\n\n\n\n\n\nThe book you need to read on bin Laden’s life post-2001\n\n\n\nbooks\n\njournalism\n\npakistan\n\n9/11\n\nhistory\n\nreview\n\n\n\nMy review of The Exile, a book that finally gives us an intimate look at bin Laden’s life in hiding through accounts from his wives and family members.\n\n\n\nJun 26, 2017\n\n\n\n\n\n\n\n\n\n\nItalian Comprehension: Putting Myself On the Hook\n\n\n\nlanguage\n\nmemory\n\nstudy\n\nlearning\n\nvocabulary\n\nitalianchallenge\n\nlanguages\n\n\n\nHow I’m learning to read Italian in two months using Clozemaster and public accountability.\n\n\n\nJun 22, 2017\n\n\n\n\n\n\n\n\n\n\nRobert Caro’s Big Long Book\n\n\n\nbooks\n\nurban-planning\n\nreview\n\n\n\nWhat I learned from finally finishing Robert Caro’s million-word biography of Robert Moses.\n\n\n\nJun 19, 2017\n\n\n\n\n\n\n\n\n\n\nHow We Forget\n\n\n\nbooks\n\nreview\n\n\n\nReflections on Emma Jane Kirby’s The Optician of Lampedusa, and how easily we forget the human tragedy unfolding at Europe’s borders.\n\n\n\nJun 4, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Meaningful Leisure\n\n\n\njordan\n\nincremental-elephant\n\narabic\n\nlanguagelearner'sjournal\n\nlanguage\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nReflections on shifting to a third phase of Arabic study focused on making genuine friendships and participating in everyday leisure activities around Amman, rather than…\n\n\n\nMay 24, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Increasing Spoken Fluency\n\n\n\narabic\n\nlanguage\n\nlanguagelearner'sjournal\n\nlanguages\n\nspeaking\n\nlanguagecoaching\n\nreading\n\n\n\nHow I’m working to improve my spoken Arabic fluency through shadowing, reading aloud, and structured practice.\n\n\n\nApr 30, 2017\n\n\n\n\n\n\n\n\n\n\nGuest Post: Can robots be language coaches?\n\n\n\nlanguage\n\nincremental-elephant\n\ncoaching\n\nlanguages\n\ncoachbot\n\nlanguagecoaching\n\n\n\nA guest post exploring whether robots can serve as language coaches, inspired by reflections on learner-centered coaching methods and self-directed learning.\n\n\n\nApr 29, 2017\n\n\n\n\n\n\n\n\n\n\nOn Reading in Arabic: The Evidence\n\n\n\narabic\n\nlanguage\n\nstudy\n\nlearning\n\nlanguages\n\nreading\n\n\n\nWhat ElSaid Badawi’s 1970-1977 study on Arabic reading instruction teaches us about language learning.\n\n\n\nApr 17, 2017\n\n\n\n\n\n\n\n\n\n\nYou need to be reading more to get ahead in Arabic\n\n\n\narabic\n\nlanguage\n\nstudy\n\nlearning\n\nlanguages\n\nlanguagecoaching\n\nreading\n\n\n\nWhy reading is essential for Arabic learners, even if your goal is speaking fluency.\n\n\n\nApr 15, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Independent Study\n\n\n\narabic\n\nlanguage\n\njordan\n\nlanguagelearner'sjournal\n\ncoaching\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nHow I’m approaching independent study in Arabic dialect, from reading unglossed media articles to seeking out immersive experiences beyond the classroom.\n\n\n\nApr 9, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Leaving Qasid\n\n\n\narabic\n\nlanguage\n\njordan\n\nlanguagelearner'sjournal\n\nlearning\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nReflections on finishing two-and-a-half months at Qasid Arabic Institute and deciding to shift focus toward studying ammiya dialect in Amman instead.\n\n\n\nMar 20, 2017\n\n\n\n\n\n\n\n\n\n\nWhy do Arabic fonts appear so small? (and how to fix it)\n\n\n\narabic\n\nlanguage\n\ncoding\n\nlanguages\n\ndesign\n\n\n\nWhy Arabic fonts appear smaller than English text on websites, and what’s causing this design oversight.\n\n\n\nMar 12, 2017\n\n\n\n\n\n\n\n\n\n\nAudio Courses with Language Transfer\n\n\n\nlanguage\n\nlanguages\n\nweb\n\ntools\n\n\n\nA recommendation for Language Transfer, a free audio course that’s great for getting your ear attuned to a new language.\n\n\n\nMar 10, 2017\n\n\n\n\n\n\n\n\n\n\nEverything You Ever Wanted to Know About Guinea Pigs\n\n\n\ngeneral\n\nbibliography\n\npets\n\nreading\n\nanimals\n\n\n\nI read six guinea pig books in a few days and here’s what I learned from each one.\n\n\n\nMar 6, 2017\n\n\n\n\n\n\n\n\n\n\nWalk Around the Block\n\n\n\nproductivity\n\nmovement\n\nwalking\n\nwork\n\n\n\nA simple trick I’ve been using to stay active while working: taking short walks around the block every 45-60 minutes.\n\n\n\nMar 3, 2017\n\n\n\n\n\n\n\n\n\n\nThree Podcast Recommendations\n\n\n\npodcast\n\npodcasts\n\n\n\nThree podcast recommendations for expanding your listening habits: science-fiction and technology, American foreign policy behind the scenes, and nuclear security.\n\n\n\nFeb 26, 2017\n\n\n\n\n\n\n\n\n\n\nCore Language Learning Beliefs\n\n\n\narabic\n\nlanguage\n\nlessonslearnt\n\nlanguages\n\nlearning\n\nlanguagecoaching\n\n\n\nMy core beliefs about language learning that shape how I study and coach others.\n\n\n\nFeb 23, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Keeping Pace\n\n\n\narabic\n\nlanguage\n\nincremental-elephant\n\nlanguagelearner'sjournal\n\nlearning\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nReflections on a week of Arabic study highs and lows, and what preparation and confidence taught me about speaking fluently.\n\n\n\nFeb 18, 2017\n\n\n\n\n\n\n\n\n\n\nHow to learn a language without a teacher\n\n\n\nlanguage\n\nincremental-elephant\n\ncoaching\n\nlanguages\n\nlearning\n\nlanguagecoaching\n\n\n\nWhy teachers aren’t necessary for learning a language, and how to take responsibility for your own learning instead.\n\n\n\nFeb 14, 2017\n\n\n\n\n\n\n\n\n\n\n‘Master Arabic’ is Out Today!\n\n\n\nbooks\n\narabic\n\nlanguage\n\nincremental-elephant\n\npublishing\n\nplateau-ebook\n\nlanguages\n\nmasterarabic\n\nlanguagecoaching\n\n\n\nI’m releasing my book ‘Master Arabic’ today, along with a comprehensive online resource guide featuring over 300 learning resources.\n\n\n\nFeb 13, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Deepening My Studies\n\n\n\narabic\n\nlanguage\n\nlanguagelearner'sjournal\n\njordan\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nHow I’m deepening my Arabic studies through daily speaking practice and immersive classroom work at Qasid.\n\n\n\nFeb 4, 2017\n\n\n\n\n\n\n\n\n\n\nActual Fluency\n\n\n\narabic\n\nlanguage\n\npodcast\n\nlearning\n\nlanguages\n\ninterview\n\nlanguagecoaching\n\n\n\nI was interviewed on the Actual Fluency podcast, discussing topics like learning spoken versus written Arabic, finding materials for languages with limited resources, and…\n\n\n\nFeb 3, 2017\n\n\n\n\n\n\n\n\n\n\nVariations on a Pomodoro\n\n\n\nphd\n\nproductivity\n\npomodoros\n\nwork\n\ntranslation\n\n\n\nHow I adapted the Pomodoro technique for translation work, trading longer focus sessions for shorter cycles that better suit the demands of the task.\n\n\n\nJan 31, 2017\n\n\n\n\n\n\n\n\n\n\nHow to learn a page of verbs\n\n\n\narabic\n\nlanguage\n\nmemory\n\nlanguages\n\nlearning\n\nlanguagecoaching\n\n\n\nMy approach to memorizing a page of Arabic verbs by attacking the task from multiple angles—using pronunciation, context, example sentences, and online resources.\n\n\n\nJan 27, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Adjustments at the Deep End\n\n\n\narabic\n\nlanguage\n\nincremental-elephant\n\ncoaching\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\nMy adjustments to studying Arabic intensively—how I’m managing a heavy workload at language school and strategies like Anki cards and grammar resources that help me actually…\n\n\n\nJan 23, 2017\n\n\n\n\n\n\n\n\n\n\nTrello to Markdown: a Chrome extension\n\n\n\nuseful-tools\n\ndata\n\ntools\n\ndatabase\n\ninternet\n\ntechnology\n\ntrello\n\n\n\nI discovered a Chrome extension that exports Trello boards to Markdown and other formats, solving my need to work with hundreds of notes offline.\n\n\n\nJan 17, 2017\n\n\n\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Introducing Taylor\n\n\n\narabic\n\nlanguage\n\ncoaching\n\nincremental-elephant\n\ntaylorjournal\n\nlanguagecoaching\n\n\n\nA freelance journalist documents her journey learning Arabic at an intensive institute in Amman, starting from near-zero and navigating the steep learning curve alongside…\n\n\n\nJan 16, 2017\n\n\n\n\n\n\n\n\n\n\nClozeMaster: learn words in context\n\n\n\nlanguage\n\ntech\n\nmemory\n\nuseful-tools\n\nstudy\n\nlanguages\n\nlearning\n\ntools\n\n\n\nExploring Clozemaster, a language learning tool that teaches vocabulary through cloze tests based on real sentences from Tatoeba, and why learning words in context is more…\n\n\n\nJan 15, 2017\n\n\n\n\n\n\n\n\n\n\nMaster Arabic: Behind the Cover\n\n\n\nart\n\narabic\n\nmasterarabic\n\n\n\nBehind the cover of my new Arabic book—a painting by my mother created after her stroke, and why its story of perseverance resonates with language learning.\n\n\n\nJan 15, 2017\n\n\n\n\n\n\n\n\n\n\nDeveloping for Android with Udacity\n\n\n\ntech\n\ncoding\n\nandroid\n\npythonsideproject\n\nudacity\n\ncoachbot\n\n\n\nI was awarded a Udacity scholarship to learn Android development and explore building an offline version of my CoachBot language-learning tool.\n\n\n\nJan 13, 2017\n\n\n\n\n\n\n\n\n\n\nMaster Arabic: What’s in the Premium Edition?\n\n\n\nbooks\n\narabic\n\nlanguage\n\nincremental-elephant\n\nmasterarabic\n\n\n\nWhat’s included in the premium edition of my Master Arabic book—from expert interviews and discount codes to resource lists, cheat sheets, and access to an exclusive…\n\n\n\nJan 12, 2017\n\n\n\n\n\n\n\n\n\n\n10 podcasts to learn about data science and programming\n\n\n\npodcast\n\ncoding\n\nuseful-tools\n\ndata\n\npython\n\npodcasts\n\n\n\nMy favorite podcasts for learning about data science and programming, from shows focused on career journeys to deep dives on industry trends.\n\n\n\nJan 11, 2017\n\n\n\n\n\n\n\n\n\n\nResetting my base line: caffeine edition\n\n\n\nproductivity\n\nfocus\n\ntea\n\n\n\nHow I realized my caffeine intake had crept up during busy work months, and what I learned about when caffeine helps versus hurts my productivity.\n\n\n\nJan 11, 2017\n\n\n\n\n\n\n\n\n\n\nClassical Arabic and a Sources and Methods Recap\n\n\n\narabic\n\nlanguage\n\npodcast\n\narabicpodcast\n\nliterarycaravans\n\n\n\nLaunching a new classical Arabic literature podcast series with Talha Ahsan exploring Ibn al-Muqafaa’, plus a Sources and Methods bonus episode reflecting on the past year.\n\n\n\nJan 9, 2017\n\n\n\n\n\n\n\n\n\n\nHow to best work with a language coach\n\n\n\nlanguage\n\nstudy\n\ncoaching\n\nlearning\n\nlanguages\n\nlanguagecoaching\n\n\n\nTips for getting the most value out of working with a language coach.\n\n\n\nJan 8, 2017\n\n\n\n\n\n\n\n\n\n\nIntroducing CoachBot: Your Personal Language Taskmaster\n\n\n\ncoding\n\nuseful-tools\n\ntech\n\nlanguage\n\nproductivity\n\nlanguages\n\npythonsideproject\n\ncoachbot\n\n\n\nI built CoachBot to solve the choice paralysis I faced when studying intermediate languages—a tool that suggests randomized language-learning tasks based on your current…\n\n\n\nJan 6, 2017\n\n\n\n\n\n\n\n\n\n\nTaskpaper –&gt; Omnifocus\n\n\n\ntech\n\nuseful-tools\n\ntaskpaper\n\nmac\n\nomnifocus\n\ntechnology\n\n\n\nHow I copy hierarchical task lists from Taskpaper into Omnifocus using a simple paste trick.\n\n\n\nJan 5, 2017\n\n\n\n\n\n\n\n\n\n\nRandomise Your Learning\n\n\n\nlanguage\n\nstudy\n\nlanguages\n\nlearning\n\n\n\nWhy randomising your learning through interleaved practice helps concepts stick better than studying topics in isolation.\n\n\n\nJan 5, 2017\n\n\n\n\n\n\n\n\n\n\nThings We Control: On Internal vs External Goals\n\n\n\nlanguage\n\nphd\n\nproductivity\n\ngoals\n\nstudy\n\nlearning\n\nlanguages\n\nmasterarabic\n\n\n\nSome reflections on distinguishing between internal and external goals, and why focusing on what you actually control leads to better outcomes.\n\n\n\nJan 4, 2017\n\n\n\n\n\n\n\n\n\n\nEverything You Need to Study Jordanian Arabic\n\n\n\nbooks\n\nlanguage\n\njordan\n\nstudy\n\nlearning\n\narabicpodcast\n\namman\n\nmasterarabic\n\nlanguages\n\narabic\n\n\n\nMy review of the best resources for learning Jordanian Arabic, from textbooks and vocabulary guides to listening materials and comedy shows.\n\n\n\nJan 3, 2017\n\n\n\n\n\n\n\n\n\n\nThe ways memory skills augment your life\n\n\n\nbooks\n\npodcast\n\nmemory\n\nhistory\n\nmemorisation\n\nlearning\n\n\n\nA conversation with Lynne Kelly about how ancient memory techniques can enhance modern life, drawing from her research into historical monuments and her personal memory…\n\n\n\nJan 2, 2017\n\n\n\n\n\n\n\n\n\n\nPet Peeve: Tech Switching\n\n\n\ntech\n\nuseful-tools\n\nsoftware\n\nmedia\n\nblogging\n\ntechnology\n\n\n\nA rant about tech writers constantly switching between applications without explaining why, and a plea for more substantive reasoning behind their recommendations.\n\n\n\nJan 2, 2017\n\n\n\n\n\n\n\n\n\n\nNew Year, New Arabic-language Podcast\n\n\n\nlanguage\n\njordan\n\npodcast\n\narabicpodcast\n\namman\n\nlanguages\n\nmasterarabic\n\narabic\n\n\n\nI started a mostly-Arabic-only podcast this year to stay committed to improving my language skills alongside publishing a book on intermediate-advanced Arabic.\n\n\n\nJan 1, 2017\n\n\n\n\n\n\n\n\n\n\nPre-Order Now: ‘Master Arabic’\n\n\n\nbooks\n\nlanguage\n\nincremental-elephant\n\nstudy\n\nwriting\n\nlanguages\n\nmasterarabic\n\narabic\n\n\n\nAnnouncing the pre-order for my book on advancing past the intermediate plateau in Arabic, with interviews from educators and students across the region.\n\n\n\nDec 30, 2016\n\n\n\n\n\n\n\n\n\n\nDinner Party Decision Matrix: A Python Tool\n\n\n\ncoding\n\nuseful-tools\n\npython\n\ntools\n\nfood\n\n\n\nI built a Python tool to solve my dinner party dilemma using weighted decision-making—and learned some valuable lessons about lists, dictionaries, and testing along the way.\n\n\n\nDec 29, 2016\n\n\n\n\n\n\n\n\n\n\nKnot 4: Empathy, Tech Scepticism and Climate Change\n\n\n\nknot\n\nenvironment\n\nlanguages\n\ntechnology\n\nclimate\n\nempathy\n\nreading\n\n\n\nReflections on empathy, technology skepticism, and climate change through a curated selection of articles and books.\n\n\n\nDec 28, 2016\n\n\n\n\n\n\n\n\n\n\nTalking DevonThink with Gabe Weatherhead\n\n\n\npodcast\n\ntech\n\nuseful-tools\n\ndata\n\ndatabase\n\ndevonthink\n\ntechnology\n\nsocial-media\n\nsoftware\n\n\n\nI talked with Gabe Weatherhead about his DevonThink workflow and how he’s stepped back from mainstream social media.\n\n\n\nDec 26, 2016\n\n\n\n\n\n\n\n\n\n\nSyria’s Revolution: The First Draft\n\n\n\nbooks\n\njournalism\n\nreview\n\nsyria\n\nhistory\n\n\n\nReflections on Burning Country, a book about Syria’s revolution told through diverse firsthand accounts and oral histories from those living through the conflict.\n\n\n\nDec 25, 2016\n\n\n\n\n\n\n\n\n\n\nShare the Gift of Language Coaching\n\n\n\nbusiness\n\nlanguage\n\nincremental-elephant\n\n\n\nI offer custom language coaching to help you—or someone you’re gift-giving for—finally start or stick with learning a language.\n\n\n\nDec 23, 2016\n\n\n\n\n\n\n\n\n\n\nThe Best Books I Read in 2016\n\n\n\nbooks\n\nreview\n\n\n\nMy favorite books from 2016, including reflections on reading fewer novels than planned and standout works like A Little Life and Elena Ferrante’s Neapolitan series.\n\n\n\nDec 21, 2016\n\n\n\n\n\n\n\n\n\n\nReading and the American Revolution\n\n\n\nbooks\n\nusa\n\nreview\n\nreading\n\n\n\nMy thoughts on a book about George Washington’s reading habits and how they shaped his leadership during the American Revolution.\n\n\n\nDec 21, 2016\n\n\n\n\n\n\n\n\n\n\nTwo Charities Needing Your Support\n\n\n\ngeneral\n\nfunding\n\ncharity\n\n\n\nTwo charities I think deserve your support right now—CAGE in the UK and the Kabul Mobile Mini-Circus for Children.\n\n\n\nDec 21, 2016\n\n\n\n\n\n\n\n\n\n\nDjango vs Flask\n\n\n\ncoding\n\nuseful-tools\n\nplateau-ebook\n\nthinking\n\n\n\nWeighing Django vs Flask for building a language tool—comparing their tradeoffs in learning curve, built-in features, and speed to prototype.\n\n\n\nDec 20, 2016\n\n\n\n\n\n\n\n\n\n\nDevonThink Resurgent\n\n\n\ntech\n\nuseful-tools\n\ndata\n\nlearning\n\ndatabase\n\ndevonthink\n\nsoftware\n\n\n\nWhy I’m excited about DevonThink right now — and why Mac users frustrated with Evernote should consider trying it.\n\n\n\nDec 19, 2016\n\n\n\n\n\n\n\n\n\n\nSeeking Python Code Mentor\n\n\n\nlanguage\n\ncoding\n\nstudy\n\nlanguages\n\nlearning\n\n\n\nLooking for a Python code mentor to help guide a web app project involving a frontend, Python backend, and PostgreSQL database.\n\n\n\nDec 18, 2016\n\n\n\n\n\n\n\n\n\n\nKnot 3: Encryption, Race and Tunnels\n\n\n\njournalism\n\nknot\n\ntech\n\nmemory\n\nbooks\n\nrace\n\nmedia\n\n\n\nA roundup of articles on encryption, privacy policy, language funding, and creative responses to geopolitics.\n\n\n\nDec 17, 2016\n\n\n\n\n\n\n\n\n\n\nInto Eternity: the construction of Onkalo’s tunnels\n\n\n\nenvironment\n\ngeneral\n\nscience\n\nconstruction\n\nfinland\n\ndesign\n\nnuclearwaste\n\n\n\nA review of the documentary Into Eternity, which follows the construction of Finland’s Onkalo nuclear waste repository designed to safely contain radioactive material for…\n\n\n\nDec 16, 2016\n\n\n\n\n\n\n\n\n\n\nBroken Pots\n\n\n\nart\n\ngeneral\n\njapan\n\nculture\n\n\n\nAn exploration of kintsugi, the Japanese art of repairing broken ceramics with gold or silver to make them more beautiful than before, and how this philosophy applies to…\n\n\n\nDec 15, 2016\n\n\n\n\n\n\n\n\n\n\nNotification Zero\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\niphone\n\ntools\n\nwriting\n\ntechnology\n\ndistraction\n\nfocus\n\n\n\nHow I eliminated digital distractions to maintain deep focus during a major project.\n\n\n\nDec 13, 2016\n\n\n\n\n\n\n\n\n\n\nDaddybot\n\n\n\ntech\n\nuseful-tools\n\ntravel\n\ntechnology\n\n\n\nI listened to a podcast episode about how a parent used a telepresence robot to stay connected with his kids while deployed overseas.\n\n\n\nDec 11, 2016\n\n\n\n\n\n\n\n\n\n\nHighlights + DevonThink = Pretty Great\n\n\n\nbooks\n\nproductivity\n\ntech\n\nuseful-tools\n\ndata\n\nreview\n\ndatabase\n\nresearch\n\ntechnology\n\nsoftware\n\nreading\n\n\n\nHow Highlights streamlined my PDF annotation workflow with DevonThink.\n\n\n\nDec 9, 2016\n\n\n\n\n\n\n\n\n\n\nKnot 2: Translation as Trauma, Taxis and Artificial Intelligence\n\n\n\nknot\n\njordan\n\ngeneral\n\namman\n\nsyria\n\nlanguages\n\nartificialintelligence\n\ntranslation\n\nwar\n\nconflict\n\n\n\nA curated collection of articles on translation and trauma in Syria, taxi disruption in Amman, the colonial origins of .io domains, and the current state of artificial…\n\n\n\nDec 9, 2016\n\n\n\n\n\n\n\n\n\n\nThe Two Books Every Intermediate Arabic Student Needs to Read\n\n\n\nbooks\n\nlanguage\n\nincremental-elephant\n\nstudy\n\nlearning\n\nplateau-ebook\n\nlanguages\n\narabic\n\n\n\nMy recommendation for intermediate Arabic learners: the Arabic Voices series by Lingualism, which uses authentic monologues from across the Arab world to navigate dialect…\n\n\n\nDec 9, 2016\n\n\n\n\n\n\n\n\n\n\nNutritional Density\n\n\n\nproductivity\n\ngeneral\n\nfood\n\nnutrition\n\nconsumption\n\nsocial-media\n\ninformation\n\n\n\nExploring how the concept of nutritional density applies beyond food—to the information we consume and how choosing substantive, timeless content over ephemeral media might…\n\n\n\nDec 8, 2016\n\n\n\n\n\n\n\n\n\n\nThe Taliban on the Clintons (May 2000)\n\n\n\nafghanistan\n\njournalism\n\narchives\n\ntaliban\n\ntalibansourcesproject\n\n\n\nAn archival piece from the Taliban Sources Project: a May 2000 commentary published in Shariat newspaper discussing the Clintons and the fallout from the Monica Lewinsky…\n\n\n\nDec 7, 2016\n\n\n\n\n\n\n\n\n\n\nClimbing Routes Without Numbers\n\n\n\nmovement\n\njordan\n\nclimbing\n\namman\n\nmental\n\n\n\nI climbed an ungraded route at my local gym and rediscovered what I’d been missing about climbing — the simple enjoyment of moving without worrying about the next difficulty…\n\n\n\nDec 2, 2016\n\n\n\n\n\n\n\n\n\n\nReading the Taliban: Himal Magazine article\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nprimarysources\n\ntaliban\n\nwriting\n\n\n\nAn article I wrote for Himal Magazine in 2014 about understanding the Taliban through their own primary sources—statements, poems, and internal memos—rather than through…\n\n\n\nDec 2, 2016\n\n\n\n\n\n\n\n\n\n\nKnot 1: Solitary Confinement & Digital Security\n\n\n\nbooks\n\nknot\n\ntech\n\nscience\n\nlinks\n\ndata\n\nreading\n\n\n\nThe inaugural post in a recurring feature linking to recent reads—covering solitary confinement in American prisons, AI applications in illegal fishing, data science in…\n\n\n\nDec 2, 2016\n\n\n\n\n\n\n\n\n\n\nPositive feedback on my Memory Skills course\n\n\n\nbusiness\n\ngeneral\n\nincremental-elephant\n\nmemory\n\nlearning\n\nmemorisation\n\nislam\n\n99-names-course\n\n\n\nPositive feedback I’ve received from users of my Memory Skills course, which teaches memory techniques using the 99 Names of God as a learning framework.\n\n\n\nDec 2, 2016\n\n\n\n\n\n\n\n\n\n\nLooking the Wrong Way: Faces in ‘Arrival’\n\n\n\nart\n\nculture\n\nfilm\n\nnarrative\n\n\n\nReflections on how Denis Villeneuve’s Arrival uses close-ups of Amy Adams’ face to create tension and convey spectacle, rather than relying on conventional special effects.\n\n\n\nNov 29, 2016\n\n\n\n\n\n\n\n\n\n\nThe Inner Game: How a Perspective Shift Can Radically Improve Your Performance\n\n\n\nbooks\n\nproductivity\n\nclimbing\n\nmovement\n\nlearning\n\nskills\n\nsport\n\npsychology\n\nmental\n\n\n\nHow a perspective shift and mental reframing can unlock better performance, inspired by W. Timothy Gallwey’s classic sports psychology book.\n\n\n\nNov 23, 2016\n\n\n\n\n\n\n\n\n\n\nLearning a Language? Try a New Approach\n\n\n\nlanguage\n\nstudy\n\nlearning\n\nskills\n\nlanguages\n\ndutch\n\narabic\n\ndari\n\npashto\n\n\n\nA reflection on twenty-five years of language learning and the different approaches I’ve tried—from vocabulary-heavy methods to immersion programs, online courses, and…\n\n\n\nNov 22, 2016\n\n\n\n\n\n\n\n\n\n\nDifferent Kinds of Climbing\n\n\n\nmovement\n\ntravel\n\nclimbing\n\nfear\n\nkuwait\n\nskills\n\n\n\nI went climbing at a hall in Kuwait and realized the biggest obstacle for beginners isn’t strength—it’s fear and mental barriers.\n\n\n\nNov 20, 2016\n\n\n\n\n\n\n\n\n\n\nEncrypt Your Dropbox Files\n\n\n\ntech\n\nuseful-tools\n\nsoftware\n\nsecurity\n\ntechnology\n\n\n\nHow I encrypted my Dropbox files using Boxcryptor to secure my digital life.\n\n\n\nNov 17, 2016\n\n\n\n\n\n\n\n\n\n\nKukicha or Twig Tea\n\n\n\ngeneral\n\ntea\n\n\n\nI discovered kukicha tea this week and was impressed enough to share what makes it special—a low-caffeine Japanese tea that’s perfect if you want some mental lift without…\n\n\n\nNov 14, 2016\n\n\n\n\n\n\n\n\n\n\nMusic, Sound & Technology\n\n\n\ntech\n\nbooks\n\nmusic\n\ntechnology\n\npodcasts\n\n\n\nReflections on a Steven Johnson podcast series exploring how technological innovation and cultural shifts have shaped our sonic landscape, with a focus on episode 3’s…\n\n\n\nNov 12, 2016\n\n\n\n\n\n\n\n\n\n\nSquarespace Goes SSL\n\n\n\ntech\n\nuseful-tools\n\nsecurity\n\ninternet\n\nweb\n\ntechnology\n\nblogging\n\n\n\nNews on Squarespace offering free SSL for all hosted sites, and why it matters for security and SEO.\n\n\n\nNov 11, 2016\n\n\n\n\n\n\n\n\n\n\nFinishing GMB’s Elements\n\n\n\nmovement\n\nbody\n\nsport\n\ngmb\n\nexercise\n\n\n\nCompleting my eight-week journey through GMB’s Elements training programme and what it’s taught me about balancing pushing and pulling movements alongside climbing.\n\n\n\nNov 5, 2016\n\n\n\n\n\n\n\n\n\n\nClimbing Fuheis: Two and a Half Ascents\n\n\n\nmovement\n\njordan\n\nclimbing\n\n\n\nA video from a climbing trip to Fuheis where I completed two and a half ascents and confronted some mental blocks about foot placement and trust, along with some reflections…\n\n\n\nNov 5, 2016\n\n\n\n\n\n\n\n\n\n\nPython Side-Project: Approach\n\n\n\ntech\n\ncoding\n\nlanguages\n\ntechnology\n\npythonsideproject\n\n\n\nHow I’m structuring my Python side-project: building the core functionality as a standalone program first, then adding a web interface later.\n\n\n\nNov 5, 2016\n\n\n\n\n\n\n\n\n\n\nScratching Below the Surface with David Heinemeier Hansson\n\n\n\ntech\n\nproductivity\n\ncoding\n\nlearning\n\ncreativity\n\nskills\n\ncuriosity\n\nwork\n\n\n\nReflections on a podcast interview with DHH on curiosity, learning, systems thinking, and flow.\n\n\n\nOct 30, 2016\n\n\n\n\n\n\n\n\n\n\nA Greener Technological Footprint\n\n\n\nenvironment\n\ntech\n\ntechnology\n\nweb\n\necology\n\n\n\nSwitching from Dreamhost to GreenGeeks to reduce my technological footprint, and what I learned about carbon offsetting in the web hosting industry.\n\n\n\nOct 28, 2016\n\n\n\n\n\n\n\n\n\n\nKael Weston on Sources and Methods\n\n\n\nafghanistan\n\nlanguage\n\npodcast\n\niraq\n\npolitics\n\nwar\n\nconflict\n\nmilitary\n\n\n\nA podcast episode with Kael Weston discussing his experience living in Fallujah, the importance of language in fieldwork, and U.S. political systems in Iraq and Afghanistan.\n\n\n\nOct 27, 2016\n\n\n\n\n\n\n\n\n\n\nOn Completing Udacity’s Intro to Programming Nanodegree\n\n\n\ncoding\n\nuseful-tools\n\nlearning\n\nskills\n\n\n\nReflections on completing Udacity’s Introduction to Programming Nanodegree and what I learned along the way.\n\n\n\nOct 24, 2016\n\n\n\n\n\n\n\n\n\n\nLetting Go\n\n\n\nclimbing\n\nfear\n\nskills\n\n\n\nConfronting my fear of falling on the climbing wall, and what I learned about progress not being linear.\n\n\n\nOct 21, 2016\n\n\n\n\n\n\n\n\n\n\nDifferent Uses for Spaced Repetition\n\n\n\nlanguage\n\ncoding\n\nuseful-tools\n\nstudy\n\ntools\n\nlearning\n\nmemorisation\n\nwork\n\ntechnology\n\nspaced-repetition\n\n\n\nExploring different applications for spaced repetition beyond vocabulary learning—from remembering birthdays to retaining key insights from books and articles.\n\n\n\nOct 18, 2016\n\n\n\n\n\n\n\n\n\n\nBots: Part of the Future of Language Learning?\n\n\n\nlanguage\n\nuseful-tools\n\nlanguages\n\nlearning\n\ntechnology\n\n\n\nI tried Duolingo’s new Bots feature for language learning and discovered it’s far more limited than the marketing suggested—the conversations run on rails, not free-flowing…\n\n\n\nOct 17, 2016\n\n\n\n\n\n\n\n\n\n\nFundamentals Versus Hacks\n\n\n\nbooks\n\nproductivity\n\nlearning\n\nwork\n\n\n\nWhy fundamentals matter more than productivity hacks, and what actually moves the needle on learning and work.\n\n\n\nOct 16, 2016\n\n\n\n\n\n\n\n\n\n\nSkills Development: Foundations\n\n\n\nbooks\n\nproductivity\n\ncoding\n\nlearning\n\nmindset\n\nskills\n\n\n\nMy notes on Allison Kaptur’s talk about effective learning strategies, covering growth mindset, deliberate practice, and how to approach skill development in unstructured…\n\n\n\nOct 15, 2016\n\n\n\n\n\n\n\n\n\n\nOn the demise of afghanistannewscenter.com\n\n\n\nafghanistan\n\njournalism\n\nresearch\n\narchives\n\ninternet\n\n\n\nReflecting on the loss of AfghanistanNewsCenter, a vital archive of English-language news reporting on Afghanistan that went offline without warning.\n\n\n\nOct 13, 2016\n\n\n\n\n\n\n\n\n\n\nKeep Moving\n\n\n\nclimbing\n\nmovement\n\n\n\nA lesson from rock climbing about the power of momentum: sometimes keeping moving forward is more important than stopping to reflect.\n\n\n\nOct 13, 2016\n\n\n\n\n\n\n\n\n\n\nSeeing The Forest But For The Trees: On Exist.io\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\ndata\n\nwork\n\nbeeminder\n\n\n\nHow I use Exist.io to make sense of all my fragmented personal data and find patterns across multiple services.\n\n\n\nOct 11, 2016\n\n\n\n\n\n\n\n\n\n\nComing Soon: The Taliban Reader\n\n\n\nbooks\n\nafghanistan\n\ntalibansourcesproject\n\ntaliban-poetry\n\ntaliban\n\n\n\nAnnouncing The Taliban Reader, a collection of primary sources on the Afghan Taliban compiled over a decade of fieldwork, coming soon from Hurst Publishers.\n\n\n\nOct 5, 2016\n\n\n\n\n\n\n\n\n\n\nLearning Without Seams\n\n\n\ngeneral\n\nclimbing\n\nstudy\n\nlearning\n\nskills\n\nwriting\n\n\n\nThoughts on learning from others by studying both the polished final results and the visible struggles in between—and why you need both to develop your own craft.\n\n\n\nSep 29, 2016\n\n\n\n\n\n\n\n\n\n\nSustainable Climbing: Three Book Reviews\n\n\n\nbooks\n\nclimbing\n\nreviews\n\necology\n\n\n\nI read three climbing books back-to-back and here are my thoughts on what they each brought to the table.\n\n\n\nSep 28, 2016\n\n\n\n\n\n\n\n\n\n\nLessons Learnt: Language Study Habits That Work\n\n\n\nlanguage\n\nstudy\n\nlessonslearnt\n\nlanguages\n\nlearning\n\n\n\nWhat I’ve learned about the study habits that actually move the needle for language learners, based on coaching dozens of students.\n\n\n\nSep 27, 2016\n\n\n\n\n\n\n\n\n\n\nTurn Off Facebook’s News Feed\n\n\n\nproductivity\n\nuseful-tools\n\nfacebook\n\ntools\n\ntechnology\n\n\n\nHow I use the News Feed Eradicator Chrome extension to disable Facebook’s feed and avoid its distractions.\n\n\n\nSep 24, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Tea\n\n\n\nphd\n\nproductivity\n\nphdtoolsseries\n\ntea\n\n\n\nHow tea became both a procrastination project and a crucial part of my PhD workflow—finding the right amount of caffeine to sustain long work sessions without overdoing it.\n\n\n\nSep 19, 2016\n\n\n\n\n\n\n\n\n\n\nCoding Chronicles: Failure\n\n\n\ntech\n\ncoding\n\nudacity\n\ntechnology\n\n\n\nReflections on hitting a wall while working through Udacity’s data analysis nanodegree—what happens when the difficulty jumps and you can’t seem to break through.\n\n\n\nSep 17, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Sleep and Movement to Nourish the Body\n\n\n\nphd\n\nproductivity\n\nmovement\n\n\n\nHow prioritizing sleep and movement helped me maintain focus and productivity during my PhD.\n\n\n\nSep 16, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Goodreads for Cross-Pollination\n\n\n\nbooks\n\nphd\n\nproductivity\n\nuseful-tools\n\nresearch\n\ngoodreads\n\nphdtoolsseries\n\nreading\n\n\n\nHow I used Goodreads to track wide reading across unrelated topics during my PhD, and why diverse books were essential to avoid getting trapped in my own narrow research…\n\n\n\nSep 15, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: ‘Always return to your primary sources’\n\n\n\nphd\n\nproductivity\n\nwriting\n\nprimarysources\n\nresearch\n\nphdtoolsseries\n\n\n\nWhy returning to primary sources became my go-to strategy for unsticking myself during PhD research and writing.\n\n\n\nSep 14, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Freewriting and Journalling to Think Through your Work\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\ntools\n\nwriting\n\nresearch\n\nphdtoolsseries\n\nthinking\n\n\n\nHow freewriting and daily journaling helped me think through research problems during my PhD.\n\n\n\nSep 13, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Pen and Paper\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\ntools\n\nwriting\n\nresearch\n\nphdtoolsseries\n\nthinking\n\n\n\nWhy pen and paper became essential to my PhD process, and how working without constant internet connectivity forced me to think differently about problems.\n\n\n\nSep 13, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Vitamin-R and the Pomodoro Technique for Getting Going\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\ntools\n\nwork\n\nwriting\n\ntechnology\n\nphdtoolsseries\n\n\n\nHow I used the Pomodoro Technique and Vitamin-R to build focused work sessions during my PhD.\n\n\n\nSep 12, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: The Secret to Finishing Your PhD\n\n\n\nphd\n\nproductivity\n\nphdtoolsseries\n\nwork\n\nwriting\n\n\n\nHow I finished my PhD by adopting a four-hour daily writing routine inspired by Cal Newport’s Deep Work.\n\n\n\nSep 9, 2016\n\n\n\n\n\n\n\n\n\n\nStarting the Spaced Repetition Foundation\n\n\n\njournalism\n\nuseful-tools\n\ngeneral\n\ntech\n\nafghanistan\n\nbooks\n\nlanguage\n\nproductivity\n\nstudy\n\nlearning\n\nanki\n\nlanguages\n\nsrsfoundation\n\nspaced-repetition\n\n\n\nI discovered spaced repetition and decided to build a system around it to improve how I learn and retain information.\n\n\n\nSep 8, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Omnifocus for Managing your Non-PhD Life\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nmac\n\nomnifocus\n\nphdtoolsseries\n\n\n\nHow I use OmniFocus and the Getting Things Done system to manage all the non-PhD tasks that pile up during a PhD.\n\n\n\nSep 7, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Backup Systems for Staving off Sadness\n\n\n\ntech\n\nuseful-tools\n\nphd\n\ntools\n\ntechnology\n\nphdtoolsseries\n\nstorage\n\nbackup\n\n\n\nThe backup systems I used during my PhD to avoid losing years of work—and why redundancy matters.\n\n\n\nSep 6, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Turn Off the Internet with Freedom\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nwriting\n\ninternet\n\nresearch\n\nfreedomapp\n\nphdtoolsseries\n\n\n\nI used Freedom to block the internet during dedicated writing time while working on my PhD.\n\n\n\nSep 5, 2016\n\n\n\n\n\n\n\n\n\n\nSvifnökkvinn minn er fullur af álum: Lessons in Icelandic\n\n\n\nlanguage\n\nstudy\n\nlearning\n\nlanguages\n\nicelandic\n\npronunciation\n\n\n\nHow I started learning Icelandic and what makes its pronunciation and grammar so notoriously difficult.\n\n\n\nSep 4, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: RescueTime for Time Tracking\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\nrescuetime\n\nwriting\n\nresearch\n\nphdtoolsseries\n\n\n\nMy experience using RescueTime to track my productivity during my PhD—how passive time tracking helped me stay honest about how much work I was actually getting done.\n\n\n\nSep 2, 2016\n\n\n\n\n\n\n\n\n\n\nExistential Battles: Climbing in Amman\n\n\n\njordan\n\nclimbing\n\nsport\n\nmovement\n\nexercise\n\n\n\nHow I’ve been using climbing to push past my comfort zone and confront my lifelong fear of heights.\n\n\n\nSep 2, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Mellel for Layout and Final Presentation\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nmellel\n\ntools\n\nwriting\n\nresearch\n\nphdtoolsseries\n\n\n\nWhy I used Mellel for formatting my PhD thesis—what made it stand out from Word and Pages for handling complex documents with multiple languages and bibliographies.\n\n\n\nSep 1, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Bookends for Managing References\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nwriting\n\nresearch\n\ntechnology\n\nphdtoolsseries\n\nbookends\n\n\n\nHow I used Bookends to manage 479 references during my PhD and avoid formatting headaches at submission time.\n\n\n\nAug 31, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Scrivener for Writing Long Things\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nresearch\n\nphdtoolsseries\n\nwriting\n\ntools\n\n\n\nHow I used Scrivener to manage the structure and word count of my PhD thesis.\n\n\n\nAug 30, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Save your web links with Pinboard\n\n\n\nproductivity\n\nuseful-tools\n\ninternet\n\nweb\n\npinboard\n\nphdtoolsseries\n\n\n\nHow I used Pinboard to save and search through thousands of web links during my PhD research.\n\n\n\nAug 29, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: DevonThink for File Storage and Discovery\n\n\n\nbooks\n\nphd\n\nproductivity\n\nuseful-tools\n\ndata\n\ndatabase\n\nwriting\n\ndevonthink\n\nresearch\n\nphdtoolsseries\n\nstorage\n\nreading\n\n\n\nHow I used DevonThink to organize and discover patterns across the hundreds of PDFs I accumulated during my PhD research.\n\n\n\nAug 26, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Think better with Tinderbox\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\ntools\n\nwriting\n\ntinderbox\n\ntechnology\n\nphdtoolsseries\n\nthinking\n\n\n\nHow I used Tinderbox to organize notes and think through complex ideas during my PhD research.\n\n\n\nAug 25, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Visualise Structure and Kanban Flow with Trello\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\nstructure\n\nwriting\n\ntrello\n\nphdtoolsseries\n\n\n\nHow I used Trello to organize my PhD chapters and track my writing progress through the drafting process.\n\n\n\nAug 24, 2016\n\n\n\n\n\n\n\n\n\n\nWalking with Words\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\nmovement\n\npodcasts\n\naudible\n\nhealth\n\n\n\nHow I got back to walking regularly by pairing afternoon strolls with podcasts and audiobooks.\n\n\n\nAug 23, 2016\n\n\n\n\n\n\n\n\n\n\nPhD Tools: Beeminder\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\ntools\n\naccountability\n\ntechnology\n\nbeeminder\n\nphdtoolsseries\n\n\n\nHow I used Beeminder to stay accountable on my PhD writing goals through financial commitment.\n\n\n\nAug 22, 2016\n\n\n\n\n\n\n\n\n\n\nLearn all the districts of Afghanistan with Anki!\n\n\n\nafghanistan\n\njournalism\n\ntech\n\nuseful-tools\n\nlearning\n\nanki\n\nmemorisation\n\ngeography\n\n\n\nI created an Anki deck to learn all the districts of Afghanistan, with audio pronunciation and a manageable daily schedule.\n\n\n\nAug 21, 2016\n\n\n\n\n\n\n\n\n\n\nWalking Amman\n\n\n\njordan\n\ntech\n\nmaps\n\ndata\n\ntravel\n\namman\n\ntechnology\n\nwalking\n\n\n\nExploring why Google Maps fails as a navigation tool in Amman and considering data science solutions to account for the city’s topography.\n\n\n\nAug 19, 2016\n\n\n\n\n\n\n\n\n\n\nJordan Diaries: Start with Geography\n\n\n\njordan\n\ntravel\n\nlearning\n\nmemorisation\n\namman\n\n\n\nStarting with geography to get oriented in Jordan—learning the governorates and their capitals as a foundation for understanding conversations and information.\n\n\n\nAug 18, 2016\n\n\n\n\n\n\n\n\n\n\nRemove Your Colour\n\n\n\nproductivity\n\ntech\n\nhack\n\niphone\n\ntechnology\n\n\n\nA simple trick for resisting digital distraction: switch your devices to grayscale and remove the color that apps and websites use to keep you hooked.\n\n\n\nAug 18, 2016\n\n\n\n\n\n\n\n\n\n\nOn Learning New Skills, #showyourwork and other housekeeping\n\n\n\nbooks\n\njordan\n\npodcast\n\ncoding\n\ntsp\n\nskills\n\nwriting\n\nlanguages\n\n\n\nReflections on learning new skills and expanding literacy in data science, statistics, and Python—and the practical challenges that come with it.\n\n\n\nAug 15, 2016\n\n\n\n\n\n\n\n\n\n\nChondrichthyan Fun: a review of Shark MOOC\n\n\n\ngeneral\n\nscience\n\necology\n\nenvironment\n\nbiology\n\nsharks\n\nnature\n\n\n\nMy reward to myself while waiting for my PhD viva: a review of an edX course on shark biology, ecology, and conservation.\n\n\n\nJul 24, 2016\n\n\n\n\n\n\n\n\n\n\nHow to become a memorisation and language ninja\n\n\n\nlanguage\n\ngeneral\n\nmemorisation\n\nlanguages\n\nislam\n\nskills\n\n\n\nAnnouncing two new projects: an email course on memorization techniques using the 99 Names of God as the learning material, and one-on-one coaching for language learning and…\n\n\n\nMay 25, 2016\n\n\n\n\n\n\n\n\n\n\nReading the Afghan Taliban: 67 Sources You Should Be Studying\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nbooks\n\ntalibansourcesproject\n\nprimarysources\n\ndocuments\n\ntaliban\n\n\n\nA curated list of 67 sources for understanding pre-2001 Afghan Taliban history, including books, reports, and primary source collections I relied on during my PhD research.\n\n\n\nDec 26, 2015\n\n\n\n\n\n\n\n\n\n\nThe Best Books I Read in 2015\n\n\n\nbooks\n\n\n\nMy year of reading 150 books—what I learned about my habits and what I’m changing for 2016.\n\n\n\nDec 16, 2015\n\n\n\n\n\n\n\n\n\n\n‘Obedience to the Amir’, or how the Afghan Taliban govern\n\n\n\nbooks\n\npakistan\n\nfirst-draft-publishing\n\nafghanistan\n\npublishing\n\ntaliban\n\n\n\nI published a translated volume of ‘Obedience to the Amir’, a primary source document that Mullah Omar distributed to visitors as the essential guide to how the Taliban…\n\n\n\nNov 29, 2015\n\n\n\n\n\n\n\n\n\n\nOn Untangling Syria’s Socially Mediated War\n\n\n\nlanguage\n\njournalism\n\ntech\n\nresearch\n\nsyria\n\nsocial-media\n\n\n\nReflections on a USIP study about monitoring Syria’s conflict through social media, and what gets lost when researchers only track English-language sources.\n\n\n\nNov 22, 2015\n\n\n\n\n\n\n\n\n\n\nUpcoming Maniac Week\n\n\n\nphd\n\nuseful-tools\n\nproductivity\n\nwork\n\nmaniacweek\n\n\n\nAnnouncing my commitment to a maniac week—an intensive work sprint where I’ll focus on my PhD dissertation for 7 days straight, with strict rules around sleep, offline time…\n\n\n\nNov 15, 2015\n\n\n\n\n\n\n\n\n\n\nMisquoting Mohammad meets Sources & Methods\n\n\n\npodcast\n\nislam\n\n\n\nAn interview with Jonathan Brown on the Sources & Methods podcast discussing authority and sourcing in Islamic tradition, his approach to choosing examples for his books…\n\n\n\nNov 8, 2015\n\n\n\n\n\n\n\n\n\n\nVanishing Interest in Afghanistan\n\n\n\nafghanistan\n\njournalism\n\ndata\n\nmedia\n\n\n\nWhy international interest in Afghanistan has steadily declined since 2009.\n\n\n\nNov 1, 2015\n\n\n\n\n\n\n\n\n\n\nFour Colours\n\n\n\npodcast\n\nuseful-tools\n\nnotes\n\ntools\n\npens\n\nwriting\n\n\n\nHow I use a four-colour pen system to take more meaningful handwritten notes.\n\n\n\nOct 13, 2015\n\n\n\n\n\n\n\n\n\n\nTaliban public punishments, 1996–2001\n\n\n\nafghanistan\n\njournalism\n\ndata\n\nresearch\n\nmedia\n\ntaliban\n\n\n\nA compilation of 101 documented incidents of Taliban public punishments between 1996–2001, compiled from news archives and cross-referenced sources to provide context for…\n\n\n\nSep 24, 2015\n\n\n\n\n\n\n\n\n\n\nEcolinguism and the ethics of learning new languages\n\n\n\nlanguage\n\nuseful-tools\n\nlanguages\n\nmedia\n\n\n\nReflections on ecolinguism and the ethics behind which languages we choose to learn, discussed through an interview about language study and community building.\n\n\n\nSep 18, 2015\n\n\n\n\n\n\n\n\n\n\nSources and Methods: Back for Season 2\n\n\n\npodcast\n\ngeneral\n\n\n\nAnnouncing the return of the Sources & Methods podcast for season 2, with a new episode featuring Will McCants discussing his book on ISIS, policy versus academia, and the…\n\n\n\nSep 9, 2015\n\n\n\n\n\n\n\n\n\n\nHow to Survive Middlebury’s Arabic Summer School Programme\n\n\n\nlanguage\n\nuseful-tools\n\nlanguages\n\narabic\n\nlearning\n\n\n\nPractical tips and reflections on my experience attending Middlebury’s eight-week intensive Arabic summer programme, including how to make the most of the language pledge…\n\n\n\nSep 2, 2015\n\n\n\n\n\n\n\n\n\n\nAFP covers the Taliban Sources Project\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\ntsp\n\nresearch\n\ntaliban\n\n\n\nMedia coverage of my Taliban Sources Project after the British Library declined to host our digitized collection of Afghan Taliban documents.\n\n\n\nAug 28, 2015\n\n\n\n\n\n\n\n\n\n\nArabic Language Update: I did it! (Almost)\n\n\n\nlanguage\n\nuseful-tools\n\nstudy\n\nlearning\n\ntools\n\nmiddlebury\n\nlanguages\n\narabic\n\n\n\nHow I prepared for an intensive Arabic language course by logging 100 hours of study in June, using iTalki lessons, podcasts, and extensive reading to refresh skills I…\n\n\n\nJun 7, 2015\n\n\n\n\n\n\n\n\n\n\nHow I use Goodreads to pick what I read\n\n\n\nbooks\n\ntech\n\nuseful-tools\n\ndata\n\nquantifiedself\n\ntechnology\n\n\n\nHow I use Goodreads to pick what I read, sorting my reading list by average rating to balance challenging books with more accessible ones.\n\n\n\nMar 21, 2015\n\n\n\n\n\n\n\n\n\n\nApocalypse Then: a short review of Filiu’s ‘Apocalypse in Islam’ (2011)\n\n\n\nbooks\n\nislam\n\napocalypse\n\n\n\nMy thoughts on Jean-Pierre Filiu’s history of apocalyptic ideas in Islamic discourse, from the Qur’an through 2011, and what his account reveals—and omits—about how these…\n\n\n\nFeb 25, 2015\n\n\n\n\n\n\n\n\n\n\nNorth Waziristan: A Reading List\n\n\n\nbooks\n\njournalism\n\npakistan\n\nwaziristan\n\nbibliography\n\nreading\n\n\n\nI compiled a reading list of English-language sources on North Waziristan, focusing on factual accounts of names, dates, places and events rather than analysis alone.\n\n\n\nDec 31, 2014\n\n\n\n\n\n\n\n\n\n\nSome Books and Other Things from 2014\n\n\n\nbooks\n\n\n\nMy year reading 117 books, and some reflections on what I learned from tracking my reading habits and favorite discoveries along the way.\n\n\n\nDec 22, 2014\n\n\n\n\n\n\n\n\n\n\nNew book: An Educator’s Tale\n\n\n\nbooks\n\nfirst-draft-publishing\n\nafghanistan\n\n\n\nI helped launch a publishing house with Felix Kuehn, and our first book is out—an oral history of Sharif Fayez, an Afghan educator who shaped higher education in the country…\n\n\n\nDec 10, 2014\n\n\n\n\n\n\n\n\n\n\nNote-Taking Jujitsu, Or How I Make Sense Of What I Read\n\n\n\nbooks\n\npodcast\n\ntech\n\nuseful-tools\n\n\n\nMy approach to note-taking when reading books, especially the challenges that came with early digital formats and how I’ve worked through them.\n\n\n\nOct 24, 2014\n\n\n\n\n\n\n\n\n\n\nSources & Methods: Podcast Follow-Up\n\n\n\npodcast\n\nuseful-tools\n\n\n\nA follow-up on the fifth episode of my podcast with Matt Trevithick, featuring an interview with Mark Bernstein about note-taking, thinking, and the tools we use to organize…\n\n\n\nOct 23, 2014\n\n\n\n\n\n\n\n\n\n\nOur First Publication: ‘I Am Akbar Agha’, Memoir of a Taliban Insider\n\n\n\nafghanistan\n\nbooks\n\n\n\nAnnouncing the publication of our first book, a memoir by Taliban insider Akbar Agha spanning from 1980s Afghanistan through the fall of the Islamic Emirate and beyond.\n\n\n\nSep 16, 2014\n\n\n\n\n\n\n\n\n\n\nSources & Methods, or why I started a podcast\n\n\n\npodcast\n\n\n\nWhy I started a podcast called Sources & Methods with Matt Trevithick, and what we’re hoping to do with it.\n\n\n\nSep 7, 2014\n\n\n\n\n\n\n\n\n\n\nAn Ankified Urdu Frequency Dictionary\n\n\n\nlanguage\n\npakistan\n\nuseful-tools\n\n\n\nI created an Anki deck from a 1969 Urdu frequency dictionary, complete with audio pronunciations for nearly 10,000 words.\n\n\n\nSep 5, 2014\n\n\n\n\n\n\n\n\n\n\nTwo new co-authored reports on Afghanistan\n\n\n\nafghanistan\n\njournalism\n\n\n\nTwo reports I co-authored on Afghanistan have just been published—one with Felix Kuehn for Chatham House on political settlement beyond Taliban talks, and another as expert…\n\n\n\nAug 9, 2014\n\n\n\n\n\n\n\n\n\n\nHow to learn a language to fluency: interview with Gabe Wyner\n\n\n\nbooks\n\nlanguage\n\npodcast\n\n\n\nI interviewed Gabe Wyner about his method for learning languages to fluency, covering spaced repetition, mnemonics, and tools like Anki and Lang-8.\n\n\n\nAug 5, 2014\n\n\n\n\n\n\n\n\n\n\nThe Best Books I Read in 2013\n\n\n\nbooks\n\n\n\nMy favorite books from 2013, ranging from Afghanistan history and literary fiction to unexpected reads about conservation and thought-mapping.\n\n\n\nDec 27, 2013\n\n\n\n\n\n\n\n\n\n\nTaliban Time Travel, or How Our Understanding Is Almost Always Two Years Old\n\n\n\nafghanistan\n\njournalism\n\n\n\nReflections on why our understanding of Taliban dynamics lags years behind what’s actually happening on the ground.\n\n\n\nSep 23, 2013\n\n\n\n\n\n\n\n\n\n\nBook of the Week: ‘Al-Shabab in Somalia’\n\n\n\nafghanistan\n\nbooks\n\nsomalia\n\n\n\nMy review of Al-Shabab in Somalia, a concise history of the group that draws surprising parallels to Afghanistan’s Taliban.\n\n\n\nJul 20, 2013\n\n\n\n\n\n\n\n\n\n\nTranslators Sought – Job Opening\n\n\n\nuncategorized\n\n\n\nHiring translators for a year-long archive translation project converting Pashto and Dari newspapers and magazines into English.\n\n\n\nApr 20, 2013\n\n\n\n\n\n\n\n\n\n\nLearning to Code\n\n\n\ntech\n\n\n\nMy suggestions for getting started with programming languages, with resources for learning Python and Ruby.\n\n\n\nFeb 4, 2013\n\n\n\n\n\n\n\n\n\n\nFollowing Pakistan’s Elections\n\n\n\njournalism\n\npakistan\n\n\n\nA journalist’s daily newsletter tracking Pakistan’s summer elections, featuring headlines from multiple news sources and candidate profiles.\n\n\n\nJan 22, 2013\n\n\n\n\n\n\n\n\n\n\nISAF’s First Fifteen Days\n\n\n\nafghanistan\n\njournalism\n\n\n\nI analyzed ISAF press releases to compare operation and casualty data across the first fifteen days of January in 2011, 2012, and 2013.\n\n\n\nJan 15, 2013\n\n\n\n\n\n\n\n\n\n\nA Jedi-Mind Trick and Three Other Approaches to Learning Vocabulary\n\n\n\nlanguage\n\nuseful-tools\n\n\n\nFour approaches to learning vocabulary, including the memory palace technique and word association strategies I’ve found effective.\n\n\n\nJan 13, 2013\n\n\n\n\n\n\n\n\n\n\nFive Things I Wish Someone Had Told Me About Learning Languages\n\n\n\nlanguage\n\nstudy\n\nlanguages\n\n\n\nReflections on what I wish I’d known about language learning, based on years of studying Arabic, Farsi, Pashto, and other languages.\n\n\n\nJan 7, 2013\n\n\n\n\n\n\n\n\n\n\nUseful Tools: Pinboard\n\n\n\ntech\n\nuseful-tools\n\n\n\nAn overview of Pinboard, the bookmarking service I use to archive everything I read online, plus two recent examples of how I’ve put it to use.\n\n\n\nJan 2, 2013\n\n\n\n\n\n\n\n\n\n\nSome Things I Read\n\n\n\nbooks\n\ngeneral\n\n\n\nSome books and longform pieces I read in 2012, including essays on jihadi thought, Taliban history, and Pakistani politics.\n\n\n\nDec 29, 2012\n\n\n\n\n\n\n\n\n\n\nCatching Up: Poetry of the Taliban\n\n\n\nafghanistan\n\nbooks\n\n\n\nI co-edited a collection of translated Taliban poetry with Felix Kuehn, and it sparked quite a bit of debate about the value of publishing these cultural artifacts.\n\n\n\nDec 22, 2012\n\n\n\n\n\n\n\n\n\n\nCatching Up: An Enemy We Created\n\n\n\nafghanistan\n\nbooks\n\n\n\nUpdates on the release and reception of my book An Enemy We Created, including positive reviews and speaking engagements at universities where it’s being used as required…\n\n\n\nDec 16, 2012\n\n\n\n\n\n\n\n\n\n\nCatching Up: AAN Report on ISAF Night Raids\n\n\n\nafghanistan\n\n\n\nI co-authored a report with Felix Kuehn analyzing ISAF’s night raids and other operations based on their press releases, published by the Afghanistan Analysts Network in…\n\n\n\nDec 10, 2012\n\n\n\n\n\n\n\n\n\n\nWriting the history of the Taliban movement\n\n\n\nuncategorized\n\n\n\nReflections on the gaps in Taliban historiography and why a definitive history of the movement remains to be written.\n\n\n\nNov 26, 2012\n\n\n\n\n\n\n\n\n\n\ntalibantwitterfight: The News Story That Wasn’t\n\n\n\nafghanistan\n\njournalism\n\ntech\n\n\n\nHow media coverage of the Taliban’s Twitter presence got the facts wrong, and why the Senate’s concern about it didn’t make sense.\n\n\n\nDec 28, 2011\n\n\n\n\n\n\n\n\n\n\nEntropy and insurgent radicalisation: an ISAF goal?\n\n\n\nafghanistan\n\njournalism\n\n\n\nReflections on the fragmentation of the Afghan insurgency and what the Taliban’s eroding control over affiliated groups reveals about the conflict’s underlying political…\n\n\n\nDec 7, 2011\n\n\n\n\n\n\n\n\n\n\nVietnam’s Kill-Capture Raids\n\n\n\nafghanistan\n\nbooks\n\n\n\nI examined Mark Moyar’s account of the Phoenix program and Vietnam’s targeted raids on insurgent cadres, drawing parallels to modern counterinsurgency operations in…\n\n\n\nSep 19, 2011\n\n\n\n\n\n\n\n\n\n\nThose ‘40 al-Qaeda insurgents’…\n\n\n\nafghanistan\n\njournalism\n\n\n\nA critical examination of how ISAF press releases inflate al-Qaeda numbers in Afghanistan, with analysis of military operations tallied against the actual evidence presented…\n\n\n\nSep 13, 2011\n\n\n\n\n\n\n\n\n\n\nAn Appeal for Funding\n\n\n\nafghanistan\n\nphd\n\n\n\nAn appeal for funding to translate and digitize a comprehensive collection of Taliban documents spanning from the 1980s to present day.\n\n\n\nAug 20, 2011\n\n\n\n\n\n\n\n\n\n\nISAF Press Release Word Clouds\n\n\n\nafghanistan\n\njournalism\n\nphd\n\n\n\nWord clouds showing the most frequently used terms in ISAF press releases from November 2009 through May 2011, broken down by time period.\n\n\n\nMay 17, 2011\n\n\n\n\n\n\n\n\n\n\nMore data on ‘Kill-Capture’ Raids\n\n\n\nafghanistan\n\njournalism\n\ntech\n\n\n\nI broke down ISAF press release data into individual incidents to show that while the number of releases decreased, the actual count of kill-capture operations remained high.\n\n\n\nMay 16, 2011\n\n\n\n\n\n\n\n\n\n\nKandahar Prison Escape: the Taliban’s Tale\n\n\n\nafghanistan\n\njournalism\n\n\n\nTranslations of two articles from the Taliban’s Al-Somood magazine detailing their account of the 2011 prison escape from Kandahar’s Sarpoza jail, including background on…\n\n\n\nMay 15, 2011\n\n\n\n\n\n\n\n\n\n\nTalQaeda: the Timeline\n\n\n\nafghanistan\n\ntech\n\nphd\n\ngeneral\n\n\n\nA timeline tracking the relationship between the Taliban and al-Qaeda, compiled from research for a book on the topic.\n\n\n\nMay 13, 2011\n\n\n\n\n\n\n\n\n\n\nThe Afghan Taliban react to bin Laden’s death\n\n\n\nafghanistan\n\n\n\nAnalyzing the Afghan Taliban’s cautious response to bin Laden’s death and what their non-committal statement reveals about their position.\n\n\n\nMay 3, 2011\n\n\n\n\n\n\n\n\n\n\nThe Petraeus Effect\n\n\n\nafghanistan\n\n\n\nI analyzed ISAF press releases quantitatively to examine trends in how military operations were reported, particularly around night raids and detentions during the Petraeus…\n\n\n\nMay 1, 2011\n\n\n\n\n\n\n\n\n\n\n‘The Kill Team’\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\n\n\nReflections on the Kill Team controversy, with recommended books on soldier experiences in Afghanistan and Vietnam.\n\n\n\nApr 3, 2011\n\n\n\n\n\n\n\n\n\n\nAfghanistan’s Child Soldiers\n\n\n\nafghanistan\n\njournalism\n\n\n\nA photo and reflection on child soldiers in Afghanistan’s security forces, following a New York Times investigation.\n\n\n\nJan 30, 2011\n\n\n\n\n\n\n\n\n\n\nHelmand Refugee Appeal Update - Distribution Day\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\n\n\nAn update on the Helmand refugee appeal, including details from the distribution day when donations were delivered to displaced families living in camps in Kabul.\n\n\n\nJan 21, 2011\n\n\n\n\n\n\n\n\n\n\n‘An Enemy We Created’: the website\n\n\n\nafghanistan\n\nbooks\n\ngeneral\n\n\n\nI launched the website for An Enemy We Created, a book I co-authored with Felix Kuehn about US involvement in Afghanistan.\n\n\n\nJan 10, 2011\n\n\n\n\n\n\n\n\n\n\nUPDATED: An Appeal\n\n\n\nafghanistan\n\njournalism\n\ntravel\n\ngeneral\n\n\n\nA call to action on behalf of Afghan refugees displaced by conflict in Helmand province, with details on how to help those living in camps near Kabul.\n\n\n\nDec 28, 2010\n\n\n\n\n\n\n\n\n\n\nDeedee Derksen picks her 2010 books\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\ngeneral\n\n\n\nDeedee Derksen shares her favorite books from 2010, including autobiographies by Keith Richards and Mullah Abdul Salam Zaeef, biographies of Somerset Maugham and Patricia…\n\n\n\nDec 22, 2010\n\n\n\n\n\n\n\n\n\n\nReal People as Agents\n\n\n\nafghanistan\n\nphd\n\nbooks\n\ngeneral\n\n\n\nReflections on historical agency and why attributing change to abstractions rather than real people obscures what’s actually happening—especially when discussing ideology…\n\n\n\nDec 22, 2010\n\n\n\n\n\n\n\n\n\n\nThe Best Books of 2010 (UPDATED)\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\ngeneral\n\n\n\nMy favorite books of 2010, including long-form journalism and nonfiction works on the Middle East and beyond, plus recommendations from friends.\n\n\n\nDec 18, 2010\n\n\n\n\n\n\n\n\n\n\nOpen Letter: The Response (UPDATED)\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\n\n\nTracking the media coverage and response to an open letter I co-signed to President Obama about Afghanistan, along with reprints, commentary from signatories, and news…\n\n\n\nDec 14, 2010\n\n\n\n\n\n\n\n\n\n\nAn Open Letter to President Obama\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\n\n\nI added my name to an open letter to President Obama about the unsustainable course of the war in Afghanistan, signed by academics, experts, and NGO workers.\n\n\n\nDec 11, 2010\n\n\n\n\n\n\n\n\n\n\nNo Comment: McChrystal and Petraeus\n\n\n\nafghanistan\n\ngeneral\n\n\n\nReflections on Stanley McChrystal and David Petraeus, two influential military commanders and their impact on counterinsurgency strategy.\n\n\n\nDec 5, 2010\n\n\n\n\n\n\n\n\n\n\nTaliban Realism over the September 11 Attacks\n\n\n\nafghanistan\n\n\n\nA shift in Taliban acknowledgment of al-Qaeda’s role in the September 11 attacks, based on an interview with former Taliban Ambassador Mawlawi Abdul Salam Zaeef.\n\n\n\nNov 26, 2010\n\n\n\n\n\n\n\n\n\n\nForeign fighters down south?\n\n\n\nafghanistan\n\njournalism\n\n\n\nQuestioning claims about the number of foreign fighters in southern Afghanistan made in a Sunday Times article.\n\n\n\nNov 23, 2010\n\n\n\n\n\n\n\n\n\n\nPetraeus, Lisbon and the Great PR Push\n\n\n\nafghanistan\n\njournalism\n\n\n\nMy thoughts on the messaging around the Lisbon NATO meeting and the upcoming December Strategic Review, and what General Petraeus is trying to make us believe about…\n\n\n\nNov 22, 2010\n\n\n\n\n\n\n\n\n\n\n‘Fly Freely’ - Afghan Women’s Poetry\n\n\n\nafghanistan\n\npoetry\n\n\n\nI’m republishing translations of poems by Afghan poet Nadia Anjuman, whose work explores resilience and beauty amid hardship.\n\n\n\nNov 13, 2010\n\n\n\n\n\n\n\n\n\n\nMarie Colvin in Kandahar for the Sunday Times\n\n\n\nafghanistan\n\njournalism\n\n\n\nA critical reading of Marie Colvin’s reporting from Kandahar, examining what her account of Taliban violence gets right and what it misses about the city’s assassination…\n\n\n\nNov 7, 2010\n\n\n\n\n\n\n\n\n\n\nIrish Parallels\n\n\n\nbooks\n\ngeneral\n\n\n\nReflections on lessons from the Irish conflict while reading about Northern Ireland’s political history and strategy.\n\n\n\nJul 7, 2010\n\n\n\n\n\n\n\n\n\n\nHope Is Not A Strategy\n\n\n\nafghanistan\n\n\n\nReflections on reading an article about Afghanistan’s peace prospects and why hope-based thinking, without concrete strategy, falls short.\n\n\n\nJul 3, 2010\n\n\n\n\n\n\n\n\n\n\nKandahar Timeline 1979-2010\n\n\n\nafghanistan\n\ntech\n\ngeneral\n\n\n\nA comprehensive timeline of events in Kandahar from 1979 to 2010, compiled from various research projects and presented using Tinderbox.\n\n\n\nJun 21, 2010\n\n\n\n\n\n\n\n\n\n\nOn Experts\n\n\n\ngeneral\n\n\n\nA reflection on expertise, prompted by Joe Alex Morris’s warning to John Cooley about claiming authority on the Middle East.\n\n\n\nMay 22, 2010\n\n\n\n\n\n\n\n\n\n\nKandahar Portraits\n\n\n\nafghanistan\n\njournalism\n\n\n\nI finally finished a long-form piece about Kandahar that got published in The National’s weekend supplement.\n\n\n\nMay 14, 2010\n\n\n\n\n\n\n\n\n\n\nFrom ‘Ghazal’ by Shin Gul Aajiz\n\n\n\nafghanistan\n\ngeneral\n\ntaliban-poetry\n\n\n\nSharing lines from a Ghazal poem by Shin Gul Aajiz that I’m editing for a collection with Felix, spanning love, politics, and other themes from Taliban-era poetry.\n\n\n\nMay 1, 2010\n\n\n\n\n\n\n\n\n\n\nKandahar Chronology (September 2001-October 2009)\n\n\n\nafghanistan\n\ngeneral\n\n\n\nI compiled a chronology of significant events in Kandahar province from September 2001 to October 2009, sourced primarily from the New York Times archives and…\n\n\n\nApr 29, 2010\n\n\n\n\n\n\n\n\n\n\nAsk the Scholars: when and where was Mullah Mohammad Omar born?\n\n\n\nafghanistan\n\ntaliban\n\n\n\nI compiled conflicting scholarly accounts of Mullah Mohammad Omar’s birthdate and birthplace, ranging across a 15-year span and multiple locations in Afghanistan.\n\n\n\nApr 26, 2010\n\n\n\n\n\n\n\n\n\n\nNew(ish) Kandahar Blogs\n\n\n\nafghanistan\n\n\n\nA roundup of three blogs posting from Kandahar worth following for on-the-ground perspectives from the region.\n\n\n\nApr 25, 2010\n\n\n\n\n\n\n\n\n\n\nJere van Dyk’s ‘Captive’\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\n\n\nA review of Jere van Dyk’s memoir about being captured by the Taliban in 2008, with particular interest in his deep knowledge of Afghan tribal dynamics from his earlier work…\n\n\n\nApr 24, 2010\n\n\n\n\n\n\n\n\n\n\nKandahar’s Electricity Problems\n\n\n\nafghanistan\n\n\n\nMy thoughts on the debate over quick-fix generator programs versus sustainable electricity solutions in Kandahar, and why delivering reliable power matters more than the…\n\n\n\nApr 23, 2010\n\n\n\n\n\n\n\n\n\n\nMullah Omar captured?\n\n\n\nafghanistan\n\ngeneral\n\n\n\nDiscussing rumors about the capture of Taliban leader Mullah Mohammad Omar in Karachi.\n\n\n\nApr 19, 2010\n\n\n\n\n\n\n\n\n\n\nKandahar Survey\n\n\n\nafghanistan\n\n\n\nI share my thoughts on a survey of Kandahar Province, noting its findings on corruption and insecurity while questioning its methodology and timeliness.\n\n\n\nApr 18, 2010\n\n\n\n\n\n\n\n\n\n\n‘So how is it?’\n\n\n\nafghanistan\n\n\n\nPersonal reflections on the deteriorating security situation in Kandahar, where people I know were caught in or affected by recent attacks.\n\n\n\nApr 16, 2010\n\n\n\n\n\n\n\n\n\n\nCivilian Casualties from Zheray\n\n\n\nafghanistan\n\n\n\nReporting on a bus attack by NATO troops on the Herat-Kandahar road and documenting civilian casualties at Kandahar hospitals.\n\n\n\nApr 12, 2010\n\n\n\n\n\n\n\n\n\n\nHearts and Minds\n\n\n\nafghanistan\n\n\n\nReflections on power outages in Kandahar and the disconnect between infrastructure needs and counterinsurgency priorities, plus observations on local rumors and religious…\n\n\n\nApr 12, 2010\n\n\n\n\n\n\n\n\n\n\noverheardinkandahar\n\n\n\nafghanistan\n\n\n\nOverheard warnings from a businessman in Kandahar City about an impending crisis and the choices people face.\n\n\n\nApr 12, 2010\n\n\n\n\n\n\n\n\n\n\nBack Home\n\n\n\nafghanistan\n\ngeneral\n\n\n\nReflections on returning to Kandahar after two months away and finding the security situation significantly deteriorated, along with the research and reading I’m diving into…\n\n\n\nApr 11, 2010\n\n\n\n\n\n\n\n\n\n\nReal People, Real War\n\n\n\nafghanistan\n\njournalism\n\n\n\nA reminder that behind the headlines of military strategy in Afghanistan are real people suffering the human cost of war.\n\n\n\nFeb 8, 2010\n\n\n\n\n\n\n\n\n\n\nFT does Kandahar\n\n\n\nuncategorized\n\n\n\nThoughts on a Financial Times article about a key power broker in Kandahar, and what the reporter missed about Amir Mohammad Agha’s connection to Mullah Omar.\n\n\n\nFeb 5, 2010\n\n\n\n\n\n\n\n\n\n\nJohn Nagl and ‘the future of counterinsurgency’\n\n\n\nafghanistan\n\ngeneral\n\n\n\nReflections on John Nagl’s lecture on counterinsurgency in Afghanistan, and where his US military perspective diverges from lived experience on the ground.\n\n\n\nFeb 2, 2010\n\n\n\n\n\n\n\n\n\n\n‘Talking to Terrorists’\n\n\n\nafghanistan\n\nbooks\n\n\n\nReflections on a book about negotiating with armed groups in Northern Ireland and the Basque Country, and what lessons it might offer for current policy discussions around…\n\n\n\nJan 30, 2010\n\n\n\n\n\n\n\n\n\n\nPresenting Mullah Zaeef\n\n\n\nafghanistan\n\nbooks\n\ntravel\n\nuntitled\n\nzaeef\n\n\n\nUpcoming presentations in the UK and USA for my book with Felix Kuehn, including talks at SOAS, LSE, the Brookings Institution, and Harvard.\n\n\n\nJan 13, 2010\n\n\n\n\n\n\n\n\n\n\nStaying Apart\n\n\n\nafghanistan\n\njournalism\n\ntravel\n\n\n\nReflections on the physical, linguistic, and institutional distances that separate foreign forces, journalists, and aid workers from the people of southern Afghanistan.\n\n\n\nJan 8, 2010\n\n\n\n\n\n\n\n\n\n\n5 Books Everyone Should Read About Afghanistan\n\n\n\nafghanistan\n\nbooks\n\ndorronsoro\n\nzaeef\n\njason-elliot\n\n\n\nFive book recommendations for understanding Afghanistan, chosen for anyone preparing to work or travel there.\n\n\n\nJan 1, 2010\n\n\n\n\n\n\n\n\n\n\nNew Year, New Website\n\n\n\ngeneral\n\n\n\nI’ve moved my blog to WordPress and am committing to post more regularly this year—with commentary on media coverage of Afghanistan, film and book reviews, and reflections…\n\n\n\nJan 1, 2010\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "all.html",
    "href": "all.html",
    "title": "All Posts",
    "section": "",
    "text": "A chronological archive of all posts—both technical and personal writings.\nBrowse by section: Technical | Personal\n\n\n\n\n\n\n\n\n\nChoice Density: A Better Way to Think About AI and Authenticity\n\n\n\nwriting\n\ntech\n\ngeneral\n\n\n\n\n\n\n2026-01-21\n\n\n\n\n\n\n\nTrying to instrument an agentic app with Arize Phoenix and litellm\n\n\n\nllms\n\nagents\n\nevals-course\n\nevaluation\n\nminiproject\n\nhinbox\n\n\n\n\n\n\n2025-06-04\n\n\n\n\n\n\n\nTesting out instrumenting LLM tracing for litellm with Braintrust and Langfuse\n\n\n\nllms\n\nagents\n\nevals-course\n\nevaluation\n\nminiproject\n\nhinbox\n\n\n\n\n\n\n2025-06-04\n\n\n\n\n\n\n\nBuilding hinbox: An agentic research tool for historical document analysis\n\n\n\nllms\n\nagents\n\nevals-course\n\nevaluation\n\nminiproject\n\nhinbox\n\nresearch\n\n\n\n\n\n\n2025-05-30\n\n\n\n\n\n\n\nError analysis to find failure modes\n\n\n\nevals-course\n\nllms\n\nllmops\n\nevaluation\n\n\n\n\n\n\n2025-05-23\n\n\n\n\n\n\n\nHow to think about evals\n\n\n\nevals-course\n\nllms\n\nllmops\n\nevaluation\n\n\n\n\n\n\n2025-05-20\n\n\n\n\n\n\n\nFirst impressions of the new Gemini Deep Research (with 2.5 Pro)\n\n\n\nagents\n\ngoogle\n\ntools\n\nopenai\n\nresearch\n\n\n\n\n\n\n2025-04-09\n\n\n\n\n\n\n\nLearnings from a week of building with local LLMs\n\n\n\nclaude\n\nllm\n\nllms\n\nminiproject\n\nopenai\n\nprompt-engineering\n\nsoftwareengineering\n\ntools\n\n\n\n\n\n\n2025-03-16\n\n\n\n\n\n\n\nBuilding an MCP Server for Beeminder: Connecting AI Assistants to Personal Data\n\n\n\ntools\n\nanthropic\n\nclaude\n\nminiproject\n\n\n\n\n\n\n2025-02-21\n\n\n\n\n\n\n\nTinbox: an LLM-based document translation tool\n\n\n\ntranslation\n\nllm\n\nllms\n\nlanguages\n\nresearch\n\nminiproject\n\npython\n\ntools\n\n\n\n\n\n\n2025-02-16\n\n\n\n\n\n\n\nStarting the Hugging Face Agents course\n\n\n\nagents\n\nhuggingface\n\nskillbuilding\n\nllmops\n\nllms\n\n\n\n\n\n\n2025-02-11\n\n\n\n\n\n\n\nAI Engineering Architecture and User Feedback\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nllmops\n\nevaluation\n\n\n\n\n\n\n2025-02-09\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ chapter 9: Inference Optimisation\n\n\n\nbooks-i-read\n\ninference\n\nllm\n\nllms\n\nhardware\n\n\n\n\n\n\n2025-02-07\n\n\n\n\n\n\n\nDataset Engineering: The Art and Science of Data Preparation\n\n\n\nbooks-i-read\n\ndatasets\n\ndatalabelling\n\nllm\n\nllms\n\nfinetuning\n\n\n\n\n\n\n2025-02-05\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning\n\n\n\nbooks-i-read\n\nfinetuning\n\nllm\n\nllms\n\n\n\n\n\n\n2025-01-26\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 6\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nagents\n\nrag\n\nevaluation\n\n\n\n\n\n\n2025-01-24\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 4\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nevaluation\n\n\n\n\n\n\n2025-01-22\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 3\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nevaluation\n\n\n\n\n\n\n2025-01-21\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 1\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nfinetuning\n\nprompt-engineering\n\n\n\n\n\n\n2025-01-19\n\n\n\n\n\n\n\nFinal notes on ‘Prompt Engineering for LLMs’\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\nevaluation\n\n\n\n\n\n\n2025-01-17\n\n\n\n\n\n\n\nAssembling the Prompt: Notes on ‘Prompt Engineering for LLMs’ ch 6\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\n\n\n\n\n\n2025-01-13\n\n\n\n\n\n\n\nPrompt Content: Notes on ‘Prompt Engineering for LLMs’ ch 5\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\nRAG\n\n\n\n\n\n\n2025-01-12\n\n\n\n\n\n\n\nStarting to read Prompt Engineering for LLMs\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\ntokenisation\n\n\n\n\n\n\n2025-01-09\n\n\n\n\n\n\n\nFirst stitches: on learning to knit\n\n\n\ncrafting\n\nlearning\n\ncrafts\n\nskills\n\nlessonslearnt\n\nknitting\n\n\n\n\n\n\n2025-01-05\n\n\n\n\n\n\n\nAll the things I learned while trending on Hacker News\n\n\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\nevaluation\n\nnlp\n\n\n\n\n\n\n2024-07-07\n\n\n\n\n\n\n\nMy finetuned models beat OpenAI’s GPT-4\n\n\n\nnlp\n\nafghanistan\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\nevaluation\n\n\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\nHow to think about creating a dataset for LLM finetuning evaluation\n\n\n\nllms\n\nfinetuning\n\nisafpr\n\nafghanistan\n\ndatasets\n\nevaluation\n\nminiproject\n\n\n\n\n\n\n2024-06-25\n\n\n\n\n\n\n\nOne-click LLM finetuning with Predibase, OpenPipe and OpenAI\n\n\n\nnlp\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\n\n\n\n\n\n2024-06-17\n\n\n\n\n\n\n\nFinetuning my first LLM(s) for structured data extraction with axolotl\n\n\n\nnlp\n\nafghanistan\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\n\n\n\n\n\n2024-06-15\n\n\n\n\n\n\n\nEvaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction\n\n\n\nnlp\n\nafghanistan\n\ndatalabelling\n\nllms\n\nisafpr\n\nminiproject\n\nevaluation\n\n\n\n\n\n\n2024-06-03\n\n\n\n\n\n\n\nStructured Data Extraction for ISAF Press Releases with Instructor\n\n\n\nnlp\n\nafghanistan\n\ndatalabelling\n\nisafpr\n\nllms\n\nminiproject\n\n\n\n\n\n\n2024-06-02\n\n\n\n\n\n\n\nIntroducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009\n\n\n\nminiproject\n\nafghanistan\n\ndatalabelling\n\ndatasets\n\nnlp\n\nllms\n\nisafpr\n\n\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\nWriting a custom Terraform provider to deploy Huggingface Spaces\n\n\n\ndevops\n\nminiproject\n\nterraform\n\ngo\n\nskillbuilding\n\n\n\n\n\n\n2024-03-31\n\n\n\n\n\n\n\nPublishing the ISAF Press Releases dataset\n\n\n\nminiproject\n\nafghanistan\n\ndatalabelling\n\ndatasets\n\nnlp\n\nllms\n\n\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\nLanguage Learning Crash Course: from slightly more than zero to slightly less than advanced\n\n\n\nlanguage\n\nstudy\n\nlearning\n\neducation\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2023-08-10\n\n\n\n\n\n\n\nAll the things I wish I knew about studying at school\n\n\n\nproductivity\n\nmemory\n\nstudy\n\nskills\n\nmathematics\n\nmemorisation\n\neducation\n\n\n\n\n\n\n2023-08-06\n\n\n\n\n\n\n\nAutomating database backups with Tarsnap\n\n\n\ndatabases\n\nskillbuilding\n\nsoftwareengineering\n\ntools\n\nminiproject\n\n\n\n\n\n\n2023-07-24\n\n\n\n\n\n\n\nBuilding MathsPrompt: a tool to help me review and practice problems for my degree\n\n\n\nopenai\n\nllms\n\nmathematics\n\nrust\n\nmu123\n\nq31\n\nskillbuilding\n\nsoftwareengineering\n\ntools\n\nminiproject\n\n\n\n\n\n\n2023-07-23\n\n\n\n\n\n\n\nAutomating social media posting for my new blogposts\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\nautomation\n\nsocial-media\n\ntechnology\n\n\n\n\n\n\n2023-07-15\n\n\n\n\n\n\n\nTerraform Input Variables\n\n\n\nterraform\n\ndevops\n\nsoftwareengineering\n\n\n\n\n\n\n2023-06-22\n\n\n\n\n\n\n\nTokenizer Links\n\n\n\nnlp\n\nbalochi-language-model\n\ntokenisation\n\nlinks\n\n\n\n\n\n\n2023-06-04\n\n\n\n\n\n\n\nTokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy\n\n\n\nnlp\n\nbalochi-language-model\n\ntokenisation\n\nbalochi\n\n\n\n\n\n\n2023-06-03\n\n\n\n\n\n\n\nThe What, Why, and How of Tokenisation in Machine Learning\n\n\n\nnlp\n\nbalochi-language-model\n\ntokenisation\n\n\n\n\n\n\n2023-06-01\n\n\n\n\n\n\n\nBuilding a Balochi Language Dataset for NLP Applications\n\n\n\nbalochi\n\nnlp\n\nbalochi-language-model\n\nethics\n\ndatasets\n\n\n\n\n\n\n2023-05-29\n\n\n\n\n\n\n\nThe Risks of Language Models in Minority Languages\n\n\n\nbalochi\n\nnlp\n\nbalochi-language-model\n\ndeep-learning\n\nethics\n\n\n\n\n\n\n2023-05-22\n\n\n\n\n\n\n\nLow-resource language models: making a start with Balochi\n\n\n\nbalochi\n\nnlp\n\nbalochi-language-model\n\ndeep-learning\n\n\n\n\n\n\n2023-05-21\n\n\n\n\n\n\n\nFinishing MU123\n\n\n\nmathematics\n\nmu123\n\nq31\n\n\n\n\n\n\n2023-05-14\n\n\n\n\n\n\n\nExponents and Logarithms: a MU123 review\n\n\n\nmathematics\n\nmu123\n\nq31\n\n\n\n\n\n\n2023-05-02\n\n\n\n\n\n\n\nTerraform for the Uninitiated: Demystifying Your First Codebase\n\n\n\nterraform\n\nsoftwareengineering\n\ndevops\n\n\n\n\n\n\n2023-04-29\n\n\n\n\n\n\n\nHow to remove a commit (or two) from your git branch\n\n\n\ngit\n\nsoftwareengineering\n\nversioncontrol\n\n\n\n\n\n\n2023-04-28\n\n\n\n\n\n\n\nThe Trick Is The Thing, Part II\n\n\n\nmathematics\n\nmu123\n\nq31\n\ndeeplearning\n\n\n\n\n\n\n2023-03-25\n\n\n\n\n\n\n\nBuilding Blocks For Better Stable Eights\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\n\n\n\n2023-03-18\n\n\n\n\n\n\n\nTricking my digits classifier with diffusion\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\n\n\n\n2023-03-05\n\n\n\n\n\n\n\nVermeer at the Rijksmuseum\n\n\n\nart\n\nreviews\n\nnetherlands\n\n\n\n\n\n\n2023-02-19\n\n\n\n\n\n\n\nOn mathematical literacy\n\n\n\nmathematics\n\nmu123\n\nq31\n\n\n\n\n\n\n2023-01-01\n\n\n\n\n\n\n\n2022 Readings\n\n\n\nbooks\n\nreading\n\n\n\n\n\n\n2022-12-27\n\n\n\n\n\n\n\nFrom the foundation up: Fashion-MNIST basics from Lesson 10\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\n\n\n\n2022-10-24\n\n\n\n\n\n\n\nDeep learning tricks all the way down, with a bit of mathematics for good measure\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\n\n\n\n2022-10-17\n\n\n\n\n\n\n\nAvoiding BIDMAS, or how J does notation\n\n\n\nj\n\nmathematics\n\nmu123\n\nq31\n\nnotation\n\n\n\n\n\n\n2022-10-16\n\n\n\n\n\n\n\nStoring Bytes: what data serialisation is and why you need it for machine learning\n\n\n\nredactionmodel\n\ncomputervision\n\nmlops\n\npython\n\ntools\n\nzenml\n\n\n\n\n\n\n2022-09-07\n\n\n\n\n\n\n\nIt takes a tribe: how I’m thinking about putting my object detection model into production\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\nmlops\n\n\n\n\n\n\n2022-05-31\n\n\n\n\n\n\n\nMore Data, More Problems: Using DVC to handle data versioning for a computer vision problem\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\nmlops\n\n\n\n\n\n\n2022-05-24\n\n\n\n\n\n\n\nRedaction Image Classifier: NLP Edition\n\n\n\nfastai\n\nnlp\n\npartone\n\n\n\n\n\n\n2022-05-21\n\n\n\n\n\n\n\nA neural network for Fashion MNIST data\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\n\n\n\n2022-05-15\n\n\n\n\n\n\n\nUsing the seven-step SGD process for Fashion MNIST\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\n\n\n\n2022-05-14\n\n\n\n\n\n\n\nStochastic Gradient Descent: a mini-example of the whole game\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\n\n\n\n2022-05-13\n\n\n\n\n\n\n\nSome foundations for machine learning with PyTorch\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\n\n\n\n2022-05-12\n\n\n\n\n\n\n\nA dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\n\n\n\n2022-05-11\n\n\n\n\n\n\n\nA painless way to create an MVP demo using computer vision models\n\n\n\nfastai\n\ncomputervision\n\nredactionmodel\n\ntools\n\n\n\n\n\n\n2022-05-07\n\n\n\n\n\n\n\nHow my pet cat taught me a lesson about validation data for image classification\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\n\n\n\n2022-05-02\n\n\n\n\n\n\n\nHow to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\ndatavalidation\n\n\n\n\n\n\n2022-04-28\n\n\n\n\n\n\n\nHow to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\ndatavalidation\n\n\n\n\n\n\n2022-04-26\n\n\n\n\n\n\n\nHow to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\ndatavalidation\n\n\n\n\n\n\n2022-04-19\n\n\n\n\n\n\n\n‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\n\n\n\n\n\n2022-04-06\n\n\n\n\n\n\n\nSome characteristics of best-in-class ML portfolio projects\n\n\n\ncomputervision\n\nskillbuilding\n\n\n\n\n\n\n2022-04-04\n\n\n\n\n\n\n\nBuilding my own image to use IceVision with Paperspace\n\n\n\ntools\n\ndocker\n\ncomputervision\n\n\n\n\n\n\n2022-03-25\n\n\n\n\n\n\n\nStarting Docker In A Month Of Lunches\n\n\n\ntools\n\ndockerinamonthoflunches\n\nbooks-i-read\n\n\n\n\n\n\n2022-03-21\n\n\n\n\n\n\n\nFiguring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of\n\n\n\nredactionmodel\n\ncomputervision\n\ntools\n\ndebugging\n\njupyter\n\n\n\n\n\n\n2022-03-12\n\n\n\n\n\n\n\nIncremental Improvements to my Redaction Detection Model\n\n\n\nredactionmodel\n\ncomputervision\n\ntools\n\n\n\n\n\n\n2022-03-03\n\n\n\n\n\n\n\nThree Python Helpers for Parsing Inputs\n\n\n\npython\n\ntools\n\n\n\n\n\n\n2022-02-27\n\n\n\n\n\n\n\nIt’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model\n\n\n\nredactionmodel\n\ncomputervision\n\npython\n\ntools\n\n\n\n\n\n\n2022-02-10\n\n\n\n\n\n\n\nWhat are invariants and how can they help make your Python classes more robust?\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-02-08\n\n\n\n\n\n\n\nUpgrade your Python dicts with data classes\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-02-05\n\n\n\n\n\n\n\nHow and where to use enums in Python\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-01-30\n\n\n\n\n\n\n\nUsing mypy for Python type checking\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-01-22\n\n\n\n\n\n\n\nUsing type annotation with collections in Python\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-01-18\n\n\n\n\n\n\n\nA Midway Report on my Computer Vision Project\n\n\n\npython\n\nfastai\n\ntools\n\nredactionmodel\n\n\n\n\n\n\n2022-01-16\n\n\n\n\n\n\n\nDifferent ways to constrain types in Python\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-01-08\n\n\n\n\n\n\n\nLearning about ‘nbdev’ while building a Python package for PDF machine learning datasets\n\n\n\npython\n\njupyter\n\nfastai\n\ntools\n\n\n\n\n\n\n2022-01-06\n\n\n\n\n\n\n\nGetting practical with type annotations and mypy\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2022-01-03\n\n\n\n\n\n\n\nCounter: a shortcut to counting iterables in Python\n\n\n\npython\n\n\n\n\n\n\n2022-01-01\n\n\n\n\n\n\n\nWhat’s special about types in Python?\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2021-12-30\n\n\n\n\n\n\n\nExploring J, an array programming language\n\n\n\nj\n\n\n\n\n\n\n2021-12-29\n\n\n\n\n\n\n\nWhat makes code robust?\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\n\n\n\n2021-12-29\n\n\n\n\n\n\n\nA Taxonomy of Redaction\n\n\n\nredactionmodel\n\n\n\n\n\n\n2021-12-15\n\n\n\n\n\n\n\n73% accuracy for redaction object detection\n\n\n\nredactionmodel\n\ncomputervision\n\nprogressreport\n\n\n\n\n\n\n2021-12-11\n\n\n\n\n\n\n\nWhat is VFNet?\n\n\n\nredactionmodel\n\ncomputervision\n\n\n\n\n\n\n2021-11-30\n\n\n\n\n\n\n\nHow to annotate image data for object detection with Prodigy\n\n\n\nredactionmodel\n\ncomputervision\n\ndatalabelling\n\n\n\n\n\n\n2021-11-29\n\n\n\n\n\n\n\nLaunching a podcast about MLOps\n\n\n\nzenml\n\npodcast\n\nappearances\n\n\n\n\n\n\n2021-11-27\n\n\n\n\n\n\n\nCheck your security vulnerabilities with safety\n\n\n\nsecurity\n\ntools\n\ncalmcode\n\n\n\n\n\n\n2021-11-27\n\n\n\n\n\n\n\nHow to set and get environment variables using Python\n\n\n\npython\n\n\n\n\n\n\n2021-11-26\n\n\n\n\n\n\n\nentr: a tool to run commands when files change\n\n\n\ndebugging\n\ntesting\n\ntools\n\ncalmcode\n\n\n\n\n\n\n2021-11-25\n\n\n\n\n\n\n\nOn failure\n\n\n\ndebugging\n\nemotions\n\n\n\n\n\n\n2021-11-21\n\n\n\n\n\n\n\nSome things I learned about debugging\n\n\n\ndebugging\n\n\n\n\n\n\n2021-10-25\n\n\n\n\n\n\n\nReading Python Code\n\n\n\npython\n\nskillbuilding\n\n\n\n\n\n\n2021-09-18\n\n\n\n\n\n\n\nWriting Code\n\n\n\npython\n\nskillbuilding\n\n\n\n\n\n\n2021-09-18\n\n\n\n\n\n\n\nTensors all the way down\n\n\n\n\n\n2021-09-16\n\n\n\n\n\n\n\nA Baseline Python Development Setup\n\n\n\npython\n\ntools\n\n\n\n\n\n\n2021-09-14\n\n\n\n\n\n\n\nSix problems TFX was trying to solve in 2017\n\n\n\ntfx\n\ntensorflow\n\ngoogle\n\nmlops\n\npapers-i-read\n\n\n\n\n\n\n2021-09-11\n\n\n\n\n\n\n\nRetrieval Practice with fastai chapters 1 and 2\n\n\n\n\n\n2021-09-10\n\n\n\n\n\n\n\nHow to set a Jupyter notebook to auto-reload external libraries\n\n\n\njupyter\n\n\n\n\n\n\n2021-09-09\n\n\n\n\n\n\n\nA Baseline Understanding of MLOps\n\n\n\nmlops\n\n\n\n\n\n\n2021-09-08\n\n\n\n\n\n\n\nTraining a classifier to detect redacted documents with fastai\n\n\n\nfastai\n\nredactionmodel\n\ncomputervision\n\ndatalabelling\n\n\n\n\n\n\n2021-09-06\n\n\n\n\n\n\n\nOn the interpretability of models\n\n\n\nscience\n\ntech\n\ndeep-learning\n\ndata\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-28\n\n\n\n\n\n\n\nFastAI Lesson Zero: video notes\n\n\n\nproductivity\n\ndeep-learning\n\nstudy\n\nfastai\n\ndeeplearning\n\n\n\n\n\n\n2021-05-27\n\n\n\n\n\n\n\nGetting Out of the Intermediate Language Plateau: Arabic Edition / Principles\n\n\n\nlanguage\n\nuseful-tools\n\nstudy\n\nlearning\n\nskills\n\nlanguages\n\narabicintermediateplateau\n\narabic\n\n\n\n\n\n\n2021-05-26\n\n\n\n\n\n\n\nArthur Samuel and the ‘Frontier of Automation’\n\n\n\nscience\n\ndeep-learning\n\nstatistics\n\nmachinelearning\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-26\n\n\n\n\n\n\n\nTelling Cats from Dogs\n\n\n\ndeep-learning\n\nsoftware\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-26\n\n\n\n\n\n\n\nDeep Learning: Best in Show?\n\n\n\ncoding\n\ntech\n\nuseful-tools\n\ndeep-learning\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\nHeld back by misunderstanding\n\n\n\ntech\n\ndeep-learning\n\nacademia\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\nPDP: a precursor to modern neural networks?\n\n\n\ntech\n\ndeep-learning\n\nhistory\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\nRemoving Barriers: Deep Learning Edition\n\n\n\ntech\n\nuseful-tools\n\ndeep-learning\n\nsoftware\n\neducation\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\nRosenblatt’s Mark I Perceptron\n\n\n\ntech\n\ncoding\n\nuseful-tools\n\ndeep-learning\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\nSmall, unexpectedly powerful boxes\n\n\n\ntech\n\nuseful-tools\n\ndeep-learning\n\ntechnology\n\ndeeplearning\n\n\n\n\n\n\n2021-05-23\n\n\n\n\n\n\n\nA basic overview of how to use Handlebars\n\n\n\ncoding\n\nuseful-tools\n\njavascript\n\nweb\n\ntechnology\n\nlaunchschool\n\n\n\n\n\n\n2020-12-10\n\n\n\n\n\n\n\nHow to use jQuery and Handlebars in your website\n\n\n\ncoding\n\nuseful-tools\n\nweb\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-12-10\n\n\n\n\n\n\n\nUsing APIs to make things happen on the web\n\n\n\ncoding\n\njavascript\n\ninternet\n\nweb\n\ntechnology\n\nlaunchschool\n\n\n\n\n\n\n2020-12-04\n\n\n\n\n\n\n\nHow events drive programming for the web\n\n\n\ncoding\n\njavascript\n\ninternet\n\nprivacy\n\nlaunchschool\n\n\n\n\n\n\n2020-12-03\n\n\n\n\n\n\n\nThe Four Positions for Inserting Elements into the DOM\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-12-02\n\n\n\n\n\n\n\nDifferent ways of accessing the text contents of DOM nodes in JavaScript\n\n\n\ncoding\n\nweb\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-11-14\n\n\n\n\n\n\n\nUsing CSS selectors with JavaScript DOM methods\n\n\n\ncoding\n\njavascript\n\nweb\n\ntechnology\n\nlaunchschool\n\n\n\n\n\n\n2020-11-12\n\n\n\n\n\n\n\nTurning array-like objects into arrays with JavaScript\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-11-10\n\n\n\n\n\n\n\nUnderstanding the use cases for Closures in JavaScript\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-10-15\n\n\n\n\n\n\n\nWhat is Lexical Scope?\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-10-13\n\n\n\n\n\n\n\nWorking with JavaScript’s Object Prototype model\n\n\n\ncoding\n\nlaunchschool\n\njavascript\n\n\n\n\n\n\n2020-10-12\n\n\n\n\n\n\n\nSholay\n\n\n\nreviews\n\nbollywood\n\nfilm\n\n\n\n\n\n\n2020-06-14\n\n\n\n\n\n\n\nDil Dhadakne Do\n\n\n\nreviews\n\nbollywood\n\nfilm\n\n\n\n\n\n\n2020-06-11\n\n\n\n\n\n\n\nKapoor & Sons\n\n\n\nreviews\n\nbollywood\n\nfilm\n\n\n\n\n\n\n2020-06-11\n\n\n\n\n\n\n\nTwo Ruby patterns around map and reduce\n\n\n\ncoding\n\nruby\n\nlaunchschool\n\n\n\n\n\n\n2020-02-12\n\n\n\n\n\n\n\nHow the Internet Works\n\n\n\ntech\n\ncoding\n\ntechnology\n\nlaunchschool\n\ninternet\n\n\n\n\n\n\n2020-01-29\n\n\n\n\n\n\n\nNew book, new ways to order\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nbooks\n\npublishing\n\ntaliban\n\n\n\n\n\n\n2019-09-10\n\n\n\n\n\n\n\nMastery-based Learning with Launch School\n\n\n\npodcast\n\ntech\n\ncoding\n\nstudy\n\nlearning\n\neducation\n\ntechnology\n\n\n\n\n\n\n2019-03-21\n\n\n\n\n\n\n\nSources and Methods Does Technology\n\n\n\npodcast\n\ntech\n\ncoding\n\n\n\n\n\n\n2019-03-12\n\n\n\n\n\n\n\nUsing Ruby’s .digits method\n\n\n\ncoding\n\nruby\n\nlaunchschool\n\n\n\n\n\n\n2019-02-25\n\n\n\n\n\n\n\nSolid Study Habits for Coders\n\n\n\nproductivity\n\ncoding\n\nstudy\n\nruby\n\nanki\n\nlaunchschool\n\n\n\n\n\n\n2019-02-16\n\n\n\n\n\n\n\nUsing Ruby’s .zip method to combine arrays\n\n\n\ncoding\n\nruby\n\nlaunchschool\n\n\n\n\n\n\n2019-02-13\n\n\n\n\n\n\n\nPain: A Love Story\n\n\n\ngeneral\n\nmovement\n\nbody\n\npain\n\nhealth\n\n\n\n\n\n\n2019-01-31\n\n\n\n\n\n\n\nRaspberryLPIC: A New Series & Setup Steps\n\n\n\nraspberrylpic\n\ntech\n\ncoding\n\nraspberrypi\n\nlpic\n\nlinux\n\ntechnology\n\n\n\n\n\n\n2018-12-31\n\n\n\n\n\n\n\nEarning a certificate in Linux Essentials from LPI\n\n\n\ntech\n\nlinux\n\ntechnology\n\n\n\n\n\n\n2018-12-02\n\n\n\n\n\n\n\nIn Afghanistan, watching someone learn to code for the first time\n\n\n\nafghanistan\n\ntech\n\ncoding\n\nlearning\n\n\n\n\n\n\n2018-11-13\n\n\n\n\n\n\n\nLearn Persian / Farsi / Dari with Podcasts\n\n\n\nlanguage\n\nfarsi\n\npodcasts\n\nlanguages\n\npersian\n\ndari\n\n\n\n\n\n\n2018-11-07\n\n\n\n\n\n\n\nLearn Pashto with Podcasts\n\n\n\nafghanistan\n\nlanguage\n\npashto\n\npodcasts\n\nlanguages\n\n\n\n\n\n\n2018-11-05\n\n\n\n\n\n\n\nUsing Regex with Python to Find Strings\n\n\n\nproductivity\n\ncoding\n\nuseful-tools\n\npython\n\nregex\n\n\n\n\n\n\n2018-07-08\n\n\n\n\n\n\n\nPython Virtual Environments, Testing Environments and Markdown Strikethrough\n\n\n\ncoding\n\npython\n\n\n\n\n\n\n2018-06-24\n\n\n\n\n\n\n\nReal-World Go with Referenced Functions\n\n\n\ncoding\n\ngolang\n\n\n\n\n\n\n2018-06-23\n\n\n\n\n\n\n\nSplitting PDFs with Python\n\n\n\n\n\n2018-06-23\n\n\n\n\n\n\n\nDiagnosing Diabetes with Weka & Machine Learning\n\n\n\ntech\n\ncoding\n\nscience\n\nweka\n\ndata\n\nmachinelearning\n\n\n\n\n\n\n2018-06-18\n\n\n\n\n\n\n\nTable Tests in Go\n\n\n\ncoding\n\ntesting\n\ngolang\n\n\n\n\n\n\n2018-06-18\n\n\n\n\n\n\n\nFiguring out the Go testing ecosystem\n\n\n\ncoding\n\ntesting\n\ngolang\n\n\n\n\n\n\n2018-06-17\n\n\n\n\n\n\n\nFrom AI to to Brahms\n\n\n\nart\n\ntech\n\ncoding\n\nmusic\n\nmachinelearning\n\ntechnology\n\n\n\n\n\n\n2018-06-13\n\n\n\n\n\n\n\nMy new book: The Taliban Reader\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nbooks\n\nproductivity\n\ntaliban\n\ntalibansourcesproject\n\nwriting\n\n\n\n\n\n\n2018-06-05\n\n\n\n\n\n\n\nMachine Learning with Weka\n\n\n\ncoding\n\nuseful-tools\n\nweka\n\ngolang\n\nmachinelearning\n\nstatistics\n\n\n\n\n\n\n2018-06-04\n\n\n\n\n\n\n\nMy First Arabic Book Translation\n\n\n\nbooks\n\narabic\n\nguantánamo\n\ntranslation\n\nsamielhajbook\n\n\n\n\n\n\n2018-05-06\n\n\n\n\n\n\n\nFuzzy Searching and Foreign Name Recognition\n\n\n\nafghanistan\n\ncoding\n\npython\n\nresearch\n\n\n\n\n\n\n2018-01-31\n\n\n\n\n\n\n\nTweeting to the Void\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\nsocial-media\n\ntools\n\ntechnology\n\n\n\n\n\n\n2018-01-25\n\n\n\n\n\n\n\nInstalling PostgreSQL on a Mac\n\n\n\ncoding\n\nuseful-tools\n\npython\n\nsql\n\ndatabase\n\n\n\n\n\n\n2018-01-24\n\n\n\n\n\n\n\nMaking and shuffling lists in Python\n\n\n\ncoding\n\npython\n\nlists\n\nnumpy\n\n\n\n\n\n\n2018-01-23\n\n\n\n\n\n\n\nTabula for extracting table data from PDFs\n\n\n\ncoding\n\nuseful-tools\n\ntech\n\nafghanistan\n\nproductivity\n\ndata\n\nsoftware\n\ntechnology\n\n\n\n\n\n\n2018-01-17\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Homestay Edition\n\n\n\narabic\n\nlanguage\n\njordan\n\nlanguagelearner'sjournal\n\namman\n\ntaylorjournal\n\nlanguagecoaching\n\n\n\n\n\n\n2017-10-20\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Phone Calls and Timed Breaks\n\n\n\narabic\n\nlanguage\n\nlanguagelearner'sjournal\n\njordan\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-08-13\n\n\n\n\n\n\n\nSpaced Repetition Without Computers\n\n\n\nuseful-tools\n\nmemorisation\n\nsrsfoundation\n\nspaced-repetition\n\nanki\n\n\n\n\n\n\n2017-06-27\n\n\n\n\n\n\n\nThe book you need to read on bin Laden’s life post-2001\n\n\n\nbooks\n\njournalism\n\npakistan\n\n9/11\n\nhistory\n\nreview\n\n\n\n\n\n\n2017-06-26\n\n\n\n\n\n\n\nItalian Comprehension: Putting Myself On the Hook\n\n\n\nlanguage\n\nmemory\n\nstudy\n\nlearning\n\nvocabulary\n\nitalianchallenge\n\nlanguages\n\n\n\n\n\n\n2017-06-22\n\n\n\n\n\n\n\nRobert Caro’s Big Long Book\n\n\n\nbooks\n\nurban-planning\n\nreview\n\n\n\n\n\n\n2017-06-19\n\n\n\n\n\n\n\nHow We Forget\n\n\n\nbooks\n\nreview\n\n\n\n\n\n\n2017-06-04\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Meaningful Leisure\n\n\n\njordan\n\nincremental-elephant\n\narabic\n\nlanguagelearner'sjournal\n\nlanguage\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-05-24\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Increasing Spoken Fluency\n\n\n\narabic\n\nlanguage\n\nlanguagelearner'sjournal\n\nlanguages\n\nspeaking\n\nlanguagecoaching\n\nreading\n\n\n\n\n\n\n2017-04-30\n\n\n\n\n\n\n\nGuest Post: Can robots be language coaches?\n\n\n\nlanguage\n\nincremental-elephant\n\ncoaching\n\nlanguages\n\ncoachbot\n\nlanguagecoaching\n\n\n\n\n\n\n2017-04-29\n\n\n\n\n\n\n\nOn Reading in Arabic: The Evidence\n\n\n\narabic\n\nlanguage\n\nstudy\n\nlearning\n\nlanguages\n\nreading\n\n\n\n\n\n\n2017-04-17\n\n\n\n\n\n\n\nYou need to be reading more to get ahead in Arabic\n\n\n\narabic\n\nlanguage\n\nstudy\n\nlearning\n\nlanguages\n\nlanguagecoaching\n\nreading\n\n\n\n\n\n\n2017-04-15\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Independent Study\n\n\n\narabic\n\nlanguage\n\njordan\n\nlanguagelearner'sjournal\n\ncoaching\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-04-09\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Leaving Qasid\n\n\n\narabic\n\nlanguage\n\njordan\n\nlanguagelearner'sjournal\n\nlearning\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-03-20\n\n\n\n\n\n\n\nWhy do Arabic fonts appear so small? (and how to fix it)\n\n\n\narabic\n\nlanguage\n\ncoding\n\nlanguages\n\ndesign\n\n\n\n\n\n\n2017-03-12\n\n\n\n\n\n\n\nAudio Courses with Language Transfer\n\n\n\nlanguage\n\nlanguages\n\nweb\n\ntools\n\n\n\n\n\n\n2017-03-10\n\n\n\n\n\n\n\nEverything You Ever Wanted to Know About Guinea Pigs\n\n\n\ngeneral\n\nbibliography\n\npets\n\nreading\n\nanimals\n\n\n\n\n\n\n2017-03-06\n\n\n\n\n\n\n\nWalk Around the Block\n\n\n\nproductivity\n\nmovement\n\nwalking\n\nwork\n\n\n\n\n\n\n2017-03-03\n\n\n\n\n\n\n\nThree Podcast Recommendations\n\n\n\npodcast\n\npodcasts\n\n\n\n\n\n\n2017-02-26\n\n\n\n\n\n\n\nCore Language Learning Beliefs\n\n\n\narabic\n\nlanguage\n\nlessonslearnt\n\nlanguages\n\nlearning\n\nlanguagecoaching\n\n\n\n\n\n\n2017-02-23\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Keeping Pace\n\n\n\narabic\n\nlanguage\n\nincremental-elephant\n\nlanguagelearner'sjournal\n\nlearning\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-02-18\n\n\n\n\n\n\n\nHow to learn a language without a teacher\n\n\n\nlanguage\n\nincremental-elephant\n\ncoaching\n\nlanguages\n\nlearning\n\nlanguagecoaching\n\n\n\n\n\n\n2017-02-14\n\n\n\n\n\n\n\n‘Master Arabic’ is Out Today!\n\n\n\nbooks\n\narabic\n\nlanguage\n\nincremental-elephant\n\npublishing\n\nplateau-ebook\n\nlanguages\n\nmasterarabic\n\nlanguagecoaching\n\n\n\n\n\n\n2017-02-13\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Deepening My Studies\n\n\n\narabic\n\nlanguage\n\nlanguagelearner'sjournal\n\njordan\n\namman\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-02-04\n\n\n\n\n\n\n\nActual Fluency\n\n\n\narabic\n\nlanguage\n\npodcast\n\nlearning\n\nlanguages\n\ninterview\n\nlanguagecoaching\n\n\n\n\n\n\n2017-02-03\n\n\n\n\n\n\n\nVariations on a Pomodoro\n\n\n\nphd\n\nproductivity\n\npomodoros\n\nwork\n\ntranslation\n\n\n\n\n\n\n2017-01-31\n\n\n\n\n\n\n\nHow to learn a page of verbs\n\n\n\narabic\n\nlanguage\n\nmemory\n\nlanguages\n\nlearning\n\nlanguagecoaching\n\n\n\n\n\n\n2017-01-27\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Adjustments at the Deep End\n\n\n\narabic\n\nlanguage\n\nincremental-elephant\n\ncoaching\n\ntaylorjournal\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-01-23\n\n\n\n\n\n\n\nTrello to Markdown: a Chrome extension\n\n\n\nuseful-tools\n\ndata\n\ntools\n\ndatabase\n\ninternet\n\ntechnology\n\ntrello\n\n\n\n\n\n\n2017-01-17\n\n\n\n\n\n\n\nLanguage Learner’s Journal: Introducing Taylor\n\n\n\narabic\n\nlanguage\n\ncoaching\n\nincremental-elephant\n\ntaylorjournal\n\nlanguagecoaching\n\n\n\n\n\n\n2017-01-16\n\n\n\n\n\n\n\nClozeMaster: learn words in context\n\n\n\nlanguage\n\ntech\n\nmemory\n\nuseful-tools\n\nstudy\n\nlanguages\n\nlearning\n\ntools\n\n\n\n\n\n\n2017-01-15\n\n\n\n\n\n\n\nMaster Arabic: Behind the Cover\n\n\n\nart\n\narabic\n\nmasterarabic\n\n\n\n\n\n\n2017-01-15\n\n\n\n\n\n\n\nDeveloping for Android with Udacity\n\n\n\ntech\n\ncoding\n\nandroid\n\npythonsideproject\n\nudacity\n\ncoachbot\n\n\n\n\n\n\n2017-01-13\n\n\n\n\n\n\n\nMaster Arabic: What’s in the Premium Edition?\n\n\n\nbooks\n\narabic\n\nlanguage\n\nincremental-elephant\n\nmasterarabic\n\n\n\n\n\n\n2017-01-12\n\n\n\n\n\n\n\n10 podcasts to learn about data science and programming\n\n\n\npodcast\n\ncoding\n\nuseful-tools\n\ndata\n\npython\n\npodcasts\n\n\n\n\n\n\n2017-01-11\n\n\n\n\n\n\n\nResetting my base line: caffeine edition\n\n\n\nproductivity\n\nfocus\n\ntea\n\n\n\n\n\n\n2017-01-11\n\n\n\n\n\n\n\nClassical Arabic and a Sources and Methods Recap\n\n\n\narabic\n\nlanguage\n\npodcast\n\narabicpodcast\n\nliterarycaravans\n\n\n\n\n\n\n2017-01-09\n\n\n\n\n\n\n\nHow to best work with a language coach\n\n\n\nlanguage\n\nstudy\n\ncoaching\n\nlearning\n\nlanguages\n\nlanguagecoaching\n\n\n\n\n\n\n2017-01-08\n\n\n\n\n\n\n\nIntroducing CoachBot: Your Personal Language Taskmaster\n\n\n\ncoding\n\nuseful-tools\n\ntech\n\nlanguage\n\nproductivity\n\nlanguages\n\npythonsideproject\n\ncoachbot\n\n\n\n\n\n\n2017-01-06\n\n\n\n\n\n\n\nTaskpaper –&gt; Omnifocus\n\n\n\ntech\n\nuseful-tools\n\ntaskpaper\n\nmac\n\nomnifocus\n\ntechnology\n\n\n\n\n\n\n2017-01-05\n\n\n\n\n\n\n\nRandomise Your Learning\n\n\n\nlanguage\n\nstudy\n\nlanguages\n\nlearning\n\n\n\n\n\n\n2017-01-05\n\n\n\n\n\n\n\nThings We Control: On Internal vs External Goals\n\n\n\nlanguage\n\nphd\n\nproductivity\n\ngoals\n\nstudy\n\nlearning\n\nlanguages\n\nmasterarabic\n\n\n\n\n\n\n2017-01-04\n\n\n\n\n\n\n\nEverything You Need to Study Jordanian Arabic\n\n\n\nbooks\n\nlanguage\n\njordan\n\nstudy\n\nlearning\n\narabicpodcast\n\namman\n\nmasterarabic\n\nlanguages\n\narabic\n\n\n\n\n\n\n2017-01-03\n\n\n\n\n\n\n\nThe ways memory skills augment your life\n\n\n\nbooks\n\npodcast\n\nmemory\n\nhistory\n\nmemorisation\n\nlearning\n\n\n\n\n\n\n2017-01-02\n\n\n\n\n\n\n\nPet Peeve: Tech Switching\n\n\n\ntech\n\nuseful-tools\n\nsoftware\n\nmedia\n\nblogging\n\ntechnology\n\n\n\n\n\n\n2017-01-02\n\n\n\n\n\n\n\nNew Year, New Arabic-language Podcast\n\n\n\nlanguage\n\njordan\n\npodcast\n\narabicpodcast\n\namman\n\nlanguages\n\nmasterarabic\n\narabic\n\n\n\n\n\n\n2017-01-01\n\n\n\n\n\n\n\nPre-Order Now: ‘Master Arabic’\n\n\n\nbooks\n\nlanguage\n\nincremental-elephant\n\nstudy\n\nwriting\n\nlanguages\n\nmasterarabic\n\narabic\n\n\n\n\n\n\n2016-12-30\n\n\n\n\n\n\n\nDinner Party Decision Matrix: A Python Tool\n\n\n\ncoding\n\nuseful-tools\n\npython\n\ntools\n\nfood\n\n\n\n\n\n\n2016-12-29\n\n\n\n\n\n\n\nKnot 4: Empathy, Tech Scepticism and Climate Change\n\n\n\nknot\n\nenvironment\n\nlanguages\n\ntechnology\n\nclimate\n\nempathy\n\nreading\n\n\n\n\n\n\n2016-12-28\n\n\n\n\n\n\n\nTalking DevonThink with Gabe Weatherhead\n\n\n\npodcast\n\ntech\n\nuseful-tools\n\ndata\n\ndatabase\n\ndevonthink\n\ntechnology\n\nsocial-media\n\nsoftware\n\n\n\n\n\n\n2016-12-26\n\n\n\n\n\n\n\nSyria’s Revolution: The First Draft\n\n\n\nbooks\n\njournalism\n\nreview\n\nsyria\n\nhistory\n\n\n\n\n\n\n2016-12-25\n\n\n\n\n\n\n\nShare the Gift of Language Coaching\n\n\n\nbusiness\n\nlanguage\n\nincremental-elephant\n\n\n\n\n\n\n2016-12-23\n\n\n\n\n\n\n\nThe Best Books I Read in 2016\n\n\n\nbooks\n\nreview\n\n\n\n\n\n\n2016-12-21\n\n\n\n\n\n\n\nReading and the American Revolution\n\n\n\nbooks\n\nusa\n\nreview\n\nreading\n\n\n\n\n\n\n2016-12-21\n\n\n\n\n\n\n\nTwo Charities Needing Your Support\n\n\n\ngeneral\n\nfunding\n\ncharity\n\n\n\n\n\n\n2016-12-21\n\n\n\n\n\n\n\nDjango vs Flask\n\n\n\ncoding\n\nuseful-tools\n\nplateau-ebook\n\nthinking\n\n\n\n\n\n\n2016-12-20\n\n\n\n\n\n\n\nDevonThink Resurgent\n\n\n\ntech\n\nuseful-tools\n\ndata\n\nlearning\n\ndatabase\n\ndevonthink\n\nsoftware\n\n\n\n\n\n\n2016-12-19\n\n\n\n\n\n\n\nSeeking Python Code Mentor\n\n\n\nlanguage\n\ncoding\n\nstudy\n\nlanguages\n\nlearning\n\n\n\n\n\n\n2016-12-18\n\n\n\n\n\n\n\nKnot 3: Encryption, Race and Tunnels\n\n\n\njournalism\n\nknot\n\ntech\n\nmemory\n\nbooks\n\nrace\n\nmedia\n\n\n\n\n\n\n2016-12-17\n\n\n\n\n\n\n\nInto Eternity: the construction of Onkalo’s tunnels\n\n\n\nenvironment\n\ngeneral\n\nscience\n\nconstruction\n\nfinland\n\ndesign\n\nnuclearwaste\n\n\n\n\n\n\n2016-12-16\n\n\n\n\n\n\n\nBroken Pots\n\n\n\nart\n\ngeneral\n\njapan\n\nculture\n\n\n\n\n\n\n2016-12-15\n\n\n\n\n\n\n\nNotification Zero\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\niphone\n\ntools\n\nwriting\n\ntechnology\n\ndistraction\n\nfocus\n\n\n\n\n\n\n2016-12-13\n\n\n\n\n\n\n\nDaddybot\n\n\n\ntech\n\nuseful-tools\n\ntravel\n\ntechnology\n\n\n\n\n\n\n2016-12-11\n\n\n\n\n\n\n\nHighlights + DevonThink = Pretty Great\n\n\n\nbooks\n\nproductivity\n\ntech\n\nuseful-tools\n\ndata\n\nreview\n\ndatabase\n\nresearch\n\ntechnology\n\nsoftware\n\nreading\n\n\n\n\n\n\n2016-12-09\n\n\n\n\n\n\n\nKnot 2: Translation as Trauma, Taxis and Artificial Intelligence\n\n\n\nknot\n\njordan\n\ngeneral\n\namman\n\nsyria\n\nlanguages\n\nartificialintelligence\n\ntranslation\n\nwar\n\nconflict\n\n\n\n\n\n\n2016-12-09\n\n\n\n\n\n\n\nThe Two Books Every Intermediate Arabic Student Needs to Read\n\n\n\nbooks\n\nlanguage\n\nincremental-elephant\n\nstudy\n\nlearning\n\nplateau-ebook\n\nlanguages\n\narabic\n\n\n\n\n\n\n2016-12-09\n\n\n\n\n\n\n\nNutritional Density\n\n\n\nproductivity\n\ngeneral\n\nfood\n\nnutrition\n\nconsumption\n\nsocial-media\n\ninformation\n\n\n\n\n\n\n2016-12-08\n\n\n\n\n\n\n\nThe Taliban on the Clintons (May 2000)\n\n\n\nafghanistan\n\njournalism\n\narchives\n\ntaliban\n\ntalibansourcesproject\n\n\n\n\n\n\n2016-12-07\n\n\n\n\n\n\n\nClimbing Routes Without Numbers\n\n\n\nmovement\n\njordan\n\nclimbing\n\namman\n\nmental\n\n\n\n\n\n\n2016-12-02\n\n\n\n\n\n\n\nReading the Taliban: Himal Magazine article\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nprimarysources\n\ntaliban\n\nwriting\n\n\n\n\n\n\n2016-12-02\n\n\n\n\n\n\n\nKnot 1: Solitary Confinement & Digital Security\n\n\n\nbooks\n\nknot\n\ntech\n\nscience\n\nlinks\n\ndata\n\nreading\n\n\n\n\n\n\n2016-12-02\n\n\n\n\n\n\n\nPositive feedback on my Memory Skills course\n\n\n\nbusiness\n\ngeneral\n\nincremental-elephant\n\nmemory\n\nlearning\n\nmemorisation\n\nislam\n\n99-names-course\n\n\n\n\n\n\n2016-12-02\n\n\n\n\n\n\n\nLooking the Wrong Way: Faces in ‘Arrival’\n\n\n\nart\n\nculture\n\nfilm\n\nnarrative\n\n\n\n\n\n\n2016-11-29\n\n\n\n\n\n\n\nThe Inner Game: How a Perspective Shift Can Radically Improve Your Performance\n\n\n\nbooks\n\nproductivity\n\nclimbing\n\nmovement\n\nlearning\n\nskills\n\nsport\n\npsychology\n\nmental\n\n\n\n\n\n\n2016-11-23\n\n\n\n\n\n\n\nLearning a Language? Try a New Approach\n\n\n\nlanguage\n\nstudy\n\nlearning\n\nskills\n\nlanguages\n\ndutch\n\narabic\n\ndari\n\npashto\n\n\n\n\n\n\n2016-11-22\n\n\n\n\n\n\n\nDifferent Kinds of Climbing\n\n\n\nmovement\n\ntravel\n\nclimbing\n\nfear\n\nkuwait\n\nskills\n\n\n\n\n\n\n2016-11-20\n\n\n\n\n\n\n\nEncrypt Your Dropbox Files\n\n\n\ntech\n\nuseful-tools\n\nsoftware\n\nsecurity\n\ntechnology\n\n\n\n\n\n\n2016-11-17\n\n\n\n\n\n\n\nKukicha or Twig Tea\n\n\n\ngeneral\n\ntea\n\n\n\n\n\n\n2016-11-14\n\n\n\n\n\n\n\nMusic, Sound & Technology\n\n\n\ntech\n\nbooks\n\nmusic\n\ntechnology\n\npodcasts\n\n\n\n\n\n\n2016-11-12\n\n\n\n\n\n\n\nSquarespace Goes SSL\n\n\n\ntech\n\nuseful-tools\n\nsecurity\n\ninternet\n\nweb\n\ntechnology\n\nblogging\n\n\n\n\n\n\n2016-11-11\n\n\n\n\n\n\n\nFinishing GMB’s Elements\n\n\n\nmovement\n\nbody\n\nsport\n\ngmb\n\nexercise\n\n\n\n\n\n\n2016-11-05\n\n\n\n\n\n\n\nClimbing Fuheis: Two and a Half Ascents\n\n\n\nmovement\n\njordan\n\nclimbing\n\n\n\n\n\n\n2016-11-05\n\n\n\n\n\n\n\nPython Side-Project: Approach\n\n\n\ntech\n\ncoding\n\nlanguages\n\ntechnology\n\npythonsideproject\n\n\n\n\n\n\n2016-11-05\n\n\n\n\n\n\n\nScratching Below the Surface with David Heinemeier Hansson\n\n\n\ntech\n\nproductivity\n\ncoding\n\nlearning\n\ncreativity\n\nskills\n\ncuriosity\n\nwork\n\n\n\n\n\n\n2016-10-30\n\n\n\n\n\n\n\nA Greener Technological Footprint\n\n\n\nenvironment\n\ntech\n\ntechnology\n\nweb\n\necology\n\n\n\n\n\n\n2016-10-28\n\n\n\n\n\n\n\nKael Weston on Sources and Methods\n\n\n\nafghanistan\n\nlanguage\n\npodcast\n\niraq\n\npolitics\n\nwar\n\nconflict\n\nmilitary\n\n\n\n\n\n\n2016-10-27\n\n\n\n\n\n\n\nOn Completing Udacity’s Intro to Programming Nanodegree\n\n\n\ncoding\n\nuseful-tools\n\nlearning\n\nskills\n\n\n\n\n\n\n2016-10-24\n\n\n\n\n\n\n\nLetting Go\n\n\n\nclimbing\n\nfear\n\nskills\n\n\n\n\n\n\n2016-10-21\n\n\n\n\n\n\n\nDifferent Uses for Spaced Repetition\n\n\n\nlanguage\n\ncoding\n\nuseful-tools\n\nstudy\n\ntools\n\nlearning\n\nmemorisation\n\nwork\n\ntechnology\n\nspaced-repetition\n\n\n\n\n\n\n2016-10-18\n\n\n\n\n\n\n\nBots: Part of the Future of Language Learning?\n\n\n\nlanguage\n\nuseful-tools\n\nlanguages\n\nlearning\n\ntechnology\n\n\n\n\n\n\n2016-10-17\n\n\n\n\n\n\n\nFundamentals Versus Hacks\n\n\n\nbooks\n\nproductivity\n\nlearning\n\nwork\n\n\n\n\n\n\n2016-10-16\n\n\n\n\n\n\n\nSkills Development: Foundations\n\n\n\nbooks\n\nproductivity\n\ncoding\n\nlearning\n\nmindset\n\nskills\n\n\n\n\n\n\n2016-10-15\n\n\n\n\n\n\n\nOn the demise of afghanistannewscenter.com\n\n\n\nafghanistan\n\njournalism\n\nresearch\n\narchives\n\ninternet\n\n\n\n\n\n\n2016-10-13\n\n\n\n\n\n\n\nKeep Moving\n\n\n\nclimbing\n\nmovement\n\n\n\n\n\n\n2016-10-13\n\n\n\n\n\n\n\nSeeing The Forest But For The Trees: On Exist.io\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\ndata\n\nwork\n\nbeeminder\n\n\n\n\n\n\n2016-10-11\n\n\n\n\n\n\n\nComing Soon: The Taliban Reader\n\n\n\nbooks\n\nafghanistan\n\ntalibansourcesproject\n\ntaliban-poetry\n\ntaliban\n\n\n\n\n\n\n2016-10-05\n\n\n\n\n\n\n\nLearning Without Seams\n\n\n\ngeneral\n\nclimbing\n\nstudy\n\nlearning\n\nskills\n\nwriting\n\n\n\n\n\n\n2016-09-29\n\n\n\n\n\n\n\nSustainable Climbing: Three Book Reviews\n\n\n\nbooks\n\nclimbing\n\nreviews\n\necology\n\n\n\n\n\n\n2016-09-28\n\n\n\n\n\n\n\nLessons Learnt: Language Study Habits That Work\n\n\n\nlanguage\n\nstudy\n\nlessonslearnt\n\nlanguages\n\nlearning\n\n\n\n\n\n\n2016-09-27\n\n\n\n\n\n\n\nTurn Off Facebook’s News Feed\n\n\n\nproductivity\n\nuseful-tools\n\nfacebook\n\ntools\n\ntechnology\n\n\n\n\n\n\n2016-09-24\n\n\n\n\n\n\n\nPhD Tools: Tea\n\n\n\nphd\n\nproductivity\n\nphdtoolsseries\n\ntea\n\n\n\n\n\n\n2016-09-19\n\n\n\n\n\n\n\nCoding Chronicles: Failure\n\n\n\ntech\n\ncoding\n\nudacity\n\ntechnology\n\n\n\n\n\n\n2016-09-17\n\n\n\n\n\n\n\nPhD Tools: Sleep and Movement to Nourish the Body\n\n\n\nphd\n\nproductivity\n\nmovement\n\n\n\n\n\n\n2016-09-16\n\n\n\n\n\n\n\nPhD Tools: Goodreads for Cross-Pollination\n\n\n\nbooks\n\nphd\n\nproductivity\n\nuseful-tools\n\nresearch\n\ngoodreads\n\nphdtoolsseries\n\nreading\n\n\n\n\n\n\n2016-09-15\n\n\n\n\n\n\n\nPhD Tools: ‘Always return to your primary sources’\n\n\n\nphd\n\nproductivity\n\nwriting\n\nprimarysources\n\nresearch\n\nphdtoolsseries\n\n\n\n\n\n\n2016-09-14\n\n\n\n\n\n\n\nPhD Tools: Freewriting and Journalling to Think Through your Work\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\ntools\n\nwriting\n\nresearch\n\nphdtoolsseries\n\nthinking\n\n\n\n\n\n\n2016-09-13\n\n\n\n\n\n\n\nPhD Tools: Pen and Paper\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\ntools\n\nwriting\n\nresearch\n\nphdtoolsseries\n\nthinking\n\n\n\n\n\n\n2016-09-13\n\n\n\n\n\n\n\nPhD Tools: Vitamin-R and the Pomodoro Technique for Getting Going\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\ntools\n\nwork\n\nwriting\n\ntechnology\n\nphdtoolsseries\n\n\n\n\n\n\n2016-09-12\n\n\n\n\n\n\n\nPhD Tools: The Secret to Finishing Your PhD\n\n\n\nphd\n\nproductivity\n\nphdtoolsseries\n\nwork\n\nwriting\n\n\n\n\n\n\n2016-09-09\n\n\n\n\n\n\n\nStarting the Spaced Repetition Foundation\n\n\n\njournalism\n\nuseful-tools\n\ngeneral\n\ntech\n\nafghanistan\n\nbooks\n\nlanguage\n\nproductivity\n\nstudy\n\nlearning\n\nanki\n\nlanguages\n\nsrsfoundation\n\nspaced-repetition\n\n\n\n\n\n\n2016-09-08\n\n\n\n\n\n\n\nPhD Tools: Omnifocus for Managing your Non-PhD Life\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nmac\n\nomnifocus\n\nphdtoolsseries\n\n\n\n\n\n\n2016-09-07\n\n\n\n\n\n\n\nPhD Tools: Backup Systems for Staving off Sadness\n\n\n\ntech\n\nuseful-tools\n\nphd\n\ntools\n\ntechnology\n\nphdtoolsseries\n\nstorage\n\nbackup\n\n\n\n\n\n\n2016-09-06\n\n\n\n\n\n\n\nPhD Tools: Turn Off the Internet with Freedom\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nwriting\n\ninternet\n\nresearch\n\nfreedomapp\n\nphdtoolsseries\n\n\n\n\n\n\n2016-09-05\n\n\n\n\n\n\n\nSvifnökkvinn minn er fullur af álum: Lessons in Icelandic\n\n\n\nlanguage\n\nstudy\n\nlearning\n\nlanguages\n\nicelandic\n\npronunciation\n\n\n\n\n\n\n2016-09-04\n\n\n\n\n\n\n\nPhD Tools: RescueTime for Time Tracking\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\nrescuetime\n\nwriting\n\nresearch\n\nphdtoolsseries\n\n\n\n\n\n\n2016-09-02\n\n\n\n\n\n\n\nExistential Battles: Climbing in Amman\n\n\n\njordan\n\nclimbing\n\nsport\n\nmovement\n\nexercise\n\n\n\n\n\n\n2016-09-02\n\n\n\n\n\n\n\nPhD Tools: Mellel for Layout and Final Presentation\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nmellel\n\ntools\n\nwriting\n\nresearch\n\nphdtoolsseries\n\n\n\n\n\n\n2016-09-01\n\n\n\n\n\n\n\nPhD Tools: Bookends for Managing References\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nwriting\n\nresearch\n\ntechnology\n\nphdtoolsseries\n\nbookends\n\n\n\n\n\n\n2016-08-31\n\n\n\n\n\n\n\nPhD Tools: Scrivener for Writing Long Things\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\nresearch\n\nphdtoolsseries\n\nwriting\n\ntools\n\n\n\n\n\n\n2016-08-30\n\n\n\n\n\n\n\nPhD Tools: Save your web links with Pinboard\n\n\n\nproductivity\n\nuseful-tools\n\ninternet\n\nweb\n\npinboard\n\nphdtoolsseries\n\n\n\n\n\n\n2016-08-29\n\n\n\n\n\n\n\nPhD Tools: DevonThink for File Storage and Discovery\n\n\n\nbooks\n\nphd\n\nproductivity\n\nuseful-tools\n\ndata\n\ndatabase\n\nwriting\n\ndevonthink\n\nresearch\n\nphdtoolsseries\n\nstorage\n\nreading\n\n\n\n\n\n\n2016-08-26\n\n\n\n\n\n\n\nPhD Tools: Think better with Tinderbox\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\ntools\n\nwriting\n\ntinderbox\n\ntechnology\n\nphdtoolsseries\n\nthinking\n\n\n\n\n\n\n2016-08-25\n\n\n\n\n\n\n\nPhD Tools: Visualise Structure and Kanban Flow with Trello\n\n\n\nphd\n\nproductivity\n\nuseful-tools\n\nstructure\n\nwriting\n\ntrello\n\nphdtoolsseries\n\n\n\n\n\n\n2016-08-24\n\n\n\n\n\n\n\nWalking with Words\n\n\n\nproductivity\n\ntech\n\nuseful-tools\n\nmovement\n\npodcasts\n\naudible\n\nhealth\n\n\n\n\n\n\n2016-08-23\n\n\n\n\n\n\n\nPhD Tools: Beeminder\n\n\n\nphd\n\nproductivity\n\ntech\n\nuseful-tools\n\ntools\n\naccountability\n\ntechnology\n\nbeeminder\n\nphdtoolsseries\n\n\n\n\n\n\n2016-08-22\n\n\n\n\n\n\n\nLearn all the districts of Afghanistan with Anki!\n\n\n\nafghanistan\n\njournalism\n\ntech\n\nuseful-tools\n\nlearning\n\nanki\n\nmemorisation\n\ngeography\n\n\n\n\n\n\n2016-08-21\n\n\n\n\n\n\n\nWalking Amman\n\n\n\njordan\n\ntech\n\nmaps\n\ndata\n\ntravel\n\namman\n\ntechnology\n\nwalking\n\n\n\n\n\n\n2016-08-19\n\n\n\n\n\n\n\nJordan Diaries: Start with Geography\n\n\n\njordan\n\ntravel\n\nlearning\n\nmemorisation\n\namman\n\n\n\n\n\n\n2016-08-18\n\n\n\n\n\n\n\nRemove Your Colour\n\n\n\nproductivity\n\ntech\n\nhack\n\niphone\n\ntechnology\n\n\n\n\n\n\n2016-08-18\n\n\n\n\n\n\n\nOn Learning New Skills, #showyourwork and other housekeeping\n\n\n\nbooks\n\njordan\n\npodcast\n\ncoding\n\ntsp\n\nskills\n\nwriting\n\nlanguages\n\n\n\n\n\n\n2016-08-15\n\n\n\n\n\n\n\nChondrichthyan Fun: a review of Shark MOOC\n\n\n\ngeneral\n\nscience\n\necology\n\nenvironment\n\nbiology\n\nsharks\n\nnature\n\n\n\n\n\n\n2016-07-24\n\n\n\n\n\n\n\nHow to become a memorisation and language ninja\n\n\n\nlanguage\n\ngeneral\n\nmemorisation\n\nlanguages\n\nislam\n\nskills\n\n\n\n\n\n\n2016-05-25\n\n\n\n\n\n\n\nReading the Afghan Taliban: 67 Sources You Should Be Studying\n\n\n\nafghanistan\n\nfirst-draft-publishing\n\nbooks\n\ntalibansourcesproject\n\nprimarysources\n\ndocuments\n\ntaliban\n\n\n\n\n\n\n2015-12-26\n\n\n\n\n\n\n\nThe Best Books I Read in 2015\n\n\n\nbooks\n\n\n\n\n\n\n2015-12-16\n\n\n\n\n\n\n\n‘Obedience to the Amir’, or how the Afghan Taliban govern\n\n\n\nbooks\n\npakistan\n\nfirst-draft-publishing\n\nafghanistan\n\npublishing\n\ntaliban\n\n\n\n\n\n\n2015-11-29\n\n\n\n\n\n\n\nOn Untangling Syria’s Socially Mediated War\n\n\n\nlanguage\n\njournalism\n\ntech\n\nresearch\n\nsyria\n\nsocial-media\n\n\n\n\n\n\n2015-11-22\n\n\n\n\n\n\n\nUpcoming Maniac Week\n\n\n\nphd\n\nuseful-tools\n\nproductivity\n\nwork\n\nmaniacweek\n\n\n\n\n\n\n2015-11-15\n\n\n\n\n\n\n\nMisquoting Mohammad meets Sources & Methods\n\n\n\npodcast\n\nislam\n\n\n\n\n\n\n2015-11-08\n\n\n\n\n\n\n\nVanishing Interest in Afghanistan\n\n\n\nafghanistan\n\njournalism\n\ndata\n\nmedia\n\n\n\n\n\n\n2015-11-01\n\n\n\n\n\n\n\nFour Colours\n\n\n\npodcast\n\nuseful-tools\n\nnotes\n\ntools\n\npens\n\nwriting\n\n\n\n\n\n\n2015-10-13\n\n\n\n\n\n\n\nTaliban public punishments, 1996–2001\n\n\n\nafghanistan\n\njournalism\n\ndata\n\nresearch\n\nmedia\n\ntaliban\n\n\n\n\n\n\n2015-09-24\n\n\n\n\n\n\n\nEcolinguism and the ethics of learning new languages\n\n\n\nlanguage\n\nuseful-tools\n\nlanguages\n\nmedia\n\n\n\n\n\n\n2015-09-18\n\n\n\n\n\n\n\nSources and Methods: Back for Season 2\n\n\n\npodcast\n\ngeneral\n\n\n\n\n\n\n2015-09-09\n\n\n\n\n\n\n\nHow to Survive Middlebury’s Arabic Summer School Programme\n\n\n\nlanguage\n\nuseful-tools\n\nlanguages\n\narabic\n\nlearning\n\n\n\n\n\n\n2015-09-02\n\n\n\n\n\n\n\nAFP covers the Taliban Sources Project\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\ntsp\n\nresearch\n\ntaliban\n\n\n\n\n\n\n2015-08-28\n\n\n\n\n\n\n\nArabic Language Update: I did it! (Almost)\n\n\n\nlanguage\n\nuseful-tools\n\nstudy\n\nlearning\n\ntools\n\nmiddlebury\n\nlanguages\n\narabic\n\n\n\n\n\n\n2015-06-07\n\n\n\n\n\n\n\nHow I use Goodreads to pick what I read\n\n\n\nbooks\n\ntech\n\nuseful-tools\n\ndata\n\nquantifiedself\n\ntechnology\n\n\n\n\n\n\n2015-03-21\n\n\n\n\n\n\n\nApocalypse Then: a short review of Filiu’s ‘Apocalypse in Islam’ (2011)\n\n\n\nbooks\n\nislam\n\napocalypse\n\n\n\n\n\n\n2015-02-25\n\n\n\n\n\n\n\nNorth Waziristan: A Reading List\n\n\n\nbooks\n\njournalism\n\npakistan\n\nwaziristan\n\nbibliography\n\nreading\n\n\n\n\n\n\n2014-12-31\n\n\n\n\n\n\n\nSome Books and Other Things from 2014\n\n\n\nbooks\n\n\n\n\n\n\n2014-12-22\n\n\n\n\n\n\n\nNew book: An Educator’s Tale\n\n\n\nbooks\n\nfirst-draft-publishing\n\nafghanistan\n\n\n\n\n\n\n2014-12-10\n\n\n\n\n\n\n\nNote-Taking Jujitsu, Or How I Make Sense Of What I Read\n\n\n\nbooks\n\npodcast\n\ntech\n\nuseful-tools\n\n\n\n\n\n\n2014-10-24\n\n\n\n\n\n\n\nSources & Methods: Podcast Follow-Up\n\n\n\npodcast\n\nuseful-tools\n\n\n\n\n\n\n2014-10-23\n\n\n\n\n\n\n\nOur First Publication: ‘I Am Akbar Agha’, Memoir of a Taliban Insider\n\n\n\nafghanistan\n\nbooks\n\n\n\n\n\n\n2014-09-16\n\n\n\n\n\n\n\nSources & Methods, or why I started a podcast\n\n\n\npodcast\n\n\n\n\n\n\n2014-09-07\n\n\n\n\n\n\n\nAn Ankified Urdu Frequency Dictionary\n\n\n\nlanguage\n\npakistan\n\nuseful-tools\n\n\n\n\n\n\n2014-09-05\n\n\n\n\n\n\n\nTwo new co-authored reports on Afghanistan\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2014-08-09\n\n\n\n\n\n\n\nHow to learn a language to fluency: interview with Gabe Wyner\n\n\n\nbooks\n\nlanguage\n\npodcast\n\n\n\n\n\n\n2014-08-05\n\n\n\n\n\n\n\nThe Best Books I Read in 2013\n\n\n\nbooks\n\n\n\n\n\n\n2013-12-27\n\n\n\n\n\n\n\nTaliban Time Travel, or How Our Understanding Is Almost Always Two Years Old\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2013-09-23\n\n\n\n\n\n\n\nBook of the Week: ‘Al-Shabab in Somalia’\n\n\n\nafghanistan\n\nbooks\n\nsomalia\n\n\n\n\n\n\n2013-07-20\n\n\n\n\n\n\n\nTranslators Sought – Job Opening\n\n\n\nuncategorized\n\n\n\n\n\n\n2013-04-20\n\n\n\n\n\n\n\nLearning to Code\n\n\n\ntech\n\n\n\n\n\n\n2013-02-04\n\n\n\n\n\n\n\nFollowing Pakistan’s Elections\n\n\n\njournalism\n\npakistan\n\n\n\n\n\n\n2013-01-22\n\n\n\n\n\n\n\nISAF’s First Fifteen Days\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2013-01-15\n\n\n\n\n\n\n\nA Jedi-Mind Trick and Three Other Approaches to Learning Vocabulary\n\n\n\nlanguage\n\nuseful-tools\n\n\n\n\n\n\n2013-01-13\n\n\n\n\n\n\n\nFive Things I Wish Someone Had Told Me About Learning Languages\n\n\n\nlanguage\n\nstudy\n\nlanguages\n\n\n\n\n\n\n2013-01-07\n\n\n\n\n\n\n\nUseful Tools: Pinboard\n\n\n\ntech\n\nuseful-tools\n\n\n\n\n\n\n2013-01-02\n\n\n\n\n\n\n\nSome Things I Read\n\n\n\nbooks\n\ngeneral\n\n\n\n\n\n\n2012-12-29\n\n\n\n\n\n\n\nCatching Up: Poetry of the Taliban\n\n\n\nafghanistan\n\nbooks\n\n\n\n\n\n\n2012-12-22\n\n\n\n\n\n\n\nCatching Up: An Enemy We Created\n\n\n\nafghanistan\n\nbooks\n\n\n\n\n\n\n2012-12-16\n\n\n\n\n\n\n\nCatching Up: AAN Report on ISAF Night Raids\n\n\n\nafghanistan\n\n\n\n\n\n\n2012-12-10\n\n\n\n\n\n\n\nWriting the history of the Taliban movement\n\n\n\nuncategorized\n\n\n\n\n\n\n2012-11-26\n\n\n\n\n\n\n\ntalibantwitterfight: The News Story That Wasn’t\n\n\n\nafghanistan\n\njournalism\n\ntech\n\n\n\n\n\n\n2011-12-28\n\n\n\n\n\n\n\nEntropy and insurgent radicalisation: an ISAF goal?\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2011-12-07\n\n\n\n\n\n\n\nVietnam’s Kill-Capture Raids\n\n\n\nafghanistan\n\nbooks\n\n\n\n\n\n\n2011-09-19\n\n\n\n\n\n\n\nThose ‘40 al-Qaeda insurgents’…\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2011-09-13\n\n\n\n\n\n\n\nAn Appeal for Funding\n\n\n\nafghanistan\n\nphd\n\n\n\n\n\n\n2011-08-20\n\n\n\n\n\n\n\nISAF Press Release Word Clouds\n\n\n\nafghanistan\n\njournalism\n\nphd\n\n\n\n\n\n\n2011-05-17\n\n\n\n\n\n\n\nMore data on ‘Kill-Capture’ Raids\n\n\n\nafghanistan\n\njournalism\n\ntech\n\n\n\n\n\n\n2011-05-16\n\n\n\n\n\n\n\nKandahar Prison Escape: the Taliban’s Tale\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2011-05-15\n\n\n\n\n\n\n\nTalQaeda: the Timeline\n\n\n\nafghanistan\n\ntech\n\nphd\n\ngeneral\n\n\n\n\n\n\n2011-05-13\n\n\n\n\n\n\n\nThe Afghan Taliban react to bin Laden’s death\n\n\n\nafghanistan\n\n\n\n\n\n\n2011-05-03\n\n\n\n\n\n\n\nThe Petraeus Effect\n\n\n\nafghanistan\n\n\n\n\n\n\n2011-05-01\n\n\n\n\n\n\n\n‘The Kill Team’\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\n\n\n\n\n\n2011-04-03\n\n\n\n\n\n\n\nAfghanistan’s Child Soldiers\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2011-01-30\n\n\n\n\n\n\n\nHelmand Refugee Appeal Update - Distribution Day\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\n\n\n\n\n\n2011-01-21\n\n\n\n\n\n\n\n‘An Enemy We Created’: the website\n\n\n\nafghanistan\n\nbooks\n\ngeneral\n\n\n\n\n\n\n2011-01-10\n\n\n\n\n\n\n\nUPDATED: An Appeal\n\n\n\nafghanistan\n\njournalism\n\ntravel\n\ngeneral\n\n\n\n\n\n\n2010-12-28\n\n\n\n\n\n\n\nDeedee Derksen picks her 2010 books\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\ngeneral\n\n\n\n\n\n\n2010-12-22\n\n\n\n\n\n\n\nReal People as Agents\n\n\n\nafghanistan\n\nphd\n\nbooks\n\ngeneral\n\n\n\n\n\n\n2010-12-22\n\n\n\n\n\n\n\nThe Best Books of 2010 (UPDATED)\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\ngeneral\n\n\n\n\n\n\n2010-12-18\n\n\n\n\n\n\n\nOpen Letter: The Response (UPDATED)\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\n\n\n\n\n\n2010-12-14\n\n\n\n\n\n\n\nAn Open Letter to President Obama\n\n\n\nafghanistan\n\njournalism\n\ngeneral\n\n\n\n\n\n\n2010-12-11\n\n\n\n\n\n\n\nNo Comment: McChrystal and Petraeus\n\n\n\nafghanistan\n\ngeneral\n\n\n\n\n\n\n2010-12-05\n\n\n\n\n\n\n\nTaliban Realism over the September 11 Attacks\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-11-26\n\n\n\n\n\n\n\nForeign fighters down south?\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2010-11-23\n\n\n\n\n\n\n\nPetraeus, Lisbon and the Great PR Push\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2010-11-22\n\n\n\n\n\n\n\n‘Fly Freely’ - Afghan Women’s Poetry\n\n\n\nafghanistan\n\npoetry\n\n\n\n\n\n\n2010-11-13\n\n\n\n\n\n\n\nMarie Colvin in Kandahar for the Sunday Times\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2010-11-07\n\n\n\n\n\n\n\nIrish Parallels\n\n\n\nbooks\n\ngeneral\n\n\n\n\n\n\n2010-07-07\n\n\n\n\n\n\n\nHope Is Not A Strategy\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-07-03\n\n\n\n\n\n\n\nKandahar Timeline 1979-2010\n\n\n\nafghanistan\n\ntech\n\ngeneral\n\n\n\n\n\n\n2010-06-21\n\n\n\n\n\n\n\nOn Experts\n\n\n\ngeneral\n\n\n\n\n\n\n2010-05-22\n\n\n\n\n\n\n\nKandahar Portraits\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2010-05-14\n\n\n\n\n\n\n\nFrom ‘Ghazal’ by Shin Gul Aajiz\n\n\n\nafghanistan\n\ngeneral\n\ntaliban-poetry\n\n\n\n\n\n\n2010-05-01\n\n\n\n\n\n\n\nKandahar Chronology (September 2001-October 2009)\n\n\n\nafghanistan\n\ngeneral\n\n\n\n\n\n\n2010-04-29\n\n\n\n\n\n\n\nAsk the Scholars: when and where was Mullah Mohammad Omar born?\n\n\n\nafghanistan\n\ntaliban\n\n\n\n\n\n\n2010-04-26\n\n\n\n\n\n\n\nNew(ish) Kandahar Blogs\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-25\n\n\n\n\n\n\n\nJere van Dyk’s ‘Captive’\n\n\n\nafghanistan\n\njournalism\n\nbooks\n\n\n\n\n\n\n2010-04-24\n\n\n\n\n\n\n\nKandahar’s Electricity Problems\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-23\n\n\n\n\n\n\n\nMullah Omar captured?\n\n\n\nafghanistan\n\ngeneral\n\n\n\n\n\n\n2010-04-19\n\n\n\n\n\n\n\nKandahar Survey\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-18\n\n\n\n\n\n\n\n‘So how is it?’\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-16\n\n\n\n\n\n\n\nCivilian Casualties from Zheray\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-12\n\n\n\n\n\n\n\nHearts and Minds\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-12\n\n\n\n\n\n\n\noverheardinkandahar\n\n\n\nafghanistan\n\n\n\n\n\n\n2010-04-12\n\n\n\n\n\n\n\nBack Home\n\n\n\nafghanistan\n\ngeneral\n\n\n\n\n\n\n2010-04-11\n\n\n\n\n\n\n\nReal People, Real War\n\n\n\nafghanistan\n\njournalism\n\n\n\n\n\n\n2010-02-08\n\n\n\n\n\n\n\nFT does Kandahar\n\n\n\nuncategorized\n\n\n\n\n\n\n2010-02-05\n\n\n\n\n\n\n\nJohn Nagl and ‘the future of counterinsurgency’\n\n\n\nafghanistan\n\ngeneral\n\n\n\n\n\n\n2010-02-02\n\n\n\n\n\n\n\n‘Talking to Terrorists’\n\n\n\nafghanistan\n\nbooks\n\n\n\n\n\n\n2010-01-30\n\n\n\n\n\n\n\nPresenting Mullah Zaeef\n\n\n\nafghanistan\n\nbooks\n\ntravel\n\nuntitled\n\nzaeef\n\n\n\n\n\n\n2010-01-13\n\n\n\n\n\n\n\nStaying Apart\n\n\n\nafghanistan\n\njournalism\n\ntravel\n\n\n\n\n\n\n2010-01-08\n\n\n\n\n\n\n\n5 Books Everyone Should Read About Afghanistan\n\n\n\nafghanistan\n\nbooks\n\ndorronsoro\n\nzaeef\n\njason-elliot\n\n\n\n\n\n\n2010-01-01\n\n\n\n\n\n\n\nNew Year, New Website\n\n\n\ngeneral\n\n\n\n\n\n\n2010-01-01\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an ML Engineer working at ZenML. This blog is a place for me to process and share what I learn along the way.\nI built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. I have multiple years of experience in the Python, Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker.\nI have a PhD in History and authored several books based on my research work in Afghanistan. I have a different long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here.\nI built Gemini by Example as a reference resource for the Gemini API / SDK."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Strick van Linschoten",
    "section": "",
    "text": "ML Engineer at ZenML. Historian turned software developer.\n      Writing about machine learning, languages, and life.\n    \n  \n  \n  \n    \n      Technical\n      Engineering Blog\n      MLOps, machine learning, software engineering, and programming.\n      \n        \n      \n    \n    \n    \n      Personal\n      Life & Letters\n      Afghanistan, books, language learning, travel, and reflections.\n      \n        \n      \n    \n  \n  \n  \n    \n      \n        \n      \n      GitHub\n    \n    \n    \n      \n        \n      \n      Mastodon\n    \n    \n    \n      \n        \n      \n      Twitter\n    \n    \n    \n      \n        \n      \n      LinkedIn\n    \n  \n  \n  About me →"
  },
  {
    "objectID": "technical.html",
    "href": "technical.html",
    "title": "Technical Blog",
    "section": "",
    "text": "Writings on MLOps, machine learning, software engineering, and programming.\nFor personal writings on Afghanistan, books, languages, and life, see the personal blog.\n\n\n\n\n\n\n\n\n\nTrying to instrument an agentic app with Arize Phoenix and litellm\n\n\n\nllms\n\nagents\n\nevals-course\n\nevaluation\n\nminiproject\n\nhinbox\n\n\n\nTrying to get Phoenix to work with litellm to instrument my LLM calls, grouping spans together as traces.\n\n\n\n\n\n2025-06-04\n\n\n\n\n\n\n\nTesting out instrumenting LLM tracing for litellm with Braintrust and Langfuse\n\n\n\nllms\n\nagents\n\nevals-course\n\nevaluation\n\nminiproject\n\nhinbox\n\n\n\nThird time’s a charm: setting up instrumentation with Braintrust, Langfuse and litellm. Braintrust ended up not being as ergonomic as Langfuse so I switch over midway.\n\n\n\n\n\n2025-06-04\n\n\n\n\n\n\n\nBuilding hinbox: An agentic research tool for historical document analysis\n\n\n\nllms\n\nagents\n\nevals-course\n\nevaluation\n\nminiproject\n\nhinbox\n\nresearch\n\n\n\nLessons learned from working on an entity extraction system for historical research that automatically processes documents to create structured knowledge databases, developed as a practical testbed for systematic AI evaluation techniques.\n\n\n\n\n\n2025-05-30\n\n\n\n\n\n\n\nError analysis to find failure modes\n\n\n\nevals-course\n\nllms\n\nllmops\n\nevaluation\n\n\n\nA systematic 5-step process for analysing LLM application failures through error analysis and clustering techniques to identify and categorise failure modes for iterative improvement.\n\n\n\n\n\n2025-05-23\n\n\n\n\n\n\n\nHow to think about evals\n\n\n\nevals-course\n\nllms\n\nllmops\n\nevaluation\n\n\n\nKey insights from the first session of the Hamel/Shreya AI Evals course, focusing on a ‘three gulfs’ mental model (specification, generalisation, and comprehension) for LLM application development and the importance of systematic evaluation and improvement processes.\n\n\n\n\n\n2025-05-20\n\n\n\n\n\n\n\nFirst impressions of the new Gemini Deep Research (with 2.5 Pro)\n\n\n\nagents\n\ngoogle\n\ntools\n\nopenai\n\nresearch\n\n\n\nSome initial fast impressions of Google Deepmind’s new iteration of Gemini Deep Research that uses their 2.5 Pro model.\n\n\n\n\n\n2025-04-09\n\n\n\n\n\n\n\nLearnings from a week of building with local LLMs\n\n\n\nclaude\n\nllm\n\nllms\n\nminiproject\n\nopenai\n\nprompt-engineering\n\nsoftwareengineering\n\ntools\n\n\n\nInsights from a week of building an LLM-based knowledge database, highlighting experiences with local models, prompt engineering patterns, development tools like Ollama and RepoPrompt, and software engineering principles that enhance AI-assisted development workflows.\n\n\n\n\n\n2025-03-16\n\n\n\n\n\n\n\nBuilding an MCP Server for Beeminder: Connecting AI Assistants to Personal Data\n\n\n\ntools\n\nanthropic\n\nclaude\n\nminiproject\n\n\n\nI built a Model Context Protocol (MCP) server for Beeminder to connect AI assistants with my personal goal tracking data. Here’s how I implemented this integration using Claude Desktop, what I learned about MCP development.\n\n\n\n\n\n2025-02-21\n\n\n\n\n\n\n\nTinbox: an LLM-based document translation tool\n\n\n\ntranslation\n\nllm\n\nllms\n\nlanguages\n\nresearch\n\nminiproject\n\npython\n\ntools\n\n\n\nExplores an open-source tool I built that tackles the challenges of large-scale document translation using LLMs. Born from my experience as both a historian working with Afghan primary sources and a developer, it offers innovative solutions to common translation problems through smart chunking algorithms and local model support, making multilingual content more accessible for researchers and developers alike.\n\n\n\n\n\n2025-02-16\n\n\n\n\n\n\n\nStarting the Hugging Face Agents course\n\n\n\nagents\n\nhuggingface\n\nskillbuilding\n\nllmops\n\nllms\n\n\n\nSome observations on completing unit one of the new course hosted by Hugging Face.\n\n\n\n\n\n2025-02-11\n\n\n\n\n\n\n\nAI Engineering Architecture and User Feedback\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nllmops\n\nevaluation\n\n\n\nMy notes on chapter 10 of Chip Huyen’s ‘AI Engineering’, an exploration of modern AI system architecture patterns and user feedback mechanisms, covering the evolution from simple API integrations to complex agent-based systems, including practical implementations of RAG, guardrails, caching strategies, and systematic approaches to gathering and utilizing user feedback for continuous improvement.\n\n\n\n\n\n2025-02-09\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ chapter 9: Inference Optimisation\n\n\n\nbooks-i-read\n\ninference\n\nllm\n\nllms\n\nhardware\n\n\n\nChapter 9 is a guide to ML inference optimization covering compute and memory bottlenecks, performance metrics, and practical implementation strategies. This technical summary explores model-level, hardware-level, and service-level optimizations, with detailed explanations of batching strategies, parallelism approaches, and attention mechanisms - essential knowledge for ML engineers working to reduce inference costs and improve system performance.\n\n\n\n\n\n2025-02-07\n\n\n\n\n\n\n\nDataset Engineering: The Art and Science of Data Preparation\n\n\n\nbooks-i-read\n\ndatasets\n\ndatalabelling\n\nllm\n\nllms\n\nfinetuning\n\n\n\nExplores Chapter 8 of Chip Huyen’s ‘AI Engineering,’ examining the intricate landscape of dataset engineering through the lenses of curation, augmentation, and processing.\n\n\n\n\n\n2025-02-05\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning\n\n\n\nbooks-i-read\n\nfinetuning\n\nllm\n\nllms\n\n\n\nExplores when and how to implement finetuning effectively, looking at key technical aspects like memory considerations and PEFT, while emphasising fine-tuning as a last-resort approach after simpler solutions like prompt engineering and RAG have been exhausted.\n\n\n\n\n\n2025-01-26\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 6\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nagents\n\nrag\n\nevaluation\n\n\n\n\n\n\n\n\n\n2025-01-24\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 4\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nevaluation\n\n\n\nA comprehensive guide to AI system evaluation, synthesising Chapter 4 of Chip Huyen’s ‘AI Engineering.’ These notes detail practical frameworks for assessing AI models, covering evaluation criteria, model selection strategies, and pipeline implementation, while maintaining a balanced perspective between academic rigour and real-world application needs.\n\n\n\n\n\n2025-01-22\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 3\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nevaluation\n\n\n\n\n\n\n\n\n\n2025-01-21\n\n\n\n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 1\n\n\n\nbooks-i-read\n\nllm\n\nllms\n\nfinetuning\n\nprompt-engineering\n\n\n\nA detailed analysis of Chapter 1 from Chip Huyen’s ‘AI Engineering’ book, covering the transition from ML Engineering to AI Engineering, the three-layer AI stack, and modern development paradigms. Includes insights from a study group discussion on enterprise adoption challenges and emerging evaluation techniques.\n\n\n\n\n\n2025-01-19\n\n\n\n\n\n\n\nFinal notes on ‘Prompt Engineering for LLMs’\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\nevaluation\n\n\n\nDetailed notes covering Chapters 10 and 11 of ‘Prompt Engineering for LLMs’ by Berryman and Ziegler, focusing on LLM application evaluation and future trends. Chapter 10 explores comprehensive testing frameworks including offline example suites and online AB testing, while Chapter 11 discusses multimodality, user interfaces, and core principles for effective prompt engineering. Includes personal insights on the book’s emphasis on completion models versus chat models.\n\n\n\n\n\n2025-01-17\n\n\n\n\n\n\n\nAssembling the Prompt: Notes on ‘Prompt Engineering for LLMs’ ch 6\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\n\n\nA detailed breakdown of Chapter 6 from ‘Prompt Engineering for LLMs,’ examining prompt structure, document types, and optimization strategies for effective prompt engineering, with practical tips on information positioning and context selection within prompts.\n\n\n\n\n\n2025-01-13\n\n\n\n\n\n\n\nPrompt Content: Notes on ‘Prompt Engineering for LLMs’ ch 5\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\nRAG\n\n\n\nChapter 5 of ‘Prompt Engineering for LLMs’ explores static content (fixed instructions and few-shot examples) versus dynamic content (runtime-assembled context like RAG) in prompts, offering tactical guidance on implementation choices, tradeoffs, and potential pitfalls while emphasising practical examples throughout.\n\n\n\n\n\n2025-01-12\n\n\n\n\n\n\n\nStarting to read Prompt Engineering for LLMs\n\n\n\nllm\n\nprompt-engineering\n\nbooks-i-read\n\ntokenisation\n\n\n\nSummary notes from the first two chapters of ‘Prompt Engineering for LLMs’.\n\n\n\n\n\n2025-01-09\n\n\n\n\n\n\n\nAll the things I learned while trending on Hacker News\n\n\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\nevaluation\n\nnlp\n\n\n\nI was on the front page of Hacker News for my two last blog posts and I learned various things forom the discussion and scrutiny of my approach to evaluating my finetuned LLMs.\n\n\n\n\n\n2024-07-07\n\n\n\n\n\n\n\nMy finetuned models beat OpenAI’s GPT-4\n\n\n\nnlp\n\nafghanistan\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\nevaluation\n\n\n\nFinetunes of Mistral, Llama3 and Solar LLMs are more accurate for my test data than OpenAI’s models.\n\n\n\n\n\n2024-07-01\n\n\n\n\n\n\n\nHow to think about creating a dataset for LLM finetuning evaluation\n\n\n\nllms\n\nfinetuning\n\nisafpr\n\nafghanistan\n\ndatasets\n\nevaluation\n\nminiproject\n\n\n\nI summarise the kinds of evaluations that are needed for a structured data generation task.\n\n\n\n\n\n2024-06-25\n\n\n\n\n\n\n\nOne-click LLM finetuning with Predibase, OpenPipe and OpenAI\n\n\n\nnlp\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\n\n\nI tried out some services that promise to simplify the process of finetuning open models. I describe my experiences with Predibase, OpenPipe and OpenAI.\n\n\n\n\n\n2024-06-17\n\n\n\n\n\n\n\nFinetuning my first LLM(s) for structured data extraction with axolotl\n\n\n\nnlp\n\nafghanistan\n\nllms\n\nminiproject\n\nfinetuning\n\nisafpr\n\n\n\nI finetuned my first LLM(s) for the task of extracting structured data from ISAF press releases. Initial tests suggest that it worked pretty well out of the box.\n\n\n\n\n\n2024-06-15\n\n\n\n\n\n\n\nEvaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction\n\n\n\nnlp\n\nafghanistan\n\ndatalabelling\n\nllms\n\nisafpr\n\nminiproject\n\nevaluation\n\n\n\nI evaluated the baseline performance of OpenAI’s GPT-4-Turbo on the ISAF Press Release dataset.\n\n\n\n\n\n2024-06-03\n\n\n\n\n\n\n\nStructured Data Extraction for ISAF Press Releases with Instructor\n\n\n\nnlp\n\nafghanistan\n\ndatalabelling\n\nisafpr\n\nllms\n\nminiproject\n\n\n\nI used Instructor to understand how well LLMs are at extracting data from the ISAF Press Releases dataset. They did pretty well, but not across the board.\n\n\n\n\n\n2024-06-02\n\n\n\n\n\n\n\nIntroducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009\n\n\n\nminiproject\n\nafghanistan\n\ndatalabelling\n\ndatasets\n\nnlp\n\nllms\n\nisafpr\n\n\n\nI’m publishing a unique new dataset of Afghan newspaper and magazine articles from the 2006-2009 period. This collection of over 7990 articles were originally translated from Dari and Pashto and published by Afghanwire, a media monitoring organisation that I co-founded and ran in Kabul at the time.\n\n\n\n\n\n2024-04-01\n\n\n\n\n\n\n\nWriting a custom Terraform provider to deploy Huggingface Spaces\n\n\n\ndevops\n\nminiproject\n\nterraform\n\ngo\n\nskillbuilding\n\n\n\nI worked on this short project to allow people to create/deploy Huggingface Spaces using Terraform (instead of via the API or using the website)\n\n\n\n\n\n2024-03-31\n\n\n\n\n\n\n\nPublishing the ISAF Press Releases dataset\n\n\n\nminiproject\n\nafghanistan\n\ndatalabelling\n\ndatasets\n\nnlp\n\nllms\n\n\n\nI published a dataset from my previous work as a researcher in Afghanistan. It consists of press releases about military operations as well as full annotations showcasing information extracted from those press releases. It has value as a historical artifact but potentially could be used as an LLM evaluation task as well.\n\n\n\n\n\n2024-03-24\n\n\n\n\n\n\n\nAutomating database backups with Tarsnap\n\n\n\ndatabases\n\nskillbuilding\n\nsoftwareengineering\n\ntools\n\nminiproject\n\n\n\nI added a cronjob to automate database backups for my MathsPrompt questions.\n\n\n\n\n\n2023-07-24\n\n\n\n\n\n\n\nBuilding MathsPrompt: a tool to help me review and practice problems for my degree\n\n\n\nopenai\n\nllms\n\nmathematics\n\nrust\n\nmu123\n\nq31\n\nskillbuilding\n\nsoftwareengineering\n\ntools\n\nminiproject\n\n\n\nI built a tool to help me practice the parts of mathematics that I find hardest. I also have been reading some books about Rust and I also wanted to play around with that so used it for the server / backend.\n\n\n\n\n\n2023-07-23\n\n\n\n\n\n\n\nTerraform Input Variables\n\n\n\nterraform\n\ndevops\n\nsoftwareengineering\n\n\n\nAll the ways you can set input and local variables when using Terraform.\n\n\n\n\n\n2023-06-22\n\n\n\n\n\n\n\nTokenizer Links\n\n\n\nnlp\n\nbalochi-language-model\n\ntokenisation\n\nlinks\n\n\n\nSome links and random observations relating to tokenisation as gathered over the past week.\n\n\n\n\n\n2023-06-04\n\n\n\n\n\n\n\nTokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy\n\n\n\nnlp\n\nbalochi-language-model\n\ntokenisation\n\nbalochi\n\n\n\nI explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language. I share the challenges I faced due to language-specific limitations, my initiative to expand language metadata, and my plans to assess and enhance tokenization efficiency.\n\n\n\n\n\n2023-06-03\n\n\n\n\n\n\n\nThe What, Why, and How of Tokenisation in Machine Learning\n\n\n\nnlp\n\nbalochi-language-model\n\ntokenisation\n\n\n\nThe basics around the tokenisation process: why we do it, the spectrum of choices when you get to choose how to do it, and the family of algorithms most commonly used at the moment.\n\n\n\n\n\n2023-06-01\n\n\n\n\n\n\n\nBuilding a Balochi Language Dataset for NLP Applications\n\n\n\nbalochi\n\nnlp\n\nbalochi-language-model\n\nethics\n\ndatasets\n\n\n\nI share my journey of building language models for Balochi, a language with few digital resources. I discuss assembling a dataset of 2.6 million Balochi words.\n\n\n\n\n\n2023-05-29\n\n\n\n\n\n\n\nThe Risks of Language Models in Minority Languages\n\n\n\nbalochi\n\nnlp\n\nbalochi-language-model\n\ndeep-learning\n\nethics\n\n\n\nThe dual-edged nature of developing a language model for the Balochi language, weighing potential benefits like improved communication, accessibility, and language preservation against serious risks such as misuse by state actors for surveillance and power consolidation, and the unintentional promotion of linguistic monoculture.\n\n\n\n\n\n2023-05-22\n\n\n\n\n\n\n\nLow-resource language models: making a start with Balochi\n\n\n\nbalochi\n\nnlp\n\nbalochi-language-model\n\ndeep-learning\n\n\n\nThe Balochi language is underrepresented in NLP. I’m interested in contributing to the field by building a language model for Balochi from scratch and contributing training resources and datasets along the way.\n\n\n\n\n\n2023-05-21\n\n\n\n\n\n\n\nFinishing MU123\n\n\n\nmathematics\n\nmu123\n\nq31\n\n\n\nI completed the first module from my maths degree with the Open University. Highlights were quadratic equations, trigonometry and exponential functions.\n\n\n\n\n\n2023-05-14\n\n\n\n\n\n\n\nExponents and Logarithms: a MU123 review\n\n\n\nmathematics\n\nmu123\n\nq31\n\n\n\nI delved into exponents and logarithms in my Open University Maths degree, discovering their practical applications and connections to concepts like Euler’s number. Gaining a deeper understanding, I enjoyed manipulating symbols and working with these fascinating mathematical tools.\n\n\n\n\n\n2023-05-02\n\n\n\n\n\n\n\nTerraform for the Uninitiated: Demystifying Your First Codebase\n\n\n\nterraform\n\nsoftwareengineering\n\ndevops\n\n\n\nLearn the essentials of working with Terraform as a beginner, including basic commands like init, plan, apply, and destroy. Gain insights into code structure, variables, outputs, and providers while exploring a new codebase.\n\n\n\n\n\n2023-04-29\n\n\n\n\n\n\n\nHow to remove a commit (or two) from your git branch\n\n\n\ngit\n\nsoftwareengineering\n\nversioncontrol\n\n\n\nInstructions how to remove a commit from your git logs.\n\n\n\n\n\n2023-04-28\n\n\n\n\n\n\n\nThe Trick Is The Thing, Part II\n\n\n\nmathematics\n\nmu123\n\nq31\n\ndeeplearning\n\n\n\nI’ve enjoyed learning about quadratic equations and trigonometry for my Maths degree, and am struck by how many incremental steps along the way contributed to the total edifice of understanding.\n\n\n\n\n\n2023-03-25\n\n\n\n\n\n\n\nBuilding Blocks For Better Stable Eights\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\nAn impromptu continuation of the last blog, where I use perceptual loss to get the updates to my random noise image that I wanted and finally manage to ‘generate’ an image of the digit eight.\n\n\n\n\n\n2023-03-18\n\n\n\n\n\n\n\nTricking my digits classifier with diffusion\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\nI accidentally built a way to adversarially generate handwritten images that seem to be of the number eight, but aren’t. This blog showcases an experiment I made around the core process going on in the generative diffusion process.\n\n\n\n\n\n2023-03-05\n\n\n\n\n\n\n\nOn mathematical literacy\n\n\n\nmathematics\n\nmu123\n\nq31\n\n\n\nThinking aloud about how to tie a collection of mathematical ‘tricks’ and operations together in some sort of logical and rounded whole.\n\n\n\n\n\n2023-01-01\n\n\n\n\n\n\n\nFrom the foundation up: Fashion-MNIST basics from Lesson 10\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\nNotes and some personal exploration following through the lesson 10 course materials from FastAI part 2. We cover the basics of loading in our data and generating our matrix.\n\n\n\n\n\n2022-10-24\n\n\n\n\n\n\n\nDeep learning tricks all the way down, with a bit of mathematics for good measure\n\n\n\ncomputervision\n\nfastai\n\nparttwo\n\n\n\nNotes and reflections based on the first lesson (aka ‘lesson 9’) of the FastAI Part II course. This covers the fundamentals of Stable Diffusion, how it works and some core concepts or techniques.\n\n\n\n\n\n2022-10-17\n\n\n\n\n\n\n\nAvoiding BIDMAS, or how J does notation\n\n\n\nj\n\nmathematics\n\nmu123\n\nq31\n\nnotation\n\n\n\nI learned about prefix, postfix and infix notation, and how J evaluates mathematical expressions which makes the BIDMAS rules unnecessary.\n\n\n\n\n\n2022-10-16\n\n\n\n\n\n\n\nStoring Bytes: what data serialisation is and why you need it for machine learning\n\n\n\nredactionmodel\n\ncomputervision\n\nmlops\n\npython\n\ntools\n\nzenml\n\n\n\nI explain the basics around data serialisation and deserialisation, why it’s a commonly-encountered topic, and showcase where I had to implement some custom logic to serialise custom Python objects used in a computer vision project.\n\n\n\n\n\n2022-09-07\n\n\n\n\n\n\n\nIt takes a tribe: how I’m thinking about putting my object detection model into production\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\nmlops\n\n\n\nThere are many pieces involved when deploying a model. This post covers the ones that relate to my object detection model and I explain how I’m going to put together the pipelines that will drive a continuous training loop once it’s all up.\n\n\n\n\n\n2022-05-31\n\n\n\n\n\n\n\nMore Data, More Problems: Using DVC to handle data versioning for a computer vision problem\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\nmlops\n\n\n\nI show you why you probably want to be versioning your data alongside your code. I introduce the basic functionality of DVC, the industry-standard tool for data versioning. I also explain specifically how I’m using DVC for my computer vision project.\n\n\n\n\n\n2022-05-24\n\n\n\n\n\n\n\nRedaction Image Classifier: NLP Edition\n\n\n\nfastai\n\nnlp\n\npartone\n\n\n\nI train an NLP model to see how well it does at predicting whether an OCRed text contains a redaction or not. I run into a bunch of issues when training, leading me to conclude that training NLP models is more complicated than I’d at first suspected.\n\n\n\n\n\n2022-05-21\n\n\n\n\n\n\n\nA neural network for Fashion MNIST data\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\nThe final step of this series looking at chapter 4 of the fastai book tackles the final step where we construct a very simple 3-layer neural network which learns to distinguish a pullover from a dress.\n\n\n\n\n\n2022-05-15\n\n\n\n\n\n\n\nUsing the seven-step SGD process for Fashion MNIST\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\nI apply all the lessons we’ve learned so far on the Fashion MNIST dataset. This requires us learning a few new concepts like optimisers, ReLU, nonlinearity and so on.\n\n\n\n\n\n2022-05-14\n\n\n\n\n\n\n\nStochastic Gradient Descent: a mini-example of the whole game\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\nThis short post shows how you iterate through a simple example of optimising three values as passed into a quadratic equation/function. We use SGD to optimise these.\n\n\n\n\n\n2022-05-13\n\n\n\n\n\n\n\nSome foundations for machine learning with PyTorch\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\nI outline the basic process that a computer uses when training a model, greatly simplified and all explained through the lens of PyTorch and how it calculates gradients. These are some pre-requisite foundations that we will later apply to our Fashion MNIST dataset.\n\n\n\n\n\n2022-05-12\n\n\n\n\n\n\n\nA dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\nI read part of chapter four of the fastai course book, learning about a naive approach to image classification (sort of!)\n\n\n\n\n\n2022-05-11\n\n\n\n\n\n\n\nA painless way to create an MVP demo using computer vision models\n\n\n\nfastai\n\ncomputervision\n\nredactionmodel\n\ntools\n\n\n\nI created a few deployed MVP demos showcasing models I’d created while participating in the fastai course, uploading them to the Huggingface Hub and using a Gradio Demo hosted on Huggingface Spaces.\n\n\n\n\n\n2022-05-07\n\n\n\n\n\n\n\nHow my pet cat taught me a lesson about validation data for image classification\n\n\n\nfastai\n\ncomputervision\n\npartone\n\n\n\nI learn a valuable lesson about how a model often will ‘cheat’ when training and sometimes the solution is a separate held-out set of ‘test’ data which can give a more accurate assessment of how well the model is performing.\n\n\n\n\n\n2022-05-02\n\n\n\n\n\n\n\nHow to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\ndatavalidation\n\n\n\nIn this third and final post on data validation for the computer vision context, I cover some alternative tools that you might want to consider, from Evidently to the humble ‘assert’ statement. I conclude by setting out some guidelines for when you might want to be doing data validation and which tools might be more or less appropriate for your specific problem.\n\n\n\n\n\n2022-04-28\n\n\n\n\n\n\n\nHow to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\ndatavalidation\n\n\n\nIn this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of the Great Expectations library to get you started with increasing your confidence in your object detection annotations.\n\n\n\n\n\n2022-04-26\n\n\n\n\n\n\n\nHow to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\ndatavalidation\n\n\n\nAn overview of the problem that data validation seeks to solve, explored through the lens of an object detection problem and some of the tradeoffs that such an approach might bring. I introduce and simplify the high-level concepts you need to use the Great Expectations library.\n\n\n\n\n\n2022-04-19\n\n\n\n\n\n\n\n‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data\n\n\n\ntools\n\nredactionmodel\n\ncomputervision\n\n\n\nI show how adding synthetic data has improved my redaction model’s performance. Once I trained with the synthetic images added, I realised a more targeted approach would do even better.\n\n\n\n\n\n2022-04-06\n\n\n\n\n\n\n\nSome characteristics of best-in-class ML portfolio projects\n\n\n\ncomputervision\n\nskillbuilding\n\n\n\nI wrote about some of the things that go into creating a really great portfolio project for machine learning. For this post I’m less interested in the technical achievements than I am in how it is presented.\n\n\n\n\n\n2022-04-04\n\n\n\n\n\n\n\nBuilding my own image to use IceVision with Paperspace\n\n\n\ntools\n\ndocker\n\ncomputervision\n\n\n\nI setup a new Paperspace project that uses a custom Docker image to provision its environment, saving me a bunch of initial installation time and dependency bug pain. A huge productivity win!\n\n\n\n\n\n2022-03-25\n\n\n\n\n\n\n\nStarting Docker In A Month Of Lunches\n\n\n\ntools\n\ndockerinamonthoflunches\n\nbooks-i-read\n\n\n\nI’m reading Elton Stoneman’s ‘Learn Docker in a Month of Lunches’ and blogging as I learn along the way. In chapters 1-3 we learn about the context for Docker as well as some basic commands for running and building containers.\n\n\n\n\n\n2022-03-21\n\n\n\n\n\n\n\nFiguring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of\n\n\n\nredactionmodel\n\ncomputervision\n\ntools\n\ndebugging\n\njupyter\n\n\n\nI used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming. For computer vision problems, it’s really useful to have visual debugging aids and FiftyOne is a well-documented and solid tool to help with that.\n\n\n\n\n\n2022-03-12\n\n\n\n\n\n\n\nIncremental Improvements to my Redaction Detection Model\n\n\n\nredactionmodel\n\ncomputervision\n\ntools\n\n\n\nI used a series of techniques to improve the performance of my model while creating a pathway to (hopefully) bigger gains going forward.\n\n\n\n\n\n2022-03-03\n\n\n\n\n\n\n\nThree Python Helpers for Parsing Inputs\n\n\n\npython\n\ntools\n\n\n\nThe parse, yarl and datefinder packages are all ways in Python to help parse input data of different formats and types. Nothing essential here, but useful nonetheless.\n\n\n\n\n\n2022-02-27\n\n\n\n\n\n\n\nIt’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model\n\n\n\nredactionmodel\n\ncomputervision\n\npython\n\ntools\n\n\n\nI iterated through several prototypes to get to a script that could autogenerate synthetic training data for my computer vision model. I hoped to bootstrap my training to get a bit jump in model performance.\n\n\n\n\n\n2022-02-10\n\n\n\n\n\n\n\nWhat are invariants and how can they help make your Python classes more robust?\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nChapter 10 covers the last of the user-defined types explored in ‘Robust Python’: classes. We learn what an ‘invariant’ is and how to decide whether to use a data class or a class when rolling your own types.\n\n\n\n\n\n2022-02-08\n\n\n\n\n\n\n\nUpgrade your Python dicts with data classes\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nChapter 9 of ‘Robust Python’ dives into the uses of data classes, a user-defined datatype in which you can store heterogenous data together. They help formalise implicit concepts within your code and as a result also improve code readability.\n\n\n\n\n\n2022-02-05\n\n\n\n\n\n\n\nHow and where to use enums in Python\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nThe eight chapter of Patrick Viafore’s book, ‘Robust Python’, gets into enums which you can use when you have a grouping of some constants that belong together.\n\n\n\n\n\n2022-01-30\n\n\n\n\n\n\n\nUsing mypy for Python type checking\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nReflections on the sixth and seventh chapters of Patrick Viafore’s book, ‘Robust Python’. We slowly wind down our discussion of type hints in Python code and think through using mypy and how to introduce type hints to a legacy codebase.\n\n\n\n\n\n2022-01-22\n\n\n\n\n\n\n\nUsing type annotation with collections in Python\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nReflections on the fifth chapter of Patrick Viafore’s book, ‘Robust Python’. We learn about how to use type annotations when collections (lists, dictionaries and sets, primarily) are involved.\n\n\n\n\n\n2022-01-18\n\n\n\n\n\n\n\nA Midway Report on my Computer Vision Project\n\n\n\npython\n\nfastai\n\ntools\n\nredactionmodel\n\n\n\nA report midway through my computer vision project to detect the presence of redactions on government documents.\n\n\n\n\n\n2022-01-16\n\n\n\n\n\n\n\nDifferent ways to constrain types in Python\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nReflections on the fourth chapter of Patrick Viafore’s recent book, ‘Robust Python’. We learn about the different options for combining types and constraining exactly which sets of types are permitted for a particular function or variable signature.\n\n\n\n\n\n2022-01-08\n\n\n\n\n\n\n\nLearning about ‘nbdev’ while building a Python package for PDF machine learning datasets\n\n\n\npython\n\njupyter\n\nfastai\n\ntools\n\n\n\nSome early thoughts on the benefits and possible drawbacks of using fastai’s ‘nbdev’ literate programming tool which is a suite of tools that allows you to Python software packages from Jupyter notebooks.\n\n\n\n\n\n2022-01-06\n\n\n\n\n\n\n\nGetting practical with type annotations and mypy\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nReflections on the third chapter of Patrick Viafore’s recent book, ‘Robust Python’. We get some quick practical examples of how to use type annotation and how to use tools like mypy to analyse how typed values pass through your code.\n\n\n\n\n\n2022-01-03\n\n\n\n\n\n\n\nCounter: a shortcut to counting iterables in Python\n\n\n\npython\n\n\n\nA nice little helper from the Python standard library\n\n\n\n\n\n2022-01-01\n\n\n\n\n\n\n\nWhat’s special about types in Python?\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nReflections on the second chapter of Patrick Viafore’s recent book, ‘Robust Python’. We learn about types and how they fit into Python.\n\n\n\n\n\n2021-12-30\n\n\n\n\n\n\n\nExploring J, an array programming language\n\n\n\nj\n\n\n\nWhat I have learned so far about why the J language exists and what problems it tries to solve.\n\n\n\n\n\n2021-12-29\n\n\n\n\n\n\n\nWhat makes code robust?\n\n\n\nrobustpython\n\npython\n\nbooks-i-read\n\n\n\nReflections on the first chapter of Patrick Viafore’s recent book, ‘Robust Python’.\n\n\n\n\n\n2021-12-29\n\n\n\n\n\n\n\nA Taxonomy of Redaction\n\n\n\nredactionmodel\n\n\n\nA brief analysis of some of the types of redactions that are commonly found in FOIA documents. I use these as the dataset used to train an object detection model for redactions.\n\n\n\n\n\n2021-12-15\n\n\n\n\n\n\n\n73% accuracy for redaction object detection\n\n\n\nredactionmodel\n\ncomputervision\n\nprogressreport\n\n\n\nI made some progress on my redaction model.\n\n\n\n\n\n2021-12-11\n\n\n\n\n\n\n\nWhat is VFNet?\n\n\n\nredactionmodel\n\ncomputervision\n\n\n\nSome basics I learned about the object detection model vfnet.\n\n\n\n\n\n2021-11-30\n\n\n\n\n\n\n\nHow to annotate image data for object detection with Prodigy\n\n\n\nredactionmodel\n\ncomputervision\n\ndatalabelling\n\n\n\nHow I used Prodigy to annotate my data ahead of training an object detection model\n\n\n\n\n\n2021-11-29\n\n\n\n\n\n\n\nLaunching a podcast about MLOps\n\n\n\nzenml\n\npodcast\n\nappearances\n\n\n\nI will be co-hosting a new podcast about MLOps called Pipeline Conversations.\n\n\n\n\n\n2021-11-27\n\n\n\n\n\n\n\nCheck your security vulnerabilities with safety\n\n\n\nsecurity\n\ntools\n\ncalmcode\n\n\n\nThe database is only updated once a month, but it is a useful check nonetheless.\n\n\n\n\n\n2021-11-27\n\n\n\n\n\n\n\nHow to set and get environment variables using Python\n\n\n\npython\n\n\n\nA short post on setting environment variables using Python.\n\n\n\n\n\n2021-11-26\n\n\n\n\n\n\n\nentr: a tool to run commands when files change\n\n\n\ndebugging\n\ntesting\n\ntools\n\ncalmcode\n\n\n\nentr is a useful tool to rerun things when watched files change. It’s especially useful when testing.\n\n\n\n\n\n2021-11-25\n\n\n\n\n\n\n\nOn failure\n\n\n\ndebugging\n\nemotions\n\n\n\nSome reflections on the idea of what it is that we do as software engineers.\n\n\n\n\n\n2021-11-21\n\n\n\n\n\n\n\nSome things I learned about debugging\n\n\n\ndebugging\n\n\n\nA few lessons I’ve learned about debugging at work in recent weeks\n\n\n\n\n\n2021-10-25\n\n\n\n\n\n\n\nReading Python Code\n\n\n\npython\n\nskillbuilding\n\n\n\nSome of the code libraries I plan on reading to improve my Pythonic style\n\n\n\n\n\n2021-09-18\n\n\n\n\n\n\n\nWriting Code\n\n\n\npython\n\nskillbuilding\n\n\n\nA reminder that early on, nothing really beats writing code for growing as a coder\n\n\n\n\n\n2021-09-18\n\n\n\n\n\n\n\nTensors all the way down\n\n\nDiscovering an optimised alternative to numpy arrays that are used from the ground up when you train deep learning models\n\n\n\n\n\n2021-09-16\n\n\n\n\n\n\n\nA Baseline Python Development Setup\n\n\n\npython\n\ntools\n\n\n\nGetting a development environment setup for Python and having to choose between pyenv vs virtualenv vs venv\n\n\n\n\n\n2021-09-14\n\n\n\n\n\n\n\nSix problems TFX was trying to solve in 2017\n\n\n\ntfx\n\ntensorflow\n\ngoogle\n\nmlops\n\npapers-i-read\n\n\n\nI extracted the core problems that TensorFlow Extended (TFX) was looking to solve from its 2017 public launch paper.\n\n\n\n\n\n2021-09-11\n\n\n\n\n\n\n\nRetrieval Practice with fastai chapters 1 and 2\n\n\nReviewing the main building blocks of the deep learning training workflow in the first chapters of the fastai book\n\n\n\n\n\n2021-09-10\n\n\n\n\n\n\n\nHow to set a Jupyter notebook to auto-reload external libraries\n\n\n\njupyter\n\n\n\nA small bit of Jupyter notebook magic\n\n\n\n\n\n2021-09-09\n\n\n\n\n\n\n\nA Baseline Understanding of MLOps\n\n\n\nmlops\n\n\n\nWhat I understand of the domain, prior to starting to work in this area full-time\n\n\n\n\n\n2021-09-08\n\n\n\n\n\n\n\nTraining a classifier to detect redacted documents with fastai\n\n\n\nfastai\n\nredactionmodel\n\ncomputervision\n\ndatalabelling\n\n\n\nHow I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training\n\n\n\n\n\n2021-09-06\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "personal/2010-01-01-5-books-everyone-should-read-about-afghanistan.html",
    "href": "personal/2010-01-01-5-books-everyone-should-read-about-afghanistan.html",
    "title": "5 Books Everyone Should Read About Afghanistan",
    "section": "",
    "text": "I often get asked for book recommendations by people who are about to deploy/work in/travel to (etc) Afghanistan. The choices here are a bit unorthodox – more on account of what I omitted rather than the choices themselves, I would imagine – but I think these five books should offer the basis for a good working understanding of some of the ‘themes’.\nThere’s no specific order to these books, although you’re probably better off leaving David Edwards’ Before Taliban till later on in your studies.\n “An Unexpected Light: Travels in Afghanistan” (Jason Elliot)\nAn Unexpected Light was the book that made me want to come to Afghanistan in the first place. I read it while I was still in secondary school and knew that this was a place I wanted to end up someday. A detailed account of Elliot’s travels in Afghanistan during the 1990s, the book offers an indispensable introduction to cultural and historical principles in Afghanistan.\n\n“Before Taliban: Genealogies of the Afghan Jihad” (David B. Edwards)\nThis is the sequel to Heroes of the Age (also a must-read), and it covers the 1980s jihad (from 1979 to 1995). It’s a fine example of what good writing and research on Afghanistan should look like – something we should all aspire to – and introduces the changes that the 1980s brought to Afghan political culture, and shows how the Taliban were an outgrowth of this period.\n “Afghanistan: The Mirage of Peace” (Chris Johnson, Jolyon Leslie)\nOne of the first books I read when I first came to Afghanistan, this explores social environment and cultural identity, especially as it relates to the NGO and assistance community. It doesn’t take you all the way up to the present day, but it’s an excellent summary (read: indictment) of the post-2001 period.\n “Revolution Unending: Afghanistan, 1979 to the Present (The CERI Series in Comparative Politics and International Studies)” (Gilles Dorronsoro)\nNone of the first three books go into much detail on the specifics of Afghan history, but Dorronsoro is the best primer on the past 30 years. Many books are written without the benefit of significant ‘field’ exposure to the Afghanistan that lies outside Kabul, embassies and MRAPs, but Dorronsoro (thankfully) does not fall into this category.\n “My Life With the Taliban” (Abdul Salam Zaeef)\nI’d be remiss if I didn’t mention my own addition to the pile (full disclosure: I was one of the editors of this book). At a time when much of the world’s attention falls on Afghanistan – for good or for ill – there are few books that convey a real and unfiltered sense of the Taliban movement and their roots in the villages of southern Afghanistan. This book does that. And the fact that I spent close to four years working to get this book translated, edited and published should tell you something about how important I think it is that policy-makers read this book."
  },
  {
    "objectID": "personal/2010-01-08-staying-apart.html",
    "href": "personal/2010-01-08-staying-apart.html",
    "title": "Staying Apart",
    "section": "",
    "text": "Distance can be cruel. Separation may only be the beginning of our problems, but it is almost certainly the root. I don’t believe in a ‘clash of civilisations’, except in the sense that we ourselves create similar dynamics with our actions.\nThis distance is easily found in Kandahar: Kabul is 305 miles away by road (that you can’t take because it is too dangerous) or an hour by plane. The urgency of the situation in southern Afghanistan is not evident when you pass through government offices in Kabul, nor is it readily apparent in Kandahar itself. The main staging area for foreign troops in the south is Kandahar Airfield or KAF, itself some 11 miles away from Kandahar City. As you can see in the video above, there is also a separation on the roads, where convoys pass by local traffic as if from another planet.\nLanguage is another problem: foreigners for the most part don’t speak Pashtu, and I found that the Afghan translators employed by foreign troops aren’t always Pashtu speakers either – I went to Arghandab a few weeks ago and found only one translator who seemed to have a firm command; the rest knew a few words before lapsing into Dari.\nThen when we talk about the Taliban, there are misunderstandings on both ‘sides’. Obama’s objectives – despite all the drawn out policy reviews and speeches intended to delineate exactly these – are opaque to almost everyone down here. Similarly when it is the turn of foreigners to look in and assess the long-term goals of ‘the Taliban’.\nJournalists themselves are separated from the people in Kandahar:\n\nI’m worried Lang’s death will lead to further scaling back of reporting from Afghanistan.\nAs it is, Canadians already get a limited view of Afghanistan. The handful of Canadian reporters who do cover the country are based at the NATO base at the Kandahar airfield and save occasional trips to Kabul, Canadian reporters are rarely assigned to cover stories outside the Canadian zone of operations.\n[From Why reporters should stay in Afghanistan]\n\nIt was with these issues in mind that I travelled to Arghestan today. It’s only two hours drive from Kandahar City to the district centre in Arghestan, but you’d be lucky to find someone who’ll go with you. This is not to say that the district is dangerous – far from it in fact – but that the distances are an obstacle even for Afghans.\n\n\n\nArghestan district, Kandahar province\n\n\nThe road out to the district centre is newly-built and, according to the district chief, one of the main development projects which have been carried out in the district since he took over some three years ago. Only in a few areas did our driver have some worries about passing through safely; for most parts of the road we were completely undisturbed – there was even very little traffic heading to and from Arghestan.\nI’ll save the details of this trip for a separate article I’m writing for publication elsewhere, but suffice it to say that Arghestan poses a number of interesting questions: why, in the first place, is the district one of the few places in Kandahar province where you can travel so freely? Is it indeed, as many allege, because the insurgency uses Arghestan as a pathway between Pakistan and southern Afghanistan, transporting wounded fighters and other supplies, or because their is some sort of unspoken deal (‘don’t scratch my back and I won’t scratch yours’)? Is the almost complete absence of foreigners (and I imply money and development projects as part of this) a reason for the calmness in the district, or is the calmness a reason for the absence of foreigners and their money?\nI certainly didn’t come back from the brief trip with any firm answers on these questions. The district chief seems to be a good choice, doing useful work and liked by the people. Arghestan, we should remember, is Afghanistan’s second biggest district by area, and for someone to be able to keep the lid on the situation (with only 80 policemen assigned to the district, reduced from 138 last year) is at least worthy of our attention and careful assessment.\nBack in the City, people are depressed and seem to have lost any of the little hope they had a few months ago – back when we were all waiting for a ‘new’ strategy that was actually ‘new’; back when it seemed people had become serious about Afghanistan.\nRecent weeks have brought their own changes and surprises: the head of the Mohammadzai tribal shura, a friend of Felix and I, was kidnapped just over a week ago from Ayno Meena, one of the safer areas around the City. A policeman was shot dead in Wesh, a residential area located close to Spin Boldak district centre, shattering illusions of what local people thought of as one of the last safe areas in Kandahar; people stopped going out at night for a week after the policeman was shot.\nIt seems nowhere is safe any more, least of all the city."
  },
  {
    "objectID": "personal/2010-01-30-talking-to-terrorists.html",
    "href": "personal/2010-01-30-talking-to-terrorists.html",
    "title": "‘Talking to Terrorists’",
    "section": "",
    "text": "“The reality was that [Afghanistan] was viewed as an unwanted headache and one which seemed increasingly impossible to solve. This much is made clear from official government documents from the period, which reflect the sense of defeatism and intellectual exhaustion that permeated the highest echelons of the British state. […] Governments had cast around for a ‘silver bullet’ to solve the crisis, oscillating between markedly divergent positions. […] thinking on [Afghanistan] now appeared more rudderless than ever.\n[…] a policy vacuum allowed the notion of ‘talking to terrorists’ to once more re-enter British calculations.”\n\nI’ve doctored the above passage a little, but it’s certainly an interesting parallel for the present day discussion. The passage is, in fact, discussing post-1975 Northern Ireland and the British government’s return to a policy of clandestine discussions through intermediaries with figures from within the Provisional IRA.\nThe book does caution against drawing parallels between different circumstances – everything is local, after all – but the fact that even a brief read in the book will remind you of what is happening with international policy towards the Taliban at the moment is an indicator that there are at least lessons to be learnt here: ending political stalemate in the greater Kandahar area at the moment should be the single priority of any efforts to find ‘a solution’, but doing so from a point of strategic bankruptcy will inevitably be to the detriment of everyone’s long-term future.\nAs such, the book “Talking to Terrorists: Making Peace in Northern Ireland and the Basque Country (Crises in World Politics)” (John Bew, Martyn Frampton, Inigo Gurruchaga) is an absolute must-read for policy-makers who see a future (or an end-game) in the possibility of some sort of negotiated settlement with the Taliban."
  },
  {
    "objectID": "personal/2010-02-05-mistakes-in-the-financial-times.html",
    "href": "personal/2010-02-05-mistakes-in-the-financial-times.html",
    "title": "FT does Kandahar",
    "section": "",
    "text": "Good article in yesterday’s Financial Times newspaper on Amir Mohammad Agha, very much a so-called ‘key player’ in Arghandab district of Kandahar province.  Matt Green outlined why he might be important to the viability of US forces in the district, possibly (although I don’t know this) invited down there by the 82nd Airborne to impress on Amir Mohammad Agha the importance of which way he decides to choose.  In any case, I don’t imagine any ‘decision’ taken by Amir Mohammad Agha to be public and clear-cut. I was greatly disappointed, however, that Matt missed out on the key point when it comes to Amir Mohammad Agha – he is Mullah Mohammad Omar’s father-in-law.  (and with that, he also missed the extremely important 1980s context and just how involved Amir Mohamad Agha was involved in the early years of the Taliban movement post-1994."
  },
  {
    "objectID": "personal/2010-04-11-back-home.html",
    "href": "personal/2010-04-11-back-home.html",
    "title": "Back Home",
    "section": "",
    "text": "Kandahar, it seems, has changed. Felix and I were away for a little over two months, and during that time security conditions in the city have worsened considerably. The threat comes not just from the Taliban – who are able to carry out occasional prominent operations and move around the city – but also criminal groups. Kidnappings, robberies, intimidation – these seem to be par for the course for residents inside the city.\n‘The surge’ is coming, too, and everyone knows it. Some families are sending women and children away, either to Quetta or to Kabul; those who could afford to do so had mostly done this already. Young people who manage to find work or study opportunities outside Kandahar are staying away. “Come back to Kandahar?” said one Kandahari friend of mine now working in Kabul. “You’ve got to be kidding, right?”\nI haven’t really had a chance to catch up on what’s going on outside the city, let alone what’s going on in the districts, but I hope reporting this summer is going to be better than this recent article (“Barrel-chested governor Canada’s 250-lb political weapon in Kandahar” by Murray Brewster). Steve Coll’s blog post on everyone’s favourite brother is a must-read.\nI’m knee-deep in research work and reading of my own. On my bed-side table for the coming couple of weeks (ok, I don’t have a bed-side table…) are:\n– Brynjar Lia’s Architect of Global Jihad\n– David Cloud and Greg Jaffe’s The Fourth Star\n– David Finkel’s The Good Soldiers\n– Philip Short’s Pol Pot: Anatomy of a Nightmare\n– Vasily Grossman’s A Writer At War: V.G. with the Red Army, 1941-1945\nAnother book I’ve been dipping into recently is Patrick Porter’s Military Orientalism (Hurst, 2009), an excellent take on the way militaries see each other and adapt to their ‘enemy’. I haven’t yet read the chapter which deals with the Taliban, but I’ll be sure to comment here when I do.\nThe things we’re working on have completely filled our plates for the next half year or so: a collection of Taliban ‘poems’ or songs that we’re putting out a translation of next year; a second volume together with Mullah Zaeef on the history of the Taliban movement 1980s-present day that we hope will address all the things everyone said he neglected to mention in the first book; and a large research project for New York University on the extent of links between the Taliban and al Qaeda (and all the various affiliates of both) which tackles everything from the 1970s onwards."
  },
  {
    "objectID": "personal/2010-04-12-hearts-and-minds.html",
    "href": "personal/2010-04-12-hearts-and-minds.html",
    "title": "Hearts and Minds",
    "section": "",
    "text": "I type to you now without the sound of a generator in the background. Yes, it’s that time of the week - we have city power. For those of you who haven’t had the pleasure of working with a generator in the next room, let me just tell you that it makes it difficult to think; by the end of the day you often feel like someone has been bashing your head all day. Turning the generator off is one of the most pleasant moments in my day.\n…which is why it baffles me that restoring Kandahar’s city-power isn’t more of a priority for the guys who are ‘winning hearts and minds’ (supposedly) this summer in the city. I get that without sorting out Kajaki and Dahla dams paying for fuel for massive generators to supply the city with power is a little like burning money, but we seem to be doing that anyway so why not go for the short-term fix on this one when it could make SUCH a difference.\nAnyway, rant over. Another interesting thing I heard yesterday is that shopkeepers are tearing up and throwing out their stocks of Seven Star cigarettes on account of a rumour that the company have written the word “Allah” inside the filter so that by smoking Seven Star somehow you are burning or desecrating the name of God. People really seem to be taking this one to heart.\nUPDATE: (25 minutes later) The power went again. We’re back on generator. Yuk.\nAGAIN UPDATE: Things like this don’t help either…"
  },
  {
    "objectID": "personal/2010-04-16-so-how-is-it.html",
    "href": "personal/2010-04-16-so-how-is-it.html",
    "title": "‘So how is it?’",
    "section": "",
    "text": "I’m running out of ways to describe how difficult Kandahar is becoming, and more so each day. A good friend was just a little over 100 metres away yesterday evening when the foreign offices were attacked. Lucky guy. With him was someone else I interviewed a few months ago for an article I’m writing for The National. The commander of a group of men in a private security company, he had told me how dozens of his friends had died over the course of his work. Last night while he was out with friends, the group that he works with now were all killed in the blast.\nWe’re only at the beginning of the summer. Four or five months to go before we realise that the surge didn’t really work. If only we could fast-forward to that point and avoid all the deaths to come."
  },
  {
    "objectID": "personal/2010-04-19-mullah-omar-captured.html",
    "href": "personal/2010-04-19-mullah-omar-captured.html",
    "title": "Mullah Omar captured?",
    "section": "",
    "text": "Today I heard for the third time that Mullah Mohammad Omar (i.e. Taliban leader) was captured by Pakistanis three weeks ago in Karachi. I don’t really believe it, but since everyone’s talking about it I thought I’d post something here. You know. Just in case…"
  },
  {
    "objectID": "personal/2010-04-24-jere-van-dyks-captive.html",
    "href": "personal/2010-04-24-jere-van-dyks-captive.html",
    "title": "Jere van Dyk’s ‘Captive’",
    "section": "",
    "text": "I’m looking forward to this book, just reviewed (below) by Publisher’s Weekly. Quite apart from the whole survival-memoir thing, Jere knows a lot about the Haqqanis (having spent time with them during the 1980s).\n\n“Captive: My Time as a Prisoner of the Taliban” (Jere Van Dyk)\nCaptive: My Time as a Prisoner of the Taliban Jere Van Dyk. Times, $25 (288p) ISBN 978-0-8050-8827-4\n\nAn American journalist exploring the war zone on the Afghanistan-Pakistan border reports unwanted lessons in its perils in this harrowing memoir. Having traveled with the “freedom fighters” in the ’80s, Van Dyk thought he had the connections and knowledge to navigate the tribal lands between Pakistan and Afghanistan, but he was captured by a fractious band of Taliban fighters in 2008. Van Dyk (In Afghanistan: An American Odyssey) and his Afghan guides spent 44 days in a dark cell. Well-fed but terrified, he felt a nightmare of helplessness and disorientation. Dependent on a jailer who mixed solicitude with jocular death threats and a ruthless Taliban commander who could free or kill him on a whim, the author performed Muslim prayers in an attempt to appease his captors; wary of murky conspiracies involving his cellmates, he “was afraid of everybody, including the children.” Van Dyk’s claustrophobic narrative jettisons journalistic detachment and views his ordeal through the distorting emotions of fear, shame, and self-pity. But in telling his story this way, he brings us viscerally into the mental universe of the Taliban, where paranoia and fanaticism reign, and survival requires currying favor with powerful men. The result is a gripping tale of endurance and a vivid evocation of Afghanistan’s grim realities. 1 map. (June 22)"
  },
  {
    "objectID": "personal/2010-04-26-ask-the-scholars-when-and-where-was-mullah-mohammad-omar-born.html",
    "href": "personal/2010-04-26-ask-the-scholars-when-and-where-was-mullah-mohammad-omar-born.html",
    "title": "Ask the Scholars: when and where was Mullah Mohammad Omar born?",
    "section": "",
    "text": "As part of the NYU study, I’ve been doing some delving into the ages of various key members of the Taliban and those affiliated with ‘Al Qaeda’ and the various associated groups. While doing this, I came across a whole host of differing accounts of Mullah Mohammad Omar’s age and birthplace. I thought I’d list some that I came across as a way of showing how the ‘scholarly community’ is often deeply divided on really basic issues.\n\nSana Haroon (in Frontier of Faith) says that his ‘hometown’ was Uruzgan\nJohn Cooley (in Unholy Wars) says that he was born in Maiwand district, Kandahar province\nBruce Riedel (in the execrable The Search for Al Qaeda) says that he comes from Uruzgan province\nKamal Matinuddin (in The Taliban Phenomenon) states that he was born in 1961 in “Nauda village of Panjwayi district”, Kandahar province; that he later moved with his family to Deh Rawud district of Uruzgan province, and then later migrated back to Sangisar in Kandahar province. Matinuddin’s account is frequently cited.\nRohan Gunaratna (in Inside Al Qaeda) states that he was born in 1962 in Uruzgan\nSteve Coll (in Ghost Wars) states that he was born in 1950 in Nodeh village in Kandahar province\nDexter Filkins (in The Forever War) dances around the issue and states merely that he was based in Sangisar\nAhmed Rashid (in Taliban) says that he was born in 1959 in Nodeh village near Kandahar and that he moved with his family during the 1980s jihad to Tirin Kot in Uruzgan province\nMichael Griffin (in Reaping the Whirlwind) states that he was “from Maiwand” in Kandahar province\nA hagiographical Arab jihadi account of Mullah Mohammad Omar’s life (“The Giant Man”, published by Al-Tibyan Publications) states that he was born in 1962 in Uruzgan\nAnother Arab jihadist profile on Azzam.com states that he was born in 1960 in Noori village in Kandahar province\nMullah Zaeef (in My Life With the Taliban ) says that he was born in Uruzgan around 1962\nThe French review Politique Internationale says – in the introduction to one of the few interviews made by a western news outlet with Mullah Mohammad Omar – that he was born in 1965 in a village near Kandahar.\n\nThat’s a variance of 15 years in the different estimates, and I haven’t even included the various speculations in newspaper and magazine print – of which there are volumes.\nIt all goes back to issues of information and openness among the Taliban. I’m reading Philip Short’s excellent biography of Pol Pot in the evenings here in Kandahar, and I came across this passage:\n\nEven then, he did so reluctantly. For two decades he had operated under multiple aliases: Pouk, Hay, Pol, ‘87’, Grand-Uncle, Elder Brother, First Brother - to be followed in later years by ‘99’ and Phem. “It is good to change your name,” he once told one of his secretaries. “The more often you change your name the better. It confuses the enemy.” Then he added, in a phrase which would become a Khmer Rouge mantra: “If you preserve secrecy, half the battle is already won.” The architect of the Cambodian nightmare was not a man who liked working in the open.\n\nI’m wary of drawing comparisons between the Khymer Rouge and the Taliban, if only because it seems easy to do so on the surface, but secrecy over basic points is certainly something that they shared.\nPlease let me know if you come across any ‘interesting’ citations of where Mullah Mohammad Omar was born; I even vaguely recall reading somewhere that he was born in Kunar province, but can’t remember where I read it.\nUPDATE: Someone very helpfully suggested I read this Dutch report from 1999 as to the childhood and early years of Mullah Mohammad Omar. Go to Google Translate if you don’t understand Dutch."
  },
  {
    "objectID": "personal/2010-05-01-from-ghazal-by-shin-gul-aajiz.html",
    "href": "personal/2010-05-01-from-ghazal-by-shin-gul-aajiz.html",
    "title": "From ‘Ghazal’ by Shin Gul Aajiz",
    "section": "",
    "text": "These lines from a poem written by Shin Gul Aajiz and published on the Taliban’s website sometime in late 2007:\n\nThe river of your love took me, I am going\nIf I am a drop, you are the sun of beauty\nI am a garden of flowers because of your love’s spring\n\nIt’s one of the poems that Felix and I are editing together for a collection to be published by Hurst Books in early 2011. Lots of different styles, forms and subject matter. The one above is about yearning for his ‘beloved’. Many are political (motivational anthems angry with the ‘kuffar’) but these by no means dominate the collection we’ve kept since 2006."
  },
  {
    "objectID": "personal/2010-05-22-on-experts.html",
    "href": "personal/2010-05-22-on-experts.html",
    "title": "On Experts",
    "section": "",
    "text": "I found this nice little story about John Cooley:\n\nIn his typical self-effacing manner, [John] Cooley prefaced Payback by recounting how Joe Alex Morris, a friend killed during the first stages of the Iranian revolution while reporting for the Los Angeles Times, cautioned him about taking his expertise too seriously.\n“Never consider yourself an expert on the Middle East. If you do, you’re already in deep trouble,” Morris told Cooley."
  },
  {
    "objectID": "personal/2010-07-03-hope-is-not-a-strategy.html",
    "href": "personal/2010-07-03-hope-is-not-a-strategy.html",
    "title": "Hope Is Not A Strategy",
    "section": "",
    "text": "I was browsing through my father’s pile of books a while back and I came across a hardback with a great title: ‘Hope Is Not A Strategy’. (It’s not about politics, or Afghanistan, so don’t bother looking it up).\nAnd they’re right. It’s not.\nWhich brings me to an article I’m reading at the moment: Alex Thier’s “Afghanistan’s Rocky Path to Peace”. You can see in this photo I took of my notes that I enjoyed the near-fairy-tale like quality of the article’s assumptions:\n[caption id=“” align=“alignnone” width=“480”] thier.jpg[/caption]\nThe words that occur with great frequency in this article are conditional: ‘could’ appears 8 times, ‘might’ appears 12 times and ‘would’ occurs 29 times (and also the word ‘will’ 29 times, as if force of suggestion will make something happen).\nIt’s probably just me, but I came away from this article with the sense – if this was as far as we might allow ourselves to think in terms of a possible negotiated settlement – that there is no way this can ever happen. For all that is presented is hope. Hope that this might change. He even says that the possibility would require “the stars to align”.\nThat’s not enough. There are enough alternative possibilities to the outline presented in this article that mean the concluding paragraph falls flat on its face.\n\n“Do the Afghan people get a say? After 30 years of war they are among the poorest and most traumatized people on earth. But they are possessed of endurance and an indomitable spirit. If the indigenous, neutral leadership that supports a just peace could find its voice, that might spur a movement that presses the parties to reconcile.”\n\nI say it again. Hope. Is. Not. A. Strategy."
  },
  {
    "objectID": "personal/2010-11-07-marie-colvin-in-kandahar-for-the-sunday-times.html",
    "href": "personal/2010-11-07-marie-colvin-in-kandahar-for-the-sunday-times.html",
    "title": "Marie Colvin in Kandahar for the Sunday Times",
    "section": "",
    "text": "There’s an interesting piece in today’s Sunday Times by Marie Colvin on Kandahar. It’s stuck behind a paywall, I’m afraid, so you’ll have to get it via LexisNexis or do a google search in a couple of days to see if someone copies it out elsewhere.\nIt’s an interesting story for the detail it brings out from Kandahar city. Colvin presents a picture of an increased Taliban focus on the city as a result of pressure from the outer districts where American/ISAF forces have been carrying out operations in recent weeks. One of the problems with this article, though, is that she gets the timing the wrong way round. A photo caption, for example, states that “the Taliban have begun assassinating government officials after infiltrating the city.” The Taliban’s assassination campaign has been up and running for several years now. There is nothing new, either, in the claim that the Taliban have decided to focus on the city as a special priority.\nAlready back in November/December 2009 a decision was taken to flood the city with Taliban supporters or sympathisers (and to reach out to those already living there). Much has been written on the areas that the Taliban gravitated to – for a mixture of tribal/qawmi and geographical-kinship reasons – but this piece suggests what’s going on is a new development. One interesting data point, though, is the extent of the violence. She visits Mirwais hospital to get a sense of the numbers:\n\n“The hospital’s reception desk keeps three separate books to record the bloodshed. One is for Taliban shotings, the second for IEDs and vehicle bombs and the third for”innocent deaths” – from road accidents and natural causes. The receptionist said that 14 or 15 injured victims of Taliban attacks, mostly men, were being brought in every day.”\n\nNot all of these are assassination attempts, of course. But certainly the numbers being targeted nowadays is high. Even back in late summer this year there it wasn’t unusual for 4 or 5 people to be killed in a single day.\nA key point left out of this article is the fact that assassinations are not exclusively carried out by the Taliban. A long-standing rumour in the city even holds that the early assassination campaign reinvigorated around 2006-7 was spearheaded by old Kandahari Hezb-e Islami affiliates/supporters from the older mujahedeen generation. A larger number still are completely unrelated and carried out independently of the Taliban’s assassination commission (yes, there’s an official ruling body to assess who gets targeted and who doesn’t), the result of an environment where anything goes, where the rule of law is absent and where there is simply too much violence happening to make everything a priority.\n***\nApologies for the absence. Have been taking some time together with Felix Kuehn to finish off a book-length study of the relationship between the Taliban and al-Qaeda (and their affiliates) 1970-2010, commissioned and part-funded by New York University’s Center on International Cooperation and the Norwegian government. More on that to follow."
  },
  {
    "objectID": "personal/2010-11-22-petraeus-lisbon-and-the-great-pr-push.html",
    "href": "personal/2010-11-22-petraeus-lisbon-and-the-great-pr-push.html",
    "title": "Petraeus, Lisbon and the Great PR Push",
    "section": "",
    "text": "Analysis and commentary on Afghanistan is pretty frustrating at the moment, mostly since I’m out of the country, but all the fuss over Petraeus, the Lisbon meeting this weekend and the upcoming December Strategic Review in the US. Finally put some thoughts to paper on how I see it:\n\nThe problem with milestones is that there’s always another one a little further down the road. Last week we had the NATO meeting in Lisbon, to be followed soon after by the long-anticipated December Strategic Review. I can recall back in February this year when think-tank “lifers” in Washington told me to sit tight in anticipation of the “big review” coming up in December which would deliver some much-needed policy changes. Now that we’re here the view seems much less rosy:\n\nLast week a team led by Lt. Gen. Douglas E. Lute, the president’s Afghanistan adviser at the White House, returned from Afghanistan and Pakistan with data that will serve as a basis for Mr. Obama’s review of the war next month. General Petraeus is also assembling masses of data.\n\nThose final five syllables should be enough to make even the most die-hard optimist take pause. Petraeus wants to present an empirically valid case for continuing along the current course – the so-called “default position” turbo-charged with all the money and weapons the heart could ever want. Petraeus wants to use all these “masses of data” to make you believe five things, all of which are also more problematic than he’d have you believe.\n\nRead the rest over at Current Intelligence."
  },
  {
    "objectID": "personal/2010-11-26-taliban-realism-over-the-september-11-attacks.html",
    "href": "personal/2010-11-26-taliban-realism-over-the-september-11-attacks.html",
    "title": "Taliban Realism over the September 11 Attacks",
    "section": "",
    "text": "One of the most difficult issues to navigate when discussing recent history with Taliban interviewees (especially those of a political bent) has always been the attacks on New York and Washington on September 11, 2001. Traditionally any attempt to suggest that Osama bin Laden was involved in the planning and funding of these attacks was met with skepticism as well as a statement along the lines of, “we don’t know, nor have we seen any convincing evidence and it could have been anyone who carried out and planned this attack.”\nNow, though, in the first semi-official acknowledgement from a Talib – in this case the former Ambassador to Pakistan, Mawlawi Abdul Salam Zaeef – we have the following statement in an interview:\n\nWhen asked for his opinion of Osama bin Laden and his relation with Mullah Omar following the events of 11 September, Zaeef said, “Following the September events, the Commander of the Faithful Mullah Omar met with Bin Laden in the presence of a large number of Taliban leaders and Al-Qaeda members, and asked him if they were behind the attacks on the twin towers and the Pentagon.\n“Osama denied the allegations but I now believe that Bin Laden planned the attacks without informing the Commander of the Faithful and then lied to him by denying his involvement in the attacks after they took place,” he said.\n\nThis is maybe all we’re going to get for the moment, but this admission is a crucial first step in tackling the issue of the Taliban and al-Qaeda. Let’s hope it’s part of a larger political development."
  },
  {
    "objectID": "personal/2010-12-11-an-open-letter-to-president-obama.html",
    "href": "personal/2010-12-11-an-open-letter-to-president-obama.html",
    "title": "An Open Letter to President Obama",
    "section": "",
    "text": "I’m very privileged to be able to add my name to this letter – signed by some very smart people who’ve been working in and around Afghanistan for many years.\n\nWe have been engaged and working inside Afghanistan, some of us for decades, as academics, experts and members of non-governmental organizations. Today we are deeply worried about the current course of the war and the lack of credible scenarios for the future. The cost of the war is now over $120 billion per year for the United States alone. This is unsustainable in the long run. In addition, human losses are increasing. Over 680 soldiers from the international coalition – along with hundreds of Afghans – have died this year in Afghanistan, and the year is not yet over. We appeal to you to use the unparalleled resources and influence which the United States now brings to bear in Afghanistan to achieve that longed-for peace.\n\nRead the rest up at the website www.afghanistancalltoreason.com and note that the list of signatures is growing and being updated as more people learn about the letter. Please support this initiative by forwarding the text of the letter onwards."
  },
  {
    "objectID": "personal/2010-12-18-the-best-books-of-2010.html",
    "href": "personal/2010-12-18-the-best-books-of-2010.html",
    "title": "The Best Books of 2010 (UPDATED)",
    "section": "",
    "text": "It’s the end of the year again – so fast! – and I thought it’d be worth taking a moment to reflect on what I’d read over the past year. I also managed to rope in a few friends in to provide their own roundups for the sake of variety. I allowed myself to include long-form journalism as well as books, since this year saw two really fantastic examples of that; of course there were many, many more, but the two below really stood out.\nFor non-fiction, I came to Noah Feldman’s Fall and Rise of the Islamic State a few years after it was published, but found it both interesting and lucidly written, as fine an example for how to explore these issues of ideology and political aspiration in Islam as I know. Students and scholars of political Islam take note.\nMatt Aikins notes how a new round of Iraq memoirs are being released, and at the top of these (although it’s only half-memoir) must be Wendell Steavenson’s The Weight of a Mustard Seed. She tells Iraq’s story through the voice and life of a relatively senior figure from within Saddam’s armed forces, interspersing it with her own efforts to to research that same story. It’s beautifully written – like her previous book on Georgia – and, along with Anthony Shadid’s Night Draws Near, is always something I recommend to people on Iraq. David Finkel’s The Good Soldiers tells the story of the American military’s struggles post-2003, again powerfully written.\nFrom Afghanistan, Elizabeth Rubin’s New York Times Magazine profile of President Karzai was simply one of the most compelling and interesting pieces of writing that I’ve read from the post-2001 period. You must read this if you haven’t already. Looking across the border, Jane Mayer wrote an absolutely devastating New Yorker piece on the drone strike campaign in Pakistan. I’m surprised it hasn’t received more attention. If you haven’t read it, stop what you’re doing; print it out and make time.\nReconciliation has been one of the most misused buzzwords of 2010. For a different perspective, look no further than Ed Moloney’s Voices from the Grave. This is an edited/commentary-rich oral history of two figures from Northern Ireland, published earlier this year now that both voices have died. It shows the inner machinations going on behind the scenes – including some amazing accounts of prison dynamics and the hunger strikes – and every pundit and politician seeking to involve themselves somehow in the debate must read this book as a historical and contextual corrective.\nI didn’t get the chance to read much fiction this year on account of work, but Shahriar Mandanipour’s Censoring an Iranian Love Story (reviewed in the New Yorker here) was definitely the most memorable. Time will tell whether it will last, but my sense is that this was something special.\nThere were countless numbers of books that I wanted to read but didn’t find the time. They will be priorities in 2011:\n– Alice Munro’s short-story collection, Too Much Happiness\n– Priya Satia’s Spies in Arabia (described to me by Matt Aikins as follows: “It’s about the cultural environment of Edwardian-era British secret agents in Arabia – their dissatisfaction with Western modernity, their search for some pre-modern, inscrutable purity in the ‘vast desert’ with its ‘timeless inhabitants’, the intuitionist methodologies they developed in response to a ‘mysterious Orient’ that scientific empiricism could not fathom, their cultivated literary mystique and ambitions, their habits of dressing in Arab garb and living so as to ‘become one with them’ – and the complex relationship this had to the military and political imperatives of empire and war.”) Who wouldn’t want to read that?\n– Nir Rosen’s Aftermath (although I’ll have to read his earlier Iraq book first…)\n– Two books on Kashmir: Arif Jamal’s Shadow War and Basharat Peer’s Curfewed Night.\n– Timothy Snyder’s Bloodlands, an account of the killings and deaths in central and eastern Europe during the 1930s and 1940s.\n– Mary Kaldor’s The Ultimate Weapon is No Weapon\n– Michael Lewis’s The Big Short, on the financial crisis and how it happened\n– and (although I reckon this’ll keep me going into 2012) Richard Taruskin’s magisterial Oxford History of Western Music. It’s five volumes, but Taruskin is one of the truly great living musicologists and cultural scholars of our day. It’s been out for a while but Oxford University Press have recently issued a paperback version selling at just under £60 on Amazon. That’s a bargain if ever there was one.\nHere are some selections from Matt Aikins, intrepid journalist and the talent behind Harper’s profile of General Razziq, The Master of Spin Boldak:\n\nEvery year it seems as if there are more good books being published and less time to read any of them. 2010 was no exception. There is a sort of ‘second wave’ of in-depth reporting coming out of the Afghan and Iraqi conflicts. Joshua E. S. Phillips’ chronicle of torture by US soldiers in Iraq, None of Us Were Like This Before, is among the best. It’s unflinching in every sense of the word: neither from incendiary portrayals of the depravities US military might inflicted on innocent Iraqis, nor from a nuanced and empathetic understanding of the torturers themselves, in many cases ordinary Americans who found themselves swept up, beyond morality, by forces within and without that they could hardly comprehend.\nFinally, two of my favorite reads from 2010 were not actually published in 2010. Jane Mayer’s The Dark Side is astonishing not only for its comprehensive indictment of the expansion of executive power under Bush, but for how well-written and engrossing it is. And Out of Afghanistan, Diego Cordovez and Selig Harrison’s out-of-print account of almost a decade of negotiations leading to the Geneva Accords, (which paved the way from Soviet withdrawal from Afghanistan) should truly be a must-read for every Afghanistan expert. It’s extremely relevant right now.\n\nAnd these from Anand Gopal, by far and away the best-connected and most interesting writer on the insurgency in Afghanistan (just see his paper on Kandahar if you need convincing):\n\nIn 2010 we finally saw some quality Af-Pak books hit the shelves, three of which are indispensable. Antonio Giustozzi’s Decoding the Taliban: Insights From the Field contains selections from some of the most careful and learned observers of the Afghan insurgency; if you don’t have time for the whole book, read Tom Coghlan’s take on Helmand. Giustozzi’s other release this year, Empires of Mud, is a fascinating study of warlordism in Afghanistan, a much-abused term that warranted the close attention. My Life in the Taliban by Mullah Abdul Salaam Zaeef provides a rare glimpse into the mind of a senior Taliban figure. In particular, the descriptions of life during the anti-Soviet insurgency in Kandahar are an important contribution to our understanding of the country’s history.\nOutside of the South Asia field, The Immortal Life of Henrietta Lacks gives a compelling look at the intersection between genetics, medical research, race and class. It traces the story of a poor, cancer-ridden African American woman and her unlikely (and unknowing) contribution to medical science: a cell sample that has been used to study cancer for decades. Jonathan Franzen’s novel Freedom looks at Bush-era American suburbia. I don’t think it quite lived up to its hype, but it is an important and enjoyable read nonetheless. Finally, for the mathematically inclined, I recommend Oded Goldreich’s P, NP and NP-completeness: The Basis of Complexity Theory, which gives of a good overview of the P-NP problem in computer science, which made the news this year for almost getting solved.\n\nAnd these from Naheed Mustafa, a friend and journalist who is hopefully soon starting work on a great project she has up her sleeve:\n\nI always feel like I’m six months to a year behind in my reading. I end up doing so much reading for work that I can’t get around to reading the things I want. But certainly there are worse problems one can have. I do read a lot of long form journalism and some of the pieces I especially enjoyed have already been mentioned above (Elizabeth Rubin’s profile of Hamid Karzai) and Jane Mayer’s drone piece.\nDaniyal Moinuddin’s collection of short stories In Other Rooms, Other Wonders was compelling and on the whole I thought it was an eloquent presentation of the fading of the traditional landowning class in Pakistan’s Punjab. The other two books I finally got around to reading and am happy that I did: Sweetness in the Belly by Camilla Gibb and The Book of Negroes by Lawrence Hill – both Canadian writers. Neither was published in 2010 but, like I said, I’m always behind.\nThere are several long form pieces I’d suggest as well. Two from Basharat Peer who I think is one of the most phenomenal journalists of our time and has an eloquent, literary style of writing: Kashmir’s Forever War in Granta 112: Pakistan and The Road Back from Ayodhya in The Caravan. The third is an astonishing portrait of Roger Ebert written by Chris Jones for Esquire entitled The Essential Man. Jones’ attention to detail and the tiny cues he picks up are brilliant. Roger Ebert wrote a response to Jones’ profile (on the whole positive) that you may want to read to get some sense of the process (I’m obsessed with “process”). Also, another Esquire piece called Eleven Lives by Tom Junod about the oil workers who were killed in the Deepwater Horizon explosion back in April of this year.\nMy last recommendation is actually a short excerpt from a memoir my dear and lovely friend Rahat Kurd is writing. It’s about growing up Muslim in Canada. The essay was printed in Maisonneuve magazine: Things That Make Us Muslim.\n\nYour suggestions and recommendations are welcome in the comments below."
  },
  {
    "objectID": "personal/2010-12-22-real-people-as-agents.html",
    "href": "personal/2010-12-22-real-people-as-agents.html",
    "title": "Real People as Agents",
    "section": "",
    "text": "I was reading in the first volume of Taruskin’s history of music – all right, procrastinating from overdue PhD chapters – and came across this useful and timely reminder: &gt; “Statements and actions in response to real or perceived conditions: these are the essential facts of human history. The discourse, so often slighted in the past, is in fact the story. It creates new social and intellectual conditions to which more statements and actions will respond, in an endless chain of agency. The historian needs to be on guard against the tendency, or the temptation, to simplify the story by neglecting this most basic fact of all. No historical event or change can be meaningfully asserted unless its agents can be specified; and agents can only be people. Attributions of agency unmediated by human action are, in effect, lies – or at the very least, evasions. They occur inadvertently in careless historiography (or historiography that has submitted unawares to a master narrative), and are invoked deliberately in propaganda (i.e., historiography that consciously colludes with a master narrative).” (Richard Taruskin, The Oxford History of Western Music, vol 1, p.xviii)\nIt’s good to be reminded of this when thinking about most things, but especially when discussing ideology and influence with regard to the war in Afghanistan and the identity of the various groups fighting. People have thoughts; ideas do more than just ‘emerge’. I’m just as guilty of this as anyone else, but I think writing on the nature of the Taliban, for example, could become a lot clearer if we stuck to the agency of real people rather than abstractions."
  },
  {
    "objectID": "personal/2011-01-10-an-enemy-we-created-the-website.html",
    "href": "personal/2011-01-10-an-enemy-we-created-the-website.html",
    "title": "‘An Enemy We Created’: the website",
    "section": "",
    "text": "aewc-cover(2010jan4).jpg\n\n\nYou can read more about An Enemy We Created on the book’s website. It went online tonight and contains an outline of the argument along with advance praise from several analysts and scholars of the region. This is the book Felix Kuehn and I spent the past year working on.  Go take a look:\nwww.anenemywecreated.com"
  },
  {
    "objectID": "personal/2011-01-30-afghanistans-child-soldiers.html",
    "href": "personal/2011-01-30-afghanistans-child-soldiers.html",
    "title": "Afghanistan’s Child Soldiers",
    "section": "",
    "text": "kandaharchildsoldier.jpg\n\n\nVery please to see the New York Times article this morning on the use of child soldiers in Afghanistan’s security services. Read it here. Above is a photo I took back in February 2008 in Kandahar after the death of Abdul Hakim Jan. The boy said he was 19 years old."
  },
  {
    "objectID": "personal/2011-05-01-the-petraeus-effect.html",
    "href": "personal/2011-05-01-the-petraeus-effect.html",
    "title": "The Petraeus Effect",
    "section": "",
    "text": "I’ve been going through old ISAF Press Releases, trying to examine them through a quantitative lens and see what they all mean when the numbers are aggregated. I’ve written before (also here) about Petraeus, data and PR campaigns, and this post is an illustration of those trends. First, a chart: \nFor this graph, I went through all of ISAF’s press releases from November 2009 looking specifically at any mention of night-raids, detentions of insurgency members, ‘facilitators’ and ‘leaders’. The total number of press releases is, therefore, much larger (including all the development/governance pieces) but it correlates to the line on this chart. The only point I’d mention as well is that the decrease in numbers of press releases recently shouldn’t be taken as a lessening of the media campaign (or of the military’s ‘operational tempo’ as I think it’s meant to be called); rather, press releases these days often take the form of a morning summary or an evening summary of events. These will often include information about several operations (or several raids) in one press release. Later this week I’ll get back to you with the same data set, but showing the overall number of night-raid-type operations mentioned in Press Releases. This will, I reckon, probably show much larger (or consistently large) numbers up till the present day.\nUPDATE: Just for a sense of the overall numbers, between November 30, 2009 and today (May 1, 2011) there were 3,167 official ISAF press releases issued. The number that refer to raids, detentions or deaths of insurgency members is just over 1,500."
  },
  {
    "objectID": "personal/2011-05-13-talqaeda-the-timeline.html",
    "href": "personal/2011-05-13-talqaeda-the-timeline.html",
    "title": "TalQaeda: the Timeline",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“1035”][](http://www.anenemywecreated.com/timeline/) An Enemy We Created - Timeline[/caption]\nLong-standing readers of this blog will know that I have been working (together with my colleague, Felix Kuehn) to get to grips with the nature of the relationship between the Taliban and al-Qaeda (and the various affiliates of both). To that end, we’ve written a book (‘An Enemy We Created’) and a summary paper dealing with the post-2001 aspects for the Center on International Cooperation (who funded much of the research).\nWe reread most of the primary and secondary-source literature on the topic, and while going through compiled long lists of dates, names and places. The key dates from those lists made their way into this timeline. There are fewer entries post-2001 at the moment on the timeline, but I’ll get round to updating that soon hopefully. Please leave any feedback on the timeline in the comments below this post, or send me an email.\nYou can view the timeline at: www.anenemywecreated.com/timeline/\nThis timeline was created in Tinderbox, the software that I used for most of my research and database work. I have to thank Mark Bernstein and particularly Mark Anderson for their help with some of the technical issues in coding the site. I’d strongly recommend Tinderbox to researchers, journalists and other ‘information workers’. I’d be lost without it.\nFor those in London, I’ll be presenting on how I use Tinderbox in my day-to-day research work at a one-day event on May 28. Register here if you’d like to come along."
  },
  {
    "objectID": "personal/2011-05-16-more-isaf-press-release-data.html",
    "href": "personal/2011-05-16-more-isaf-press-release-data.html",
    "title": "More data on ‘Kill-Capture’ Raids",
    "section": "",
    "text": "Last week I charted the numbers of press releases that ISAF have put out relating to “security operations” of one kind or another. I promised to split this data up into numbers of incidents mentioned in those press releases (since some press releases contain multiple incidents). The data from last week showed a decrease in the numbers of press releases (particularly in 2011). When these numbers are split up into individual incidents, however, you can see that there hasn’t been a decrease in incident numbers. Actually, April 2011 had almost as many operations as September 2010 (the highest for the data set). Of course, all of this is just a picture as presented by ISAF, but since they don’t release these figures in aggregate form to the public/media it’s all we have to go on. Here’s the new chart:\n\nA word on the title: the incidents here are collected from ISAF press releases that refer to an event where an Afghan (or a ‘foreign fighter’) was killed or captured. Sometimes this happened while troops were on patrol, but more often was the result of a targeted raid/operation.\nI’m working on something longer that will go into the specifics of this data set in more detail (incident data by type/province/month etc) so this will probably be my last post on the subject until the report is released."
  },
  {
    "objectID": "personal/2011-08-20-an-appeal-for-funding.html",
    "href": "personal/2011-08-20-an-appeal-for-funding.html",
    "title": "An Appeal for Funding",
    "section": "",
    "text": "(This is a joint post by myself, Felix Kuehn and Anand Gopal) I wouldn’t normally put something like this up on the blog, but after over a year or so of asking around (without success) we’re trying all options.\nFor several years now, Felix, Anand and I have been collecting old (and new) Taliban documents. Felix and I made a point of finding things that covered pre-2001, and Anand found things post-2001. Some of the research you may have read about on this blog or in books came from this material. For example, the poems written by Talibs in Poetry of the Taliban (published later this year) were all gathered together in this way.\nOur collection is pretty wide and comprehensive. I won’t say too much about the kinds of sources we have, but suffice it to say that we have complete collections of most publications and books that the Taliban were associated with (and various other documents/videos/audiotapes). These sources date from the 1980s until the present day.\nWe are looking – we have been looking – for funds to translate these sources into English and place them (and scans of the originals) online so that researchers and anyone else can access them. Almost none of this material is used (or has been used) in the study of the Taliban (particularly pre-2001) and this project would allow a far deeper understanding.\nIf you are a donor and are interested in funding this project, please get in touch with me, Felix or Anand at the ‘Contact Alex’ section on the right."
  },
  {
    "objectID": "personal/2011-09-19-vietnams-kill-capture-raids.html",
    "href": "personal/2011-09-19-vietnams-kill-capture-raids.html",
    "title": "Vietnam’s Kill-Capture Raids",
    "section": "",
    "text": "Have been dipping into Mark Moyar’s Phoenix and the Birds of Prey: Counterinsurgency and Counterterrorism in Vietnam in recent days. I find the Afghanistan-Vietnam comparison a bit of a non-starter (for various reasons) but the extensive use of targetted operations perhaps akin to the capture-and-kill raids being employed across Afghanistan mean it’s at least worth exploring. (One recent article for the Foreign Policy Research Institute makes that explicit connection).\nWhat follows is an extended sequence of quotations taken from that book; I’ll leave you to draw your own conclusions. Bracketed numbers after quotations are page references:\n\n“The [insurgent] shadow government, it is certain, expanded greatly from 1960 to 1965, then shrank somewhat from 1965 to 1967. The most reasonable estimate of political cadre strength in 1967 probably came from the Central Intelligence Agency (CIA), which concluded that it lay somewhere between 80,000 and 150,000.” (11)\n\n\n“The CIA will not release its own statistics, but the statements of certain CIA officers strongly suggest that the CIA actually understated the numbers of VC/VCI captured or killed in the documents made available to other U.S. government organizations and to the public. The CIA did so, in all probability, to avoid negative publicity. The figures in these documents, especially the 1971 figures, have a high ratio of captured to killed. Some have stated that the PRUs consistently killed more Communists than they captured. Enders spoke of his years as regional PRU adviser in both I Corps and III Corps: “It was very hard for us to bring the number of captures above the number of kills — we tried hard to do that — because it was difficult to capture people. I’d say that the ratio of killed to captured generally was about two to one.” Jack Harrell stated, “In II Corps and III Corps, when I was in those areas, the ratio for the PRUs was probably two Communists killed to one captured. In the documents, the CIA did not wildly exaggerate the numbers of VC/VCI captured; doing so might have attracted unwanted attention. The number killed, therefore, must have been much higher than the documents state.\nA number of CIA advisers have indicated explicitly that they thought the PRUs generally captured or killed more VC/VCI per month than the available documents list. […] PRU National Director William Redel, for instance, told CIA officer Orrin DeForest at the end of 1968 that the PRUs had been responsible “for approximately seven thousand Vietcong killed per year for the past four or five years,” which averages out to 583 per month. Rear Adm. Irve LeMoyne gave a figure that suggests even larger nationwide neutralization totals: “We were capturing, in the delta, a thousand to twelve hundred VCI monthly.”\nMy best estimate, based on my own conversations with many PRU advisers and on other sources, is that the CIA leadership believed that the PRUs capptured or killed anywhere from 700 to 1,500 Communists during most months from 1967 to 1972.” (172-3)\n\nThere then follows two chapters which provide evidence for the “statistical falsification” which “inadvertently allowed the Allies to use seemingly impressive statistics to make misleading claims of success” (235). See chapters 16 and 17 to read those in full. Chapter 18 shows how “the Allies captured many VCI and reported them as neutralized but then released them from captivity after a short period of time. Some rejoined the shadow government, but the Allies had no way of knowing exactly how many.” (236) See Gareth Porter’s story for an Afghanistan parallel.\nThere’s a little bit on what I guess would be the equivalent of Afghanistan’s reintegration programme.\n\n“The GVN’s Third Party Inducement program proved much more detrimental to statistical accuracy than these other factors. Under the terms of this program, which lasted from 1967 to 1969, the GVN offered rewards to anyone who induced a Communist to rally. The program caused the number of ralliers to mushroom. Some South Vietnamese convinced friends or relatives in the VC to rally, but others presented non-Communist friends or relatives as VC or VCI ralliers. A fraudulent rallier typically received part of the reward from the third party. GVN officials often took their cuts, as well, in return for tolerating the deceit and even encouraged people to rally fraudulently so as to increase the flow of American money into their pockets. In some places, the Americans estimated that as many as half of the people who rallied through this program did not belong to the VC. Some Communists and non-Communists also rallied in more than one province in order to collect multiple rewards, thus inflating the statistics even further. Brig. Gen. James Herbert commented: “The Vietnamese are very flexible, and they know how to beat the system. If you had a reward for becoming a Hoi Chanh [rallier], some VC would rally and collect the reward. After a period of time, they’d be fed back into the society. They might wander somewhere else and give up there. They’d give a different alias each time, and communication among the various provinces was not very good, so they could do it easily. I think a VC could almost make a living giving himself up across the country.” Considerable disagreement among the Americans over the VCI rallier numbers provides further evidence that the statistics were far from precise. A Rand Corporation study gives figures for VCI ralliers that differ sharply from those of Phoenix.” (238-9)\n\nAnd something on the classification of those being killed or captured:\n\n“Not only did the Allies report many non-VCI as VCI, they also assigned false ranks to many of the VCI whom they did neutralize. The South Vietnamese often assigned high ranks to corpses and, to a lesser extent, to prisoners and ralliers, who actually were low-ranking Communists or not Communists at all. In so doing, of course, they wanted to make their reports look better. Jack Harrell commented, “I would say that in many cases, few of the people we captured or killed were as important or as highly placed in the VCI as they were classified.”” (241)\n\nI’m looking forward to reading some statistical studies of these kinds of operations as found in Iraq and Afghanistan in recent years. There are various people formerly serving in the military in both countries who are writing up their experiences as Masters or PhD theses. Please leave a comment below if you know of any related research in the works."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "",
    "text": "The Taliban twitter account (sic) is back in the news again, this time courtesy of the US Senate:\nThe article then goes on to restate some of the usual assumptions and apparently unchecked facts of the story that have been mentioned in the more recent slew of press. I’ve rounded up links to most of these articles here for you; but seriously, don’t waste your time.\nI’ll leave it to others to explain why the Senate getting excited about ‘the Taliban twitter account’ doesn’t seem to make a lot of sense – I don’t claim any particular understanding of that world – but I really hope reporting on the matter starts to improve. By way of example, more from that article by Ben Farmer:\nI’m not sure what it takes for the Taliban to ‘embrace’ social media, but apparently not much. The Taliban set up some official twitter accounts back as far as 2009 and these accounts have been autoposting since then (more below). That’s it. It would be more accurate to say that media reports have enthusiastically embraced reporting on the Taliban’s activities on Twitter.\nNo. Just no. The account @abalkhi appears to have nothing to do with the Taliban (see below). I’d also be interested to see the evidence for the statement that ‘Taliban spokesmen also frequently spar with Nato press officers’. I have not seen a single instance of this. Every other story on these accounts repeats this claim. And it’s presumably quite an important distinction: an official spokesman (we might assume it is a man) engaged in verbal attacks on the official ISAF account is a different thing from some fanboy in his bedroom doing the same thing.\nSo, in the hope that this story can die the death it should have MONTHS ago, here are some facts.\nThe following is a list of the Twitter accounts most frequently associated with the Taliban, presented in the order they were first created:"
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alsomood",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alsomood",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@alsomood",
    "text": "@alsomood\nstarted: June 3, 2009 // regularity of tweets: 2 or times per week // language: Arabic // name: Majallat al-Somood\nfollowing: 10 // followers: 574 // number of tweets: 379\nThis was the very first twitter account that the Taliban seem to have set up. (Or, if they set accounts up earlier, they have not been used). @alsomood is the official account for one of the Taliban’s magazines, al-Somood. This is an Arabic-language magazine that caters to audiences in the Gulf, for the most part. Printed copies of the magazine have even shown up from time to time. For the most part, however, it’s just a PDF edition, released once every month. It mostly includes longer articles and commentary not found elsewhere on the Taliban’s main site, although one or two articles are usually translated from al-Somood and shared via the main web outlet. The @alsomood account tweets once or twice a week in Arabic, and every single time these tweets are automated by twitterfeed. Twitterfeed is a site that allows you to automatically post a tweet every time something changes on your website, for example. You give it an RSS feed to follow, and every time there’s a new link it autoposts. Which is to say, there does not ever have to be anyone operating this account. It is fully automated."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarah3",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarah3",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@alemarah3",
    "text": "@alemarah3\nstarted: October 22, 2010 // regularity of tweets: stopped // language: English // name: Islamic Emirate of Afghanistan\nfollowing: 3 // followers: 50 // number of tweets: 6\nThis appears to have been an experimental account. It was only used for 6 tweets, and stopped on October 27, 2010."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarahweb",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarahweb",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@alemarahweb",
    "text": "@alemarahweb\nstarted: December 19, 2010 // regularity of tweets: daily // language: English // name: Mostafa Ahmedi\nfollowing: 4 // followers: 6420 // number of tweets: 3014\nThis is one of the accounts that is followed by journalists. It is exclusively posted to by twitterfeed. There appears to be no direct manual tweeting on this account. This is an official account. It is also one of the two accounts that @ISAFmedia believe to “have some tie to the Taliban.”"
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarah222",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarah222",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@alemarah222",
    "text": "@alemarah222\nstarted: February 18, 2011 // regularity of tweets: stopped // language: English // name: Ahmad\nfollowing: 8 // followers: 14 // number of tweets: 4\nThis sees to have been another experimental account. It only tweeted 4 times, each of which were of a Taliban video."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarahmedia",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#alemarahmedia",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@alemarahmedia",
    "text": "@alemarahmedia\nstarted: February 21, 2011 // regularity of tweets: Irregular // language: English // name: Alemarah Media\nfollowing: 2 // followers: 30 // number of tweets: 36\nThis account was abandoned on March 11, 2011. There was some manual tweeting, including a mix of videos from the Pakistani Taliban. It appears to be unofficial."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#hanif_hamad",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#hanif_hamad",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@hanif_hamad",
    "text": "@hanif_hamad\nstarted: May 3, 2011 // regularity of tweets: Daily // language: English // name: Afghanistan news\nfollowing: 296 // followers: 78 // number of tweets: 1090\nThis is another official account that runs off twitterfeed. There is no manual tweeting from this account. Moreover, when @alemarahweb updates, @hanif_hamad updates simultaneously with the same message. This means that they are running off the same RSS feed (and probably the same twitterfeed account). It was started the day after bin Laden was killed."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#abalkhi",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#abalkhi",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@ABalkhi",
    "text": "@ABalkhi\nstarted: May 12, 2011 // regularity of tweets: Daily // language: English // name: Abdulqahar Balkhi\nfollowing: 24 // followers: 4293 // number of tweets: 865\nThis is the most well-known of the alleged ‘Taliban’ accounts, yet everything seems to suggest that @Abalkhi (and the account later created, @Abalkhii with two ‘i’s) is unofficial. He never tweets any material which isn’t already up on the Taliban’s website. He seems to speak Pashtu and/or Dari (translating material from the news section of the Pashtu site before it has been translated and uploaded on the English site). This might be (at a stretch) one reason why journalists continue to refer to his account as being ’official’. He also tweets completely manually – presumably because he has no access to the official site’s RSS stream (which is not provided to normal users of the website). He set up his account a week after the death of bin Laden, and my hunch is that the operator of this account probably doesn’t even live in Afghanistan (or Pakistan)."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#muhammadzabiull",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#muhammadzabiull",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@MuhammadZabiull",
    "text": "@MuhammadZabiull\nstarted: September 20, 2011 // regularity of tweets: Irregular // language: English // name: Muhammad Zabiullah\nfollowing: 137 // followers: 46 // number of tweets: 27\nThis also seems to be an unofficial account. The user states their location as being in ‘Paktia Afghanistan’, but some of his tweets imply that he is outside the country (although seem to suggest that he is Afghan by nationality). He tweets a fair amount manually, sometimes only providing links, and other times corresponding with @Abalkhi. This account was set up relatively recently."
  },
  {
    "objectID": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#abalkhii",
    "href": "personal/2011-12-28-talibantwitterfight-the-news-story-that-wasnt.html#abalkhii",
    "title": "#talibantwitterfight: The News Story That Wasn’t",
    "section": "@ABalkhii",
    "text": "@ABalkhii\nstarted: September 20, 2011 // regularity of tweets: Weekly // language: English // name: Abdulqahar Balkhi\nfollowing: 54 // followers: 54 // number of tweets: 80\nThis user has a very similar name to @Abalkhi, and it is possible both accounts are operated by the same person. The only difference is that @Abalkhi seems to update almost exclusively through the web app, and @Abalkhii seems to update mostly on his/(her?) iPhone. While @Abalkhi almost exclusively tweets stories from the Taliban website, @Abalkhii (created in September 2011) tweets stories from the international news media and engages in a fair amount of discussion with other twitter users. Moreover, the language style @Abalkhii uses is quite different from that of @Abalkhi.\nYou can view a timeline of when these accounts were created here.\nAt any rate, I hope this puts to rest the whole ‘Taliban spokesmen are on the internet engaged in big twitter discussions with ISAF’. The truth is that they are not. There is one account which occasionally responds to @ISAFmedia, but (for reasons outlined above) it does not seem to be official.\nIn fact, the only people who seem to really be enjoying this all are @ISAFmedia themselves and the media outlets covering the story. Almost every day now, @ISAFmedia puts out a tweet to @Alemarahweb saying that something that was posted was wrong. This is one example:\n\nAnd, in a way, it sort of represents the futility of a lot of what goes on in Afghanistan these days: someone sitting behind a desk in ISAF headquarters, tweeting away at a Taliban twitter account, hoping to goad someone in response, but there is nobody to respond to since @Alemarahweb is tweeting automatically without anyone needed to run their account."
  },
  {
    "objectID": "personal/2012-12-10-catching-up-aan-report-on-isaf-night-raids.html",
    "href": "personal/2012-12-10-catching-up-aan-report-on-isaf-night-raids.html",
    "title": "Catching Up: AAN Report on ISAF Night Raids",
    "section": "",
    "text": "I haven’t blogged in a really long time. Expect that to change. Before I get round to writing something new, I have some long overdue announcements and posts relating to writing work I was busy with last year. Most of you will be aware of the publications. If so, carry on. If not, perhaps take a look. In October 2011, I wrote a report together with Felix Kuehn on ISAF’s night raids and other operations described in their press releases over a 2+ year period. That was for the Afghanistan Analysts Network; if you’re not reading their blog, you’re missing a lot. (Seriously, subscribe to their RSS feed and save yourself the headache of having to check back.)\nWe partnered with The Guardian’s DataBlog team, who offered visualisations of some of the data presented in the report. Check out their stories here, here and here.\nOur report was met with an extremely forceful response from ISAF, to which AAN issued a collective reply. We never heard back from ISAF after that.\nWe’re currently at work on an update to this report which will be published by AAN relatively soon (most likely early next year). Watch this space."
  },
  {
    "objectID": "personal/2012-12-22-catching-up-poetry-of-the-taliban.html",
    "href": "personal/2012-12-22-catching-up-poetry-of-the-taliban.html",
    "title": "Catching Up: Poetry of the Taliban",
    "section": "",
    "text": "The second book that came out this year – I co-edited it together with Felix Kuehn – was Poetry of the Taliban. Some people weren’t entirely happy with that idea, but on the whole the reviews were pretty positive, both about the collection and about the idea of publishing these translated songs.\nThe very existence of these cultural artefacts provoked a discussion – ultimately one of the things we had hoped would happen – and if you browse through the list of responses you can get a sense of the diversity of that debate.\nIf you haven’t managed to get hold of a copy, it should be available in most good bookshops (or on online retailers).\n(Hint: this would make an excellent Christmas stocking filler.)"
  },
  {
    "objectID": "personal/2013-01-02-useful-tools-pinboard.html",
    "href": "personal/2013-01-02-useful-tools-pinboard.html",
    "title": "Useful Tools: Pinboard",
    "section": "",
    "text": "This is the first in a series of posts I’ll be doing on this blog detailing some software or web services that I use. I’ll try to end each post with two examples of things I’ve used the software for recently. Pinboard is an online bookmarking service. I save all the articles I read online there with a handy bookmarklet, and everything I read in Instapaper and via twitter also gets saved there. Even better, if you upgrade to a premium subscription, Pinboard’s servers will make an archive copy of the site so even if it is taken offline, you’ll still have a copy of the site. And before you say that other services do it better, read this.\nIt’s handy for sharing collated link collections with people and it’s useful just as an archive of everything you’ve been reading.\nTwo recent uses of Pinboard:\n\nI keep a rolling list of all the reviews and comments on my recent edited book, Poetry of the Taliban. You can see this here. These kinds of lists are great for sharing with other people.\nThe other day I remembered I had read something online, but couldn’t quite remember where, so I searched within my Pinboard archive (including the text of all the websites I’d visited and read articles from in the past two weeks), finding the article within seconds."
  },
  {
    "objectID": "personal/2013-01-13-a-jedi-mind-trick-and-three-other-approaches-to-learning-vocabulary.html",
    "href": "personal/2013-01-13-a-jedi-mind-trick-and-three-other-approaches-to-learning-vocabulary.html",
    "title": "A Jedi-Mind Trick and Three Other Approaches to Learning Vocabulary",
    "section": "",
    "text": "UPDATE: I now offer one-on-one language coaching. Read more about what it involves and what kinds of problems it’s best suited to addressing.\n[My last post on the study of languages seems to have been well-received so I thought I’d add a few thoughts on the study of vocabulary. For some reason it’s another stumbling block for many people who lack a system to be able to manage their vocabulary learning. I hope that by the time you finish reading this post you’ll have some approaches and tools to think about, at the very least.] There are three things you need to know about and do when it comes down to learning vocabulary, possibly four.\n\n1. Word Association\nThis is pretty basic stuff as far as vocab learning goes, but if you don’t know it it can be something of a revelation. See this (and click through the following links) for a basic outline.\nThe basic task here is to associate the meaning along with the sound of the word.\nThe trick with word association is to make the images in your head as crazy as possible. You need to make it stick in your mind, so the more outrageous the image, the more it’s going to stick. You might think it takes too long to imagine these scenes/images (that I’ll describe) but it’ll pay off in the long-run.\nSo, what you have to do is take a word and mentally associate it with its meaning. Take the Arabic word mumill (meaning ‘boring’). Close your eyes if you need to. When I see that word, I think of two things – MOO (the sound that the cow makes) and then a flour MILL. And somehow I have to try to associate those two things with the concept of ‘boring’. So I imagine a flour mill, an old slightly dusty stone flour mill. You can hear the slow grinding of the mill on the flour, grinding it down to a fine powder. You can smell a bit of the flour in the air; perhaps the particles in the air are brushing against your face, getting in your hair. You can see the dark stone. When you touch the mill itself, it’s a bit warm to the touch from all the grinding it’s been doing. When you turn and look to see who’s driving/pushing the millstone, to your surprise you see that it’s a cow, who makes a gratifyingly loud ‘MOO’ sound when she sees you. When the cow walks past, you can smell the ‘cow smell’ and she’s warm to the touch as well (having worked so hard). You see that she’s extremely bored doing her milling, and you see that in one paw/hand/foot she has a sudoku game (or whatever) that she’s doing while she pushes the millstone around and around to stop herself from being too bored.\nAnyway, that’s more or less what you have to do for every word. Split it up into sounds and then do this word association. The important things are to:\n\nuse all the senses (sight, smell, touch, taste, hearing) in your association, because that’s what will make it really stick in your mind and\nmake the images/scenes you create as wild as possible.\n\nLearning professionals usually suggest to play on the deeper patterns, including things that are embarrassing etc etc, so try to bring all of that into your visualisation. Also, personalise the images. It needs to resonate FOR YOU. Use objects that evoke very specific and strong emotions: love, sex, war, your late relative, object of your infatuation, whatever it is; it is well known that emotional states and the full sensory palette can facilitate recall.\nThe point with all of this isn’t that you are going to remember all of this image when you hear the word ‘mumill’. Your brain will move much faster than that, and you’ll just get a glimpse at the image and that’ll be enough to jog your memory to provide the translation ‘boring’. After a while (I’ll explain below), you won’t need the image any more, but it’ll be there if you need it.\nFor Arabic, one of the issues is often that you have a long list of verbs, all with 3 letters with the same vowelling – darasa, faqada, hamala etc etc – and that can sometimes make it difficult to distinguish the words. But, like I said, split them up into two parts if you can, or find some way to make them stand out or associate them with something you already know.\n\n\n2. Iversen’s Lists\nYou should have gone through your list of words that you have to memorise and do this for every single word. I will assume that you will have a bit of learning to do each week or each day, incrementally, rather than getting all your words at once to learn for the entire year all in one go.\nSo do the association technique first. Then take a blank piece of paper – A4 is good (or whatever the US equivalent is) – and write a list of the words you have to learn today on the left side of the page. Try not to take up too much space. Maybe it’s 20 words. Write them down on the left side of the page.\nThen take a ruler or draw a line alongside that list and to the right of the line, (perhaps in a different colour pen), write the translation of that word. Do that for all the words. If you don’t know the word, then the memorisation image/association hasn’t stuck, so you can look up the correct answer and make sure that your association sticks this time.\nOnce you’ve completed this first test, take another piece of paper (or, better still, something thicker like a book so you can’t cheat) and cover up the first column. Now you only have your answers to look at, and you should draw another line and then write the translations. (i.e. translating things back into the original language). Do all the translations for the list, then check whether you got them right.\nThen you should repeat this until the entire piece of paper (both sides) are covered with translations back and forth. If you write small-ish, you should be able to get a good 6 or 7 rounds of translations/testing in (if not more).\nPerhaps don’t do all of these sessions at once. Do one side of the page in one go, and then leave other sessions for later in the day (for reasons I’ll explain now).\n\n\n3. Spaced Repetition & Anki\nThe really Jedi vocabulary learning trick requires that you know a bit about how the mind works and how quickly we forget. Take a look at this graph. This basically shows how the memory forgets.\n[caption id=“” align=“alignnone” width=“324”] forgettingcurve[/caption]\nSo, following the red line, when you first learn a word (or a piece of information, or anything) if you try to remember it within an hour or so, the memory is pretty good. If you try to recall that fact 6 days later (without any study in between), you’ll see (by following the red line to day 6) that the memory for such facts can swiftly decline pretty quickly.\nThere is a way to avoid this, though, which is something called spaced repetition. Because this half-life or rate of decline of memory is predictable (i.e. everyone has this curve, and how quickly it takes you to forget stuff is more or less stable/calculable), if you remind yourself of the word or fact within certain times, then you will be able to ‘reset’ the forgetting curve. The great thing about this ‘reset’ process, is that each time you do it, it takes longer to forget.\nFor example, let’s say you learned the word for mumill just now. You’d ideally want to recall that word 30 minutes after you learned it. Then an hour later, then 3 hours later, then 6 hours later, then 12 hours later, then 24 hours later, then 3 days later, then 1 week later, then 3 weeks later, then 5 weeks, then 2.5 months etc etc).\nSo if you keep catching the words at the point just before you forget them, you can steadily put the fact/word deeper into your long-term memory.\nBut, you may ask, how do you remember when the last time you tested yourself on a particular word? How do you ensure that you catch this ‘forgetting curve’ and know how far along you are with memorisation…\nLuckily, a bunch of people have already taken care of this and thought it through, and there’s a piece of software which will save your life. I wish I’d had it when I was learning languages at university.\nIt’s called Anki, and you can download it here. They’ve just released version 2.0. It’s free. There are other imitations, but Anki is really the gold standard here. Don’t bother looking around. Anki is the real deal.\nSo with this programme you create a ‘collection’ of words. You have to input your vocabulary (just one side – i.e. just Arabic-English or English-Arabic) into the programme. Once one person has done it, then you can share the decks of flashcards (either online or as offline files), so you can immediately become the most popular person on your course if you do a full collection for your course, I would guess. (For more obscure languages, and I include Pashto in this pantheon, there are some true heros who assemble vocabulary lists and upload them to sharing sites for others to use).\nAnyway, once you’ve inputted the cards, it will test you on the words in both directions (i.e. Eng-Ar as well as Ar-Eng) automatically.\nThen you just start learning. You can state how many new words you want to learn each day. I’d recommend no more than 20. And, IMPORTANT POINT, you should do steps 1 and 2 (as I explained above) BEFORE you do a round with Anki. i.e. first do the word association stuff, then do a day learning the words with the lists and the blank piece of paper and the columns, and then the next day you should learn those words in Anki. It’ll auto-test you the words and you pick an option whether the word was easy to remember, hard, very hard, or whether you didn’t remember it at all.\nDepending on which option you pick for each word (when you see the answer), it’ll then remember which forgetting curve to assign to the word, and it’ll remind you that you need to review that piece of vocabulary/fact at the appropriate time.\nSo let’s say in 1 month from now, the word mumill shows up on the screen, and you eventually remember it, but it took a bit of time. You press ‘very hard to remember’ and it’ll remind you of that word in 18 days (approx) since it recognises that you need a bit more time before it really ends up in your longer-term memory.\nThen once you’ve started with Anki, you just have to make sure to return to Anki once a day and study the words that show up for review. Luckily there is a mobile version of Anki available (for iPhone/iPad as well as Android). I’ll assume you have a phone which is either an iphone or an android. The mobile versions you have to pay for. But it’s completely worth it.\nThis means that whenever you’re stuck in a bus, or waiting in a queue or something, you just need to pull out your phone and you can review a few words on Anki. It has all the same features as the desktop version (apart from the ability to add words, I think). It’s also a good idea to include audio along with each vocab entry which will be another sensory association and input that will help imprint the word in your mind.\nThe trick here, and it’s really important, is to do it every day. If you only do it once a week, then you’ll forget words more often (as the forgetting curve means you’ll have missed the chance to reactivate or ‘reset’ the curve on words during the week). I really strongly recommend you do your Anki words once a day. Some days there won’t be any words, or very few (depending on how many your course has you learning).\n\n\n4. (Writing/Reading for Extra Imprinting)\nIf you really want to get to a high level in your vocab learning, then use it to support your more general skills. i.e. you should use the vocab words in context.\nWhen I use Anki, I sometimes like to take each word (when it comes up on the screen for testing/study) and before I give my answer, I first make myself use that word in a sentence. This allows me to practice grammar structures, and also creates new associations for that word with other pieces of material. (For the memory, more associations are better, since things are recalled via these webs/networks of signals in the mind). Even better, write these sentences down (although by now we’re talking study/exercises that take a bit of time, rather than just Anki etc, which would take maximum 20 minutes per day).\nThe ideal place for practicing your writing is lang-8.com, where you can get a free account. The principle here is that everyone corrects everyone else. i.e. when you put up a few sentences of writing practice, native Arabic speakers (or whatever language) will correct your sentences, but ideally you should correct their English sentences etc to return the favour. It’s all free, and done on an honour system, so you get as much as you give etc.\nI use it for my Urdu, Arabic and Pashto studies, and you’ll usually get a correction for things you post there within 12-24 hours, which is pretty amazing when you think about it.\nAnother really good way to reinforce your vocabulary is to read a lot. Most of the studies of language study and learning (see last week’s post) now agree that ‘intensive reading practice’ is the best way to build up your vocabulary. Obviously, you need to start with texts that are somewhat comprehensible and then slowly build up, and it can be really difficult. Sometimes you think that you’re just reading gobbledegook. But slowly, if you stick at it, weeks later, you’ll be able to read more and more, and you’ll be learning words without the need to memorise and go through all the systems above (although if the word’s giving you problems, or if you’d really like to remember it, then by all means add it to Anki etc) because you’ll be using and seeing that word in the context of the sentence / words around it etc.\n…\nAnyway, none of this is a substitute for the somewhat-hard work that goes into learning vocabulary, but it certainly can shortcut things. Particularly if you start inputting your vocabulary into Anki early on in the course of your language studies, and reviewing it every day throughout the year, by the end of a year you’ll be in a really good place compared to others."
  },
  {
    "objectID": "personal/2013-01-22-following-pakistans-elections.html",
    "href": "personal/2013-01-22-following-pakistans-elections.html",
    "title": "Following Pakistan’s Elections",
    "section": "",
    "text": "Saba Imtiaz, a freelance journalist based in Karachi, has started a daily newsletter of updates relating to Pakistan’s upcoming elections – scheduled to take place this summer. She explains what she’ll be offering in a recent blogpost: &gt; Along with a round-up of the headlines and commentary from English, Urdu and Sindhi news sources, I’ll also be writing smaller profiles of candidates and major news issues, as well as doing smaller data dumps on voting patterns\nView some of the updates here and subscribe to the newsletter via email here and via RSS here.\nIf you follow Colin Cookman’s excellent Daily New Briefs then you’ll probably want to subscribe to Saba’s election update."
  },
  {
    "objectID": "personal/2013-04-20-translators-sought-job-opening.html",
    "href": "personal/2013-04-20-translators-sought-job-opening.html",
    "title": "Translators Sought – Job Opening",
    "section": "",
    "text": "I’m about to start a large archive translation project (together with Felix Kuehn and Anand Gopal). We’re looking for some high-quality translators to work full-time on a project translating old newspapers and magazines from Pashto (and a small amount of Dari) into English.\nThe work would be 5 days per week, but you will be free to work from home. I.e. there will almost certainly not be any office for you to work from.\nThe project will take place over the course of a year, with a possible extension to a second year. We would initially hire you for a trial period before committing to hire you for the full year.\nIf you’re interested, please head on over to the form here and fill it out. We’ll get back to you in due course, if and when we’re ready to start the hiring process.\nUPDATE (JULY): We’ve stopped taking applications and are working on finalising the hiring process. Thanks for all your interest!"
  },
  {
    "objectID": "personal/2013-09-23-taliban-time-travel-or-how-our-understanding-is-almost-always-two-years-old.html",
    "href": "personal/2013-09-23-taliban-time-travel-or-how-our-understanding-is-almost-always-two-years-old.html",
    "title": "Taliban Time Travel, or How Our Understanding Is Almost Always Two Years Old",
    "section": "",
    "text": "I’ve been noticing something on twitter and in the public debate surrounding the Afghan Taliban over the past few days. An interview of Motassim in the Guardian last week was shared widely online, and people particularly seemed to find his comments about Mullah Mohammad Omar’s loss of control of the Taliban interesting. To those following the Taliban closely this isn’t news. It’s been the case for two or three years at least. What is interesting is that it has taken that long for that kind of analysis and comment to become accepted by mainstream commentators and to become part of the public debate on the Afghan Taliban.\nMy very unscientific guess is that there’s usually a lag of 6-18 months from a trend starting to emerge within the Taliban to the point where those outside the movement start to notice it. And from there there’s another 1-2 years before a particular feature or analytical point becomes accepted and part of public discourse.\nNeedless to say, this incredibly slow dispersal of understanding makes it hard for analysis and action to mesh together usefully."
  },
  {
    "objectID": "personal/2014-08-05-how-to-learn-a-language-to-fluency-interview-with-gabe-wyner.html",
    "href": "personal/2014-08-05-how-to-learn-a-language-to-fluency-interview-with-gabe-wyner.html",
    "title": "How to learn a language to fluency: interview with Gabe Wyner",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“652”] fluentforever[/caption]\nGabe Wyner has a book out today, and I took the time to interview him about his method. It relies on several things I’ve mentioned on this blog before – namely Lang-8, Anki, spaced repetition, mnemonics etc – so I’m trying out something new: posting the audio of the interview I did a few days ago.\nGabe’s book is worth your time, especially if you’re getting back into learning languages, or if you’re starting for the first time. If you have less patience for books and the written word, you can watch his course over on CreativeLive. It’s 18-hours of instruction (in an classroom environment) on how to learn a language using his method. All the steps outlined in his book are expanded upon in this course. I have watched it all, and can attest to its value.\nThe interview can be listened to here.\nUPDATE: I now offer one-on-one language coaching. Read more about what it involves and what kinds of problems it’s best suited to addressing."
  },
  {
    "objectID": "personal/2014-09-05-an-ankified-urdu-frequency-dictionary.html",
    "href": "personal/2014-09-05-an-ankified-urdu-frequency-dictionary.html",
    "title": "An Ankified Urdu Frequency Dictionary",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“550”] Urdu Frequency Dictionary [/caption]\nUntil recently I had been trying to spend more time in Pakistan’s megatropolis, Karachi. As part of this move I had been trying to learn Urdu. There are a variety of excellent study materials for Urdu, but I won’t write about those today. Rather, I want to offer up a resource for serious students of the language.\nReaders of my previous language posts will know about Anki. For those who don’t know, it’s basically software that allows you to memorise pieces of information (such as foreign language vocabulary).\nFrequency dictionaries are wonderful things. They present words of a language in the order of frequency used (usually in writing). They are assembled by amassing huge databases/corpora of text and these are analysed to see which words are used most often.\nFor a beginning learner of a language, they can be a real help. You start with the most frequently used words and work your way out to the ones you’ll encounter less.\nNothing like this exists for Urdu. Or so I thought. I was passed a series of scanned PDF images of an old frequency dictionary published in Canada in 1969. This was made on the basis of an analysis of newspaper copy/texts. Obviously the language used is a bit dated, but as a solid start, this is a good selection. (For those with deep pockets, you can search bookfinder.com for “An Urdu Newspaper Word Count” by Mohammad Abd-Al-Rahman Barker but I’ll warn you that the cheapest copies available are $100+ USD).\nUnfortunately, in the part of the dictionary I was sent, the words are listed in alphabetical (by Urdu) order. This means that the order is not ideal. There are, however, 9,956 words in this collection. If you’re serious about Urdu you could do a lot worse than learning all of them. You’ll skew your vocabulary a little towards the literary side, but that’s not necessarily a bad thing.\nI had someone type up the whole dictionary into Anki and add a spoken audio file for every word. (Many thanks to Affan Ahmad for this massive labour.) You can even set Anki to deliver you words randomly served from the frequency dictionary.\nSo, without any further ado, the files are here. They are split up because Anki became a bit difficult when inputting the files, but you can combine them on your own computer into a single “Urdu” folder.\nEnjoy! And please post any feedback below in the comments. I mainly wanted to get this out into the public so people can use it (rather than gathering dust on my laptop).\nUPDATE: I now offer one-on-one language coaching. Read more about what it involves and what kinds of problems it’s best suited to addressing."
  },
  {
    "objectID": "personal/2014-09-16-our-first-publication-i-am-akbar-agha.html",
    "href": "personal/2014-09-16-our-first-publication-i-am-akbar-agha.html",
    "title": "Our First Publication: ‘I Am Akbar Agha’, Memoir of a Taliban Insider",
    "section": "",
    "text": "Very happy to announce the publication of First Draft Publishing’s inaugural book, I Am Akbar Agha. Felix and I have been working on this initiative for a while, and it’s nice to have it see the light of day.\nAkbar Agha’s book is the latest in an (increasingly) long line of memoirs by individuals associated with the (Afghan) Taliban. So far we have insider accounts from Zaeef, Mutawakil, Mujhda, Mustas’ad, Abu al-Walid al-Masri and rumours of works being written by figures like Mawlawi Qalamuddin and others.\nAs for what’s in the book, I’ll refer you to the blurb on the back:\n\nFollowing in the tradition of Mullah Zaeef’s My Life With the Taliban, Akbar Agha’s memoir tells a story of war, friendship and political intrigue. Starting in 1980s Kandahar, the difficulties and successes of the mujahedeen come through clearly as Akbar Agha struggles to administer a group of fighters. He details the different groups fighting in Kandahar, their cooperation and the scale of the Soviet Union’s efforts to crush them. Not directly a participant in the Taliban government that ruled post-1994, Akbar Agha offers a sometimes-critical account of the administration built by many of his former fighters. After the fall of the Islamic Emirate in 2001, Akbar Agha was involved in the Jaish ul-Muslimeen opposition group and for the first time he has revealed his account of what happened in the kidnapping of UN aid workers. I Am Akbar Agha ends with an analysis of the problems afflicting Afghanistan and outlines a vision for the political future of the country post-elections and post-2015. Anand Gopal has written an introduction to the book.\n\nIf you’re interested in Afghanistan and want to delve a little deeper into the Taliban’s (pre-)history, you’ll enjoy this book. Lots of new material.\nWe also published an exclusive interview with Akbar Agha over on our blog.\nAnand Gopal has written a great foreword to introduce and contextualise the book. If you’re interested in learning more about the book but don’t know whether to commit to buying it, I’d recommend downloading the free preview/sample of the text via iBooks or the Amazon Kindle Store where you can read the full text of Anand’s foreword.\nIf you end up reading it, please leave a review on Amazon and Goodreads as a way of supporting our publishing initiative to get more of these primary source texts translated and made available to a wider audience.\nBuy a paperback or electronic copy here: LINK\nFind out more about First Draft Publishing here: LINK"
  },
  {
    "objectID": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html",
    "href": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html",
    "title": "Note-Taking Jujitsu, Or How I Make Sense Of What I Read",
    "section": "",
    "text": "Note-taking is a problem. It’s an interesting problem, but still a problem. Many people have switched over from paper books to digital copies. I am certainly one of the early adopters in this trend, having wrangled Graeme Smith and his sister into facilitating a first iteration of Amazon’s Kindle to be delivered to my house in Kandahar.\nMy colleague Felix Kuehn and I used Kindle versions of books heavily in our research for An Enemy We Created. Using those references in footnotes was difficult at the time: the format was so new that established footnoting styles (APA/Chicago etc) hadn’t developed the standards for referencing kindle documents. All this was made harder by the fact that Kindle copies of books added a whole new problem into the mix by abandoning page numbers for ‘Kindle location numbers’. This changed a few years later, and current users probably won’t have this problem, but if you go look at the footnotes for An Enemy We Created, you’ll still find that many, if not most, of the references are to Kindle locations and not page numbers. In fact, I think our book was probably the first serious history work to rely so extensively on digital Kindle references in the footnotes; I remember having discussions with our publisher about it.\nAll this isn’t to say paper copies don’t have their uses. But some books just aren’t available in digital format. I’ll get into the workaround for that later. The best way to make this less of a problem is to gently nudge publishers to issue their books on a kindle format.1 But I am already getting off track.\nAll this seemed to come to a head this past week, where a podcast I hosted together with Matt Trevithick took up the topic of notes and note-taking. Mark Bernstein, our guest on the show, wrote a really excellent book on the topic some years ago entitled The Tinderbox Way. I’d strongly recommend you read if you’re involved in knowledge work in any way. Here’s a short excerpt defining the importance and use patterns for notes:\nLater in the week, Maria Popova (of Brainpickings fame) was on Tim Ferriss’ podcast to talk about her website, her reading and her workflow. Both Tim and Maria expressed frustration over the lack of tools for people wanting to download and interact with their Kindle clippings:\nShe then goes on to some more detailed points of how this doesn’t work, and Tim commiserates, suggesting that maybe they should hire some people to fix this problem. But the good thing is that there are solutions. The problems Maria and Tim bemoan are things that every other Kindle user has had to deal with since day one, so thankfully there are a number of workarounds that simplify the process of reading, annotating and sifting within one’s notes of a book or document.4\nSo notes are important, we get that. But how do we use them to their utmost? How do we even gather them together and store them? How do we use them for our writing, for our thinking? These are all important questions which I don’t feel have been properly answered, and where those answers have been given, they’re buried or hidden somewhere out on the internet.\nI want this post to get into the weeds about how to get your materials off a Kindle device, how to store it usefully on a Mac (my apologies, PC/Linux users), and how to repurpose those notes to be creative, to write, and to think.\nThis post has three parts:\nIt will by necessity be an overview of some useful tools and options for researchers, but if you leave comments I can probably expand on individual points/sections in follow-up posts if needed."
  },
  {
    "objectID": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html#storage",
    "href": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html#storage",
    "title": "Note-Taking Jujitsu, Or How I Make Sense Of What I Read",
    "section": "1. Storage",
    "text": "1. Storage\nThis is a problem that wasn’t explicitly raised in the things that motivated this post, but it’s something I get asked frequently. Maria and Tim both seem to be avid Evernote users, and I know many others also use this, but there are other options. It’s worth starting here because the tools will determine what you can do with your notes.\nI’ve offered advice to other Mac users on what software to use for research projects that require a certain deftness in handling large quantities of sometimes disparate materials. The same applies to people who are just trying to keep track of the things they read, trying to draw together connections, and to derive meaning from it all. I’ll get into the meaning-creation in the final section, but for the moment, let me briefly describe our four options for file/note storage as I see it.5\n\nFinder/PathFinder. This is the lowest-tech option. Basically, once you split your files up (see section two) you store them in folders and refer to them that way. I don’t find this option very attractive or useful, because it’s like a filing cabinet. Your ability to discover connections and to remember what’s in those folders is pretty limited. I don’t recommend this at all, but from conversations with other researchers and writers, it seems this is the default option.\nEvernote. I include this here because it’s part of a workflow that we’ll cover later on. Evernote is great for all the reasons you can read about on their site. It syncs across all your mobile and desktop devices, it OCRs images so you can search for text captured inside photos you upload into your library of notes.\nDevonThink. This is my default ‘bucket’ for information, documents and notes. You can read up on the many (MANY) things that DevonThink Pro Office or DTPO (the version you should get, if you’re getting this software) does. Not only does DTPO store your documents, but it allows you to access that information in a number of extremely useful formats. There is a mobile app, too, though it could do with a bit more work. The most interesting feature of DTPO is its search and discovery functionality (using some magic sauce algorithms). They don’t make as much of this on their website as they used to, but I’d strongly recommend you check out these two articles (one, and two) by Steve Berlin Johnson which explain a little of the wonderful things DevonThink can do for your notes. As with the next recommendation, it’s not cheap. But powerful doesn’t always come cheap. It’s a solid investment if you spend the time getting to know this piece of software.\nTinderbox. I discussed this at some length on the Sources & Methods podcast with Mark Bernstein, so I’d recommend you listen to that as your first port of call. Tinderbox is not an everything-bucket in the way that Evernote and DevonThink are, and I use it slightly differently, but it’s a great place to actually do the work of thinking, organising and writing once you have something (i.e. a project of some sort) for which you want to use all your notes. I’ll explain more about this in section three.\n\nI’d recommend getting to know the different bits of software to get a sense of what they can do. DevonThink has a handy section of their website where you can see how people use it in their work lives. Tinderbox has something similar, with some case studies of usage.\nFor DevonThink, it’s generally good to keep your ‘buckets’/databases of files separated by topic. I have a mix of these kinds of databases (50 in total): some are country-specific, some are project-specific (i.e. to contain the research that goes into a book or a long report), and some are topic-specific (i.e. I have one for clippings and notes relating to Mathematics, one for things relating to Cardiology etc). I’d also recommend you give Steve Berlin Johnson’s book Where Good Ideas Come From a read, particularly chapter 4.\nGiven the learning curve with some aspects of the workflow that follows, you might want to consider introducing these pieces of software one-by-one, or as needed. That way you’re using only what you understand and can implement things without being too overwhelmed by the novelty of the systems. It took me years (almost a decade) to implement and iterate the systems described below, and I’m still not finished modifying as the tools change."
  },
  {
    "objectID": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html#clipping-splitting",
    "href": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html#clipping-splitting",
    "title": "Note-Taking Jujitsu, Or How I Make Sense Of What I Read",
    "section": "2. Clipping & Splitting",
    "text": "2. Clipping & Splitting\nThis section is all about getting materials off mobile devices and onto your computer where you can put them into some sort of overarching database.\n\nAccessing Your Amazon Kindle Clippings\nFirst let’s sort out how best to get notes from a kindle onto your Mac. Don’t use Amazon’s website. It’s going to create all sorts of problems for you in terms of formatting.\nFirst thing’s first: sync your kindle to the cloud. Just turn on the wifi/3G and select the “Sync” option. This will ensure all your highlights are backed up to the cloud.\nThen plug your Kindle into your computer via USB. Then go into the “Documents” folder, and search for a file called “My Clippings.txt”. If you’ve been using your kindle for a while, it’s probably going to be quite large. Nevertheless, copy that file to your desktop. Feel free to eject your Kindle from your laptop now. We won’t be needing it any more.\n[caption id=“” align=“alignnone” width=“1634”] An example of what you might see when you open your “My Clippings.txt” file [/caption]\nIf you open the txt file that is now saved to your desktop, you’ll find all your clippings and annotations preserved in a useful plaintext format. This may solve your problems straightaway, in which case, congratulations: you now have all your annotations in a useful format that you can use however you wish.\nIf you want to take it to the next level, though, you’ll want to split this file up. At the moment, you have a very large plaintext file which contains all your notes. You’re likely to have notes from a wide variety of topics and books in here, so it doesn’t make sense for you to keep them all in a single location. The ideal solution is for you to have a single file for every clipping, a single file for every annotation.6\nThis is where Split-ter.scpt comes in. I’m afraid I don’t know who to credit for this wonderful piece of code. I downloaded it somewhere on the internet some years back and can’t seem to find a link to the author either in the code or elsewhere online. (Whoever you are, thank you!)\nThis script works with another piece of software mentioned above — DevonThink Pro Office. For now, I’ll ask you to ignore that bit, and focus on what’s happening to the file. I use the script to convert our “My Clippings.txt” file into multiple files. It goes in, finds a delimiter (any piece of text or syntax that repeats itself in the original file) and creates a new note/file every time it comes across this delimiter. In this way, you’ll quite quickly from the file shown above to something like this:\n\nNow you have a note for every annotation and/or clipping. This is then something you can dump into Evernote, or keep in DevonThink. Again, more about the difference between these programmes in the next section. (Note, that you can use Tinderbox to split up the “MyClippings.txt” file as well using the “Explode” tool).\nUPDATE (a little later on Friday night): Seb Pearce has just let me know that there are other options available for dealing with the ‘My Clippings.txt’ file. Check them out on his site.\nThe second problem raised on the Tim Ferriss podcast was Amazon’s limitations for clippings. This differs from publisher to publisher, it seems, so there’s no way of predicting it. An unfortunate rule of thumb: the more useful the book, the more likely the publisher has locked it down. When you’re making clippings inside the book, Amazon gives you no notification that you’ve reached the book’s limitations. But when you go to check your “My Clippings.txt” file to start using your notes, then you may find the note says:\n\n“”\n\nAll the work you’ve done selecting pieces of text are for nothing, it would seem. The publisher has prevented you from using your book.\nOne solution is to remove the DRM from the book before you put it on your kindle. This is legal so long as you’re not sharing the book with other people (as this process would theoretically allow you to do).7 Follow this link to find out how to de-DRM your Kindle and iBooks documents. You can also visit &lt;libgen.org&gt; to download an already-DRMed copy of the book you’ve purchased. These will often be in .epub format so you’ll have to convert these over to a .mobi format if you want to use them on your kindle device. (To convert from .epub to .mobi, use the free Calibre cross-platform software.)\nIf you read a de-DRMed copy of a kindle book on your kindle device, there will be no limitations as to how much you can annotate. The publishers limitations will all be gone. So that’s one option.\nFor those who aren’t comfortable removing the DRM on your books, you can get all your annotations out, but it comes with a little bit of hassle.\nHere’s an example of what I mean (screenshot from my DevonThink library). I was reading in Hegghammer’s excellent Jihad in Saudi Arabia and making highlights (at 4:06am, apparently) but at some point I hit the limit imposed by the publisher.\n\nThe workaround to bypass this limit from the publisher is to first export all your notes out of your “MyClippings.txt” file. So all your clippings are saved, even though some of them may not work. Let’s say, for the sake of argument, that the final three notes aren’t working because of the publisher’s limitatations. That’s the case in the screenshot above. What you do is (again, once you’ve backed up the clippings txt file) delete three of the earlier clippings that you already have. Then you sync your Kindle to the server and it will think that you have clipped three less quotes, so these will then become available (both in the myclippings.txt file and on the website. Like I said, it’s a bit fiddly. I would much rather remove the DRM completely and not have this hassle at all, though when you do that Amazon will not sync your clippings to the cloud and to their &lt;kindle.amazon.com&gt; database. You’ll have to export them using the tools I mentioned above.\n\n\nKeeping Up With The Joneses, or How to Use Instapaper to Clip Web Articles\nThis may be something completely idiosyncratic to my own workflow, but I don’t enjoy reading articles in a web browser. I’d also prefer not to be hijacked into reading all these articles. For instance, when I’m in Tweetbot/Twitter or Facebook and I see a link that I like, I will almost never read that article then and there. Rather, I’ll send it to my Instapaper queue.\nFirst, a quick word about Instapaper vs Pocket. I use Instapaper. I started off with them, switched over to Pocket for about two years, and now I’m back with Instapaper. They’re both more or less the same. Instapaper happens to be what I’ve chosen for myself because of their handy Kindle service. (If you have articles in your queue, you can have Instapaper send the most recent articles to your Kindle at a particular time (i.e. first thing in the morning) which you can then clip and archive to your heart’s content.) Both Pocket and Instapaper work with what follows, so just pick one and stick to it. I’d recommend Instapaper because they allow for the sharing of the full texts of articles and because of the Kindle digest feature.\nI find I have so much to stay on top of and keep tracking online, I can’t just click around and read things as and when I see them online. I schedule time apart for reading of my Instapaper queue (and for reading books on my Kindle) and only read during those times. (I do the same with email, only checking and responding to email between the hours of 12-1pm and 5-6pm each day. The rest of the day email is off and disabled. I even deleted my email account on my iPhone as inspired by this medium.com post.)\nMy workflow with web articles is to follow as much as possible via RSS. I prune the sites I’m following every three months, but in general the number is stable around 650. I use Newsblur as my RSS reader, and every time I find an article I’d like to read (later), I use the handy ‘send to instapaper’ bookmarklet. This sends the article to my Instapaper queue.\nThe same goes for twitter. I follow enough people on Twitter for it to be impossible for me to read every post that passes through my stream. I will dip once or twice a day, however, to see what people are saying. I use two services to monitor my Twitter and Facebook streams to pick out the most-shared articles to ensure that I don’t miss the big sharks of the day. They’re both free, and I’d strongly recommend you signing up and getting their daily summaries of what people were talking about on Twitter that day. News.me has been around for a while and I trust their article selection. Nuzzel is newer, but it seems to have a few more options. I guess you could probably do with picking only one of the two.\nAfter reading articles on my Kindle (or sometimes on a mobile device like my iPad or iPhone), you can clip the article if you want to save it (just like making a clipping inside a book, only the entire article is saved).\n[caption id=“” align=“alignnone” width=“2448”] This is what you see in an article when you click to “Clip This Article” on a kindle… [/caption]\nThen your clippings will be captured in the ‘MyClippings.txt’ file as explained above and you can export them directly to DevonThink or Evernote or Tinderbox. (The main downside to doing things this way is that when the kindle clips it, all formatting is lost (including paragraph breaks)).\nAlternatively, you can ‘Favourite’ the article. I use this setting because it then sends the article and URL to my @stricklinks twitter account, something I created to share the best things I was reading. It also saves the full text of the article to Pinboard (a service I’ve already written about on my blog here) and to Evernote. (I use If This Then That to facilitate this.)\nOnce I’m done reading, I can go into Evernote and all my articles are waiting for me to be sorted through. Because I use DevonThink as my everything-bucket, and because all the sorting and discoverability features are there, I have a separate stage of exporting my notes out of Evernote into DevonThink. I’ve already probably taken you a little too far down the rabbit-hole of my workflow, but this is an important stage because otherwise you can’t do anything with your notes.\nLuckily, someone has written a script which makes this possible. Many many thanks to the good people at Veritrope for updating the script every time updates to the software get released. It’s fairly self-explanatory. You select the notes that you want to export, choose which DevonThink folder you want to export to and then it goes to work. It can occasionally be buggy and stop half-way through, but usually a little trial-and-error will let you pinpoint which Evernote note is causing the problem and you can transport that one over manually.\nI usually do an export session to bring everything from Evernote into my DevonThink inbox once a week. This way the number of clippings doesn’t get too out of control, and I’m not constantly playing around with this during the week. You might find this all is overkill, but it has become an essential part of my workflow to store the various things I’m reading on a daily basis.\n\n\nPillaging Your Hard Copies, AKA Living the Paperless Dream\nYou may have hardcover copies of books that you want to use as part of this system. One way to use them is to scan the books into your DevonThink library. DevonThink Pro Office comes with an OCR package (via ABBYY FineReader) so whatever you scan can then become searchable and useful.\nIn the past, particularly with books I’ve purchased in Afghanistan and Pakistan that are unlikely (read: never) to be made available as electronic versions, I take a Stanley knife to the bindings, feed the pages into my ScanSnap scanner which scans both sides and compiles all the scans into a single PDF document that is searchable on my laptop. The whole process is destructive of the book, but it gives the text inside a new life. Given how fast the new ScanSnap models work (around 25 pages per minute, both sides), this is an attractive way to get digital access to materials that are only available in paper form.\nYou can highlight text within the resulting PDFs and then later export your clippings from those PDFs as notes into DevonThink. There’s another useful script to help with that. It only works with the free Skim PDF reader, but that’s my default PDF reader so it works out well.\nFor more on paperless workflows, check out David Sparks’ Field Guide on the topic."
  },
  {
    "objectID": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html#discovery-meaning",
    "href": "personal/2014-10-24-note-taking-jujitsu-or-how-i-make-sense-of-what-i-read.html#discovery-meaning",
    "title": "Note-Taking Jujitsu, Or How I Make Sense Of What I Read",
    "section": "3. Discovery & Meaning",
    "text": "3. Discovery & Meaning\nIf you made it this far, congratulations. This is the section where all the fiddling with export starts to take on some meaning. After all, we’re not reading and exporting these notes purely because we are hoarders or to fetishise the art of collection (though in some cases, that may be what’s going on). No, we are taking notes because we are trying to understand difficult topics, because we are trying to solve important problems.\n\nDiscovering Links and Connections\nThe Steve Berlin Johnson articles referenced earlier are an essential first stop, particularly in demonstrating how DevonThink can add some serendipity into how you use your individual notes. To give you an example of how this works, here’s a screenshot from my ‘TalQaeda’ database that I put together while working on An Enemy We Created:\n\nIn the upper part you can see a bunch of notes relating to the Haqqani family. The lower left part is the contents of a note (Note: exported from Instapaper). The bottom right list of documents (under “See Also”) is a list of notes that may be related to this particular quote. This is the magic algorithmic sauce I mentioned earlier that makes DevonThink so powerful.\nIf I click through to some of those suggested notes, I’m taken to similar quotes on the same topics, two PDFs of reports (of dubious analytic value, but that’s a separate issue), three clippings from Kindle books where people are making reference to the relationship between the Haqqanis and al-Qaeda (the subject of the original note). Note that I didn’t have to pre-tag documents for this ‘see also’ functionality to work its magic. It analyses inside the text and makes its suggestions based on the similarities it identifies. (Needless to say, it’s not simply a matter of matching individual words. Some of the suggested notes don’t mention al-Qaeda or the Haqqanis by name, but they are implied; DevonThink catches this all).\nOnce you start to build up a decent database of notes (my Afghanistan database has just under 65 million words of notes, including 12,800+ PDFs) this ‘See Also’ functionality really allows for some unexpected links to be made, especially when you’re at the stage of writing up a project/book. One note will lead to another note, which will lead to another note. If you follow these trails of notes (like breadcrumbs) you can develop a pretty idiosyncratic picture.\nI do not know of a manual method which allows for this kind of process.\nDevonThink has an extremely robust search function which allows you to find things along similar principles (including a very useful ‘fuzzy spelling’ option, perfect when checking my database for notes on someone whose first name could be spelt Mohammad, Muhammad, Mohammed or any of the other variations).\n\n\nFiguring Out What It All Means\nOnce you have an idea of the outlines of the topic, once you’ve been taking notes for a while, your database in DevonThink is probably starting to fill with useful information.\nIf you’re writing a book, though, you’ll want to start writing alongside this gathering process. (Check out Michael Lopp’s overview of the process of writing a large research book, which, to my mind, is fairly accurate.)\nI don’t find DevonThink a particularly pleasant place to write, so I do that elsewhere. Before I write things out in long form, I usually do some outlining, particularly if it’s something where the dense collection of factual detail is important to the development of the argument (as was the case with An Enemy We Created). For this, I find Tinderbox indispensable for working up an overview of what I know, for figuring out how I’m going to structure it, and for helping me put together my first draft.\nTinderbox can display notes in a number of different ways. You can view your documents as outlines, as maps, or even as timelines:\n\nIn this image you can see the information arranged as an outline, but here (below) you see the same information organised as a map (mirroring the actual layout of the map of those districts in a particular part of Kandahar):\n\nJust to show you that it can handle complexity, here’s a map created by Felix to help him figure out how people involved in militant Islamism were/are connected across different geographical sectors:\n[caption id=“” align=“alignnone” width=“1966”] It’s complicated… [/caption]\nI’ll often use Tinderbox maps to store outlines for how I’ll write a particular section or chapter, making notes inside the document, dragging quotes in from DevonThink to supplement the argument that’s being constructed.\nGetting to the point where you can actually start writing on the basis of your notes is the whole point of all of this. Technology is useful, but mainly when directed at a specific problem or goal. All the tips, tricks and software described in this post has helped me write books, reports and (coming soon!) even my doctoral thesis/PhD. I have encountered only a few (barely a handful) researchers who use their computers for this collation, sifting and discovery process. There’s no way to keep it all in your head. Here’s hoping more people start adopting these tools…\nFootnotes:\n\nFor many years, Amazon offered users the ability to let publishers know that you wanted to see title X or Y on a Kindle format, but they failed to make this piece of interaction useful by keeping track of what you’d requested of publishers (so as then to be able to let you know when it was finally released in Kindle format). ↩︎\nExcerpt From: Mark Bernstein. “The Tinderbox Way.” iBooks. ↩︎\nSelective transcript from around the 50-minute mark in the podcast audio. Needless to say, the rest of this blogpost constitutions a ‘viable solution’. ↩︎\nMost of these are derived from other people, I should say. I try to give credit where I can, but sometimes I can’t remember where I first read something or who first recommended a particular tool or trick. ↩︎\nYes yes, I know, I’m going to leave out some mentions for useful software here. This is an overview, and I’m just trying to describe some options for what might work in certain situations. ↩︎\nA clipping is when you have selected and copied a passage from the book for safe-keeping, and an annotation is when you yourself write a note connected to a particular passage. ↩︎\nNeedless to say, don’t take legal advice from me. ↩︎"
  },
  {
    "objectID": "personal/2014-12-22-some-books-and-other-things-from-2014.html",
    "href": "personal/2014-12-22-some-books-and-other-things-from-2014.html",
    "title": "Some Books and Other Things from 2014",
    "section": "",
    "text": "I have read 117 books so far this year. I think there are another five still likely to happen before the end of the month. Goodreads tells me that amounts to 28,616 pages or an average of roughly 80 pages per day. Seems like a lot, but it didn’t feel that way. I set my goal for the year at 100 and it was consciously a very high number, almost unrealistically so. But in the end I never felt rushed, nor did it feel like a chore. As a result, next year I’m going to notch it up to 150. I enjoy reading books more than I fear the very occasional sense of pressure when I get behind on my reading. Beeminder has helped keep me honest and on track when it comes to my reading goals.\n[caption id=“” align=“alignnone” width=“1354”] My Beeminder chart c. late November. I disabled it after I hit the 100 mark… [/caption]\nThe breakdown of fiction:non-fiction in the books I read this year was pretty low (1:9 approx) but the gender balance was about 4:6 female-male, which isn’t bad considering I was only side-glancing at that proportion as the year went on. For 2015, I will be consciously making sure to get more fiction in my reading diet, hopefully by as much as one-third or two-fifths of the total.\n\nThe only fiction book this year that really blew me away was Karen Joy Fowler’s We Are All Completely Beside Ourselves. Imagine a cross between J.M. Coetzee, Barbara Kingsolver and A.M. Homes. That’s sort of what this book is, but to try to pre-introduce you to the plot or premise would do you a disservice. I’d strongly advise you not to read any reviews, comments or synopses of the book – even the one written by the publisher. Just go read it. It’s not only extremely moving, but it will also make you think.\n\nI read Leslie Jamison’s The Empathy Exams much earlier this year, and it made me think about essay writing afresh. The book is a collection of pieces examining the idea of empathy from different vantage points. The showstopper first essay (from which the collection takes its name) was originally published in The Believer. You can read it here. I suspect after reading that you’ll go and get the book. Her essays combine the confessional with the abstract and overthought. It’s a very attractive mix, and, with the exception of one or two pieces, the book is compulsively readable.\n\nThe second non-fiction book that, really, everyone should go read right now is by my friend and colleague, Anand Gopal. No Good Men Among The Living covers the conflict in Afghanistan through the voices and perspectives of three Afghans – one man who ends up fighting for the Taliban, a urban-educated woman who ends up in Uruzgan province, and a US-backed military strongman. Just for starters, it’s refreshing to read something in which the author doesn’t insert him or herself into the narrative like a sore thumb. It’s sad that has to be said, but this is the exception not the rule these days. Anand dismantles the evidence surrounding the resurgence of the Taliban post-2001 and what he finds – I’ll let you follow him down that path – is extremely disturbing if not entirely unexpected. This book is certainly one of the best things written about post-2001 Afghanistan and given the amount of energy and money that has been spent, it’s worth taking the time to consider what worked and what didn’t.\n\nFor some books, as you near the end you speed up as the plot comes to its inevitable conclusion, eager to find out how the story ends. For others, you slow down, not only because the themes of the book have started to intertwine around and in between one another, but also because you realise that soon this book will come to an end and this companion of the past few hours or days will start becoming but a memory. Rohini Mohan’s The Seasons of Trouble:  Life Amid the Ruins of Sri Lanka’s Civil War is a superb book. I can hardly imagine how the author managed to put it together. It reads like a novel yet feels so immediate as well. Mohan takes a similar approach to Anand, removing herself completely from the story and choosing instead to focus on the stories of three individuals who were caught up in Sri Lanka’s unfolding civil conflict. I had read Ondaatje’s Anil’s Ghost in the past, but wasn’t left with a very strong sense of Sri Lanka or its history. With this book, I feel I’ve learnt a lot. Mohan writes beautifully and the book has a strong narrative drive, such that I was up into the early hours of the morning finishing it. Just take my word for it. Buy this book. Read it. Thank me later. You’ll be better off having read it.\n\nNot everything in life is civil war and strife, though. This year I’ve been reconnecting with my body in various ways: by reading, through gymnastics classes and by working on my free-standing handstand. In this vein, I really connected with two books. Katy Bowman’s Move Your DNA approaches movement from a bio-mechanical perspective (hinges, joints, and how the body works together as a system). Once you’ve read it, you can’t think about your body and/or how you move it the same way again. The book introduces the idea of “diseases of captivity”, which is to say, diseases brought about because of biomechanically skewed or minimised movement patterns. It’s a really interesting concept, and as you read you’ll find yourself sufficiently freaked out ever few pages to get up, check your posture, check your feet alignment and/or go for a walk. Unfortunately, Bowman doesn’t always write in the clearest manner. Part of the premise of the book is that modern people have lost touch with their bodies (how to feel, how to describe, how to move) so I was hoping she would have found a language that would allow us alienated masses to better reconnect. That said, this is a great starting place. I could have done with a book with a thousand videos for each of the exercises she describes (and 3D interactive diagrams for the anatomy lessons) but this is a minor quibble. I will have to learn all that in due course. Bowman also offers courses, which I imagine are excellent in that there is a good deal of hands-on and individual experiential aspects to this book that I didn’t quite get just from a read-through. I’ll be working on the exercises over the coming months (years?). Certainly one of the most interesting/challenging books I read this year.\n\nJill Miller’s The Roll Model: A Step-by-Step Guide to Erase Pain, Improve Mobility, and Live Better in Your Body addresses similar things as the Bowman book but from a highly practical standpoint. There is a lot of theory and anatomy explained, but the core of the text are hundreds of pages of beautifully illustrated exercises. Miller recommends rolling on somewhat-soft balls to release the fascia (basically, the connective tissue in the skin and muscles) throughout the body. If you haven’t done something like this before, the first time is really instructive. As someone who spent a good part of the past decade sitting behind a computer or book, writing or researching, I can testify to the toll this has taken on my body. This book is a really excellent first step in moving away from that tension, dysfunction and inflexibility.\nMy movement journey wouldn’t have been possible without an initial boost from the kind people of Gold Medal Bodies. They’re all genuinely nice people and they have provided useful guidance and support in this project of reconnecting with my body that I mentioned earlier. I’d particularly like to thank Verity Bradford for the help she’s been giving me over the past couple of months while I’ve been working on my free-standing handstand.\n\nFinally, a few words about death and time. The past year has been full of confrontations with the passing of time and the inevitability of death. These confrontations have taken various shapes and forms and have been closer to home as well as further afield. This is always a useful reminder, I feel. I reread Seneca’s essay On the Shortness of Life this year and that helped drive the message home. But it’s easy to forget. To help with that, I’m really glad to have been using a MyTikker watch for the second half of the year. It’s a little bit like that chart of a human life in weeks that did the rounds earlier this year. On the watch, one row of digits shows the current time, and the other two show a countdown (based on statistical estimates calculated using a questionnaire) of the time you have left in your life. Of course, things can always happen out of the blue. You can’t do much to prevent that. But to push back against the passing of time, death works to jog the mind. So now, every time I look down to check the time, I’m reminded that time is passing, that our days are short.\n[Read previous year-end posts here: 2013, 2012, and 2010.]"
  },
  {
    "objectID": "personal/2015-02-25-2015-2-apocalypse-then-a-short-review-of-filius-apocalypse-in-islam-2011.html",
    "href": "personal/2015-02-25-2015-2-apocalypse-then-a-short-review-of-filius-apocalypse-in-islam-2011.html",
    "title": "Apocalypse Then: a short review of Filiu’s ‘Apocalypse in Islam’ (2011)",
    "section": "",
    "text": "An enjoyable account of the idea of apocalypse in Islamic discourse, from the Qur’an all the way up until 2011. Filiu gathered together a huge melange of written sources on the apocalypse and he presents an overview of how differing conceptions have been cultivated by Sunnis / Shi’is over time. There is a slight bias in that most of the sources are in Arabic, and his focus is, broadly, the Middle East so South Asia is not particularly part of this story at all, not to mention East Asia proper which gets nary a mention.\nI was almost completely ignorant of much of the developments detailed in the book, perhaps because I’ve focused more on South Asia in my own research/work. Indeed, I finished the book with a question on my mind as to why Afghanistan seems not to have the same obsession with ideas of the apocalypse as Filiu is suggesting is present in the Middle East. Perhaps it has something to do with Deobandism, though I’m not really sure of that… something to look into.\nOne other detraction: this is a historiography of transmitted ideas, but mostly of those written down. Filiu has lived for a long time in the Middle East (mostly in Syria, if I’m not mistaken) but you don’t really get much sense of this in the book, nor of how all the books and ideas he discusses were received by actual people. Instead, there’s a dialogue among authors and publishers – a fascinating one, at that – but I was left with the sense that something was missing.\nFiliu tells of the construction of the idea of apocalypse, how circumstance and context contributed to the development of the ideas. There’s nothing particularly ground-breaking in that: the events of a particular age shape the way ideas are framed. But the details of how publishers saw a market in apocalyptic literature were fascinating to read. Similarly, it was interesting to see how Shi’i interpretations seem to have followed a fairly different (though parallel) track of development.\nThe book has the really helpful feature of one-or-two-page summaries at the end of each chapter to help remind you of the overall argument that was covered. All-in-all a really clear presentation from Filiu of his ideas/argument.\nSome other things I learnt while reading: (helped out by quotes from the book)\n\n“The Qur’an has rather little to say about the end of the world, and still less about the omens of the Last Hour, whose prediction and description later came to be based on prophetic reports.” (28)\n“The apocalyptic narrative was decisively influenced by the conflicts that filled Islam’s early years, campaigns of jihad against the Byzantine Empire and recurrent civil wars among Muslims” (28)\n\nAfter page 70, the book gets into the post-1979 world, looking at three events that really spurred the development of apocalyptic ideas: the Siege of Mecca, the Iranian Revolution and the arrival of Soviet troops into Afghanistan. I hadn’t realised, for example, that the war in Afghanistan was one factor that spurred the 1982 Hama uprising as their spiritual leader saw in it “certain signs of the Hour” (81). The book is filled with many such fascinating asides.\nCovering the 1990s, Filiu shows how ideas from Christian messianism start to creep into the books being written about apocalypse in the Middle East, also including things like UFOs and the Bermuda Triangle. This is when he also starts detailing how certain publishers and authors become factories for apocalypse literature, churning out books to satiate an eager audience.\nAll this is further accelerated by 9/11 and the 2003 invasion of Iraq, with more intermingling of sources and ideas. Filiu also chronicles how certain Islamic orthodox establishment figures (and their state sponsors) sought to play down apocalypse narratives. Interestingly, he shows how it wasn’t really a significant theme for al-Qaeda either, at least not for its senior ideologues or leadership.\nYou can see how prescient Filiu was in reading the apocalyptic tea leaves when you get to the end of the book. Remember, he was writing this in 2007/2008 (when it was published in French), but he concludes by speculating that a merger of jihadism with messianism was probably due and that the mutation would be particularly difficult to manage:\n\n“No inevitability pushes humanity in the direction of catastrophe, even if the popular fascination with disaster may seem somehow to favor a sudden leap into mass horror. And yet, coming after the gold of the Euphrates, widely interpreted in the wake of the American invasion of Iraq as a sign of the Hour, a fire in Hijaz may be all that is needed to set in motion a new cycle of eschatological tension, inaugurating an age of widespread fear and expectation that the end of the world is at hand. If an inflammatory and incandescent event of this sort were to occur, the chance that global jihad might undergo an apocalyptic mutation would give grounds for genuine apprehension.” (193)\n\nAnyway, for all the detractions mentioned above, this was a quick and fun read that gets you up to speed on thoughts about the apocalypse in the Islamicate world. Recommended, if this sort of thing gets you excited…"
  },
  {
    "objectID": "personal/2015-06-07-2015-6-arabic-language-update-i-did-it-almost.html",
    "href": "personal/2015-06-07-2015-6-arabic-language-update-i-did-it-almost.html",
    "title": "Arabic Language Update: I did it! (Almost)",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“1366”] My Beeminder accountability graph showing how I reached my goal for a study challenge in June [/caption]\nJust a short post as I’m off away on an intensive language course for most of the next three months. This is the programme run by Middlebury College, but held in Oakland, California (USA) at Mills College. I was extremely lucky to win a Kathryn Davis Fellowship which covers the costs of the course and food and accommodation while I’m there. I have a BA degree in Arabic and Farsi from London’s School of Oriental and African Studies, but 10 years in Afghanistan spent writing books and studying Dari and Pashto meant that my Arabic has atrophied considerably. I thought it was time to resurrect those old skills, in part as a way of deepening my understanding of some of the religious aspects of the Afghan Taliban and in part – let’s be honest here – as a way of covering my bases prior to Afghanistan completely falling off the map a few months from now.\nI’ll be writing a much longer post on how to get a high-beginner-to-mid-intermediate level out of the well-known “intermediate language plateau” after the course finishes, specifically focusing on what resources are available to Arabic-language students who have good basic skills but want to go beyond that to more advanced materials. (Read these three posts for more on getting out of language plateaus in general terms.)\nThe Middlebury course caters to various levels of language ability, and since I didn’t want to waste the opportunity just revising things I had already learnt at university, I had to do a good deal of preparatory work these past few months. I started getting serious about this preparation in February. This involved over 75 hours of spoken/conversation practice (and some grammar work) with a number of different native Arabic speakers over Skype (lessons made possible through iTalki.com), as well as a lot of reading and listening. In June, as you can see on the Beeminder graph displayed above, I challenged myself to get 100 hours of exposure to the Arabic language over a period of 30 days; this included some iTalki lessons, but was also a lot of listening to Arabic-language podcasts, time spent writing on lang-8 and lots of time spent doing so-called “extensive reading” (much more to follow on that in August/September). I managed 99.5 hours, in total, just short of the total required to successfully complete the challenge I’d set myself, but enough to really make my language proficiency come along in leaps and bounds.\nAn additional note to those who would like to get in touch with me during this period: as part of the Middlebury course, they expect participants to take a language pledge where you only speak the language of study (i.e. Arabic for me) for the duration of the period of study. Read more here. For non-Arabic speakers, if you want to get in touch with me, please visit Google Translate and translate your message into Arabic there before copying the full text and pasting that into the email. It’s not perfect, but it allows me to continue to stay connected with the world without violating the language pledge. If I reply, I’ll be doing that in Arabic, too, so you’ll have to copy the text back into Google Translate to get a sense of what I replied.\nI’ll be away on the course until the end of August, and will thus ignore all non-essential email until then. If you write to me in English, I will also ignore your email until September. Thank you.\nUPDATE: I now offer one-on-one language coaching. Read more about what it involves and what kinds of problems it’s best suited to addressing."
  },
  {
    "objectID": "personal/2015-09-02-2015-9-surviving-middlebury-how-to-get-the-most-out-of-an-arabic-summer-school.html",
    "href": "personal/2015-09-02-2015-9-surviving-middlebury-how-to-get-the-most-out-of-an-arabic-summer-school.html",
    "title": "How to Survive Middlebury’s Arabic Summer School Programme",
    "section": "",
    "text": "[UPDATE: I now offer one-on-one language coaching designed to help students prepare for Middlebury’s intensive summer programme.Read moreabout what it involves and what kinds of problems it’s best suited to addressing.]\n[UPDATE 2: (Jan 2017) I’ve just finished writing a new book, Master Arabic, on getting from an intermediate level in Arabic to advanced. Read more here.]\nI returned from my summer course in Middlebury a few weeks ago, and I thought it might be worth writing up a few of my thoughts about the programme, whether I’d recommend it to others studying Arabic (and other languages) along with some other more practical tips for students who will be joining the summer programme in the future.\nOverall, I had a good time and I learned a lot. I think that alone should be sufficient recommendation. It’s two months where you get to study something to the exclusion of everything else, so in that sense it’s a real chance to focus, to immerse yourself in the skill. To some extent, the programme almost could have been anything at all and students would benefit, especially if you’re able to work alone and at the level where you’re able to profit from self-directed study. I can’t remember the last time when I focused on one thing and one thing only over a period of two months. I was also really lucky to have two excellent teachers leading our class, and to have been awarded one of the we-pay-for-everything Kathryn Davis fellowships; for both, I’m extremely grateful.\nYou can read more about the programme itself here but basically, it’s an eight-week programme of intensive language instruction and practice. They implement a language pledge (to read more about my implementation of that, read this) which means for the entire duration you’re not supposed to talk or use any other language than your target language (Arabic, in my case), even when outside the classroom. I took it a step further and stopped using anything other than Arabic on twitter, email and so on.\nOne of the interesting (sometimes frustrating) features of the Arabic language is the so-called ‘diglossia’ (and here). This means that the language used in writing and in conversation in the mass media and among educated Arabs is different from the more colloquial spoken or dialect version of the language. For the beginner, this is a frustrating realisation, especially since so few programmes and textbooks seem to place much emphasis on learning the colloqial/spoken part of the language. When I studied at SOAS (the BA Arabic programme) there was virtually no emphasis placed on the dialect, aside from the year abroad (which I spent in Damascus and thus had some contact and exposure to the Syrian / shami dialect). In any case, I was expecting that the Middlebury course would have more to offer in terms of dialect tuition, especially having spent hours reading their website and course syllabi, but this was not the case. There are dialect classes four days per week, but they are not taught to any particular syllabi and it very much is a question of chance as to whether your teacher knows how to teach (and encourage the practice of) their dialect or not. The only thing I’d say, though, is that there are lots of varieties on offer, from the more common Egyption and shami to Jordanian, Moroccan and even Sudanese. Anyway, this is all just a warning to say probably don’t go to Middlebury if all you care about is developing a really good set of spoken skills.\nStudents take a long placement exam the day after they arrive, and then the 140-or-so of them are split into levels. Normally four is the highest level offered, but this year they added 4.5 to accommodate people who had slightly longer experience/exposure. (I was in that class).\nFormal classes run from 8.45am-1:15pm (with some 5-15 minute breaks in-between to space it up) followed by lunch. After lunch, there are colloquial / dialect classes for an hour, and then you more or less spend the rest of the day doing assigned work in preparation for the next day, revision of the work you did that day and other kinds of homework. I think the teachers estimate that you should be doing four or five hours of work outside class every day, or that’s what they aim to provide/assign. That’s what’s meant by the whole “intensive” part of the Middlebury programme. (For more on whether I found the intensity of the programme useful, see below).\nWhat follows consists of recommendations for those planning to take the programme in the future. It’s mostly things I did while I was on the course, but there are some things that I’ve since realised would have been useful as well or instead of the approach I took. Feel free to pick and choose which sections you read, as they’re all meant to more or less stand alone, depending on your interest.\n\nPreparation\nI’ve already written up some thoughts, here, on what I did before the programme started to ensure that I wasn’t just revising things I had already studied. My case was perhaps unusual in that I had a long period of (somewhat-focused) study under my belt in terms of my BA degree, but years of non-use meant that I didn’t really feel confident using the language any more.\nWhy is it useful to do some extra study before the start of the course? I’m going to take it as a given that you’re not a complete beginner, in which case, you more or less know what you need to be studying: a little bit of grammar perhaps, lots and lots of words, lots of spoken practice, a decent amount of reading at the appropriate level, and so on. So why not get a headstart and do some study beforehand so you can benefit from the focused months coming your way?\nIf you’re a complete beginner, there’s still a lot of value in doing some work before the programme starts. A lot of the things you do in the early days of learning Arabic are somewhat menial (learning the alphabet, practicing pronunciation and some of the unique sounds that Arabic uses, and so on) and there’s no real reason why you can’t have this all in the bag before you start formally with the Middlebury programme. If you’re extra enthusiastic, you could learn a bunch of basic phrases that you’ll always need to use — perhaps start with this list — and/or learn the first 500-2000 (depending on your bravery) words in the Arabic frequency list. Either use the versions over at Memrise or Anki for this (both are “no-typing” courses, with audio, I think, so they should be ok for those who’ve just learnt the alphabet). And for a true bonus, pick a teacher over at iTalki and do an easy 30-45 minute lesson every week or two in the winter/spring months before the summer.\nTake a look at my last post, and combine with this list for some suggestions on things you could be doing prior to the beginning of the course in June:\n\nspoken practice via iTalki or some other language exchange site/programme (shout-out to the newcomer Natakallam, which pairs Syrian refugees in Lebanon with Arabic learners, benefitting both parties). From January-early June I was doing 4-5 hours of iTalki lessons every week (an hour session almost every weekday)\nwriting practice over at Lang-8 (I’ve written about this before, but basically you write entries (about anything) and people correct them for you in exchange for you correcting things in your native language (presumably English)).\nreading practice — I read through all the Sahlawayhi books from January-June. In case you haven’t heard of this excellent series of graded readers for beginning-intermediate levels (and their paired audiobooks), you’re really missing out. The stories are quirky, and not so difficult that you’re looking up every other word in good ol’ Hans Wehr. Even if the language level in the final levels is beyond what you’re capable of, there’s still lots to devour in the early books. Strongly recommended, though this isn’t really for absolute beginners.\nadministrative preparation - Make sure you’ve tied up all your loose ends, as far as you are able. This means delegating work, pausing projects and so on. You might not be able to do this, but try as much as possible. There were some poor souls on the programme who had to work on their ‘jobs’ on the side of the programme; the amount of homework doesn’t really allow for that and for you to sleep, so one will suffer if you try to continue things from your pre-Middlebury life.\ntools and skills preparation - make sure you’re familiar with Anki, Lang-8 and other such tools before the programme begins. This’ll save you time and headaches that you could be using to learn actual Arabic words, phrases and more. (See below on some of the tools that I consider essential).\n(typing practice - this one’s optional, but given how much you’ll find yourself having to type, I’d strongly recommend you practice this and get comfortable finding your way around the keyboard prior to attending the course, especially at the higher levels where you have to write dissertations of 2000+ words in Arabic (typed, of course). I don’t know of many options for PC-users, but for Mac users you can check out XType (on the Mac App Store) for a structured lesson plan for learning typing using the Arabic alphabet. There’s also Typing Master Arabic available online, but it’s a 100% Arabic-only course so that may or may not be appropriate for you.)\n\n\n\nWhen to Study\nJust a quick point here about sleep: it’s really important, especially when you’re cramming words down your throat day and night. Formal classes were usually finished by 3pm, but some people would take time off and only begin homework after dinner at seven or eight in the evening.\nI’d strongly recommend you begin your homework immediately following the end of formal classes at 2.30/3pm. This way, you have some work to do after dinner, but it’s not an insurmountable pile of work, and you’re not going to be studying until 3am every day. You can keep up that kind of schedule for a week maybe, but not eight straight weeks of the programme. You’ll either drop out or lose your mind — both, I think, happened this year, to some extent. Don’t be that person.\nTry where possible to do the unpleasant / necessary thing now so as not to have to do it later. The Middlebury Arabic programme does not really work well with any kind of procrastination behaviour. Enough said on that.\n\n\nThe Language Pledge\nThis is quite important, I think. There’s a qualitative difference in terms of how you approach the course if you’re committed to the language pledge and if you’re not.\nA lot of people violated the pledge this year (Summer 2015), both on and off campus. I get that it’s sometimes good to let off steam and so on, but if you leave campus and talk English among yourselves, you’re missing out and you’re setting your study back.\n(Total beginners aren’t subject to the language pledge, in any case, so don’t worry if that’s you).\n\n\nKeeping Up with Vocabulary\nThis is a big one. The course, particularly in the higher levels, is big on encouraging the learning of words. And, in fact, the more I progress in my Arabic studies, the more I think the learning of words (in context, where possible) is perhaps the key thing to progress forwards. This also connects to my final conclusion about Middlebury, that an intensive programme is only useful if you’re taking the things you learnt on into the long-term.\nThe students of level 4.5 learnt around 3000 new words during the eight weeks of the programme. This doesn’t include words learnt while reading the assigned novel (see below), but I know it’s roughly 3000 because I have them all entered into Anki and I can see exactly how many times I’ve reviewed each one and so on.\nApproximately 85 words per day (on each of the five study days per week) is not for the faint of heart. It’s unrelenting and it’s tough and it’s often dispiriting.\nI could not have done it without spaced-repetition and my faithful Anki.\nIf you want to ensure that you’re not panicking every time there’s a weekly vocabulary test, and (more importantly) if you want to make sure that you’ll have a way to keep learning and reviewing the things you learnt while on the Middlebury programme, you have to use some sort of spaced-repetition software. I really don’t see any other way.\nAgain, I’ve written about spaced-repetition elsewhere so go check that out and then come back.\nSo now you know that Anki is basically a flash-card programme, one that presents words for review at exactly the right time so you’re not needlessly studying, reviewing and testing yourself on things that you know pretty well. As I said earlier, if you aren’t embracing some sort of software-based approach to storing the things you learn on the course, you’ll just forget it over the months after you leave the programme in which case, why did you pay $13,000 to attend in the first place?\nSo, to put you in my shoes, every day we’d study new texts, listen to things, write things and sometimes even get vocab lists themselves. Lots of input. After class each day, I’d take an hour (sometimes up to two) inputting the new bits of information into Anki to ensure that I can keep reviewing things during the course without stressing about which words to devote the most time to, and so on.\nA typical entry might look something like this:\n\nwhich will, in due course, create cards like this, to test my recall:\n\n(That card is giving me a picture of a political prisoner along with the context of a sentence in which a word (mu3taqaliin) was originally present; it is my job to remember which word is suitable for that context, prompted by the picture to remind me. Note that the cards are 100% Arabic-only. This is important in general, regardless of the Middlebury language pledge.)\n(By the way, this method of 100% target-language-only cards, and the formatting of the cards and a lot of the ‘system’ I’m describing here draws heavily on the system described by Gabriel Wyner in his fantastic CreativeLive course on learning languages. I’ve recommended that in the past, too.)\nSo I’ll do that for every new word we learned, sometimes formatting them those context-heavy cards, and sometimes cards without the context (especially when first learning a word, and when the word or phrase is something tangible). Like this:\n\n…which is a prompt for me to recall the Arabic word for “hummingbird” (which I saw fly by me one day while walking around the grounds of the campus). When I click the card to check the answer, I also hear the word pronounced.\nFor words I’m learning for the first time, or for really abstract terms that I can guess are going to have a complex usage within different contexts, I would then write out — I still do all this, by the way, every time I learn a new word — a sentence or two to check that I’ve really grasped the meaning of the word and how to use it in the context of the real world. At the end of my study session, I’ll post all my new practice sentences over on Lang-8, and usually within about two or three hours they’ll all be corrected. I’ll then take the corrections, add them into Anki so now I have:\n\nwords presented without context\nwords presented with the context of whatever text we were studying at the time\nwords presented with an entirely new context, one which has personal relevance and immediacy to me because I wrote it.\n\nThis is how you learn lots of words.\nIf you’re not keen on posting things to lang-8, just take your practice sentences to your teachers during office hours each day (usually around 9 or 10pm) and get them to correct them together with you. Straight after you’ve been to see your teacher, go add those corrections into Anki so you can be tested on them (see more on this in the next section). Our teachers were happy to see students doing this, practicing the new words in context and seeing people taking the extra effort to master the material.\nSo if you’re doing this, you’ll actually be studying some hundred or so new cards every day — sometimes more, if you include the new words you study during colloquial classes — and in order to keep on top of this, you’ll need to start reviewing first thing every day. Seriously, just get up at seven in the morning and spend an hour every day before class reviewing the cards that Anki selects for you. Do another hour in the afternoon/evening and you’ll on the right path. It seems a lot, but the payoff is amazing. (I usually was reviewing Anki cards between 1-3 hours every day, depending on how many words I had added the previous day) throughout the eight-week period.)\n(Side-bar: If you’re in a class that likes to work/collaborate together, you can — of course — spread out the workload of inputting words into Anki, and then everyone can benefit from sharing the files).\nThe night before tests, I would often see people around campus ‘cramming’ words, and the next morning I’d hear about how they were studying until 4am and so on. This is not a recipe for success in the long-term. This won’t help you retain the words after the course ends.\nSo start using spaced-repetition software and make it more likely that you’ll remember the things you learn…\n\n\nLearning from Homework Corrections\nI mentioned this in the previous section. The Middlebury course includes a decent amount of written homework every day (particularly in the higher levels). If you’re not finding some way of systematically reviewing the mistakes you made (after you get your corrections back), then you’re really missing out. In fact, you might as well not bother doing the homework if you’re not going to bother reviewing the corrected versions.\nIt works something like this. I’ll do it in English so you can see what I mean. I’d write a sentence like this, perhaps:\nI go to the supermarket last week.\nThe phrase “last week” alerts us to the fact that the verb should be in the past tense. Thus, the corrected version should be:\nI went to the supermarket last week.\nSo I’ll have a card that prompts me with:\nI __ to the supermarket last week.\nI’ll probably also have a picture of someone walking or “going” alongside that prompt, and the correct answer will be “went”.\nI made cards for every single homework correction or sentence where there was a mistake. Again, inputting it all can take a good chunk of time, and I would only input one or two instances of each error (i.e. if I misspelled the same word thirty times in an essay, I wouldn’t make thirty different cards testing the spelling of the word), but it saves time in the long-run. This is the speedy way to excise commonly-made errors from your life.\n\n\nLearning Grammar\nI approached grammar slightly differently during the Middlebury course. By the time you’re at level 4.5, you’ve already studied all the basic grammar you’re ever going to need, probably more than once. So the grammatical points we studied were a mixture of revision of the basics and some obscure aspects of the language.\nAgain, I’d make sure I wrote sample sentences practicing the grammar points under discussion and those would (post-corrections) make their way into Anki. If there were lists of things to be learnt — the seven forms of the siffa mushabahha, for example — I’d make a card that asks me for those seven forms, then I’d pick sample words in those verbal forms, then add them to a memory palace, and then the Anki system would keep reviewing my command of those seven forms. (Don’t worry if the details of what I just wrote were incomprehensible; just take away the fact that I was finding ways to test myself on the grammar we learned, in abstract form (i.e. a list of the forms) as well as in applied form (sample sentences using the things I’ve learned in real-world context).\n\n\nReadings\nAnother thing the Middlebury course — like any decent language programme — includes a lot of is reading. Where possible, I used Steve Ridout’s excellent online service called Readlang.\nI’ll let Steve explain the overall principle:\nNow you know how Readlang works, you’ll have figured out why it’s such a valuable service. You read texts, you figure out the words you don’t know, and then you test yourself on those words using the context of the passage you just studied (and testing employs the spaced-repetition algorithm, too!). Readlang also allows you to export your ‘learned’ words into Anki.\nThere are some slight amendments I’d recommend for Arabic-learners. If you go to your account settings you’ll have the opportunity to enter in details for a custom Arabic dictionary. Delete whatever is the default there at the moment (probably Google) and use the following text to refill the form.\nURL: http://arabic.britannicaenglish.com/en/{{query}}\nIt should look something like this:\n\nIt’s one of the best online dictionaries for Arabic and works very well in conjunction with Readlang.\n\n\nColloquial/Dialect Classes\nI don’t have too much to say about these. If your colloquial class teaches you a lot of new words, make sure to add them all to Anki and review as appropriate. You’ll probably have to pronounce the words yourself when making new cards — this explains the basics of how to do that — so make sure to make annotations on class handouts as to the vocalisation of the words (i.e. the vowelling) so that you’re not mispronouncing.\n\n\nThe Novel\nWe read Tayyib Saleh’s Mawsim al-hijra illa al-shomal (‘Season of Migration to the North’) over the course of the eight-week programme. It’s not particularly long, but the language used is difficult, especially in terms of the huge number of new words.\nThe idea of studying a novel in the context of the Middlebury programme — as explained by the director and by our teachers — is exposure. Exposure to grammatical structures, exposure to phrases, exposure to culture, and exposure to vocabulary. You aren’t really meant to be learning all the new words, just trying your hardest to figure out what’s going on in the context of the story.\nI used Readlang (along with a copy of the original Arabic text of the novel) to read the assigned chapters each week. That way I wasn’t spending all my time looking words up in Hans Wehr. The number of new/unknown words made that an untenable prospect.\nI also got hold of an audio-recording of the text of the novel. This will be tricky to do, but it really pays off. As you may know, it’s hard to read a passage out loud in Arabic if you don’t know the meaning of the words and — sometimes — the grammar of the passage. (This has to do with how the language works). I’m also a fairly slow reader, so having an audio recording of the passages being studied is a real help.\nI’d usually listen to the audio of the section due for a particular week’s study once or twice before even starting with the reading. Then I’d use readlang and go through line by line. And then I’d listen to the chapter another time, sometimes several times.\nYou’ll find it difficult to find recordings of novels in Arabic. For some reason there isn’t a market for them, and nobody is producing recordings, so you’ll probably have to make your own. This may involve asking your teacher on iTalki to do it (this is what I did), or asking a freelancer on Upwork or any of the many thousands of contracting/freelancer sites. Or just post a request on Facebook perhaps. You’ll probably be able to get it done for under $100, which in the context of your investment into the Middlebury programme isn’t actually a great deal, even more so if you split the cost between the members of your class and share the audio recordings.\nGetting your hands on an audiobook version of the novel you’re studying is something I’d really strongly recommend.\n\n\nPoetry Night\nTowards the end of the programme, the course organizers put on an evening of poetry recitation. Students are given the opportunity (a few weeks earlier) to volunteer to recite a poem as part of the evening’s activities. It’s a long evening, and amongst all the other work you are busy with on the course, it’s hard to see why you’d volunteer to take on another burden, but it’s well worth it. It’s a chance to expose yourself to (often) highly stylized language. It’s a change to really polish your pronunciation. And, by the time you’re done rehearsing, you’ll probably have memorized the text so it’s also a chance to learn a poem in Arabic by heart. I recorded my recitation on that evening here. Apologies for any mistakes etc.\n\n\nFurther Online Resources\n\nContext-Reverso — I have one of my fellow classmates to thank (Hi Yasmine!) for this gem. You put in a word, and it spits out example sentences (drawn from a big database of sample real-word texts) using that word in context. For the intermediate-advanced learner, it’s an amazing resource, far more useful than Google Translate or any others of its kind.\niTalki — I mentioned this above, especially in the context of your preparations for study at Middlebury. I also (where classmates were off campus on weekends, or in order to prepare for oral presentations and so on) scheduled one or two lessons during the programme itself so that I could talk through my ideas, or a difficult piece of text etc, with my teacher over Skype.\nLang-8 - I mentioned this above. Use it. Love it. Share it.\nElectronic Hans Wehr — You can search the Hans Wehr dictionary online by root.\nForvo — great for getting audio pronunciation of words for use in your Anki deck.\n\n\n\nConclusion: How useful are intensive language study programmes?\nI’m not sure how useful the “intensive” part of the Middlebury programme really is. Most students didn’t have a system to manage the massive levels of input, so a lot of their time and efforts were wasted. If Middlebury taught and offered ways for students to be better prepared — such s some of the things I’ve attempted to outline above — then perhaps I’d feel more charitable to the programme, but as such I feel many students were let down in some way.\nSo, after all that, was it worth it? On balance, yes, but mainly because I didn’t have to spend all that money for the course fees (on account of my scholarship) and also because I had systems in place to ensure that I wouldn’t forget the things that I was learning during the course.\nIf you’re a complete beginner and want to leapfrog ahead to the point where you can start teaching yourself, I’d recommend this programme. If you’re already at intermediate levels, you might want to look into other ways, especially if you aren’t lucky enough to have an employer or a grant that pays for your attendance.\nI’ll be writing a separate post on resources available to the intermediate-advanced Arabic language student in due course, as well as on how to get out of the plateau into which it’s easy to find oneself. Till then, I hope you found this useful…"
  },
  {
    "objectID": "personal/2015-09-18-2015-9-ecolinguism-and-the-ethics-of-learning-new-languages.html",
    "href": "personal/2015-09-18-2015-9-ecolinguism-and-the-ethics-of-learning-new-languages.html",
    "title": "Ecolinguism and the ethics of learning new languages",
    "section": "",
    "text": "I was interviewed by Tammy Bjelland of the Business of Language podcast a few weeks ago, and the episode recently went live. Readers of this blog will know that I write about the study of language with some regularity – see the archives for some previous posts – but I don’t talk about it a great deal on my own podcast nor is it really the focus of my work. So it was nice to have a chance to talk through my background in learning languages and the challenges of learning languages with few materials available for self-study. There isn’t enough written about this.\nIt was also gratifying to find a forum to discuss Richard Benton’s ideas about ecolinguism. He wrote a blogpost summarising some of his ideas here:\n\nI am an ecolinguist because I want my work to preserve the complexity of our world’s language and culture ecosystem. How do you create a strong community made up of hardened, poor refugees and rich, privileged natives? The privileged must work hard to create new connections. In middle school, the band geek or math nerd can’t simply decide to enter the “cool crowd.” Only those with strong social capital can invite in those on the outside.\n\n\nThe strength of our communities depends on the decisions of the privileged and the powerful. When insiders opt to forgo their comfort to commune with those who go without, they unite communities who would be isolated. When a well-educated privileged professional chooses to learn a language, for example, he forgoes his advantage in communicating in way where he feels most comfortable. The white Minnesotan, speaking elementary, broken Somali, puts the outsider, the refugee, in the position of power. Struggling to learn this difficult language allows new connections to grow.\n\nThe choices we make as to which language to learn next have a broader impact beyond our own lives. For the full discussion, visit Tammy’s website to listen to the full episode or subscribe via your preferred podcast client.\nUPDATE: I now offer one-on-one language coaching. Read more about what it involves and what kinds of problems it’s best suited to addressing."
  },
  {
    "objectID": "personal/2015-10-13-four-colours.html",
    "href": "personal/2015-10-13-four-colours.html",
    "title": "Four Colours",
    "section": "",
    "text": "A few years ago, I read a book that changed the way I took notes. That book was “How to Make a Complete Map of Every Thought You Think” by Lion Kimbro. Thanks to my podcast, Sources and Methods, I had the chance to chat with Lion a few weeks ago. The episode will be out in November but I wanted to share one of the ideas that I’ve found most useful. He wrote about it in his book and we discussed it again on the podcast.\nIt involves taking notes with a four-colour pen. I’m talking about pen-and-paper here, not digital notes, though I suppose it might work there too with some tweaking. You use a different colour to ascribe different meanings to your notes. Thus, quoting from his book:\n\nRED: Error, Warning, Correction\n\n\nBLUE: Structure, Diagram, Picture, Links, Keys (in key-value pairs)\n\n\nGREEN: Meta, Definition, Naming, Brief Annotation, Glyphs\n\n\nBLACK: Main Content [p. 26]\n\nMost notes will thus be in Black, but other things can stand out by sticking to the system outlined in the quote. It takes a bit of getting used to, including sticking up reminders on walls showing the colour scheme, but after a week or two it’s instinctual and really helps when revisiting notes at a later date.\nLion shares lots of other note-taking tips in our podcast, which I’ll post here when it’s out.\nA short practical tip: ever since reading Lion’s book, I’ve been using a Bic four-colour biro which are quite easy to find in most stationery stores. Lion mentioned a different type which I’ve now been using for a week or two and have had a really good experience so far. It’s the Zebra Sarasa4 model (pictured above). If you want to get into taking notes using four-coloured pens, I’d really recommend it."
  },
  {
    "objectID": "personal/2015-11-08-misquoting-mohammad-meets-sources-methods.html",
    "href": "personal/2015-11-08-misquoting-mohammad-meets-sources-methods.html",
    "title": "Misquoting Mohammad meets Sources & Methods",
    "section": "",
    "text": "Matt and I were lucky to get Jonathan Brown on the podcast for an episode that came out this week. Check it out for his work on authority and sourcing in the Islamic tradition, a discussion of how he picks the examples that end up in his books and the current state of Islamic studies."
  },
  {
    "objectID": "personal/2015-11-22-on-untangling-syrias-socially-mediated-war.html",
    "href": "personal/2015-11-22-on-untangling-syrias-socially-mediated-war.html",
    "title": "On Untangling Syria’s Socially Mediated War",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“351”][](http://iraqburning.blogspot.no/2006/04/walk-in-damascus-amidst-symbols-of.html) Some old photos from when I used to live in Damascus [/caption]  \nHow do we figure out what is going on in a country like Syria, when journalists, researchers and civilians alike are targeted with frustrating ease? Is it enough to track what is being posted on social media outlets? These two questions are at the core of a fascinating recent(ish) study published by the United States Institute for Peace (USIP).\nSyria’s Socially Mediated Civil War – by Marc Lynch, Deen Freelon and Sean Aday – came out in January 2014 and analyses an Arabic-and-English-language data set spanning a few years. It offers a useful overview of the social media trends as they relate to the ongoing conflict in Syria. It is especially relevant for those of us who aren’t inside Syria right now, and who are trying to understand things at one remove, whether that is through following social media output or talking to those who have left the country. (This means journalists, researchers and the like.)\nSome stark conclusions emerge from the report. The ones I’m choosing to highlight here relate to how international media and research outlets have often been blind to structural issues that obscure their ability to understand Syria from outside the country.\n\n“Social media create a dangerous illusion of unmediated information flows.” [5]\n\nThe role of translation or the importance of having research teams that are competent in both English and Arabic comes out very strongly from the research.\n\n“The rapid growth in Arabic social media use poses serious problems for any research that draws only on English-language sources.” [page 3]\n\nThe report details how tweets about Syria in Arabic and English came to be different universes, how the discourse rarely overlapped between the two and that to monitor one was to have no idea of what was going on in the other:\n\n“Arabic-language tweets quickly came to dominate the online discourse. Early in the Arab Spring, English-language social media played a crucial role in transmitting the regional uprisings to a Western audience. By June 2011, Arabic had overtaken English as the dominant language, and social media increasingly focused inward on local and identity-based communities. Studies using English-only datasets can no longer be considered acceptable.” [6]\n\nAlso:\n\n“The English-language Twitter conversation about Syria is particularly insular and increasingly interacts only with itself, creating a badly skewed impression of the broader Arabic discourse. It focused on different topics, emphasized different themes, and circulated different imagery. This has important implications for understanding mainstream media’s limitations in covering Syria and other non-Western foreign crises and raises troubling questions about the skewed image that coverage might be presenting to audiences.” [6]\n\nAlso:\n\n“researchers using only English-language tweets would be significantly misreading the content and nature of the online Twitter discourse.” [17]\n\nAnd:\n\n“These findings demonstrate once again the insularity of English-language journalists and the rapid growth of the Arabic- speaking networks. Both findings are potentially troubling for at least two reasons. First, they imply a journalistic community whose coverage may be influenced more by its cultural and professional biases than by the myriad constituencies within Syria and across the region. Second, they point to the power of social media to draw people into like-minded networks that interpret the news through the prism of their own information bubbles.” [26]\n\nThe general ideas in here won’t necessarily come as a surprise but I found it instructive to see just how different those two discourse universes are in the report.\nIn a separate but not-unrelated note, I have been thinking of ways that I can stay engaged in what’s going on in Syria beyond just consuming reports at one step removed. I’m working with a beta-testing team using a piece of software called Bridge – made by the lovely team at Meedan – which allows for the translation of social media and the use of those translations as part of an embedded presentation online. I will be translating strands and snippets from certain parts of Syria’s social media universe in Arabic. More on this soon, I hope."
  },
  {
    "objectID": "personal/2015-12-16-the-best-books-i-read-in-2015.html",
    "href": "personal/2015-12-16-the-best-books-i-read-in-2015.html",
    "title": "The Best Books I Read in 2015",
    "section": "",
    "text": "This year was my biggest yet. 150 books in total and 15 days still to go.\nOf those, 29 were fiction and the remaining 121 non-fiction. I’m not really happy with that disparity, but more on that below. Five of the books this year were in Arabic, including two novels. Readers of this blog will know that I’ve been steadily working my way up to the level where I’m comfortable reading Abdul Rahman Munif’s Cities of Salt trilogy in the original. I’m not quite there yet, but getting there. The five books in 2015 is a start. Watch this space.\nGoodreads tells me that my 150 books total to some 32,686 pages. That’s almost 90 pages every day given a 365-day year, but I was away studying Arabic for two months so actually it’s more like 107 pages per day. Look at the dip in my Beeminder graph where I read nothing:\n\n49 of the books I read were written by women. One-third is better than last year’s 1:9 ratio, but still not many as I’d have liked. (Last year I wrote that I’d be happy with one-third.)\nLast year I upped my goal from 100 to 150 for 2015 and on one hand I’m glad I did that. I’ve long felt that the more I read, the more I expose myself to. On the other hand, my two-months-where-I’m-only-allowed-to-speak-Arabic gave me a solid disadvantage going in. I felt the pressure, and I felt that I rushed through some books that I’d maybe have liked to savour more. So next year I’m going back to 100. It’s only a mild challenge to ensure that I read a hundred, but it’s enough that I’ll get exposed to new things.\nI’m also going to make sure that 50% of what I read is fiction. I liked the way I tried to do this towards the end of the year, and I realise I miss reading fiction. Or perhaps a lot of the non-fiction I read these days has a ‘samey’ quality to it. I also realised that I have a lot of books in my library (digital or physical) already, so I’m not going to spend any money on books in 2016. (The only way I can read a book that was published in 2016 is if someone buys it for me, or if I get a gift card as a present etc). If you catch me buying any books, feel free to scold me.\n\nAs for the things I really enjoyed this past year, you’ve heard me praise Jonathan Brown’s Misquoting Mohammad already. I’ll stick with what I originally wrote for Pacific Standard:\n\n“Misquoting Muhammad is a book I wish I had the money to buy for all my friends and colleagues, because he presents readers with a guide to Islamic thought that portrays it not as a fixed entity but as a complex product of utterly human machinations… Ultimately, Brown teaches a simple, if vital, lesson: Authenticity is elusive in religion, and those who claim it tend not to be searching for the truth but grasping for power.”\n\nSeriously. Go read that book.\n\nThe Paris conference has just concluded. World leaders are taking a moment to bask in their self-reflected satisfaction at the agreement they produced. I don’t know enough to be able to offer a substantive critique, but I’m highly doubtful that anyone actually took the kinds of hard decisions to go against financial or national interests. Two things have offered a kind of salve to my pessimism about the environment. Paul Kingsnorth’s Dark Mountain Project offers a bracingly dark vision of our future, but one that meshes with what my gut tells me might be a useful path to go down. Roy Scranton’s short book, Learning to Die in the Anthropocene: Reflections on the End of Civilisation, has two core messages: “We’re fucked” and “Prepare to die”. Scranton suggests that we embrace philosophy as a way to respond to the crisis – the crisis of humanity’s extinction, no less. If nothing else, Scranton gets you to ponder your future death, and that’s never a bad way to spend one’s afternoon.\n\nAs co-host of a podcast that is at least nominally about doing better work, I’d be remiss if I didn’t mention two books related to this theme. Ed Catmull’s Creativity Inc. was a surprisingly deep read, offering thoughts on developing ideas, managing teams and growing as a leader. As it turns out, the team pushing out films at Pixar have spent a lot of time thinking about what does and doesn’t work when it comes to collaborating together on creative projects. If you have to do that, I’d strongly recommend you give this a read.\n\nMake it Stick: The Science of Successful Learning (by Peter C. Brown, Henry L. Roediger III and Mark A. McDaniel) is now the book I give to people starting out on a new course of study or reading. I wish I’d read it year ago myself. The authors explain how to study, how to learn things for long-term retention, and how to tweak the school experience to encourage retention. The authors strive to make examples practical and applicable. Spaced-repetition software is never mentioned in the book – in fact technology really isn’t the focus – but it’s possible to read it as a love letter to Anki. (Read more of my full review/summary here).\nThe exchange of letters between Paul Auster and J.M. Coetzee felt like something I wasn’t meant to read. The latter is high up there in the pantheon of my favourite writers and Here and Now was a true pleasure to read. If you don’t care for either of the correspondents, though, this book may fall flat.\n\nAs a society, we think too little about death and dying and what it means to become old. Being Mortal: Medicine and What Matters in the End is about the care that we give to people at the ends of their lives. (He also encourages us to think about what that might look like for our own lives). The practice and profession of medicine is slanted a certain way, but Atul Gawande shows fairly clearly how that isn’t necessarily beneficial. Short, but hits hard.\n\nAs my only fiction pick for this year, Shandana Minhas’ Survival Tips for Lunatics is a delightful romp through Pakistan. This is a children’s book on the surface, but appropriate for ‘adults’ who have a sense of fun and are willing to follow Minhas on her crazy journey. Some great characters in this book (including, but not limited to, a bear, a velociraptor and a sparrow). The plot is simple (two children get left behind on a camping trip and have to get back to their parents, via Baluchistan and a Pakistani military border post/facility on Afghan soil) but the adventures are many. Strongly recommended by anyone who is bored by the usual books given and read to pre-teen children.\nLet me know if you enjoyed any of these over on Twitter or on Goodreads. And happy reading for the coming year!"
  },
  {
    "objectID": "personal/2016-05-25-2016-5-memorisation-language-ninja.html",
    "href": "personal/2016-05-25-2016-5-memorisation-language-ninja.html",
    "title": "How to become a memorisation and language ninja",
    "section": "",
    "text": "I’m very glad to be able to announce two new things I’ve been busy with over the past few months.\n\nFirstly, I’m launching an email course showing how to learn long lists of items by heart. This course is outwardly directed towards Muslims, since the list that you learn over the course of a week, is a list of 99 Names of God — the so-called Asmaa ul-Husnaa. But the broad principles are the same for learning any long list of things, so don’t think you need to be a Muslim to take the course. The materials come with lots of handouts and supplementary information about memory and the like.\nNote that this first course is part of something new I’m calling Incremental Elephant, a place where I can offer more courses related to memory, language-learning and productivity.\nSecondly, as regular readers of this blog will know, I’ve been blogging about technology, productivity, language learning and the intersection between the three for several years. Along the way, I’ve fielded dozens of questions from readers about which programme to use for this or that scenario, or which textbook to use when starting out with language x or y. I’ve increasingly been taking on longer-term clients to coach through these issues, so I’m taking the opportunity now to announce officially that I offer one-on-one coaching for language learning or productivity-related issues.\nThe language you’re learning doesn’t need to be one that I already know, because my coaching is usually targeting the meta-issues of how you’re studying rather than what you’re studying.\nI offer weekly or biweekly Skype coaching sessions. This will include a mix of reviews of work you did the past week, planning your studies for the coming week and brainstorming techniques to get you over specific problems that are preventing you from moving forward. (For example, I’ve recently been working with someone who has problems declining verbs, so we’ve been tackling that from several angles using a variety of techniques).\nMore news on the Ph.D. front in a few months, I hope, but for now, go check out the 99 Names course and get in touch if you would like to discuss working with me to improve how you go about learning languages.\nUPDATE: I’ve written up a more extensive explanation of what one-on-one language coaching involves, and what kinds of problems it’s best suited to tackling. Read more here."
  },
  {
    "objectID": "personal/2016-08-15-2016-8-on-learning-new-skills.html",
    "href": "personal/2016-08-15-2016-8-on-learning-new-skills.html",
    "title": "On Learning New Skills, #showyourwork and other housekeeping",
    "section": "",
    "text": "Newly released from life under the PhD regime, I have been exploring new interests and indulging my curiosity down new pathways. Last time I wrote, I explained how I had taken to studying sharks and various aspects of conservation and marine biology. There was a certain amount of training around higher-level concepts in that course: in particular, on the scientific method, ways to problem-solve and so on, but in general the emphasis was on the material rather than specific skills.\nI think that learning new skills – expanding the lenses you have through which to view the world, or giving yourself more options for a variety of perspectives – is an important part of becoming more useful. To that end, language learning was always an important part of getting to know any subject or place (either the implicit language of a particular domain, or the explicit language of a people or country).\nRecently, I’ve taken to trying to expand my numeric and digital literacy. I’ve always been technically literate, at least to the extent that I was able to solve my problems, fiddle with code a little and play around with technical tools and programmes for which I have no formal training. That said, there is a wide gulf between that position and a more confident data literacy, where I can interpret things through the lens(es) of statistics, use tools to analyse what is going on and so on. I’ve always felt like this was a core weakness in my ways of understanding the world, so I’m now trying to address or remedy that.\nMy first step is to address the gaping holes. These are a mix of mathematics, statistics, probability as well as things like computational languages that will allow me to engage in these disciplines with more than a basic-level understanding. There is a fair amount of debate as to whether the name is useful or not, but for now, I’ll state that I’m training myself in ‘data science’.\nI learn these kinds of skills best through a practice- and feedback-rich environment, so I’m taking a project-based approach to my studies. Khan Academy (what a resource!) is my mentor for mathematics, and Udacity (whose courses I’ve taken in the past and been very impressed by) is helping with the Python/coding aspect. Each skill group I learn is followed by a mini-project that allows me to use the skills in an applied context.\nAll of this is by way of introduction to what I really wanted to talk about today, which is a recent problem that I encountered while completing my coursework. The details don’t matter that much, so I won’t get into them, but suffice it to say for now that I was trying to write a python function that calculated the number of days between one date and another date.\nOne feature of coding that I find frustrating is the way loops (bits of code that repeat for as long as certain conditions remain met/unmet) can be included within loops, and sometimes maybe another set as well within the loop-within-a-loop. I’m new to this stuff, so this level of recursion (is that even the right word? I’m pretty sure recursion is used for something else) makes my brain hurt when I try to think about it too much. But think about it I must.\nSo there I was, with some broken bit of code, no idea how to move forward. The answer to the problem was just one click away, but I didn’t want to just see the answer. I wanted to learn the thing that was absent. I wanted to learn how to get from my state of confusion to a state of understanding.\nThis seems to be a common enough problem in my new studies that I thought it might be useful to describe. You can explain the solution, at which point you understand how that particular problem was solved, but how do you improve the likelihood that you’ll be able to solve a different problem in the future? Does looking at the solution help? Does spending a week staring at a piece of paper, wishing the solution to magically appear, help?\nIn the end, what I’m trying to get better at is problem-solving. As far as I’ve been able to read so far (in books like Sebastian Gutierrez’s fantastic Data Scientists at Work), this skill is valued far above any specific skills like competence in one programming or computational language vs another. The specific languages can all be learnt. Getting better at solving problems, interviewees seem to constantly imply, that is much harder.\nWhich is where I am, trying to solve hard (hard for me, at least) problems.\nMy solution involved thinking about the problem more deliberately. Udacity hosts an online forum where students can post bits of code, questions etc. There’s usually someone who answers the question within 24 hours. The emphasis is on finding new ways to think about problems rather than just providing the answer. So, teaching you how to fish instead of giving you a fish. That old chestnut.\nI posted a question. In writing out the question, I was forced to be more deliberate in how I thought out the problem. By specifying the exact point where I saw something going wrong, I was forced to be more sequential, more methodical. I started to see new pathways forward to progress with the problem. I would then go off and work a bit more until I hit another roadblock, at which point I’d post a follow-up question. Then a bit more insight would follow the process of writing up the problem. Then a bit more work, another roadblock. You get the idea. I think I submitted 4 different questions/writeups in as many hours. By the end, I had thought my way out of the problem. Nobody needed to reply to my query; I had solved it myself.\nThis is quite a simple insight, in many ways: writing things down, making them concrete… these are things that help with the process of thought. Thought, taken out of the mushy-tissue-filled black box that is our brains and put on paper, is something that benefits from being methodical. I’d long known (at least, theoretically or implicitly) that writing helps thought. Every time I’ve worked on an essay or on a book, the process has clarified my thought in ways that wouldn’t have been possible had I just remained satisfied with ‘thinking’ about things.\nThere are a bunch of small project ideas I have and want to use as ways of practising my new / fledgeling skills. I have certain data sets from my time in Afghanistan that I’d like to work with. There are other interests that I’d like to stimulate by applying this new tool / set of tools."
  },
  {
    "objectID": "personal/2016-08-15-2016-8-on-learning-new-skills.html#showyourwork",
    "href": "personal/2016-08-15-2016-8-on-learning-new-skills.html#showyourwork",
    "title": "On Learning New Skills, #showyourwork and other housekeeping",
    "section": "#showyourwork",
    "text": "#showyourwork\nThis brings me neatly to my second point. I finished a book by Austin Kleon recently and it revolves around similar-ish ideas. Kleon talks about the importance of sharing the process of work as well as the work itself. He explains a bunch of the reasons why this is a smart idea, and he discusses various ways that you can enact this as a guiding practice going forward.\nIt reminded me that the last major learning experience I went through (not including languages or personal growth) was when I started travelling in and learning about Afghanistan. I was writing lots back then, either in emails to a group of friends and family, or later in blog form or in articles written for free for local magazines. (In my work for AfghanWire, too, I drafted a mini-encyclopedia of sorts, with entries for the important pieces of context (people, places, events). Researching all of that was a huge task, but looking back I’m certain that I learnt a lot through the process.)\nThe same thing happened when I started studying sharks, as I explained last time. I used twitter as a way of connecting with scholars and people who had already spent years studying the discipline. I used my podcast, Sources & Methods, to connect to at least one group (the Gills Club –&gt; see the podcast here) and to connect to talk about that new interest.\nAll of which is a roundabout way of saying that I will be blogging more. Most blogs will not be as long as this, or previous posts. In the past, I would only consider it worth blogging if I had taken a week or few to gather together all the relevant insights to a particular topic or issue, but I’m not sure that’s reasonable any more (thanks to Kleon’s book) and I want to benefit from the power that comes from writing through problems on a more regular basis."
  },
  {
    "objectID": "personal/2016-08-15-2016-8-on-learning-new-skills.html#housekeeping",
    "href": "personal/2016-08-15-2016-8-on-learning-new-skills.html#housekeeping",
    "title": "On Learning New Skills, #showyourwork and other housekeeping",
    "section": "Housekeeping",
    "text": "Housekeeping\nI mentioned that I’m done with the PhD. So now the rest of my life starts. I haven’t decided on the next step just yet, and there still some loops that need tying off. I have some research/report-writing to finish, a novel written last year during a fit of PhD-procrastination to edit/rework, and more to be done with my Jordanian colloquial Arabic.\nOne somewhat important development is that I’m moving to Amman (Jordan) this coming week. I’m not sure how long I’ll be there, but consider it my base from now on. If you’re ever passing through town, please do drop me a line.\nMy final piece of news is that Felix and I have signed a contract to work on a new book for Hurst (title and all other details TBC) which will be an anthology of Taliban writings/statements. It’ll cover the whole gamut of their experience from the 1980s up until the present day. We’ll be drawing from the materials of the Taliban Sources Project as well as other sources we’ve gathered along the way in previous years. I’m quite excited about the project, and it’ll be an amazing resource once out. The problem for a lot of research, as I’ve mentioned before, is that the primary sources aren’t used half as much as they ought. We hope this project will offer a corrective in this respect."
  },
  {
    "objectID": "personal/2016-08-18-2016-8-remove-your-colour.html",
    "href": "personal/2016-08-18-2016-8-remove-your-colour.html",
    "title": "Remove Your Colour",
    "section": "",
    "text": "This is just a quick little trick that may or may not be useful for you. I can’t seem to track down the site where I originally read about this, but if I discover it subsequently I’ll be sure to credit the author/blogger because I’ve derived a lot of benefit from this.\nThe idea is simple: switch all your digital devices’ displays to black/white or grayscale (i.e. turn off all colours). This includes phones and laptops and whatever else may have a bright colourful screen that allows you to change the colour configuration. You do this because, as I discovered, so many of the (cheap) tricks employed by websites and computer systems to get you to keep using them, all of these things use colour as a key tactic. By turning the colour off, you take away a lot of the emotional pull that these services have on you, and thus are more able to switch off, detach and so on.\nReaders of this blog will know that I’ve been a long-standing advocate of turning everything off. My phone is on Airplane Mode for almost the entire day, usually, and my laptop is only connected to the internet if there’s a particular reason. Connection to the internet, for me, is a privilege not a right. It’s a rule that I’ve defined for myself – (this will / may not work for you!) – but it’s one that I’ve defined because I had problems with these things in the past. For someone interested in learning more, with half an iota of curiosity about the world, the internet is a feast of opportunity and possibility. Social media more so: every connection is an opportunity to discover something new, and the stream never ends!\nSo I’ve become pretty good about switching these things off, but sometimes you have no choice. Having all screens as black-and-white means that I have a bit more objectivity, or power to stand back from the allure of programme x or service y.\nYou really notice how much colour plays a role in keeping you using your phone or a particular app when you turn the colour back on after a few days with it turned off. (Occasionally you have to identify a particular colour in an emailed photo, or some such). Suddenly everything is pretty. Reds and blues and greens glisten. The screen on most modern devices is really a marvel, and it’s clearly designed to work well with colour.\nStill, turn it off. That’s how they get you. (Instructions for Macs here, Android phones here, PCs here (sort of), and iPhone/iPads here). You might be surprised how it gives you a bit more power over your own choices."
  },
  {
    "objectID": "personal/2016-08-21-learn-afghanistan-districts-anki.html",
    "href": "personal/2016-08-21-learn-afghanistan-districts-anki.html",
    "title": "Learn all the districts of Afghanistan with Anki!",
    "section": "",
    "text": "A friend was asking about using Anki to learn to recognise the districts of Afghanistan so I made her a deck that provides tests in the following way;\nOn the front of the card the question is presented along with a computer-generated audio pronunciation of the district name:\n\nThen if you know it, you’ll answer Badakhshan and then you’ll click/tap through to the next screen to see if you got it right. You’ll see this:\n\nThen you can mark whether you got it right or not. There are around 400 districts to learn, so if you learn 13-15 new cards each day you’ll finish the whole lot in a month.\nWhy learn all the districts of Afghanistan? Sometimes you’ll hear someone talking about a particular place or part of the country, and without knowing which province they’re talking about you might not understand the context or the conversation. Plus, a little bit of geography never hurt anyone.\nGive it a try. And let me know if you manage to complete the deck. You can download the full Anki file here. Enjoy!"
  },
  {
    "objectID": "personal/2016-08-23-2016-8-walking-with-words.html",
    "href": "personal/2016-08-23-2016-8-walking-with-words.html",
    "title": "Walking with Words",
    "section": "",
    "text": "Earlier this year I glanced at my Fitbit stats and saw that I was walking less than usual. There were some reasons for this, but as always, the data itself functioned as a sort of reminder, a nudge in the right direction. I had a think about the main thing keeping me from writing and realised it was because I was working full-out writing my PhD.\nMost of the second (rewrite)/redraft of my thesis was written in a little coffee shop in Delft (a small historic town in the south of Holland). Each morning I’d cycle to the cafe at 7:30, ready to start at 8am. I’d work for four hours and then cycle back home at 12. I’ll write more about this particular workflow/pattern in a separate post, but suffice it for now to say that I had lots of time in the afternoons when I could be walking or finding more ways to be moving my body, but this wasn’t happening.\nWhen I tried to remedy this by going for more walks in the afternoon, I found I developed a strong antipathy to the time outdoors. It shouldn’t have been that way. I was in the middle of nature, walking through nature preserves and forests close to home, but that apparently wasn’t enough.\nPodcasts and audiobooks offered a way through and round this problem. I took a bit of time to explore some different shows with regularly published episodes and I filled up my Audible queue with interesting books that I wouldn’t otherwise read. It turns out that this was enough. (I feel like I’ve learnt this lesson in the past, albeit in slightly different circumstances. I guess some lessons don’t stick as well.)\nIn case you’re curious, four podcasts I still enjoy are:\n1) The Frontline Club – recordings of their live events in London. The quality is variable, but generally you learn something about a place or an issue that you hadn’t considered (either ever, or for a very long while). Time well spent.\n2) Invisibilia – the first series was far better than the current one, but they’re still almost always interesting and stimulating. The hosts discuss various aspects of psychology, behaviour and social activity.\n3) Back to Work – This might only make sense if you’ve been listening from the beginning, but Merlin Mann and co-host Dan Benjamin are an interesting pair to discuss technology, its role in our lives and a huge number of various connected issues.\n4) Reveal – this is connected with The Center for Investigative Reporting and each episode offers audio versions or discussions of major investigative reporting. Like with the Frontline Club, you learn a lot from this one. It’s a little US-centric, but not exclusively so.\nFor Audible books, the sky is the limit. I listen to books at 1.25 or sometimes 1.5x speed (and podcasts at 1.5-2x speed), so even very long books can only take 12 or 13 hours to get through. That’s one or two weeks of walks for a book you wouldn’t otherwise have read. Get through a book or two every two weeks and you’re reading a couple of dozen books a year.\nNow when I think about whether I want to go for a walk, I am enthusiastic about going out. I’m happy not only to be moving, but also to be coming into content designed to inform and entertain. If you aren’t moving as much as you feel like you might want, maybe see if you can add in some audio media to make the activity more pleasant."
  },
  {
    "objectID": "personal/2016-08-25-phd-tools-tinderbox.html",
    "href": "personal/2016-08-25-phd-tools-tinderbox.html",
    "title": "PhD Tools: Think better with Tinderbox",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nTinderbox is a tool for writers and thinkers that can handle most things that you throw at it. Anything to do with thinking, it can probably do what you want. That said, there is a slight learning curve to the programme, and it may not be to everyone’s particular style. With those caveats stated, let’s dive in.\nAny PhD student generally takes a lot of notes. Notes on books or articles you’re reading, or notes about points you want to make in the argument of your text / writeup. There are purely text-based / database-style systems that can handle these kinds of notes (like DevonThink, about which more soon) but none with the flexibility or visual features of Tinderbox.\nA list of notes, for example, can be transformed into a visual / spaced-and-linked map of meaning like traditional ‘mind maps’. You can switch back and forth between outlines and maps easily (or even display both on the same screen/window) and display notes as well.\nIt’s fast, it doesn’t break or crash or slow down your computer, and it helps you think things through in the way that is best suited to your needs. Too often, software forces you to think in a particular way (i.e. the way of the software creator), but Tinderbox adapts to the way you were thinking and allows you to draft notes and structure accordingly.\nI’ve written elsewhere about some things I’ve done using Tinderbox, so no need to mention all that here, but some things I found specifically useful for my PhD:\n\nSmall databases, constructed on the fly, while taking notes from books. An example of this is a database of key players or individuals from within the Taliban who occurred at various places in my notes. This grew to a pretty extensive document, but Tinderbox allows you to make these kind of structured data sets without needing to think too much about how the data might eventually be presented or used. Changing things is easy.\nTimelines – Tinderbox can display lists of events with start and/or end dates on a timeline. I used this to create the TalQaeda timeline, for example, or the list of moments where the Afghan or international military forces claimed to have killed or identified a Chechen fighter in Afghanistan (chechensinafghanistan.com).\nWorking through ‘unstructurable’ ideas – There’s often a gap between the ideas you think you have in your head and the ideas as they are expressed on the page. I have found Tinderbox extremely useful in allowing me to find a way to make the two align closer together, or to figure out a structure or a sequence to parts of an idea in a way that makes most sense.\n\nYou can also use Tinderbox as a day-planner or a task outliner (like I discussed in my post about Trello), though I think it might be less suited to this task when compared to Trello.\nThe forum for Tinderbox and related products is a great place to discuss method, process and different ways of structuring ideas. Users are a mix of complete beginners and others who have been drafting books, novels and essays using Tinderbox for years. I find the discussions in the forum are often stimulating; asking questions there is an interesting way to rethink a particular mental quandary you might find yourself in.\nBONUS: Listen to my podcast interview with Tinderbox’s creator, Mark Bernstein, for more on the vision behind the software and for some practical tips on structuring ideas."
  },
  {
    "objectID": "personal/2016-08-29-2016-8-phd-tools-pinboard.html",
    "href": "personal/2016-08-29-2016-8-phd-tools-pinboard.html",
    "title": "PhD Tools: Save your web links with Pinboard",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nPinboard is the successor to Delicious and various other social link repositories. It’s a service I get a lot of use out of because some parts of following the news, monitoring various government / non-governmental sites etc means reading lots of small articles each day. If, in the future, I want to return to a particular article, I generally don’t want to have to go through the hassle of searching for it afresh (sidebar: use DuckDuckGo instead of Google! It’s great!) so I just click a button to save a page in Pinboard when I think there’s a chance I’ll find this useful or I want to preserve it in some way. (Visit the ‘tour’ part of the site to learn more about how Pinboard works.)\nPinboard also auto-adds links, if I’ve starred, retweeted or saved anything in Twitter. Also if I’ve added a link from twitter or elsewhere into my Instapaper (Pocket is also used by some people, and is supported by Pinboard), then these articles are also autosaved into Pinboard. I reason that if I’ve taken the trouble to save it for reading later, then there was probably something in there that I might find useful in the future, or something that I might want to reference later.\nThe great thing about searching your repository of pinboard links is that you can do in-text searches. So you’re not just searching the name and URL of the link, but you’re searching the full text of the page. This is really useful, especially if you have many links saved. I just checked my account stats and see that I have over 60,000 bookmarks saved in my pinboard account. This is over 15 years worth of bookmarking.\nPinboard also offers a paid upgrade service where it will archive copies of a page and store that archived image. That way, even if the site is later taken down, or someone deletes the page, or anything at all happens to the page, then you still have a copy of the page and can search in it, can download it etc. Needless to say, this is really useful for monitoring the Taliban’s websites, for example, which are frequently targeted in take-down attempts and where data is periodically deleted from servers or changed in various ways. I often double down and make a manual archive copy of important messages/pages to be stored in DevonThink, but I generally rely on Pinboard to handle the bulk of this work for me.\nThe creator of Pinboard, Maciej Cegłowski, is a smart guy and who writes interesting things, and the service reflects this. (Check out his interview on the Longform podcast). It’s a paid service, but is relatively inexpensive as far as these services go. Moreover, the paid nature of the service means that there’s relatively little (if at all?) creep factor to using it. Pinboard isn’t selling your link database on to anyone else, they aren’t marketing data profiles of their users etc. It’s a solid service.\nThe interface is pretty minimal, which I like, but in case it’s not to your taste you can browse your links in any one of a dozen or so apps which can hook up into your Pinboard data. Links are fully taggable, including tag nesting etc, so you’re fully covered on that front. There’s a social element to pinboard that I don’t use much (mainly because I don’t know many others who use pinboard for saving links) but I can see that that might be a useful feature if you have a community working on a particular topic or area.\nPinboard is easy to use, reliable and relatively inexpensive. It can save you time and help you find things you read on the web. Check it out here."
  },
  {
    "objectID": "personal/2016-08-31-phd-tools-bookends.html",
    "href": "personal/2016-08-31-phd-tools-bookends.html",
    "title": "PhD Tools: Bookends for Managing References",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nMy PhD included references to 479 individual sources. It’s well known that formatting issues often plague students just prior to submission of their dissertations. A reference manager can help solve most of these problems.\nWhen I began my PhD, I was using Sente, a Mac-only programme, but towards the end I transitioned to Bookends. There’s no particular reason for the change, mainly that Bookends is a slightly sparer-design.\nDifferent journals and universities require different formatting of references and sources. Bookends (or whatever you choose) is an easy way to stay on top of these formatting issues.\nIt connects easily (via a shortcut) to Scrivener or many other word processing tools that are commonly used. If you have many references like me, you can colour code them to make it easy to see what’s what at a glance (see the image above for part of the database I used for my PhD).\nI don’t, however, use Bookends as a repository for PDFs and documents. You can do this, technically, but it’s not ideal. You’re far better off keeping your reference manage for what it does well, and then having a separate file system for your PDFs and other documents (like DevonThink, for example)."
  },
  {
    "objectID": "personal/2016-09-02-2016-8-phd-tools-rescuetime.html",
    "href": "personal/2016-09-02-2016-8-phd-tools-rescuetime.html",
    "title": "PhD Tools: RescueTime for Time Tracking",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nRescueTime is a passive activity tracker for what you do on your laptop (or Android phone – limitations in Apple’s iOS mean that it’s not possible to have the same detail in app usage tracking from iPhones and iPads). It sits in the background, watching where you spend your time. You can visit the website to see your stats, or it also sends you a weekly summary of what you did.\nI’ve experimented with various kinds of activity and time trackers in the past, and my experience is that if you have to actively turn it on and off when you start and stop what you’re doing, you’ll probably forget. Also, that method of time tracking isn’t particularly good at noting when you go down a hole of Youtube distraction that one time you have to search for something online. With RescueTime, you can be sure to be delivered an accurate summary of all the ways you are inefficient and wasting your time. (So much shame).\nSo it’s good for tracking the amount of time you’re writing (tasks are rated from very unproductive to very productive on a 5-point scale) and it’s good for tracking what sites you’re visiting during the day. This can be linked up to other sites, like Beeminder, to enforce some kind of time limit. RescueTime also has a version of site blocking where you can say, for example, if I spend over 1 hour on “very distracting time” or “watching videos”, block all my internet for the next 3 hours (or something like that). Or you can hook it up to Beeminder and say, as I did, if I’m not writing for 2 hours every day (as in, actually typing and adding words to the page (it knows when you’re staring at a page versus actually typing, by the way) ) then take my money.\nA lot of your PhD writeup and research will probably be digital-based, so RescueTime is ideal for keeping you honest as to exactly how much work you’re getting done. It’s easy to have a false sense of all the hours of work you’re supposedly doing.\nYou can keep a little window open somewhere on your screen that shows your real-time ‘productivity score’ (also compared with the previous day or week). That way, if you’re at all competitive, you’ll try to beat your own score and try to keep your score high. It may seem stupid, but these little tricks are unfortunately necessary in some cases, particularly when dealing with a long multi-year project. You don’t get any marks for having developed a useful workflow that allows you to get your work done, but still, PhDs are as much a test of your ability to carry out this kind of long-term research as they are a test of your specific research skills and argumentative/analytic capability.\nOne other thing I used from the RescueTime features: internet autoblock first thing in the day. This was before I discovered Freedom App (more later on this), mind, but it was a good substitute. I found that if I somehow managed to hold off from using the internet in the mornings, then my work day would be measurably better (better meaning I actually wrote things and got engaged in the tasks at hand instead of falling down some rabbit hole of distraction, or responding to some “urgent” email). So I set up RescueTime to turn off the internet for 2 hours once my computer had been on and active for 1 minute each day. That way, by the time my laptop had started up and I was ready to do things, the internet was already off. I’ll talk more about how it’s useful to turn off the internet in a separate whole post on “Deep Work” in a few days. Another good setting: allocating 30 minutes or 1 hour to “very distracting” sites per day. That way you have some leeway to waste time, but not enough that it’s going to markedly ruin your ability to work that day.\nRescueTime is free for most of the features I described above. Anyone working as a writer/academic etc of some kind ought probably to have it installed, I think, if only to be more aware of how they’re spending their time. Go try it out! It’s free so you have no excuse!"
  },
  {
    "objectID": "personal/2016-09-04-learning-icelandic.html",
    "href": "personal/2016-09-04-learning-icelandic.html",
    "title": "Svifnökkvinn minn er fullur af álum: Lessons in Icelandic",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“1134”] Some pronunciation rules for vowels [/caption]\nYesterday, mostly out of curiosity, I took my first lesson in Icelandic. I’m not anticipating this becoming a major driving force in my life, but I was provoked by someone’s explanation of the language’s pronunciation weirdnesses to learn more.\nThe alphabet is the same as English (mostly), though there are some new combinations of letters as well as some tricksters that look like you know what they should sound like but that come out as something completely different. Some letters change sounds depending on where they are in a sentence.\nThe combination of two Ls comes out as a sort of clicking sound. When the two Ls are derived from a foreign word, then it sounds like it might in English. But for everything else – and I’m quoting from my excellent teacher, Thor the Tutor over at iTalki – you want a “wet sound in which the sound of air escapes from underneath and from both sides of the tongue when it’s low in the mouth”. It’s a bit like the combination “TL” in English, except not really.\nI also learnt that inhabitants of Iceland have not developed the skill of hearing foreigners speaking their language with bad pronunciation. So while in France or Germany perhaps local / mother-tongue speakers will be able to figure out what you’re saying even if you are butchering the pronunciation, that isn’t the case for Iceland. Luckily, that sets the bar high for pronunciation, which I’ve always held is important to get right from an early stage.\nI’d read a little online about the complexities of learning Icelandic:\n\n“The difficulty of different languages manifests at different stages,” Jóhanna says. In Icelandic’s case, taking that first crack at the grammar is daunting. In Icelandic, verbs are conjugated variously for tense, mood, person, number and voice—active, passive or middle. Heavy inflection generates a staggering list of possible ways to say, in one well-known example, the numbers one through four. And although the Icelandic vocabulary has far fewer lexemes than that of a language like English, a single Icelandic word can have a phenomenal range of meanings depending on the particles with which it is used. Consider “halda,” literally “to keep,” which can become “halda fram” for “claim/maintain,” “halda upp á” for “celebrate,” “halda uppi” for “support” and so on.\n\nA textbook I found online had the following caveat to those who sought to embark on a programme of study:\n\nNone of this bothers me too much. My teacher said something similar, that the faster we get speaking without necessarily allowing ourselves to fall into the abyss of inflection and grammar then the faster we can progress to the level that it makes sense to add in all the complexity.\nBy the end of the lesson I was sounding out commonly-used phrases and the trick vowels were catching me out less and less. Now, my task is to learn those phrases and internalise the pronunciation rules through practice. Then I’ll be ready for my next lesson.\nOh, and the meaning of the blog title? My hovercraft is full of eels."
  },
  {
    "objectID": "personal/2016-09-06-phd-tools-backup.html",
    "href": "personal/2016-09-06-phd-tools-backup.html",
    "title": "PhD Tools: Backup Systems for Staving off Sadness",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nHaving some kind of backup system is essential for all PhD students (and probably anyone else using a computer for writing of one kind or another). The less friction to your backup system, the better. If you have to plug in a USB or Firewire external hard drive in order to start your backup process, you’re probably not going to be doing it enough and you’re probably going to lose files and data.\nI’ve learnt the hard way how hard drives can fail. A few years ago, I lost roughly a decade’s worth of digital photos when my backup system failed. My work files were ok – because I’d taken steps to check that this was working - but for whatever reason I hadn’t taken the same care for my non-work files. Cue sadness.\nI use multiple types of backup. Ideally, you’ll also use at least two. One should be a regular backup to a hard drive – something like Apple’s Time Machine in conjunction with an external disk – and the other should be a cloud backup.\nI use Backblaze and Spideroak for my cloud backups. You may find it overkill to have two separate systems for storing my backups in the cloud, but space and the services are cheap enough that it’s possible. In fact, if I was living somewhere with faster internet I’d probably add in AWS Glacier as an additional backup service.\nI also use SuperDuper to make a clone copy of my hard drive. I’ve been burnt by Apple’s Time Machine backup in the past (see above) so I don’t use it any more because I lost my trust. But I heard it’s better now. Caveat emptor.\nProgrammes like Scrivener (see earlier blogpost) have built-in auto-backups. Use them, and test them to make sure it’s doing what it says it’s doing. You don’t want to have to find this out after something’s gone wrong.\nIn fact, I encourage you to make a recurring calendar appointment with yourself to stress-test your backup systems once every two or three months. Different scenarios to try out: your hard drive fails; try to get hold of your main PhD working draft from your backup system. Or, another good one, your laptop gets stolen; are you able to access all your files regardless, and eventually (once you replace your computer) restore your system as it was before the theft? Actually do these tests! I’ve often found that a system that I thought was working properly turns out to be failing in some small but essential way.\nTowards the end of the writeup, your paranoia around file failure is likely to be sufficiently intense as to inspire all sorts of manual backup routines. Earlier this year while I was nearing that point myself, I would email myself zipped copies of the scrivener file as well as store copies on Evernote and Google Drive and Dropbox. This, note, in addition to the other backups I had going.\nA lot of this is common sense. Backups are important. We all know it. But it’s good to have a system that you know and can be confident works. Don’t tarry! Take steps to set something up today, even if it’s just a background cloud backup service like Backblaze."
  },
  {
    "objectID": "personal/2016-09-08-spaced-repetition-foundation.html",
    "href": "personal/2016-09-08-spaced-repetition-foundation.html",
    "title": "Starting the Spaced Repetition Foundation",
    "section": "",
    "text": "When I first learned about the idea of spaced repetition, I felt like I’d discovered a magic trick or a secret of some kind. Here was a piece of research that could transform how quickly I learned new words (I was studying Pashto at the time) and how well I recalled them. On the other side of that elation, I felt a simultaneous frustration and sense of disappointment that this piece of pedagogy / educational science hadn’t come up before in my schooling career. All those vocab cards I’d created for my BA degree in Arabic and Farsi! So much inefficiency! Not to mention all the things I had studied at primary and secondary school. Most of that specific knowledge remains but it is like the shadow of its original form. I’ve recently started studying Maths again and I’m painfully aware just how much I’ve forgotten.\nSpaced repetition is a principle that advocates a specific series of intervals between reviews of a particular fact or ‘thing’ that you want to memorise. A German scholar called Herman Ebbinghaus discovered that there is a pattern to how most humans learn and forget materials, and he outlined a specific algorithm to ensure optimum recall and retention of whatever you are trying to learn. For that, we must be thankful for the (seemingly) boring experiments he had to go through to get to that piece of insight.\nAnki is a piece of software that manages your study and recall of individual pieces of information. The spaced repetition algorithm or insight is at the core of how Anki works and helps you. I’ve used Anki to learn languages, to memorise smaller facts and useful pieces of information, to cement my understanding of broader concepts, among other things. It’s pretty adaptable to whatever your specific learning needs are. This is why I’ve stuck with Anki over its competitors. (I’ve tried pretty much all of them, and continue to do so, but Anki remains the most customisable and useful IMHO).\nOver the years, I’ve had the privilege to share Anki with others along my travels. Sometimes this has come in the context of language learning, other times like with the friend who wanted to learn all the names of all the bones in the human body.\nIn fact, Anki was the original bridge or glue that started my friendship with Matt Trevithick, Sources and Methods co-host and all-round busy person. I walked into his office in Kabul – he was working at the American University of Afghanistan at the time – and spied a huge stack of flashcards in a box in the corner. I pointed to them and he explained with great pride how he’d learnt them all. We talked through his process a bit and soon enough I was singing the praises of Anki and how much time he’d save and…and…\nWe’ve had many conversations over the years relating to language-learning, the different ways Anki can be used and our overwhelming and continuing sense of disbelief that the principles of spaced repetition aren’t more widely adopted in the field of education. People who get deep into self-study eventually stumble on SRS, since you often try to find ways to get faster or better in the absence of a teacher telling you what to do. The Quantified Self movement has been instrumental in spreading the gospel of SRS, too, in their special sessions devoted to the topic in their yearly conference, or through the pride-of-place given to show-and-tell explanations of how SRS has allowed someone to learn Chinese, or hieroglyphics or so on. (Click here for the QS list of talks referencing SRS. I’d highly recommend dipping your toe into some of these talks. Most are extremely engaging.)\nBringing matters up to the present day, I have a bit more time and bandwidth for focus now that the PhD is finished and Matt and I had been talking about this sense that more people should be using SRS for a few years, so we have finally bitten the bullet and started something to try to make that happen.\nThe Spaced Repetition Foundation is our institutional first step to start to spread the lessons to a broader audience and to more varied contexts. Our mission statement reads:\n\n“The Spaced Repetition Foundation, an independent, not-for-profit center, is dedicated to advancing the adoption of spaced repetition as a supplementary learning tool.\n\n\n“Despite the scientifically proven ability of spaced repetition to vastly increase the retention of information in the brain as a supplemental learning tool, the use of this technique is, at present, largely restricted to select individuals and students for personal enhancement or academic achievement. To date, no single center exists to advocate for the increased adoption of this approach in educational settings at the macro level, to increase public awareness of this useful tool, and to serve as a focal point for interested individuals to come together and work on these issues.\n\n\n“Attempting to solve this problem, Natalie, Alex and Matt, long-time users of spaced-repetition applications, decided in the summer of 2016 to start the Spaced Repetition Foundation, which seeks to function as an independent advocate dedicated to increasing the adoption of spaced-repetition technology wherever learning is happening.”\n\nNote that we are not completely and exclusively technology-centric in our approach. At least part of the work involves outreach, involves testing use cases and scenarios, and if it really is to find a place in the broader world (away from electricity and flashy phones, perhaps) it must be adaptable to low-tech or no-tech approaches.\nGoing forward, we’ll be spreading the word about the science behind spaced repetition (spoiler: it works), ways everyone can use some form of review-recall testing in their lives, and generally galvanising different communities to find ways to rethink their approaches to knowledge acquisition and long-term recall through spaced repetition. We have some specific first steps planned, but I’ll hold off with those details until they are more pertinent.\nI’m extremely glad, too, that we will get to work with Natalie McKnight, the other co-founder, who brings a wealth of institutional and practical educational experience beyond simply the solitary learner that Matt and I represent. Natalie is Professor of Humanities and Dean of the College of General Studies at Boston University. I’m excited to work together with her to expand the kinds of communities and settings we can reach.\nIf you have any interest in the work we’re doing, please sign up for our newsletter here or if you are interested in learning more, please do get in touch. Full details are available on our website at http://www.spacedrepfoundation.org."
  },
  {
    "objectID": "personal/2016-09-12-2016-9-phd-tools-vitamin-r-and-the-pomodoro-technique-for-getting-going.html",
    "href": "personal/2016-09-12-2016-9-phd-tools-vitamin-r-and-the-pomodoro-technique-for-getting-going.html",
    "title": "PhD Tools: Vitamin-R and the Pomodoro Technique for Getting Going",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nIn my last post I mentioned the way I divide my work into timed segments. The ideal timing for me, I felt, was 45 mins on : 15 minutes off. The canonical division, however, is 25 minutes on : 5 minutes off. This is a technique commonly referred to as the Pomodoro Technique (named after a tomato-shaped kitchen timer, I think). You might find that starting off at 45:15 is too much at the beginning, particularly if you’re not used to focused stretches of work, and that you have to slowly work your way up to that ratio, increasing the minutes incrementally.\nI like the idea of splitting work into timed units as an alternative to the usual task-based approach. This way, you make sure to take regular breaks, and you develop a healthy appreciation for the fact that some tasks take longer than you were expecting. I used to be someone who would claim to work from 8am-6pm on a particular project. I now realise that that is an illusion. Nobody can concentrate for that long, and the work you’ll be producing by the end of that session will most likely be worthless. Far better to have focused core sessions and then be honest about where you’re spending your time. Working 8am-6pm day-in-day-out is also a surefire way to burn out from what you’re doing.\nAnother advantage to pomodoros is that they are small enough to appear unthreatening to your emotional lizard brain. Confronted with two options (either working for 25 minutes on a particular problem, or an unboxed task instruction to ‘complete this particular task’) I know I feel far more comfortable taking a bash at starting to work if I just have to get through 25 minutes. If I place the entire responsibility and expectation of completing a section or a problem from the outset, I’m far more likely to find ways to avoid starting, to procrastinate (even if everything is switched off and I have no access to the internet; it’s amazing how creative the mind can be at avoiding hunkering down and tackling a difficult task).\nThere are many (many) pomodoro timers available online. FWIW, the ones that I’ve used and found work well for me are: FocusTime, PomoDone (which hooks into Trello boards).\nAround the time when I started my routine of ‘Four Perfect Hours’ each day, I discovered something called Vitamin-R. This is probably overkill for many of you, but if you’re inclined to monitor your data and your stats and your progress, then it might be worth exploring.\nThe programme works on your laptop and your phone (though I almost exclusively used the Mac app) and you set up your time ratios (i.e. my 45 mins on, 15 minutes rest routine). You specify what you’ll be doing during the coming 45 minutes. This is useful in forcing you to clarify what you will be doing, since being specific about this makes it likely that you’ll make progress instead of just browsing about a bit in your sources and so on. It gives you alerts and alarms at the start and end of your pomodoros, as well as periodic ‘tick-tock’ noises at random moments to just remind you that this is a period of focused. Some people might find this annoying; I found it useful to occasionally break me out of a daydream or from going down some not-particularly-useful line of approach.\nAt the end of each session, it asks you how focused you felt while working. This is really useful for building up (over time) a picture of which times of the day are more useful than others in terms of your focus.\n[caption id=“” align=“alignnone” width=“1450”] One of the charts that Vitamin-R generates [/caption]\nYou can see that my early mornings were generally my core work time. You will usually have an instinctual understanding of this truth, but Vitamin-R allows you to confirm it and to keep track of just how many hours you’re spending in ‘Deep Work’.\nI happened to have a Beeminder goal for ‘Deep Work’ at the time, and I filled it with data from Vitamin-R. At the end of every day, I’d update it with however many minutes Vitamin-R said I’d tracked as having been devoted to that deep work. That kept me honest, and it was also nice to see the cumulative core hours add up over time.\n[caption id=“” align=“alignnone” width=“1348”] Here you can see the 187 or so hours I tracked in the first half of 2016 [/caption]\nMost won’t need or want this level of specificity or tracking. Any phone (even a dumb phone’) comes with a countdown timer, and that’s enough to get started with the pomodoro technique. I recommend it because it encourages regular breaks. If you find this useful, please do let me know. It’s always good to hear from others in the ‘trenches’ of knowledge work."
  },
  {
    "objectID": "personal/2016-09-13-phdtools-pen-paper.html",
    "href": "personal/2016-09-13-phdtools-pen-paper.html",
    "title": "PhD Tools: Pen and Paper",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nIt’s worth also talking in general terms about pen and paper. Readers of this blog would be right in considering me as someone who uses many different digital tools. Yet I am also a firm advocate for the use of paper and pen.\nI’ve written before about my use of a four-color pen. This was one of the more useful discoveries of 2015.\nUsing pen and paper offers the opportunity for slowing down and thinking in different ways about particular problems. Needless to say, pen and paper as a tool is firmly ‘distraction-free’, perhaps unless you’re someone who likes to doodle.\nI like working on problems from different perspectives throughout my attempts to tackle whatever complexities arise. For this reason, I’ll spend some time outlining, some time free-writing, some time structuring and restructuring things I’ve already written, some time talking things through with a third-party, and some time making mindmaps or lists of ideas with pen and paper.\n[caption id=“” align=“alignnone” width=“1280”] The full handwritten overview of all my PhD chapters, glued to a large white sheet of paper [/caption]\nThis cycling through different ways of composition / thinking on paper is something I developed over time, and it was in part a product of my time in Kandahar. Electricity was in limited supply, as was the internet, and some days there would simply be no way to write on a laptop. Sometimes even the laptop wouldn’t start because the temperature in our little room on the roof was too hot. So I developed things to do during those downtimes, so that I wasn’t completely hampered from working. The interruptions and lack of power was such a prominent feature of life that to allow yourself to be dictated by that would be to never complete anything.\nSo I would read books or articles on my Kindle. I would make lists in my notebooks. I would make lists of things to look up when the internet or electricity came back. I would make lists of tasks. I would outline sections of whatever I was writing. I would have focused discussions with Felix about a particular section or issue. Pen and Paper was at the centre of all of this, and I took that on to my life when I returned to places with constant streams of electricity and internet connectivity.\nI’ve actually found that I’m the most useful and productive (in a holistic sense) when I’m in that disconnected mode, without the reliance on the internet to look everything up, and forced to just forge ahead with the hard work of thinking.\nA particular model for this was the work of Erich Auerbach and his book Mimesis: The Representation of Reality in Western Literature, which he wrote from Istanbul during the Second World War without access to many sources. As Edward Said explains in his Introduction:\n\n“He explains in the concluding chapter of Mimesis that, even had he wanted to, he could not have made use of the available scholarly resources, first of all because he was in wartime Istanbul when the book was written and no Western research libraries were accessible for him to consult, second because had he been able to use references from the extremely voluminous secondary literature, the material would have swamped him and he would never have written the book. Thus along with the primary texts that he had with him, Auerbach relied mainly on memory and what seems like an infallible interpretive skill for elucidating relationships between books and the world they belonged to.”\n\nMy hunch is that the limitations on his work process, and access to sources, was one of the things that made that book so great.\nPen and paper don’t need batteries. So give it a try. Go somewhere new, or somewhere you feel like your energy gets recharged, take a notebook with you and make notes. You can always type them up later on, but for now, just write and think."
  },
  {
    "objectID": "personal/2016-09-15-phd-tools-goodreads.html",
    "href": "personal/2016-09-15-phd-tools-goodreads.html",
    "title": "PhD Tools: Goodreads for Cross-Pollination",
    "section": "",
    "text": "[This is part of a series on the tools I used to write my PhD. Check out the other parts here.]\n\nDuring the period I was working most intensely on my PhD writeup, I read over 100 books. I put that number out there not as a confrontation, but as an illustration that reading is important to ensure you don’t get lost in a small box of your own creation. Judging purely from my own experience and from sporadic conversations with a loose handful of fellow PhD candidates, this can be a real problem.\nReading widely and about issues and problems wholly unrelated to your field of study is, I believe, the hallmark of a curious mind. If I meet someone for the first time and I’m assessing their work, I’m far more likely to be interested in the last ten books they’ve read than many other data points. Even the fact that someone is taking time to read, and to read diversely, is an important indicator for me.\nI think I can date my adoption of this books-and-ideas-for-cross-fertilisation to when I read Steven Johnson’s book Where Good Ideas Come From. He makes a strong case for a more deliberate approach to how you develop and cultivate ideas in your thinking life. (The book is short and highly suggestive of specific approaches to work. I’d recommend it if this kind of thing interests you).\nI’ve found that things that I don’t track and monitor tend to fall beside the wayside. Hence Goodreads and Beeminder and a number of other tracking tools. Goodreads allows you to set how many books you want to read each year and then keeps a convenient little widget reminding your how far ahead or behind you are of your goal. If you want a bit more of a ‘sting’ for non-compliance, you can hook up Beeminder and you’ll be kept honest that way.\nReading books on unrelated topics was something I would do in the afternoons or evenings after my Four Perfect Hours. The time would be mine and I could read without any sense of guilt or that I wasn’t making progress on my PhD writeup. No, I’d done my work in the morning, so now I could read to my heart’s content.\nEncounters with books are encounters with other ideas, other minds. It refreshes your approach and your sense of perspective – both so important for your PhD. Give it a try! See how you can add in some reading time to your daily routine. Even 30 minutes before bed each evening adds up in the end."
  },
  {
    "objectID": "personal/2016-09-17-coding-chronicles-failure.html",
    "href": "personal/2016-09-17-coding-chronicles-failure.html",
    "title": "Coding Chronicles: Failure",
    "section": "",
    "text": "A true journal of my progress in learning data analysis skills must include notes about failure. Nothing is purely sunshine and achievement milestones, so this is a short write-up of a recent (ongoing) problem I’m having with my data analysis / coding nanodegree.\nAs I mentioned previously, I’ve been working my way through Udacity’s Intro to Programming nanodegree. The course is split up into stages, and I’m currently working on the final one. You get to choose an area of specialism, and I picked data analysis. The work you do as this final stage actually counts towards the subsequent full-blown data analysis nanodegree.\nI managed to get through the first four (actually five, since there’s a “stage 0”) stages in a month or two. It went extremely quickly and while there were challenges, I never felt completely overwhelmed. This final stage, though, is sapping my very life spirit. I work on the lessons for an hour or two, six days a week. While previously I was getting through perhaps half a dozen lessons each day, now I’m lucky if I manage to get through a single one.\nThis past week, I’ve been stuck on a single lesson for the past three or four days. I’ve written queries on the Udacity forum, where more experienced coders and alumni of the nanodegrees offer support and advice. You usually get an answer within two or three hours, which is excellent. The only thing this time is that I’m finding I don’t understand the answers to my questions.\nThere seems to be a layer of misunderstanding or some fundamental skill or piece to the puzzle that I’m missing, that means I’m unable to progress through the course. The difficulty level seems to have jumped substantially from one course to the next.\nThe specifics of the problem are as follows. We’re using Jupyter Notebook to work through a series of problems and questions we have around a data set supplied in the form of several .csv files. At first, we did some formatting changes and converted the data into mutually recognisable formats. Then we’ve slowly been isolating parts of the data so as to answer specific questions. The data set itself relates to the Udacity course’s students – how long people stay enrolled, when people quit, that sort of thing – so for the current question/class I’m supposed to make a list of students who have completed at least one week of the course.\nDay one of my struggle: I just got stuck in. Tried extremely hard. Failed extremely hard. This was more of a brute force approach. It had worked on some previous occasions, but after two hours I was still nowhere. I wasn’t entirely sure I even understood the question.\nDay two: I wrote up a question on the forum and decided to go back to basics. I had noted that there seemed to be some layer, some skill or fundamental principle that I wasn’t understanding. Given that the class required a good grasp of lists and dictionaries (both Python programming language concepts), I thought I’d go outside the Udacity course and take a few lessons elsewhere. Of course, we’d covered lists and dictionaries in previous lessons, but apparently not enough for me to be confident in their use. I spent a few hours working my way through various courses and their practice examples: CodeAcademy has a useful series of lessons, as does CodeSchool; chapter 5 of “Automate the Boring Stuff” also has a useful section on lists and dictionaries. So I’d worked my way through all this and I felt like I was in a much better place when it came to the fundamentals.\nBut on day 3: more failure and a lack of comprehension. My frustration was compounded when the response to my query on the Udacity forum made me even more confused than when I started. I feel like I have a somewhat solid idea about what I have to do to solve the problem, but all my implementations of code don’t take me where I expected, and nothing has brought me to the answer to the test problem.\nSo that’s where I am right now. Stagnation and failure. I’m booked in to chat with one of the Udacity team on a Skype call tomorrow, and hopefully that will set me straight. I’m pretty sure that all of this is something that time and patience will solve, or at least ameliorate, but for now it’s frustrating not to know which pieces of the puzzle I’m missing."
  },
  {
    "objectID": "personal/2016-09-24-turn-off-facebook.html",
    "href": "personal/2016-09-24-turn-off-facebook.html",
    "title": "Turn Off Facebook’s News Feed",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“640”] http://static1.squarespace.com/static/55e7927fe4b0a4a21bb6869b/t/5628e6b9e4b0126c0335864d/1445521081699/ [/caption]\nJudging by the stats generated by Google Analytics and Squarespace’s own tool, over 40% of those who read my blog come here from Facebook. I’m slightly astounded by that number. It also means if I left Facebook (as I’m sometimes wont to do), I’d presumably lose all of those Facebook readers since most don’t seem to use RSS readers or anything else similar.\nAll of this is just by way of preamble to point you towards a neat little tool called News Feed Eradicator. I’ve been using it for the past 5 or 6 months, and it has delivered many benefits. Its function is pretty simple: turn off the News Feed in Facebook. The only thing you’re left with is the bar on the top with the little red glowy number indicator (that you won’t be seeing as a red if you turned your screen to greyscale as I previously suggested) for all the millions of notifications Facebook seems to think are important.\nI only use Facebook as a sort of broadcasting platform since I do know that some people exclusively use it as a source of news and reading material. But if I have to log in to post something, I don’t want to be confronted by my News Feed and its temptations (or dissatisfactions), then I have this Chrome extension to switch everything off.\nSo I write this now to encourage you all to turn off your News Feeds with the extension, even though I realise this might mean you might not see the next time I post something. (There are, in any case, many other ways to follow my writings/blog: RSS, twitter and via email newsletter, for instance)."
  },
  {
    "objectID": "personal/2016-09-28-three-climbing-books.html",
    "href": "personal/2016-09-28-three-climbing-books.html",
    "title": "Sustainable Climbing: Three Book Reviews",
    "section": "",
    "text": "Today, three quick and dirty reviews. I read these books back-to-back. They reinforce each other, despite their authors being unconnected in any practical sense.\nThe first, ‘Espresso Lessons’ is a short version of Arno Ilgner’s classic ‘The Rock Warrior’s Way’:\n\n“Things you don’t think about when you’re climbing. Arno Ilgner has been working and writing about fear, technique and falling in climbing for years. This book is a distillation of some key insights he’s had. I imagine this is a book I’ll be returning to as I progress in the sport, but for a beginner there were some really useful parts. The key section for me was the one on falling, and on different ways of approaching this reality that confronts everyone, no matter what variety of climbing you’re doing.”\n\nI really liked how this book was more geared towards the mental work / challenge of climbing rather than purely the physical side (which many others have covered).\nNext, Alex Honnold’s ‘Alone on the Wall’. If you don’t know Honnold, you should check out this video:\nI never get tired of watching that. It’s an astounding feat. The book itself is a summary of the first thirty years of his life, mainly focusing on his big-ticket climbing activities:\n\n“Alex Honnold’s reputation precedes him. If you don’t know who he is, go visit youtube and have your mind blown. This book is obviously not going to do the same thing as his climbing experiences, and it’s written in a weird format. He gets a chance to explain himself on a bunch of things, returning continually to this question of risk and reward (that others often challenge him on). I could probably have done without the co-author’s contextualisation and extrapolations on top of what Alex himself wrote. Also, the publisher made a really weird choice (at least for the Kindle version) in having all of Alex’s sections in italic, which is a little annoying since his words are the core of the book and you have to read them all in italic throughout.\n“Those quibbles aside, Alex Honnold is clearly at the top of his game in the climbing world and he has some really interesting rationalisations and thoughts on risk and competence. He tends to downplay everything, so it’s hard to judge some statements since he’ll just say”that was ok” or “that was easy” when to others it was probably much more difficult etc.\n“He also has a really interesting and thought-provoking position on being ‘minimal’, as well as his commitment to donate 1/3 or so of his earnings to charity and a eco-foundation that he set up.\n“Fans of his climbing will find lots to enjoy in this book. Others, perhaps not so much; for you, check out Mountains of the Mind: A History of a Fascination or Into Thin Air: A Personal Account of the Mount Everest Disaster.”\n\nThe final book was Yvon Chouinard’s ‘Let My People Go Surfing: The Education of a Reluctant Businessman’. Chouinard is the founder of the Patagonia outdoor clothing and equipment company. This was a surprisingly thought-provoking read:\n\n“ An incredibly inspiring and thought-provoking account of the intersection of business practices, sustainable environmental activism and wilderness exploration.\n“I hadn’t heard of Yvon Chouinard, the founder of Patagonia, prior to reading this book, though I now see that he’s been recently profiled in the New Yorker and many other places over the years.\n“The first third of the book is an autobiographical account of how the company, Patagonia, was started along with some of the key inflection points along the way. Alongside this is a longer attempt to explain the principles and philosophies that guide how the company does its business. This includes everything from design principles (aiming for a quality minimal design over something that is perhaps cheaper but will last less long, for example) to their management philosophy and environmental activism work.\n“It is this latter part of the book that gives a lot of food for thought for anyone running a business. More and more is being written on the issues surrounding growth — and I’m probably revealing my ignorance of a huge literature going back decades, if not longer — and Patagonia and the practices and philosophies here offer a challenge of sorts to anyone doing business in the twenty-first century. I will be chewing on the things in this book for a long while.”\n\nIt has been days since I finished this book, yet I’m still mulling it over. That’s as good a testament to the power of ideas as any."
  },
  {
    "objectID": "personal/2016-10-05-taliban-reader-announcement.html",
    "href": "personal/2016-10-05-taliban-reader-announcement.html",
    "title": "Coming Soon: The Taliban Reader",
    "section": "",
    "text": "Hot off the press. Hurst Publishers (my publisher for My Life With the Taliban, An Enemy We Created and Poetry of the Taliban) just released their spring/summer catalogue. As always, some great titles in the works.\nFelix and I have our own humble contribution to scholarship on the Afghan Taliban: The Taliban Reader: War, Islam and Politics In Their Own Words. I’ll add a link here to somewhere online where you can pre-order, but the blurb runs as follows:\n\nWho are the Taliban? Are they a militant movement? Are they religious scholars? The fact that these and other questions are still raised is testimony to the way the movement has been studied, often at arm’s length and with scant use of primary sources.\nThe Taliban Reader forges a new path, bringing together an extensive range of largely unseen sources in a guide to the Afghan Islamist movement from a unique insider perspective. Ideal for students, journalists and scholars alike, this book is the result of an unprecedented, decade-long effort to encourage the emergence of participant-centred accounts of Afghan history.\nThis ground-breaking collection, ranging from news articles and opinion pieces to online publications and poems transcribed by hand in the field, sets the stage for a recalibration of how we understand and study the Afghan Taliban. It challenges researchers to forge new norms in the documentation of conflict and provides insight into the future trajectory of political Islamism in South Asia and the Middle East.\n\nI’m really excited to have this project (somewhat soon) available. As noted in the blurb, it’s the product of years of work, not to mention collaboration with various friends and colleagues.\nI’ll be writing more about the book closer to the release date (and once it’s actually finished and submitted to the publishers)."
  },
  {
    "objectID": "personal/2016-10-13-demise-afghanistannewscenter.html",
    "href": "personal/2016-10-13-demise-afghanistannewscenter.html",
    "title": "On the demise of afghanistannewscenter.com",
    "section": "",
    "text": "Earlier this year, without any fanfare, a truly great resource and archive relating to Afghanistan’s recent past went offline.\nThe AfghanistanNewsCenter wasn’t very well known, and it doesn’t seem to have done too much to publicise its activities. The founder – Fawad Ahmad Muslim – as far as I’ve been able to figure out, simply kept updating the site as more news came in. I subscribed to his Yahoo Groups newsletter since at least 2007. If I search through my database of reports published on Afghanistan, many (many) of them include links to stories made available through afghanistannewscenter.com so I’m sure it wasn’t just me who used it.\nAt its core, its mission was very simple: gather together all the English-language reporting on Afghanistan (from wire services, magazines and also sometimes press releases) and store it in a database. This database was browseable by date, and the archive went back to 1992. With a bit of Google Kung Fu, you could even perform text searches on the full archive, getting year-by-year outputs of articles about a particular person or place.\nAs a researcher without the backing of a large institution, affording me access to resources like LexisNexis and the like, AfghanistanNewsCenter was an essential and valuable part of my resource quiver. I think I must have used it to research pretty much every project that I’ve produced since first arriving in Afghanistan in 2004, from the bigger books to research papers and my own newspaper articles.\nA few months ago, I noticed that the site went down. I’ve been unable to contact the owner, even though there is a sort of ghost remnant (presumably automated) version on Facebook, which posts recent wire copy articles. Needless to say, Facebook being Facebook, there’s no way to do anything useful with the articles on that page, and the more useful google-driven searches by year and by keyword are impossible. I am pretty sure that most of the archive wasn’t even stored on Facebook in this way.\nSo this is just a paean to a resource that’s now lost to us. All the articles still exist, thankfully, distributed across a thousand different news outlets and services, and you can still collate all these together, but the ability to search and index these articles is now lost to us. Its sudden and unexpected demise should, moreover, offer a lesson to others who hope to offer such services in the future.Please, PLEASE, find a way to ensure that these useful services are managed so as to be sustainable over the long-term. Even if this means finding an institutional home for the data, it’ll be worth it over the long-term."
  },
  {
    "objectID": "personal/2016-10-15-skills-development-foundations.html",
    "href": "personal/2016-10-15-skills-development-foundations.html",
    "title": "Skills Development: Foundations",
    "section": "",
    "text": "I watched this video a few months ago, but thought it worth returning to since it covers a lot of really useful ground for anyone who has to learn new skills / develop etc. Which is to say, it’s relevant to everyone on planet Earth.\nOr just read these notes to get an overview of some useful things she talks about. (If you want to read a transcript, go here.)\n“Allison Kaptur: Effective Learning for Programmers” — Notes from YouTube\nWe learn early on that Kaptur’s job at the Recurse Center was to help and support students grow and learn amidst the freedom that the programme there afforded them. This kind of unstructured environment can come as a shock if you’ve only been able to work in structured settings in the past (schools, universities, big companies etc). The unstructured freedom to which Kaptur refers around the 1:30 mark also happens to be a hallmark that defines self-study work.\n“Growth Mindset”\n\nKaptur introduces the work of sociologist Carol Dweck and the distinction she’s drawn between fixed mindsets and growth mindsets.\n\nA fixed mindset “holds that intelligence is a trait that some people have in some fixed amount, and they can’t really affect how much of it they have.”\nA growth mindset “says that intelligence is something that you can work on and something you can develop with effort.”\n\nWhether someone adopts a fixed or a growth mindset then can determine how they view various other aspects of work. With regards to ‘effort’, people who believe in fixed mindsets hold that “if you are good at something, then it should be easy”. (And, conversely, if you’re bad at something, then it should be hard.) People with a growth mindset believe that you need to work hard at something to become better at it.\nKaptur mentions how Dweck’s work has also shown that people who are praised for their effort in the task being performed tend to get better results [slight simplification of what she said] than those who are praised for what they achieved. This is a fairly well-known and well-publicised aspect of Dweck’s work.\nKaptur notes that having a fixed vs growth mindset is something which (it seems) can be changed. And the switch from fixed to growth mindset can sometimes happen with deceptively easy tactics.\n\nSometimes it’s as simple as being aware of the things you’re saying (e.g. “Oh, I could never learn physics”). Kaptur suggests when you say “I am…” or “Some people are just…”, these might be times to examine whether you’re stuck in a fixed mindset pattern.\n\nFour strategies to change a fixed mindset:\n\n\n“Reframe praise and success” — if someone praises you for something you said by saying “you’re so smart”, you can mentally (most of the time you will say this to yourself internally) reframe this as “yes, I did a great job on that project. I worked very hard and I used an effective strategy.”\n\n\n“Reframe failure” — this is basically the opposite of the first strategy. Listen to your self-talk when you fail at something. If you’re saying “I failed because I’m bad” or “maybe I’m not cut out for this kind of work”, then try reframing it by asking yourself what you learnt from this attempt and what strategies you could change or use next time you try something similar.\n\n\n“Celebrate challenges” — if you can find ways to frame places where you struggle as a victory or an accomplishment in and of itself, this will really help drive you into that growth mindset. Accordingly, when the going gets tough, celebrate the difficult as an opportunity for growth, development and learning.\n\n\n“Ask about processes” — asking “how did you do that” can often be really illuminating, and is better (when viewing someone else’s work, for example) than saying “of course they did x or y; they are a genius/wizard”.\n\n\nOn Confidence & Imposter Syndrome\n\nDweck’s research shows that confidence doesn’t help you respond to challenges. A lot of advice counsels feeling more confident in response to difficulties in work etc, but the angle Dweck is explores is the idea that “if you’re doing something new, confidence about something old doesn’t help you with that.”\nIf you hold a fixed-mindset, any moment is basically a chance to prove whether or not you are a failure. “So running into challenges is particularly stressful in that context.”\nThe trick to getting past all of this (of course) is to adopt a growth mindset. (20:10)\n\n\nStrategies\nDweck’s research also shows that those who really embody a ‘growth mindset’ are also focused on strategies (and not just outcomes).\n\n“Make It Stick” — Kaptur offers some useful tips that she gathered from this great book. You can read my review here, in which I also extract some of my favourite actionable points.\n\n“Learning is an acquired skill” — the premise of the book\n\n“Effortful retrieval &gt; rereading” — this can be something like self-tests administered through Anki, or it can just be writing a review of a book after you’ve read it. Or it could be trying to summarise a recently-mastered topic by teaching it to someone else.\n\n\n“Spaced practice &gt; massed practice” — Kaptur references three main ways to space out practice — spaced / varied / and interleaved.\n\n\nSpaced = spacing practice sessions out over time rather than bunching it all together in a single mega-session\nVaried = find a way to vary the kinds of practice you’re getting so that you’re not getting falsely sure of your command of the topic.\nInterleaved = shuffling the kinds of exercise / practice you’re doing so that it’s somewhat random is better than always sticking to the same order (or a predictable order).\n\n\n“Difficulty is (usually) desirable” — (with the related point that making errors is usually desirable). One difficulty that isn’t desirable, however, is anxiety around performance. This comes out of Dweck’s research.\n\n\n\nThis is all difficult — a consequence of the fact that all of these strategies are difficult is that people don’t do it. They don’t challenge their recall, they don’t push into the areas they don’t know and so on, even after they’ve been specifically instructed in the ways that these strategies are more effective.\nKaptur encourages us to find ways to make effortful retrieval part of our everyday lives and work. This may mean you have to:\n\nuse a flashcard programme to test you\ntake guesses\nbe systematic about how you attack your problems. She is speaking in the context of programming, so she talks about debugging but it works for most problems. Have a hypothesis about what’s going wrong, and then tackle each part systematically.\n\nShe also suggests we find ways to implement spaced practice. The harder you work to retrieve a fact from your memory, the better this is for your grasp of that fact, so in the end while it feels horrible to test yourself on recall of materials you don’t know so well, it’s actually better for you.\nWith a growth mindset, errors are something to be welcomed (because they imply that there’s some sort of a feedback loop going on, from which you can, in turn, learn). Thus finding ways to get more feedback (about your writing, your code, etc) is to be encouraged."
  },
  {
    "objectID": "personal/2016-10-17-future-language-learning.html",
    "href": "personal/2016-10-17-future-language-learning.html",
    "title": "Bots: Part of the Future of Language Learning?",
    "section": "",
    "text": "Duolingo just released a new feature on their iOS app called ‘Bots’. (In case you don’t know it already, Duolingo is a really useful free resource for learning a bunch of different languages in a systematic and structured fashion). Their new Bots feature only works for Spanish, French and German languages/courses at the moment.\nTheir press release included a sample picture (the first one on the left) suggesting the kind of conversation you might have with their bot.\nI tried the app out this afternoon, using the French version as I don’t speak Spanish (yet). You can see (the beginning of) my conversation in the second image.\n \nWhat I hadn’t realised (and the press release and the deluge of news articles that recycled said press release didn’t help with this) was that the app functions on rails. Which is to say, you can’t actually have a free-flowing conversation with the bot. You can’t even say “I am sad” in the universe of Duolingo’s bots. As you can see from the picture above, triste is “not an accepted word”.\nClearly this is early days for the technology. It’ll get better, and they’ll offer it for the rest of their languages and it’ll all be wonderful. (Maybe).\nI’ll be curious to see how people end up interacting with these bots. There have been some great stories / podcasts on how this kind of thing turns out, like the Radiolab 3-parter or the reporting on Microsoft’s ‘Tay’ bot.\nAnd if chatbots increase, so will ways to speak to computers via voice recognition. Different ways of practicing language are to be welcomed, but, as my brief time with the Duolingo bots suggests, it’s early early days…"
  },
  {
    "objectID": "personal/2016-10-21-letting-go.html",
    "href": "personal/2016-10-21-letting-go.html",
    "title": "Letting Go",
    "section": "",
    "text": "Over the past week, I have felt the progress I’ve been making on the climbing wall is in jeopardy somehow. My energy levels have fluctuated quite a bit. I can never be sure how much I’ll be able to climb when I get to the wall, and old fears have started to raise their heads again.\nI thought I was over my fear of falling. But it has returned in a new guise. Part of the improvement in skill and strength that I’ve made has allowed me to try new walls and new routes. Some of these are on an inclined wall. The wall leans over you rather than just going straight up, which places more strain on your arms.\nIt also, I have discovered, is somehow a trigger straight to the core of my fear of falling. Gravity pulls straight down, so when you let go on an inclined wall (either in a fall or while taking a rest break) you’re pulled away from the wall into free space behind you. There’s nothing different about the experience. Either way, the rope, your harness and the knots you tied will hold you. But the experience is visceral. I’ll be fine all the way up the wall, but when I let go and pull out into open space behind me, I feel like falling is inevitable.\nIt’s a hard feeling to describe, and there must be something (at least on some level) hard-wired about it. The feeling comes about so instantly and with such force.\nToday I did some exercises to try to become more comfortable with the feeling. I did some intentional falls (basically just leaping backwards and trying to stay mindful of the experience of having the rope catch me). I climbed up the inclined wall, every so often turning round and allowing myself to rest on the rope while I turned my body backwards, looking out into the main hall of the centre.\nI won’t claim to have had any breakthroughs. If anything, I feel like I’m back where I started. But I’ll keep going back, and I’ll keep climbing those walls. After all, feelings are just that: feelings. I no more have to follow the beat of their drum than I have to buy a can of Coca Cola every time I see their advertisement on a wall. But I do have to acknowledge they exist, and, maybe, to get comfortable with their taking up space in my head from time to time.\nEvery day is different. Progress isn’t necessarily linear. The things I thought I’d dealt with come back to confront me. But as long as the process — doing the climbing — is still of interest to me, then this isn’t a problem. I just have to keep showing up, keep climbing those walls."
  },
  {
    "objectID": "personal/2016-10-27-kael-weston-podcast.html",
    "href": "personal/2016-10-27-kael-weston-podcast.html",
    "title": "Kael Weston on Sources and Methods",
    "section": "",
    "text": "Matt and I put out a new episode of Sources and Methods podcast today. We spoke to Kael Weston, discussing his time spent living in Fallujah, the importance of speaking the language of the place in which you work, as well as the political systems countries like the USA employ in far-off places like Iraq and Afghanistan. He also recently wrote a book, The Mirror Test, which is worth reading. You can find the episode over on iTunes or listen directly on the Sources and Methods website."
  },
  {
    "objectID": "personal/2016-10-30-scratch-below-surface-dhh.html",
    "href": "personal/2016-10-30-scratch-below-surface-dhh.html",
    "title": "Scratching Below the Surface with David Heinemeier Hansson",
    "section": "",
    "text": "I’m part-way through listening to a podcast interview with David “DHH” Heinemeier Hansson, creator of Ruby on Rails, author of Rework and Remote and co-founder of Basecamp. I wanted to write down some brief notes on what came up so far before I forget. I may return to this thread after I’ve finished listening to the rest. These are just some of the points that stood out for me from the conversation; I’d strongly recommend you give it a listen yourself, especially if you’re interested in coding, systems thinking, improving and learning new skills and so on.\n\nCuriosity — this is a skill worth cultivating. There are various metaphors often used for curiosity, but in the podcast DHH talks about the need to ‘dig / scratch below the surface’. He talks about how finding the thing in the topic / skill / domain that interests him is often a case of just keeping burrowing down until you find it.\nLearn What You Need — DHH talks about learning to code as a way of solving a particular problem, and about learning only just enough rather than diving into that learning as a pure pursuit needing you to learn everything.\nSystems Improvement — this isn’t explored that deeply in the part I listened, but it was a good reminder that there is a way of thinking about improvement that sees improving the system as the way to improve the skill.\nTime / Flow — DHH talks about driving racing cars and how time really flies when he does this. This is an observation that this, for him, is a skill he is enjoying learning, and one that gives him a great sense of that satisfaction that comes from doing a rewarding activity at just the right level of difficulty.\n\nMore on this to follow (maybe)."
  },
  {
    "objectID": "personal/2016-11-05-fuheis-climbing-2.html",
    "href": "personal/2016-11-05-fuheis-climbing-2.html",
    "title": "Climbing Fuheis: Two and a Half Ascents",
    "section": "",
    "text": "This is a video from a recent climbing trip to Fuheis. (Filming courtesy of Felix Kuehn). The weather is starting to turn cold so I feel like maybe this is the last time I’ll get to go climbing outdoors in Jordan until spring comes.\nI climbed three times yesterday. I don’t really have a strong sense of what the ratings were on the routes, but I think they were all in the 5s (i.e.5a–5c). You can see the ratings for the route at the wall here.\nAs I wrote a short while back, I’ve been dealing with things that I thought I’d sorted out weeks ago. My sense of trust of the rope and the comfort up at heights is still pretty good and I’m not too worried about how that is progressing, but yesterday I was unable to complete a route that I had ascended the last time I was there.\nI had energy to spare, so it wasn’t a question of not being able to summon the strength to get up; it was more a mental block. I was unable to trust in my feet, in the grip of the shoes on the wall. As one more experienced climber said to me after I had descended, “you need to feel more stable and confident in how you place your feet on the wall”.\nI can see the wisdom in what he’s saying, but I think it’s something that comes with time and experience rather than just a mental shift or a technical correction that you can make immediately.\nI was also reminded yesterday of the non-linear path that learning often takes, where progression in one area can often be followed by regression a short while later. Over the long run, you’re getting better, but it’s hard to retain that sense of overall perspective.\nLast time I went to Fuheis, I had a really good two or three weeks afterwards, spurred by my sense of regret and failure that I experienced while climbing outdoors. I’m more willing to seize the opportunity, to go the extra mile (or inch), so I’m looking forward to the coming few weeks to see what I get up to. I’m feeling confident in where I am and in my general strength and skill progression."
  },
  {
    "objectID": "personal/2016-11-11-squarespace-goes-ssl.html",
    "href": "personal/2016-11-11-squarespace-goes-ssl.html",
    "title": "Squarespace Goes SSL",
    "section": "",
    "text": "This news is a few days old, but it’s worth restating. Squarespace is the name of the comapny that hosts this blog and most of the other websites that I currently manage. A few days ago, they announced that they’d be offering SSL for all Squarespace sites:\n\nSecure Sockets Layer, or SSL, is a technology that secures the connection between your browser and the website you’re visiting. It allows you and your website visitors to feel confident that their information is secure. And we believe that confidence is an important part of your online identity. So, starting today, we’re proud to offer free SSL on all Squarespace websites. Website owners should not have to pay extra or wrestle with complex technical issues to offer this basic security to their users. Every website can enable SSL, which will automatically direct users and search engines to a secure version of that site. The result is that millions of more domains on the Internet will be secured via SSL, our customers can take advantage of the confidence that secure websites bring users, and we will have helped the Internet take a huge step forward in promoting security by default. As an added benefit, websites hosted on Squarespace may enjoy a boost in search rankings. Squarespace is taking care of almost everything, making this an easy transition for customers. To seamlessly manage SSL certificates for all of our websites, we’ve partnered with Let’s Encrypt, a free and open certificate authority (CA) run for the public’s benefit that provides free SSL certificates.\n\nThis is great news, and they deserve some credit for rolling this out. Google is making a push to encourage most websites to be secured in this way – not doing so may even harm your search rankings – so it’s probably a trend that’s coming for most people eventually. In any case, if you’re using Squarespace as your host, make sure you turn it on!"
  },
  {
    "objectID": "personal/2016-11-14-kukicha-twig-tea.html",
    "href": "personal/2016-11-14-kukicha-twig-tea.html",
    "title": "Kukicha or Twig Tea",
    "section": "",
    "text": "This week I discovered a new type of tea, and I’ve been enjoying it so much that I thought I’d write about it here. Kukicha tea, also known as ‘twig tea’ and bōcha, is a Japanese import. It is a cross between black and green tea, though more similar to the latter than the former.\nIts advantage is the low amount of caffeine per cup. This is mainly because the tea seems to be a mixture of scraps, offcuts and stems that weren’t used in ‘real tea’ cultivation. The back of the box I bought states the average caffeine content per cup as follows:\n—&gt; Filter coffee: 120mg\n—&gt; Black tea: 60mg\n—&gt; Kukicha tea: &lt;10mg\nGiven my sensitivity to green tea, this is a great alternative that gives some mental kick but doesn’t completely kick me over. I’ve since noted that Rishi Tea, my supplier of choice, also sells bags of kukicha, and I look forward to drinking more going forward."
  },
  {
    "objectID": "personal/2016-11-20-different-kinds-of-climbing.html",
    "href": "personal/2016-11-20-different-kinds-of-climbing.html",
    "title": "Different Kinds of Climbing",
    "section": "",
    "text": "This past week I went with my niece to a climbing hall (and trampolining centre) in Kuwait. You can see photos of the walls here and here. The whole experience was designed to encourage play and fun, naturally, rather than for some kind of skill-building. There weren’t any other adults climbing; just children with a throw-yourself-at-it mentality that was infectious.\nThe routes themselves weren’t particularly difficult, but it was harder trying to ascend without climbing shoes and hindered by the world’s most uncomfortable harness. The walls weren’t that high, and this seemed to reward a sprint-like style of climbing. Speed was more important than technique or form. This was climbing as challenge, as a way to enjoy scrambling up and leaping off the top, rather than anything else. (The centre uses auto-belay devices that stop you from falling all the way down to the bottom).\nThe experience — mine, and watching that of the children 10–20 years younger than me — reinforced the conclusion that has been steadily growing in my mind over time. Climbing is not really about strength; fear and the mental battles are the biggest things holding the beginner climbing back.\nI’ll have to still bash my head against the wall for a while longer before I fully absorb and believe this lesson, but the sooner I do, the sooner I’ll be able to advance onwards to the next level of challenge."
  },
  {
    "objectID": "personal/2016-11-23-inner-game-review.html",
    "href": "personal/2016-11-23-inner-game-review.html",
    "title": "The Inner Game: How a Perspective Shift Can Radically Improve Your Performance",
    "section": "",
    "text": "I’ve recently become interested in mental models, or different types of lenses you can bring to view a particular situation or reality. We all have an immense flexibility in how we can view the world; sometimes changing our approach in this way can change how we experience our reality.\nI found the process of realising and accepting this reality — that how we choose to respond to things changes how we experience them — incredibly empowering and liberating. It gives you options and choices in situations that may have seemed oppressive or challenging in the past. It opens you up to new approaches, new possibilities. I’ve written about my struggles with various parts of climbing, but one of the places I often return to is this idea that I can choose how I respond to that difficult. Similarly, I can reframe how I decide to view that reality: I can see it through the lens of fear, or I can view it through the lens of opportunity and growth. I can see it through my default hesitant-cum-reluctant perspective or I can try to imagine how someone else sees it (a friend, perhaps, who doesn’t struggle with the same mental blocks, or even someone much further along in the sport).\nI recently read a fantastic book that introduces some of these ideas as they relate to the sport of tennis. The book, The Inner Game of Tennis: The Classic Guide to the Mental Side of Peak Performance, was written by W. Timothy Gallwey and was originally published in 1972. Back then, it made quite a splash (selling over a million copies to date) and the book is rightly well-read as a classic of sports psychology literature. Gallwey has since published a number of spin-off books applying his principles to other domains (work, stress, music, golf, skiing) but you needn’t be a tennis player at all to get a lot from the original version.\nThe core idea that Gallwey introduces is that the mind has “two selves”, Self 1 and Self 2. I’m not entirely sure about the full extent to which these overlap with Kahneman’s System 1 and System 2, but there is some common ground.\nSelf 1 (also known as the ‘teller’) is the voice in your head which tries to actively manage how you do certain things. It is responsible for efforts to ‘try hard’, and is often quite critical. Self 2, in contrast, is the ‘doer’, the part of you that (in the end) ends up doing things but that is sensitive to the attitude and comments made by Self 1. The book is about the relationship between these two voices, since Self 1 doesn’t trust Self 2 to be able to deliver. Gallwey argues that in order to be effective and do our best work (or play our best match of tennis), we need a better way of translating our knowledge into effective action, and thus a better way for the two selves to communicate and interact.\nSome of the details of the book are specific to movement-based skills. I think that anyone doing any kind of sport would benefit from reading this book. Gallwey outlines four key skills that go into effective movement: - letting go of judgement — once you stop allowing self 1 to dictate the terms of the conversation, you can rely much more on the instinctual learning capacity of self 2. This also hooks into the idea of ’seeing clearly’, which pretty much immediately improves your ability to respond and react to the reality of the present moment. - creating images — visualising movement, really noticing what’s going on (when viewing others do it) and working from imitation and from feeling your way into the movements. - ‘letting it happen’ — getting out of the way. This has a lot to do with the first skill. You have to trust Self 2’s ability to deliver, and you just need to let it happen, or let whatever happens happen. The important thing here is the nonjudgemental observation of change and the results. - concentration — this is the final stage, since just ‘letting go’ is not quite enough. The relaxed, nonjudgemental mind needs focusing. For movement, this means focusing your observation and sensitivity to sensations in the body (for example) or an increased consciousness of the senses. (“Focus intensifies the light of consciousness”).\nHe ends the book by addressing the idea of games — games that people play, the meaning of competition, and the broader implications of the way we approach this ‘inner game’ (balancing self 1 and self 2) in our life. (I was really impressed how so many of the ideas presented presage current trends and predilections: the importance of mindfulness, the value of play, and so on).\nIf the first part of the book was less applicable to things outside the domain of movement skills and practical learning, the second half is much more relevant to the problems I think most of us face. There’s a certain amount of pop psychology that probably hasn’t lasted the test of time, but mostly it is really solid perspectives on: - finding games where the score doesn’t matter - the debate in Western society as to the value of competition as compared with cooperation. - issues that result when ‘success’ is equated to value and self-worth (which can in turn lead to various compulsions to succeed, or, paradoxically, to rebel)\nGallwey ends the book with the paradox that is at the core of the ‘inner game’, that in order to allow Self 2 the freedom to learn and act naturally, this requires a level of abandonment. So it is a matter of “caring, yet not caring […] effort, but effortless.” He ties it in to all sorts of other areas outside tennis that really convinced me as to the applicability beyond purely motor / movement skills.\nThere was a lot to unpack about this book, and I may return to some of the specific points in the future. For anyone working on their movement skills or finding ways to enjoy that part of their life, I’d put this on the list of essential books. For anyone else, maybe give the first few chapters a read; I think you’ll be surprised what Gallwey brings to the table."
  },
  {
    "objectID": "personal/2016-12-02-climbing-routes-without-numbers.html",
    "href": "personal/2016-12-02-climbing-routes-without-numbers.html",
    "title": "Climbing Routes Without Numbers",
    "section": "",
    "text": "On Wednesday I climbed up an ungraded route. The indoor climbing centre where I go once or twice a week has been updating and renewing the different holds they bolt to the wall. We arrived before they had printed out the little square reference signs that run along the bottom of the wall.\nIn the place of my longtime-favourite warm-up route, a nice yellow-coloured 5A, was a new route populated with deep black jugs. It looked easy from the bottom, but there was no confirmation of this from the grading at the bottom.\nEven routes in mountains — particularly in areas where lots of climbers visit — will often have been graded, but I found that I enjoyed climbing up the route without the grade. It returned me to that feeling — one I’ve been struggling to recreate — early on in my time climbing when it wasn’t always about doing the next level or the higher difficulty progression: it was just about enjoying the climbing, enjoying moving my body and whatever thoughts come up when you’re ten or fifteen metres above the ground.\nI’ll be trying to pay less attention to the numbers in future sessions and more to the felt and sensed experience of climbing."
  },
  {
    "objectID": "personal/2016-12-02-knot1-solitary-confinement-digital-security.html",
    "href": "personal/2016-12-02-knot1-solitary-confinement-digital-security.html",
    "title": "Knot 1: Solitary Confinement & Digital Security",
    "section": "",
    "text": "This is the first of what I hope will be a regular feature on this blog. Knot will link to recent things that I’ve been reading. It will include short articles as well as books and other things on the internet."
  },
  {
    "objectID": "personal/2016-12-02-knot1-solitary-confinement-digital-security.html#articles",
    "href": "personal/2016-12-02-knot1-solitary-confinement-digital-security.html#articles",
    "title": "Knot 1: Solitary Confinement & Digital Security",
    "section": "Articles",
    "text": "Articles\n\nMartin Garbus — “America’s Invisible Inferno” (NYRB)\n\nThis NYRB review of ‘Hell Is A Very Small Place’ covers the use of solitary confinement in the American prison system. It ends with unearned optimism, but nevertheless is a useful reminder of the importance of the issue.\n\nMary Catherine O’Connor — “The latest weapon in the fight against illegal fishing? Artificial intelligence” (The Guardian)\n\nInteresting short piece on the use of machine learning combined with data science competition site, Kaggle, to try to improve the efficiency and accuracy of fishing inspections. Hardly a day passes when a new initiative like this is launched, merging data science and crowdsourced solutions. I look forward to being able to contribute (one day in the hopefully-not-so-distant future).\n\nMeeri Kim — “How a researcher used big data to beat her own ovarian cancer” (Washington Post)\n\nMore data science, though this is more at the N=1 edge of things than the previous fishery data story. The more data we are able to access and generate, the more there will be need for ways of analysing and processing these complex interactions. A lot of money is being invested in finding ways to automate this for non-technical users, though I imagine we’re still a little way away from that utopia.\n\nCheck Point — “More Than 1 Million Google Accounts Breached by Gooligan” (CheckPoint Blog)\n\nI’m not quite sure why this isn’t a bigger story. I’ve long believed that anything stored by Google (in particular, emails) will at some point be leaked in a hack or by some disgruntled employee. This latest hack is pretty close to that scenario, with the caveat that we don’t know what was taken. Takeaway: move away from the Google ecosystem where possible.\n\nScott Gilbertson — “HTTPS is not a magic bullet for Web security” (Ars Technica)\n\nThis is a few months old but offers a useful overview on HTTPS technology, what it can help with (and what it is less useful for). The headline is actually a bit deceptive since the article seems to make a strong case for the use of HTTPS.\n\nShane Parrish — “The Simple Plan To Read More” (Farnham Street / Personal Growth)\n\nI’ve long ago taken this advice on board, but it’s a useful reminder. I enjoy reading, and I enjoy the diversity that comes from reading widely and broadly, so keeping up the pace and seeing it more as a habit to be done most days is, to my mind, the right way of looking at it."
  },
  {
    "objectID": "personal/2016-12-02-knot1-solitary-confinement-digital-security.html#books",
    "href": "personal/2016-12-02-knot1-solitary-confinement-digital-security.html#books",
    "title": "Knot 1: Solitary Confinement & Digital Security",
    "section": "Books",
    "text": "Books\nI read three books this past week. Jim Klopman’s Balance Is Power was my easy introduction to some of the science behind why it is useful to train balance in a focused way as a skill.\nI finished the second volume in Elena Ferrante’s tetralogy, The Story of a New Name, though I didn’t enjoy it quite as much as the first. I suspect I’ll have to return to it in a few weeks after the dust has settled.\nFinally, I devoured Paul Kalanathi’s When Breath Becomes Air, a beautifully written exploration of death, medicine, how we go through all these things as people, how illness affects not only the body but the mind and the spirit as well. It reminded me of Atul Gawande’s Being Mortal, which covered similar themes albeit with less of an intimately tragic outcome. I highly recommend giving When Breath Becomes Air a read."
  },
  {
    "objectID": "personal/2016-12-07-taliban-on-the-clintons.html",
    "href": "personal/2016-12-07-taliban-on-the-clintons.html",
    "title": "The Taliban on the Clintons (May 2000)",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“295”] One of the scanning machines we used during the course of the Taliban Sources Project [/caption]\nThe US election is over; my work on materials deriving from the Taliban Sources Project continues. As I waded through the archives – released in book form and online this coming spring – I found this little gem. Published on May 22, 2000, in Shariat newspaper (the flagship publication that ran throughout the years of Taliban rule), this is a commentary piece on Hillary, Bill and Chelsea Clinton in the wake of the Monica Lewinsky scandal. It ran with the headline, “Friend” in their op-ed section of the day:\n\nAccording to reports Mrs Hillary Clinton is the candidate for US senate.\n\n\nSince long Hillary has been trying to fulfil this wish. But on one hand the accomplishment of her wish is a success for her, on the other hand it is a lethal blow to Mr. Clinton. As president of the country his days are numbered. After seven months he will no more be the president of the country.\n\n\nAfter the end of his tenure as president he must leave the Whitehouse. After his retirement from the post presidency the Whitehouse will no more be the place of entertainment for him and Monika. But Hillary is also not a weak lady who is totally dependent on Mr. Clinton. During his presidency she was continuously ignored because of Monika.\n\n\nNowadays sometimes Mrs. Clinton is appears in very colourful poses beside vice president Al Gore in the media. One cannot blame her. She suffered a lot because of the Clinton and Monica affair.\n\n\nOur topic was her candidature for the senate. Now she spends considerable time in New York in order to increase the number of her voters and supporters. In her absence Mr. Clinton has to live totally alone.\n\n\nTheir twenty year old daughter, Chelsea, is busy with her studies at the university. In front of the prosecutor he denied any relationship with Monika. That is why she is also no more interested to revive the old friendship with him.\n\n\nFor a long time the American journalist were wondering how Mr. Clinton would manage to deal the days and nights of total loneliness. According to Reuter’s news agency finally these questions were also heard by Mr. Clinton himself. When he was departing in a helicopter from New York to Washington, Hillary demonstrated a very cold behaviour towards him. She left him alone. When the journalists asked Mr. Clinton about his loneliness, he replied that he would no more miss his wife Hilary, because he found a new very loyal friend who stays and sleeps all the time with him. He told that his new friend was his dog who was called Buddy. He further explained that he enjoys every moment of his life in the company of Buddy.\n\n\nThe journalist fully believed what Mr. Clinton told them. However, for further validity he quoted the former US president Henry Truman who once said “if a person wants to have a loyal friend in Washington he must find a dog, which remains loyal in all circumstances and never leave his owner alone”.\n\n\nMr. Clinton prefers his dog, Buddy, over his wife, Hilary. Because his dog is an animal which is free of all human characterises but always remain fully loyal to his owner. It does not matter for the dog whether his owner the US president or the Whitehouse gatekeeper. He fulfils his loyalty to the extreme end. This kind of loyalty is neither in the capacity of Hilary, Monica nor any other western lady.\n\nThis piece didn’t make the cut for the print anthology / reader, so I am publishing the translation here in full."
  },
  {
    "objectID": "personal/2016-12-09-highlights-app-review.html",
    "href": "personal/2016-12-09-highlights-app-review.html",
    "title": "Highlights + DevonThink = Pretty Great",
    "section": "",
    "text": "I’m late to the Highlights party, but I’m glad I got here.\n\nLike many readers of this blog, I get sent (and occasionally read) a lot of PDFs. In fact, I did a quick search in DevonThink, and I am informed that I have 52,244 PDFs in my library. These are a mix of reports, archived copies of websites, scanned-and-OCRed photos and a thousand-and-one things in between.\nThus far, my workflow has been to read PDFs on my Mac. Any notes I took while reading the file were written up manually in separate files. I would laboriously copy and paste whatever text snippet or quotation I wanted to preserve along with its page reference. These would be fed into DevonThink’s AI engine and magic would happen.\nNow, post-Highlights-installation, my workflow is much less laborious. I can take highlights in-app, export all the quotations as separate text or HTML files and have have DevonThink go do its thing without all the intermediary hassle. If you’re  a professional researcher or writer using DevonThink as your notes database — and quite frankly, if not, why not? — the Highlights app will probably please you."
  },
  {
    "objectID": "personal/2016-12-09-the-two-books-every-intermediate-arabic-student-needs-to-read.html",
    "href": "personal/2016-12-09-the-two-books-every-intermediate-arabic-student-needs-to-read.html",
    "title": "The Two Books Every Intermediate Arabic Student Needs to Read",
    "section": "",
    "text": "In the beginning, the study of Arabic is about coming to terms with how the language works. You start to grapple with the diglossia, with the verb system (roots everywhere!) and start working your way through one of the big textbooks that most programmes prescribe.\nAfter a few months, you want to start communicating with people, but every attempt is met with some kind of difficulty or failure. Either you’re not speaking the right dialect, or you’re not attempting to speak using dialect at all. You study a bit more, and you’re finally able to communicate with your Lebanese or Tunisian friend, but the moment you come across someone speaking a different dialect variant you’re back at square one.\nWhat’s the solution to this problem? Do students of Arabic have to learn every dialect? Thankfully, no, though you’ll want to get a basic familiarity in some of the main broad patterns that different dialect groups employ.\nThis is where the good people at Lingualism come in, and their two-volume Arabic Voices series. (Get Volume 1 and Volume 2 from Amazon here, or order direct from Lingualism).\nThese two books contain 57 monologues covering the whole Arab world and North Africa. You can learn about Qat from a Yemeni, identity from a Lebanese man, or what it’s like to date as a young Tunisian woman. To get a taste for the content of the material, everything is available for free on Lingualism’s YouTube channel. Here’s Wasem, for example, talking about the life of Arabs living in Israel:\nThe great thing about this book is that every monologue comes with a wide variety of exercises and supporting materials to help you get the most out of the language used.\nYou start off with some basic comprehension questions alongside some of the key vocabulary, then you dive into some of the specifics that are unique to that particular dialect. Here’s one of the exercises where you’re supposed to draw lines connecting the English to the Egyptian and to the Fusha / ‘Modern Standard’ translations.\n\nI’m not super keen on textbooks that deal too much in translations, but if you know how to use them, this is a real goldmine. I’ll usually put one or two of what seem like key phrases or expressions into Anki to memorise. The exercises are followed by a full transcript in Arabic and English translation. Here’s one example:\n\nSome of the monologues also include grammar notes at the end if something particularly interesting is worth noting. Here’s an example of that ‘focus’ material.\n\nOnce you’ve gone through all the exercises, listened to the video to figure out what the various new phrases might mean, you can start covering up the Arabic part of the transcript to see if you can reproduce parts of what was said on the basis of the English translation. This is hard work, but incredibly rewarding.\nIf you want to get a full overview for how the language that Arabs speak day-to-day, these two books are the resource for you. What’s more, the monologues are short enough that you can probably blaze through the content of both books in two to four months. I don’t know of any other one-stop-shop where you can gain familiarity with the linguistic variety of the Arab-speaking world, plus learn a thing or two about people’s lives and culture(s).\n[Over the coming weeks I’ll be writing more about the different resources available to intermediate-level Arabic students. I’ve just finished writing an eBook and electronic resource for people at that stage in their learning. For more information and discount offers, sign up to the mailing list here. To enquire about one-on-one language coaching sessions, click here.]"
  },
  {
    "objectID": "personal/2016-12-13-notification-zero.html",
    "href": "personal/2016-12-13-notification-zero.html",
    "title": "Notification Zero",
    "section": "",
    "text": "As I reach the final stages of a major project, I’ve been saying ‘no’ more often. I’ve been turning down work, conferences and all the little things that take away from my ability to focus for hours at a stretch.\nThe other day I realised that my usual clampdown on notifications on my various digital devices wasn’t as tight as I ideally prefer, so I turned them all off. (You can read how to turn off iOS notifications here, and Android notifications here.) I’m writing down the various parts of this in case it’s useful to someone else in the midst of a big project.\nIn practice, this means the following:\n\nno badges on apps on my screen. (The software on your phone is designed to be addictive, to make you spend more time using it. I already live my life in black-and-white so that’s a start, but removing app badges is the next step.)\npermanently set to airplane mode. (This is easy for me as my work or life doesn’t ever really involve receiving phone calls, but it removes the chance of business / telemarketing distractions.)\nall other apps’ notification permissions turned off. (If someone messages me on WhatsApp or Signal, I will never ever get a notification about it. My phone won’t make a noise, nor will there be any badge showing that there are people who tried to get in touch with me. The only way I’ll ever see if anyone wrote to me on a messaging app is to open up that app. I do this perhaps once a day, sometimes only a couple of times a week, but I’ve conditioned myself to this level of withdrawal over the past year or two).\nEmail permanently disabled on my phone. (Email is the big focus killer. If I generalise, it amounts to a box of other people’s requests on my time. So I have no way to receive email on my phone. I have things set up that I can send email (i.e. I’ve installed the SMTP login details but not enabled IMAP), but no way to receive it.)\n\nI have a fixed schedule for things like checking email: two windows from 12-1pm and 5-6pm. Outside those windows, my email is disabled (using Freedom) so there’s no way for me to access it. On my laptop, a whole bunch of other sites whose addresses are easy to type and get lost on, I pre-empt my worst self by just disabling them during the day. Problem solved.\nWhenever I write a post on this blog, it auto-posts to Twitter, Facebook, Linkedin and Tumblr. While the temptation might be to check to see who has commented on it and so on, by locking myself out on a specific schedule, this means I can choose to look at these things at the end of the day, if I really need/want to."
  },
  {
    "objectID": "personal/2016-12-16-into-eternity-review.html",
    "href": "personal/2016-12-16-into-eternity-review.html",
    "title": "Into Eternity: the construction of Onkalo’s tunnels",
    "section": "",
    "text": "I watched a gripping documentary this evening on the construction of a tunnel / radioactive waste burial facility. Onkalo is located on the west coast of Finland and is designed to store the country’s national nuclear waste until it is no longer harmful to life. This amounts to around 100,000 years.\n\nIf you think that human life has been around for a very small fraction of that (see the images above for a sense of the scale), and most of the designed ‘things’ have never lasted that long, this makes the design and implementation of such a storage facility a really huge proposition.\nThe tunnel and related storage areas will only be completed post-2100, but the design team take into account things like their assumption that there Earth will be due another ice age in around 60,000 years from now. They consider the various possibilities of new or different civilisations coming and going.\nInto Eternity is a fascinating documentary. Find it online if you get the chance."
  },
  {
    "objectID": "personal/2016-12-18-seeking-python-code-mentor.html",
    "href": "personal/2016-12-18-seeking-python-code-mentor.html",
    "title": "Seeking Python Code Mentor",
    "section": "",
    "text": "My initial foray into the world of coding began earlier this year. I’m now at the point where I am at the least cognisant of the major structures and features of this language. If you give me some Python (or Javascript) code I can most likely start to figure out what’s going on.\nI’ve wanted to make progress on a small Python side-project involving a front-facing website, a Python application hooked up to a database (most likely PostgresQL since that’s what I know).\nI have been stuck for a few weeks, not knowing how to move forward and unwilling to commit to any one of several options in case it turns out to be a dead end. I want to keep learning, and not keep getting stuck so that long time periods pass between attempts to tackle this (very small) project.\nSo I’m looking for someone who has made web apps and is familiar with Python and very simple databases with whom I can talk my project through. I’d still like to code it all myself, but I need someone who might have thirty to sixty minutes every now and then to point me in the direction of tools or libraries that I might not have heard of. I’m aware that a lot is discoverable via Google and StackOverflow, but nothing beats not having to explain myself and my project from scratch every time I ask a question.\nI know there are a lot of developers with experience and an eagerness to give back to the community by helping beginners, so consider this my call for your help. My project is a tool to help language learners, moreover, so your help will have an effect beyond just me improving in my coding skills.\nIf that’s you, and you’re willing to spend up to an hour per week helping me improve, please contact me on Twitter or via email. If you know someone who might be interested, please share this blog post with them. Location doesn’t matter if you’re comfortable sharing code during a video call.\nThank you!\nP.S. You can read some blogposts where I discuss my coding progress, and you can also find me on Github."
  },
  {
    "objectID": "personal/2016-12-20-django-vs-flask.html",
    "href": "personal/2016-12-20-django-vs-flask.html",
    "title": "Django vs Flask",
    "section": "",
    "text": "I am diving back into the coding of my language-related web tool, as I mentioned a few days back. In and of itself, the functionality is quite simple, but there are three parts that have to all work together, so it’s harder for me to put it all together than I had expected.\nOne of the decisions I’m currently mulling over is whether to use Django or Flask to allow my Python code to interact and get displayed on the website. For those of you who aren’t familiar with coding and how websites work, these two frameworks are what allows me to write the bulk of the tool in Python, but then use either Django or Flask to display it on the web and interact with the web server. (I realise I probably butchered that explanation.)\nDjango seems to be what most people recommend I use, in part because it comes with a number of useful things built-in. As a result, or maybe independently of that, it has a steeper learning curve and feels like a larger proposition than perhaps I need for my (somewhat small) project.\nFlask is pretty basic and pared down. It doesn’t contain all the bells and whistles that Django offers, but as a result it’s easier to get your head round and make a working prototype sooner.\nIf I had a bit more time, I think I’d go with Django. The user community seems bigger and I’d probably end up using the built-in features, but Flask allows me to get a working prototype hosted somewhere much faster and thus seems like probably the option I’ll choose.\nFor now, I’m getting a bare-bones Python-only version of the tool ready, then I’ll hook that up to Flask and see how that interaction works."
  },
  {
    "objectID": "personal/2016-12-21-reading-american-revolution.html",
    "href": "personal/2016-12-21-reading-american-revolution.html",
    "title": "Reading and the American Revolution",
    "section": "",
    "text": "‘A Powerful Mind: The Self-Education of George Washington’ tells the story of Washington’s reading habits (and how they benefited him). You can read only the introduction and get the main message: he read a lot and the things he read allowed him to grow as a military and political leader. The rest of the book is a detailed exploration of this thesis alongside some of the supporting evidence.\nExtensive and deep reading alike were important throughout his career. In one of the most fascinating sub-sections, Harrison shows how Washington’s military campaign benefited from some of the latest military theories of the day, both in terms of tactics and strategy.\nI benefited from the way the book proceeded chronologically through Washington’s life, not being familiar (at all) with the events of the American Revolution and the time period being examined. One of my first questions to other readers (over on Goodreads) was the extent to which someone not immersed in this American history could read this book without trouble, and I’m pleased to report that relative newbies like myself shouldn’t find much issue.\nThis book is repurposed from a PhD thesis, and it tells. The narrative is sometimes diverted to a discussion of sources or the evidence on which certain claims rest. This is all fine and proper, though it disrupts the flow. I wonder whether those discussions might not have been relegated to an appendix. That said, I also occasionally felt that too much was made out of the baseline evidence on which the book seems to rest, i.e. the records of which books were owned, annotated and/or read by Washington. Harrison writes a lot about the various ways we can deduce what Washington might have been reading at period X or Y, but as a reader myself I felt disappointed that somehow we hardly ever seemed to actually catch Washington directly in the act.\nOne of the points that I found most interesting was how Washington seemed to go about selecting his books. There were fewer titles at the time, so he seemed to have more opportunity or necessity for deep reading (annotating copies line-by-line, for example) than in the present day where it’s probably easier to develop habits of wide/extensive reading that are ultimately shallow. A fascinating section in this respect was where Washington decides he needs to overhaul his farm and agricultural practices, and Harrison shows how he used what he read to take a risky but ultimately profitable decision to reform and overhaul how he managed his land."
  },
  {
    "objectID": "personal/2016-12-23-gift-language-coaching.html",
    "href": "personal/2016-12-23-gift-language-coaching.html",
    "title": "Share the Gift of Language Coaching",
    "section": "",
    "text": "If you’re still looking around for last-minute Christmas presents, look no further. How about the friend who has been struggling to learn a language? Know someone who is always talking about how they want to learn a language but don’t have the time? Or even you yourself: are you always putting off studying Arabic or French (or even Mandarin) that you know would benefit you, but you just don’t know how to get started?\nI offer custom-made language coaching to suit your schedule and needs. You might need help with motivation, accountability, coming up with a grand plan and/or just require someone to be a sounding post for a study plan you’re already working on.\nWhatever you language-learning needs, I’ll be happy to work with you. My goal is to help you reach your goals as directly as possible. We’ll work to your strengths and find ways to make the process less of a chore.\nYou can read more about the whole process here.\nTo purchase a gift subscription, please get in touch here."
  },
  {
    "objectID": "personal/2016-12-26-podcast-gabe-weatherhead.html",
    "href": "personal/2016-12-26-podcast-gabe-weatherhead.html",
    "title": "Talking DevonThink with Gabe Weatherhead",
    "section": "",
    "text": "I’ve been on a bit of a DevonThink kick these past weeks, and the catalyst for all of this was a conversation I had with Gabe Weatherhead (@macdrifter over on Twitter, though that account is no longer active).\nYou can listen to the full episode on your podcast player of choice or over on the Sources and Methods site. Towards the end of the episode we get into the weeds on how he uses DevonThink Pro Office and several other pieces of software. I’m looking forward to hearing Gabe’s much-anticipated appearance on MacPowerUsers in January, since I imagine he’ll go into even more detail there.\nWe also discussed social media and some of the ways he found himself drifting away from commonly-used sites like Facebook and Twitter. For me, this was the most interesting part of the podcast."
  },
  {
    "objectID": "personal/2016-12-29-dinner-party-decision-matrix.html",
    "href": "personal/2016-12-29-dinner-party-decision-matrix.html",
    "title": "Dinner Party Decision Matrix: A Python Tool",
    "section": "",
    "text": "Some friends were coming round for dinner this evening and I couldn’t decide what to cook. I couldn’t decide because there were too many variables floating around that would impact the decision.\nSo I decided to make an app.\nI wanted something tasty but that also was relatively hassle-free. I wanted something that didn’t take too much time to prepare, but it still needed to be different from the kind of food I’d cook in a rush at the end of a long day.\nI’d come across the idea of weighted decision-making a while back, and really like its premise; you say how important various factors are to you and then rate all the different options according to those same factors. At the end, you’re left with a score that can be said to be more objective than just your gut feeling.\nSo yesterday morning I listed a bunch of the factors that were important to me in coming to a decision about what to cook. I figured I could get some coding practice by making a tool that would work to calculate the best option from any combination of choices.\nI put the finishing touches on the decision-making tool this morning. You can check it out on Github, though you’ll need to know a little about how Python works to get it going for yourself. I’m fairly sure my code looks horrific to a seasoned professional. There are probably ways I could have standardised the flow of questions and improved the output, but for now, I’m satisfied.\nThe app ended up picking the following for dinner (photo below):\n\nCourgette, chickpea and feta filo pastry pie\nRoast beetroot and pistachio salad\nKale, pomegranate and shredded chicken salad\nand a beetroot and chocolate cake\n\n\nI learnt a bunch of things about Python while coding the tool:\n\nWhen you run a x= raw_input(“Ask question here”) command, the information allocated to x will be in a string format. If you are collecting numbers, you’ll need to convert it to an integer format by using int().\nLists and dictionaries can be tricky, particularly when you’re looping over them or looping over certain keys etc.\nIt helps to test while you code. This particular function wasn’t too complex to understand, so I just pushed on without testing so much. At some point towards the end, I realised something in the middle was wrong: cue much debugging. I keep reading about the importance of having in-built tests. I’m not sure what that would mean / have meant in the context of this particular app, but I imagine I’ll find out in due course.\nCoding is fun and appeals to the compulsive side of my brain. I rarely need to be urged back to the task at hand when I’m coding. This probably has something to do with the quick feedback loop that coding enables. In any case, I’m hooked (again)."
  },
  {
    "objectID": "personal/2017-01-01-2017-1-new-year-new-arabic-language-podcast.html",
    "href": "personal/2017-01-01-2017-1-new-year-new-arabic-language-podcast.html",
    "title": "New Year, New Arabic-language Podcast",
    "section": "",
    "text": "This year, in combination with my publishing a book about getting from intermediate-advanced level in Arabic, I wanted to find a way to stay committed to improving my own language. So I’ve started a mostly-Arabic-only podcast.\nThere are two episodes up already, both from the ‘Jordanian Dialect’ series. This series is recorded with co-host Lina Obeidat, my good friend and Arabic teacher of several years. (Book lessons with Lina on iTalki here. She’s also a regular contributor to talkinarabic.com’s Levantine materials).\nDuring the first episode, we talked about the winter and the rain in Jordan (where we’re both based), and I learnt about the existence of a period of 40 days when the weather is much colder. In our second episode, we talked about taxis in Amman. We struggled to find many positive things to say about the system or the drivers of taxis, but tried to see things from their perspective. At the end, I learned a new proverb that was appropriate to our discussion.\nIn the coming weeks I’ll be adding more shows to the podcast. There will be a show focusing on Classical Arabic texts. There will be a show on Modern Literature. Potentially we’ll have a show for Egyptian Colloquial (sigh!) but that depends on my finding someone to host it (let me know if you’re interested).\nI’m looking forward to recording more episodes. Please leave a comment over on Soundcloud with any feedback you might have. Also, subscribe on iTunes by clicking on this link or search my name and you should find it. The podcast is also available on Overcast. Just search the name and it’ll show up."
  },
  {
    "objectID": "personal/2017-01-02-tech-switching.html",
    "href": "personal/2017-01-02-tech-switching.html",
    "title": "Pet Peeve: Tech Switching",
    "section": "",
    "text": "I read a decent amount of tech media/press. Barely a day goes by when there isn’t someone in my RSS feed explaining how they dropped application X for application Y. This seems to happen most often for frequently-used applications or workflows like scheduling/calendars or email.\nI won’t call out the specific blog post that set me writing this post, but suffice it to say that I wish there was a clause (in the contract of life) forcing tech writers or bloggers to state why the application they’re singing the praises of is better than the one they were using up to now. Specifically, are there any new features, or does it just look shinier? Also, have you been using it for longer than a day or two?\nI’m pretty solid and stable in the applications I use. It’ll take something pretty seismic to rid me of DevonThink or Tinderbox or Mailmate. But if you catch me flip-flopping in my tech-related writing, please call me out on it."
  },
  {
    "objectID": "personal/2017-01-04-internal-vs-external-goals.html",
    "href": "personal/2017-01-04-internal-vs-external-goals.html",
    "title": "Things We Control: On Internal vs External Goals",
    "section": "",
    "text": "When it comes to goal setting, I’ve found it helps to distinguish between the kinds of goals you envision and create. I first came across this concept in a book on Stoic philosophy: A Guide to the Good Life, by William Irvine. (If you’re interested in the full details/writeup, read chapter five.)\nThe basic premise Irvine lays out goes as follows:\nInstead of seeking contentment by trying to change the external world, rather we should think about how to change ourselves (i.e. internally).\nHe splits up the internal / external distinction a bit further:\n\nHere you can see that there are some things over which we have complete control. There are also things which we don’t completely control. This can be split into things we have no control over at all, and things we have some control over but not total. (This is actually a trichotomy derived from the philosopher Epictetus.)\nAn example of something we have no control over at all is when the sun rises. Things that we have some control over but not total are generally impulses, desires and aversions, so you can control your attitude in a certain situation or what kind of effort you put into a language-learning exercise. Things we have complete control over include our opinions, the goals we set, the values we consider important and our character.\nThis is a really useful way of splitting things up, and you can maybe see why it might be useful. We should spend no time at all thinking about or worrying about the things that we have no control over at all. In terms of language learning, this might relate to any putative ‘innate’ or genetically-based learning ability or aptitude. You’re stuck with who you are, so there’s no point being upset, particularly when it comes to comparing yourself to others. (Plus, a lot of this discussion of ‘innate talent’ for learning languages misses the point: there are many things that one can do to speed up one’s progress.)\nThe other two categories are really what we should be concerning ourselves with, since this is where we have the ability to influence the outcome. We can set our goals in a useful manner (as described above) and we can change our impulses and perspectives on particular tasks.\nThe practical realisation from all of this, for a language learner, is that it really makes sense to find ways to set internal goals as opposed to external goals.\nThink of a game of tennis, for example: you might have a goal to ‘win the match’. This would be an externally facing goal. It is one over which you don’t have complete control. Your opponent may have advantages over you that you can’t anticipate or prepare for.\nAlternatively, an internal goal might be to ‘play my best’. This has the advantage of being something you have full control over (i.e. how your effort is directed) and it is not something that can be refuted or quashed by your opponent on the court. Even if you lose the match, by taking this perspective of ‘doing one’s best’ you’ll be able to take solace in the fact that there was actually nothing you could have done to change this outcome. Plus, you met your goal (presuming you did actually try your best).\nIn terms of language-learning, this translates to formulating your goals in terms of process rather than fixed targets. So: rather ‘study for 10 minutes every day’ than ‘reach a B2 level by September’. This way, you’re setting yourself up for success and you can focus on what matters in your studies without the feeling of pressure. Same thing with something like writing a PhD. (I’ve written elsewhere about how you can think of writing a PhD in terms of time rather than wordcounts or sections completed.)\nI hope I’ve convinced you that goal-setting can be an important lever in how you tackle the meta-goals of your language-learning journey. It’s certainly been an important realisation for me in how I setup goals and intentions. It also works really well with commitment tools like Beeminder, though that’s a topic for another day!\n[This is an edited excerpt from my new book, Master Arabic, now available for discounted pre-order here. Use the offer code ‘preorderthankyou’ at checkout to get 10% off.]"
  },
  {
    "objectID": "personal/2017-01-05-randomise-your-learning.html",
    "href": "personal/2017-01-05-randomise-your-learning.html",
    "title": "Randomise Your Learning",
    "section": "",
    "text": "Scientists of various stripes have been studying how best to learn for many years. We can draw a variety of conclusions from this academic literature, but one of the more surprising results is that randomised practice is better than when you focus on a particular topic to the exclusion of everything else.\nThis means that when you’re revising for end-of-year exams covering a number of subjects, it helps to test yourself on all the subjects at the same time, randomly switching between the different areas, instead of picking just one subject and working through each one individually.\nThe same applies to language learning. More and more people are choosing to use spaced-repetition applications like Anki, but most times I look at how the decks are organised, I see cards and information organised by topic.\nThis misses out on the opportunity to ‘interleave’ (the term used by the literature.) The benefit from interleaving your testing of cards probably comes from the fact that it’s slightly harder to switch from a French vocabulary context into Japanese or Arabic. Your brain has to work a little harder to recall the information, so the network connections are stronger than if you’d studied all the words from the same language together.\nWhen I open up Anki, I see the following:\n\nI have one deck (called “All The Things”) which contains all my cards. There is a lot you can do with in-built and plugin support for tags so I’m able to dice and segment the cards as I wish. This means when I’m reviewing my decks, I’ll get one card with Arabic vocabulary one minute, then something testing me on shark anatomy the next, followed by some Python coding-related knowledge I’m learning.\nReviews are harder, but the concepts and facts also stick much better in my mind. Try interleaving your study and see if it helps.\nTo read more on this, check out this wonderful meta-review: “Improving Students’ Learning With Effective Learning Techniques: Promising Directions From Cognitive and Educational Psychology” by John Dunlosky, Katherine A. Rawson, Elizabeth J. Marsh, Mitchell J. Nathan, and Daniel T. Willingham in the journal ‘Psychological Science in the Public Interest’."
  },
  {
    "objectID": "personal/2017-01-08-language-coach-how-to.html",
    "href": "personal/2017-01-08-language-coach-how-to.html",
    "title": "How to best work with a language coach",
    "section": "",
    "text": "I work to help language students make more effective and efficient progress. Sometimes I work with those who want to learn other skills like coding, or perhaps a more specific productivity issue or project that requires top performance.\nBut how can you make sure you get the most value out of working with me or another professional coach?\nShow Up\nCome with questions and a purpose. Coaching works best when student and coach each show up with things to contribute to the discussion. Two important prerequisites on the part of the student are some sense of their goals as well as a set of questions. (If there weren’t any questions, there wouldn’t be any need for a coach). The more explicit and engaged these questions and purpose can be, the better the coaching will be.\nKeep Notes\nIdeally you’ll keep a notebook where you can gather more specific insights, questions and reactions to the work you’re doing. If you don’t note them down while you’re doing the work, you’ll probably forget and miss out on the opportunity to get feedback or suggestions on the particular problem that affected you.\nMore Feedback, More Communication\nAs a coach, it’s most difficult to help someone when you don’t hear much about what they’re doing or how they’re feeling about it. There probably isn’t an upper limit on how much feedback to give. More is better, just so long as it doesn’t take you away from your actual language study. This can take the form of a daily email once you’ve completed your study session for the day, or writing notes in our shared tracking spreadsheet.\nReflect\nReflection brings perspective. By taking a step back from your coaching and your language studies, you might be able to figure out some broader patterns relating to how you study. Reflection can take any form. I like to write out a problem or a situation using pen/paper, thinking through how I feel things are going. Think of it as a solo version of the coaching sessions themselves which have reflection built-in.\nFollow-up During The Week\nI welcome email queries during the week. Very few take me up on the offer, though it’s an excellent time to get quick answers to any particular question or issue that comes up. If the weekly check-in is the only time we hear from each other, course corrections are much harder to make.\nDo The Work\nIf you’re working with someone, you’re going to get the most benefit if you follow through with whatever you both agree you’ll get done in a particular week. Doing the work will bring up new challenges and questions, and these can feed into the ongoing coaching. In order to decide whether a given path is or isn’t working for you, you have to give it a serious try in the first place.\nFinally, on a more prosaic note, showing up on time and giving adequate notice if you need to reschedule are always practices I really appreciate in someone I’m working with. I often travel from different parts of the town to make sure I’m ready for a call.\nOut of all of this, communication is the key.\n[Read more about getting language/skills coaching with me here. The page includes testimonials from current and former students.]"
  },
  {
    "objectID": "personal/2017-01-11-podcasts-data-science-programming.html",
    "href": "personal/2017-01-11-podcasts-data-science-programming.html",
    "title": "10 podcasts to learn about data science and programming",
    "section": "",
    "text": "When learning a new skill or indulging a new interest, I like to find out who is podcasting about that thing. I do this early on in the process since it is hard to get a sense of the full landscape of a particular skill or issue without reading widely. The internet, of course, is a great way to find out about all the different nooks and crannies of a particular community. That doesn’t help me much when I’m cooking, though, or when I’m walking in town. Podcasts are an opportunity to broaden my exposure to those topics while doing something else, usually something physical.\nHere are some of my favourite podcasts relating to data science and programming:\nBecoming a Data Science Podcast\nHosted by Renee Teate, this was my first exposure to smart data professionals talking about their work. I think the podcast is meant to be modelled on the book Data Scientists at Work. (I’ve read the book and thoroughly enjoyed it, for much the same reasons as I enjoyed this podcast). I’d recommend going back to the first episode and listening to them all. You can get a good overview from listening to just these episodes. Renee has recently resumed podcasting for season two and she also runs a data-science resource online empire that is filled with useful materials.\nCodeNewbie\nI’m working my way through all of these these, listening through from the very beginning (over 120 of them). At the beginning, I find it’s useful to hear from people who started from nothing and who found a way to use code in their life or career. Not all of the guests are equally interesting, but the podcast is well produced and there’s still loads of useful information buried in the overall corpus.\nLearn to Code With Me\nThis podcast is in its second season. Just like CodeNewbie, the podcasts tend to focus on the journeys of people who have started from nothing. The guests are variable in terms of how interesting I find them, but I enjoy it nonetheless.\nPartially Derivative\nThis is one of the more popular data science podcasts out there at the moment. Episodes range from discussion panels to interviews to deep-dives on a particular topic. This is great for keeping up with the buzz or gossip in the data science world and they often have pretty senior guests from the world of data science and business.\nStart Here: Web Development\nThis podcast also has a Ruby/Rails sidekick podcast. I’ve just started the Web Development series and am finding it a useful overview. I’m not coming at this all as a complete beginner, so I’m not sure of the extent to which it’d be useful if that’s you, but each episode covers a particular area of programming and web development. Episodes end with an assignment or set of tasks to do to practice or learn whatever they were discussing. This is a nice combination of approaches and as a quick revision of some web development fundamentals, this is a really good place to start.\nTalk Python To Me\nPython is the language that I’ve studied and coded most with, so this podcast allows me some contact with people who are light-years ahead of me in terms of their skills. It also introduces frameworks and personalities that are important within the Python community, so if Python’s your thing, this is probably a useful podcast to listen to.\nData Skeptic\nThe contents of this podcast fairly frequently go right over my head, but it’s good to be exposed to the ideas being discussed. There are interviews with people about particular issues and mini-dives explaining a particular data science feature or area.\nFull Stack Radio\nThis is less content-rich than some of the other podcasts listed in the list, and there’s sometimes too much focus on the marketing/business side of things, but it’s nevertheless an enjoyable set of discussions about web development and programming in general. Guests often speak about meta-issues relating to productivity, management and so on.\nGreater Than Code\nThis is one of my favourites. It’s quite a new podcast, with only 14 episodes as of writing, but the hosts take care to bring in a diverse range of guests. In particular, it’s a breath of fresh air to have lots of female coders on, since most of the ‘successful’ coding podcasts tend to be heavily dominated by men and their male guests.\nO’Reilly Data Show\nThis podcast includes a lot of the latest technologies, big trends and big-name guests. Some episodes are too vague and unspecific, but as a big name in the data science podcast crowd, this is a fairly good place to go for the orthodoxy of many institutions and individuals involved in the space.\nPlease let me know if I’m missing any good podcasts that would be appropriate for someone at the beginning end of their programming journey."
  },
  {
    "objectID": "personal/2017-01-12-master-arabic-whats-in-the-premium-edition.html",
    "href": "personal/2017-01-12-master-arabic-whats-in-the-premium-edition.html",
    "title": "Master Arabic: What’s in the Premium Edition?",
    "section": "",
    "text": "Master Arabic is the book I wish I’d had when I got the end of my university programme studying Arabic. I’m really happy with how it’s turned out, especially the resource list. I discovered all sorts of new books and online materials that I’ll be using in my own studies going forward.\nI’ve updated the page for Master Arabic to more accurately reflect the differences between the basic and premium edition. Here’s what you get when you order the premium edition.\n\n13 Expert Interviews - You’ll receive a link to download MP3 files for you to listen at your own convenience. I chose people who have been through the challenges associated with intermediate learning plateaus, and they each offer unique tips to break through to more advanced levels.\nDiscount Codes - Exclusive discount codes from some useful language services and publishers. Language learning can be expensive; these codes offer some respite.\nOnline Resource List - I’ve scoured the web and physical bookstores to pick out the best resources available for the intermediate language learner. This is a ‘living document’, continually updated as I come across new materials. The resources are organised into topics / areas, and there are dialect-specific sections as well.\nCheat Sheet - Most Common 1000 Words - learning this list will make more of the things you read and listen to comprehensible.\nCheat Sheet - Tim Ferriss’ Deconstruction Dozen - Twelve sentences that reveal the core grammatical structures and principles at work in the Arabic language.\nCheat Sheet - Tool Overview - An overview of some of the main tools and tricks you can use to work on specific skills or areas of your study.\nCheat Sheet - Goal-Setting Worksheet - a one-page chart to help you think about the goals you set. (Corresponds to a section in the book on goal-setting.)\nCheat Sheet - Arabizi - a one-page quick reference chart for the symbols and letters used to type unique Arabic letters using English script. Essential for txting and chatting with friends!\nAccess to the exclusive Incremental Elephant forum to discuss language learning and your Arabic progress\nSample Study Plans - I’ve included suggested activities you can use to customise your own plan. Includes samples for those who only have 30 minutes each day as well as someone working on an intensive study programme (either in-country or at a university)\nSpread the Learning! - Buy the premium edition and I’ll send a PDF copy of the book to a friend, free of charge!\nLifetime updates - Receive lifetime updates to the book’s text. I continue to work on the text and when a new release is available, premium customers can download the update for free.\n\nYou can view a list of the thirteen people I interviewed here.\nThere are also six discount codes offered with the premium package:\n\nBliu Bliu - Get 3 months Premium for free when you enroll in the Arabic Challenge.\niTalki - Get $10 of language classes for free with your first purchase and the exclusive premium code. (PRE-ORDER ONLY)\nGlossika - 25% off for purchases in their online store\nForeigncy - Get a 10% discount on six-month and annual subscriptions with the exclusive premium code. (PRE-ORDER ONLY)\nTalk in Arabic - Discounted subscriptions.\nBeeminder - Get a 20% discount on all premium plans with the exclusive premium code.\n\n(Note, two of the premium codes are only valid if you pre-order the book.)"
  },
  {
    "objectID": "personal/2017-01-15-clozemaster.html",
    "href": "personal/2017-01-15-clozemaster.html",
    "title": "ClozeMaster: learn words in context",
    "section": "",
    "text": "Clozemaster is a service I discovered recently while researching the resource guide for my new book, Master Arabic. Clozemaster is intended to be a next step for students who have mastered some of the basics of their language of study. It teaches the language through a game where you identify the missing word. Here’s what one of their tests look like, for example:\n\nYou can see that it has a space where a word is blocked out. Four options are available below, from which you have to choose. Of course, number two is the correct option and by clicking it you move onto the next question.\nThe sentences are taken from a wonderful open-source project called Tatoeba. Tatoeba is a collection of sentences and translations which are curated and gathered through crowd-sourcing.\nClozemaster hooks into the corpus of sentences available in Tatoeba and serves up tests ordered by how commonly the word is found (i.e. its frequency ranking).\nIf you’re wondering why this is useful, it helps to know a little about how we best learn vocabulary. The gold standard of vocabulary acquisition happens through context. You learn the word when it is used in a real sentence or scenario, and where you derive all sorts of clues from the words around it. This is how we learn words when we’re growing up, and it’s how we continue to learn words and concepts in adulthood.\nClozemaster, therefore, is a great tool for someone who has mastered the basics of grammar in their language of study but who wants to grow their vocabulary further. At that stage, it doesn’t help to learn words in isolation like services like Memrise have you doing. Instead, learn through reading, or learn through services like Clozemaster. The service is free and would be ideal for someone who’s just finished Duolingo in a particular language (for example)."
  },
  {
    "objectID": "personal/2017-01-16-introducing-taylor.html",
    "href": "personal/2017-01-16-introducing-taylor.html",
    "title": "Language Learner’s Journal: Introducing Taylor",
    "section": "",
    "text": "[This is a guest blog, written by freelance journalist and (Arabic) language student, Taylor Barnes. I’m working with Taylor to get her up and running as quickly and effectively as possible, and I suggested she might want to write about her learning progress along the way. She’ll be posting here every week or two.]\nRio de Janeiro is an unusual place for an American journalist to begin studying Arabic, but at least it made my beginning steps in this challenging language deeply associated with laughter and leisure. Brazilians of all ages have a habit I admire of seemingly always being in a continuing education cursinho, be it studying German twice a week or enrolling in online courses from the Justice Ministry about drug policy. (I did this once along with a colleague in the Brazilian media, though I can’t say I finished it.)\nI lived and worked in Brazil for six years. There, I watched in myself what it meant to become fluent in a new language as an adult – I could take phone calls from strangers, have a store sign come into my visual field and not be able to not read it, hear kids prattling and usually make sense of it. When I began to contemplate a move to a new region, I was delighted to find a very affordable new Arabic course offered at Rio’s Lebanese Consulate. My Brazilian teacher there was enthusiastic about developing a communicative approach to teaching the language, so we studied songs, watched kids’ TV programs, and wrote plays about animals and princesses while we glossed over things like case endings and conjugations for pronouns like antumaa and huna. We called ourselves the habibinhos, a word so adorable in Portuguese that it makes us sound like we’re in diapers. After an enormously challenging day of doing addition and subtraction with Arabic numerals, one habibinho said that we may never learn this language, but at least we were delaying our onset of Alzheimer’s.\nWhen the stars aligned for me to move and be able to bump up my Arabic studies, I chose the Qasid Institute here in Amman for its intensity, immersion classes, and communicative approach to teaching. It’s kicking my butt. Two weeks in, my head is not yet above water.\nWe have class for three hours a day and at least as many hours of homework, and I work a part-time job to pay for my studies. On top of that, I’m starting an Ammiya supplement at Qasid twice a week. While my classmates often have university studies of Arabic under their belt and are comfortable speaking about the Syrian war or identifying whether a verb in mansub or marfua, I’m struggling to speak about anything beyond my daily life (family, friends, food and animals!).\nI thought a lot about my motivations before I left my comfortable and lovely life in Rio to chase a goal that may lead me down a rabbit hole. I’ve spoken with many friends and colleagues who studied Arabic at some point, and I hear repeated stories of dissatisfaction. It’s important for me to define clear goals and make meaningful progress toward what fluency means for me – I’ve decided that means communicating with a variety of people in a new language and reading the kind of texts we see in daily life, like a newspaper article or a Facebook post.\nThis time in Amman is unlike any other in which I’ve been a student: I chose to leave better pay, professional reputation, and satisfying work in order to work toward a new goal. It makes the move more compelling since I feel a strong sense of ownership, but it also raises the stakes. If I took my time, money, and left my friends and family, this is no longer just a leisurely hobby.\nRather than just study Arabic, I’ve dedicated significant time to studying how to learn a challenging language. In my independent study, I use many methods that will be familiar to followers of Alex’s blog and podcast, like Anki spaced repetition flashcards and learning new words in context or in stories. I also use many of the tools laid out in Gabriel Wyner’s Fluent Forever, like using Google image searches to try to discover the meaning of unknown words without translation and finding native speaker pronunciations through sites like Forvo. I am also a very big fan of TalkInArabic.com’s videos and transcripts for Ammiya/colloquial practice.\nI’ll be working with Alex as a coach over my upcoming months as I study at Qasid and meet weekly with a conversation partner I’ve picked up here in Amman. You’ll see my blogs on a weekly or biweekly basis, and I hope you see some forward movement in them. Expect upcoming posts to be shorter and more technical.\n\nEven if I said this is no longer just a leisurely pursuit, I also don’t want to lose that quality that I said I admire in Brazilians – taking small sips of a new subject just because it’s interesting and novel, without having a deadline or an endgame. At our orientation at Qasid, one of our administrators said that Arabic learning is a marathon, not a sprint. I’m an athlete, and that comparison felt spot-on.\nAnd in that spirit, I’m ending my introduction here with a selfie I took at the Rio Summer Olympics with my favourite athlete, American marathoner Meb Keflezighi. It wasn’t an easy selfie to get: I was exhausted from work during the games (thank to Ryan Lochte), but gathered enough awakedness to get up early on the final Sunday of the event to watch the men’s marathon. I used my press pass to sneak into the cool-down area after the finish line in Rio’s samba stadium and approached the legend himself to ask for the picture. It was worth it."
  },
  {
    "objectID": "personal/2017-01-23-adjustments-deep-end.html",
    "href": "personal/2017-01-23-adjustments-deep-end.html",
    "title": "Language Learner’s Journal: Adjustments at the Deep End",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and coaching work together with me. For other posts in the series, click here.]\nI’d be lying if I said this wasn’t a hard week for me. Our homework load is heavy at Qasid, and it means most days I go straight from class to work to homework. On a big picture level, I’m trying to keep my head above water by maintaining perspective that studying is a privilege and it’s not every adult who has the flexibility (thanks to the nature of freelance journalism and independent contracting) to go back to school.\nOn a more detail-oriented level, a few strategies are making me feel more orderly about my studies. One of my recurring issues that that I often feel so deep into tedious grammar that getting homework done is less about comprehending the subject and more like squirming like sightless creepy-crawly that by trial and error finally happens upon ground level. That doesn’t do me much good, so when I see that feeling coming on, I try to stop and find a resource, like my grammar book to remember some basics, like what’s the actual meaning of the pronouns I’m using or what again are the meanings associated with each verb wazin. If I’m going to take an hour to write out verb charts, I can take an extra ten minutes to make sure I understand, even if it’s a fleeting understanding, each line I’m writing.\nSome of Alex’s tips have helped me make more of the time I have in the classroom and independent study:\n\nSped up my anki card-making by using the cloze template and by challenging myself to have fewer clues (i.e., not searching for pictures or a perfect simple sentence with a new word, but mostly using the Al Kitaab and Qasid’s Ammiya course’s sample sentences and audio). This has meant a lot – it means I maintain a lot of my Al Kitaab and Ammiya vocabulary from my supplementary course in my day-to-day reviews. Also, I notice many of my classmates using Quizlet, and it is indeed tempting when we are short on time. Al-Kitaab vocabularly has already been uploaded many times over so I could download a premade deck, but usually it’s just in translation (i.e., an Arabic-to-English flashcard with a single word on each side). Making my own Anki cards certainly takes time, but I get a lot of reading and listening done in the process.\n(A note on Al Kitaab: A high point for me is continually realizing that the textbook is well designed and routinely feeding back to me vocabulary I’ve already seen. I study each chapter’s new vocabulary by looking through their sample sentences in the answer key / read aloud in the audio CD; even if I think I don’t understand a sentence at first glance, about three-fourths of the time I am able to figure it out if I read it again or listen to their slow, deliberate pronunciations. Gabriel Wyner talks about the power of the “ah-ha!” moment in language learning, or, straining a bit without any outside help from a translator and finally being able to call up the meaning to a word to which you’ve been exposed. It may sound simple, but it means in practice: Don’t run to a dictionary each time I don’t understand a sentence. And it’s been key to how I’ve been able to keep recognising a good bit of our new vocabulary which would otherwise be like pouring sand into a sieve, given the volume of work we do.)\nWhen I have trouble speaking in class, “plan spontaneity.” My teacher starts each class by asking each individual student how their days have gone and what they have done; I’m using that as an opportunity to come up with any sentence slightly more interesting than, “I did homework” and try to put a new verb into practice. Also, when time permits, I glance ahead to the exercises on our syllabus for the next day so I can have an idea of what they are about as soon as we open the page to them.\nSetting a stopwatch to see how long my homework is taking me and tracking my time spend on various language learning activities each day (reading, writing, watching TV/videos…). This doesn’t necessarily speed me up, but it does, if I’m pooped out by the end of the day, give me a sense of how I spent my time after class and work. As Alex predicted, I’m spending the vast majority of my time on reading and writing exercises, which is fine at this stage, though I want to change that in the future.\nI paid extra attention to the ten verb forms and the patterns used to derive other parts of speech (like the masdr, the ism fael, and the ism mafaul). My initial instinct had been that it was nice to appreciate these patterns but that it would certainly be a mental overhaul to try to systematically memorize them. Alex called the wazin the base of the Arabic language, and even if I don’t have them memorized, I am seeing that recognizing some common patterns and being able to derive meaning from them is useful (like, “this is the masdr, it looks similar to a conjugated verb but it’s a ‘verbal noun’ and is often coming where I expect a conjugated verb.”)\nPlan language exchanges with topics and questions beforehand. I meet with a friend from my weekly running group for English-Ammiya exchanges, and we chat throughout the week on Facebook about what we’d like to talk about. It means everything to have a compass to guide us through what the other may be saying.\n\n[If you’re interested in coaching for your own language study, click here to find out more information.]"
  },
  {
    "objectID": "personal/2017-01-31-variations-on-a-pomodoro.html",
    "href": "personal/2017-01-31-variations-on-a-pomodoro.html",
    "title": "Variations on a Pomodoro",
    "section": "",
    "text": "I’ve recently started work on a large translation project (about which more in due course). This sees me grappling with the Arabic text for several hours each day. Early on, I tried replicating my ‘4 perfect hours’ approach that had worked so well when writing my PhD dissertation.\nTranslation is hard work, and tougher on the brain in ways I hadn’t first anticipated, not having done it full time before. So I’ve found myself adjusting my 45-minutes-work, 15-minutes-rest cycle to something closer to 25-minutes-work, 5-minutes-rest (with a longer 15-20 minute gap every 4 cycles).\nFor writing and anything where you’re trying to hold lots of pieces of information in your head, it makes sense to go for longer stretches of time if you can. 45 minutes was ideal for writing my PhD, since any shorter and I felt like I was loosing too much time rebooting all the information at the beginning of each new session.\nTranslation is far more piecemeal, in a way. I can work on one sentence at a time, at least in this stage of the process, so shorter sessions work just fine.\nThis is all just an extended way of saying that customisation is important, and whatever you read online (or hear on a podcast or whatever) needs to fit into your own life, needs and patterns rather than the other way round."
  },
  {
    "objectID": "personal/2017-02-04-deepening-my-studies.html",
    "href": "personal/2017-02-04-deepening-my-studies.html",
    "title": "Language Learner’s Journal: Deepening My Studies",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and coaching work together with me. For other posts in the series, click here.]\nAn encouraging friend said to me recently that learning a language is like peeling back the layers of an onion. I’ve been studying at Qasid for a month now, and her comparison helped me take stock of what’s happened since then – on a given day, often I can’t call up a specific new fact to tell you that I learned and mastered that day, but indeed I am learning, a lot. This week I gave a twenty-minute presentation to my class that I doubtlessly would have been incapable of doing before I started this program.\nSpeaking has been my biggest hurdle, and I’ve put a tip of Alex’s into practice over the past two weeks — to find five minutes a day to speak without stopping. I’d like to do even more than that, and Alex also suggested prompting myself with a picture and talking about it for an additional five minutes. But for the time being, the former is all I’m capable of fitting in between class, work, and homework. I’ve been meeting with one of my fabulous Qasid teachers and try to recap the day’s news for her, since I write a global newsletter each day and those events are fresh and relevant to me. We’ve gone over the Gambian electoral standoff, Donald Trump and the #MuslimBan, and the Chilean wildfires in recent days.\nIn general, I’ve also been taking up anyone on the opportunity to speak — the same teacher asked if I’d be a practice student for her to become an oral proficiency certifier, which meant we had a 45-minute one-on-one conversation, while another instructor offered to stay with students after our evening ammiya course to have a purely conversational section. To strengthen my speaking muscles, in the presentation I mentioned above, I went with Alex’s “planned spontaneity” approach rather that write it word-for-word ahead of time. I chose a topic that is easy for me — the history of Arab migration to Latin America — and wrote down a list of words I wanted to use during it, like تراث (heritage) and بارز (prominent), and glanced over my list between my powerpoint slides of pictures.\nThe next step I’d like to put into practice is getting more comfortable using the verbs I know and conjugating them correctly without having to pause and think them out. I see that I have my go-to words that I use the vast majority of the time that I speak, and I’d like to resist getting into a rut.\nAs per the advice of a teacher, I’ve also been incorporating an extra nice exercise into class — I keep a running list of the vocabulary I come across in class that are new to me (on my list now: مطابق, identical, الفرد, individual, مرادف, synonym) and, when our teacher prompts us to write a few sentences using whatever new grammatical concept we’re going over, I glance through my list and try to use a word or two. It helps me resist the tendency to rely on my go-to words. It’s nice because I have an easy time learning obvious picture vocabulary — like animals, food, etc — whereas I need to pay better attention to words that are a little more abstract.\nAlso, very nicely, since I have a midterm coming up that will cover five chapters worth of vocabulary, I’m feeling appreciative that Anki’s spaced repetition methods means that material is still in the cards I see each day. As per Alex’s encouragement, I’m pausing to come up with mnemonic devices for words that don’t stick easily for me. This is something I did routinely when I studied at a more leisurely pace and that I’ve left behind as I feel a little breathless.\nAlex has also been helping me think about my next steps after Qasid and finding the balance that’s right for me between speaking/dialect and reading/writing/fusha. I’ll share more about that once I finalize my next plans.\nI’m still as pooped as I was when I wrote my first entry, but whenever I catch myself veering into a self-indulgent pity-me territory, I do stop to remind myself: I may be having little in the way of leisure time these days, but, indeed, I did live in Rio de Janeiro for six years. And when I was in Rio, I dreamed of the day I’d be able to throw myself into my studies."
  },
  {
    "objectID": "personal/2017-02-14-how-to-learn-languages-without-teachers.html",
    "href": "personal/2017-02-14-how-to-learn-languages-without-teachers.html",
    "title": "How to learn a language without a teacher",
    "section": "",
    "text": "A key belief at the foundation of my language coaching work has always been that teachers aren’t needed to learn a language. Younger people and students of mine seem to understand this far more readily than people who grew up in a culture where ‘learning a language’ meant taking a course or enrolling with a teacher.\nI was very pleased, therefore, when I discovered that someone else had given a talk on this topic at last year’s Polyglot Gathering. Lýdia Machová offers ‘Language Mentoring’, which is essentially the same idea as the language coaching that I do. I found the talk fascinating as a fellow practitioner, but I think there’s lots of wisdom there for anyone seeking to study languages outside the classroom. Indeed, the whole point of the talk is sort of that ‘outside the classroom’ is in fact the place where language learning should mainly be taking place.\nSome people, she explains, want to be taught a language, but what she tries to do in her work is show them how to learn. It may sound like a semantic difference to most, but it is crucially important.\nNobody can teach you a language. You have to learn it yourself. Wishing otherwise is to avoid the fact that real work has to happen when you’re learning a language. This is a hard lesson for many to hear, but it is certainly something I stress to all new students I work with: you have to do the work.\nCheck out the rest of her talk for more on this solid approach to self-study and why working with a coach can help boost your language-learning efforts.\n[For more information on working with me to learn a language, visit* [languagecoach](http://languagecoach.io/coaching).io/coaching*. If you’re learning Arabic, check out my new book, Master Arabic.]"
  },
  {
    "objectID": "personal/2017-02-23-core-language-learning-beliefs.html",
    "href": "personal/2017-02-23-core-language-learning-beliefs.html",
    "title": "Core Language Learning Beliefs",
    "section": "",
    "text": "I have previously written about my core principles relating to learning. I contrasted fundamental principles and beliefs with the idea of ‘life hacks’. Today I wanted to get more specific and write an initial set of core beliefs that motivate and shape both how I study languages myself and how I encourage others to do the same through my coaching work.\nNot all of these beliefs are based in rigorous science, by any means, and one of the things I hope to return to is the extent to which my personal experience (i.e. ‘the way I’ve always done it’) has dictated these beliefs and choices about language learning.\nI’ll return to these over time to check whether I still agree with them. But for now:\n\nAnyone can learn a new language.\nThe speed which you learn a new language depends on the approach you take, the amount of time you invest and the resources at your disposal.\nJust fixing the approach (your attitude & your work plan) can transform your speed and your ability to learn.\n(But), in the end, you have to invest the time. (Ideally, especially for total beginners, 1-2 hours per day, 5-6 days per week).\nBeginning a new language is both the most fun and boring stage; fun because every day you’re increasing what you can do at a really amazing rate and you always feel like you’re improving, boring because there’s very little creativity to learning a bunch of words and drilling semantic structures.\nWe live in a golden age of resources and opportunity. (This will get even better in the coming years, with computing power and applied linguistics producing custom materials to support your learning.)\nIf you only speak English, perhaps pick an easy language that’s closer to English (like French) so you can gain confidence before biting off something harder like Arabic or Mandarin Chinese.\nStudents (mostly) want to speak as their first priority, but of all the skills, reading is a better and more useful focus.\nNobody can teach you a language; you have to learn it.\nConfidence is key. Polyglots are confident about language learning because they’ve done it before and thus know that it is possible. (Conversely, most newbie language learners set the bar far too low in terms of their expectations of themselves).\nEveryone is unique, but you should probably cover the bases in terms of what you study and how you study it.\nHaving professional capacity in a second (or third) language is probably the best way to distinguish yourself in the research and ‘knowledge work’ marketplace.\nLife is a subjectively fuller and richer experience with more languages."
  },
  {
    "objectID": "personal/2017-03-03-walk-around-the-block.html",
    "href": "personal/2017-03-03-walk-around-the-block.html",
    "title": "Walk Around the Block",
    "section": "",
    "text": "People increasingly live in cities. There’s a good chance that if you’re reading this, you live in a city. That’s good for exchanging ideas and stimulating opportunities, but it often means we don’t get as much movement as we’d like or as would probably benefit us.\nSo here’s a little trick I’ve used for the past three or four years. First off, you shouldn’t be working uninterrupted for much longer than 45-60 minutes at a time. When you take a break from work, get up and move your body. Ideally, leave the house or place where you’re working, walk round the block or the building. That shouldn’t take you much longer than 5 or 10 minutes. If you’re working for 45-minute segments, it’s an ideal amount of time and it’ll refresh you physically so that your next session isn’t hampered by waning energy levels.\nTry it out and let me know how you find it…"
  },
  {
    "objectID": "personal/2017-03-10-language-transfer.html",
    "href": "personal/2017-03-10-language-transfer.html",
    "title": "Audio Courses with Language Transfer",
    "section": "",
    "text": "For beginners, one of the most important tasks is to get a sense for how a new language sounds. You can do this by immersion and osmosis – living in a country, perhaps – but it’s hard and you’ll usually just get exposed to the same set of words and phrases: numbers, directions and greetings.\nAnother great way of hearing a decent amount of sounds and suitable materials is to quickly run through an audio-only course designed for total beginners. If your language of choice exists in Michel Thomas and/or Pimsleur, you can do those. They’re a great way to get your tongue around the language. But they can be pricey, which is why I wanted to mention another option: Language Transfer.\nThese are free courses in eight different languages which follow a similar model to Michel Thomas in that they are recordings of a small group class in that particular language. The courses differ in how far they take you, but generally speaking they offer an overview of the grammar and some useful phrases and vocabulary.\nIf you’re looking for a cheap way to get a start in something like German or even Turkish or Swahili, check it out."
  },
  {
    "objectID": "personal/2017-03-20-leaving-qasid.html",
    "href": "personal/2017-03-20-leaving-qasid.html",
    "title": "Language Learner’s Journal: Leaving Qasid",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and* coaching work together with me. For other posts in the series, click here.*]\nAfter two-and-a-half intense months, I’ve finished my course at Qasid. Though this didn’t always feel easy to see on a day-to-day basis, it’s extraordinary how much students there learn over a short period of time. On my first day, I couldn’t produce full sentences other than my go-to greetings and “I’m an American journalist in Brazil,” and over the weeks there, comprehended and participated in discussions about women’s rights, marriage customs in different cultures, literature, colonialism and occupation.\nEven if it kicked my butt (or because it did), I leave with a great opinion of the school. Qasid’s teachers are extremely well trained in how to instruct students in an immersive method – we only occasionally resorted to English words when, say, our teacher wanted to make sure we really understood a grammatical point at hand. My listening comprehension soared, as did my ability to read texts (each week’s lesson in our textbook revolved around one or two native texts). Also, I had the great fortune of my class whittling down to only two students, which meant that for three hours each morning, my classmate and I were responsible for answering every question and participating fully in every discussion. So much opportunity to speak in a comfortable, mistakes-are-fine-and-expected environment turned me into something of a chatterbox, though my enthusiasm is several steps ahead of my accuracy.\nAlso, a delightful unexpected benefit about Qasid is that a group of students and teachers stay the afternoon there in their study halls. That meant that while I worked on my computer after class, I was often surrounded by chatter in Arabic, both teachers engaging their students in fusha and many students who were native dialect speakers chatting amongst themselves.\nThat said, after speaking with several other language students and journalist colleagues, consulting Alex, and thinking about my goals, I decided to switch tracks from my original plan to study two terms at Qasid and then move back to the U.S. for a summer language institute to instead focus on ammiya here in Amman. I work a part-time job to support my studies, while most Qasid students are full-time exchange students. If all students there were exhausted from their homework load, I was 150 percent so. Journalist after journalist tells me they wish they had better dialect skills and, not being someone who has a “good ear,” i.e., I don’t pick up much language without studying it in a methodical way, I think it will be important to focus on a dialect in a structured setting.\nStill, I’d consider going to Qasid again in the future. In fact, I was part of a test group to try out a new study tool the are developing that would supply easy-to-access audio and videos to accompany texts and vocabulary we study in Qasid’s textbooks. It looks like a promising way to bridge the gap between reading comprehension and pronunciation of the words in the text, i.e., I often recognize words in a text based on their consonants and long vowels but am mentally (and inappropriately) filling in a fatha each time I don’t know the short vowels.\nAs for my next steps: I’ve enrolled in a twice-a-week ammiya course at Sijal and am already enamoured with the class. I tested into the advanced level, though the other students in the class are far ahead of me in dialect. That said, unlike with Qasid the first time around (when I asked to be placed down a level because I was having difficulty following the class), I felt comfortable sticking to this level since I indeed understand the majority of the lesson. I’ll also be taking private lessons to complement the group course.\nAnother choice I’ve been happy with is that I’ve also moved to a far more happening place than my last home in Shmeisani, which has meant a world of difference in terms of just having daily interactions. I try to look up the words of things I’m looking for before I hit the streets (most recently, شمعة، سبانخ، و لوز بدون ملح). I find most people are very willing to speak with a foreigner in Arabic, though this sometimes involves my telling strangers who respond to me in English “بحكي الإنكليزية شيء، انا برازلية). I will reflect on the merits of this and some broader thoughts on expat language learning/daily usage in a future post.\nI’ve also become a social media and technology ascetic, logging out of my accounts and using them only when something necessary is at hand. In addition to being an old soul who believes that technology is eating away at humanity and rewiring our brains like substance addiction, seeing the Facebook I see every day anywhere else in the world is not one of the reasons I came to Amman. It’s pleasant to let my eyes wander while I sit in a taxi or service and try to speed read the signs around me before they’re out of sight. I don’t think I risk جهالة anytime soon – I read plenty of news (it’s part of my job), but it’s confined to a couple of hours of work a day, and then I’m free.\nAnd as for that free time, another upcoming blog will be about independent study methods post-Qasid that I will develop with Alex to make sure I keep up the reading skills I learned there even as I switch into a dialect course."
  },
  {
    "objectID": "personal/2017-04-15-arabic-reading-importance-part1.html",
    "href": "personal/2017-04-15-arabic-reading-importance-part1.html",
    "title": "You need to be reading more to get ahead in Arabic",
    "section": "",
    "text": "“Millions of language students are trapped in vicious circles. They complain that they cannot understand what they read in the foreign language because they do not know enough words. So they do not read and they do not increase their vocabulary, and so they continue not to be able to understand. Then perhaps someone tells them how important reading is and persuades them to try again. So they sit down with their dictionaries, and they look up every single word that is new to them, and very often many words that are not new but that they ‘want to be quite sure about’. At the end of three hours they have got through half a page in a book, or half a column in a newspaper. They do this for three or four days, and then give up in despair, oppressed by the tediousness of it all. They are convinced ­ quite correctly ­ that they do not know enough words to understand ordinary books and newspapers. As a result their vocabularies stay more or less the same size as they were, and they complain that they are making no progress. They either become permanently frustrated and depressed, or just give in and give up. And it all happens because they have spent more time with the dictionary than with the language itself.” (Source: Gethin / Gunnemark - The Art and Science of Learning Languages, p. 89)\n\nReading is unjustly maligned. Lots of students of Arabic pass through Amman, Jordan, where I’m currently based, and it’s not uncommon to hear the refrain, ‘I’m interested in learning how to speak, not to read and write.’ This blog post is about how you should be reading, EVEN IF spoken is your ultimate goal. I hope I can convince you of this fact, and entice you with some of the ways reading will enhance your study and understanding of culture and history.\nA large part of my book on getting out of the intermediate plateau in Arabic language learning is about the importance of reading.\nReading is the most useful activity to help an Arabic student stuck at the intermediate level. Even if your priority is to be functionally competent in a dialect, reading is still useful because it brings holistic improvements to your language abilities as a whole. The basic approach that I outlined in my book runs as follows:\n\nFind good materials\nRead lots of material at an appropriate level\nKeep going and be regular\nCross-check for performance\n\nOnly the final point needs explanation. When you are reading lots of things in Arabic over a period of a few months, you may not be aware of how you are improving, or at what rate. For that reason, it’s worth putting in place some checks and points to review every month or two.\nThere are a number of different tactics we can use when reading, and I’ll get into the details of these below, but for now you should bear in mind that it helps to switch around the reading tactics that you’re employing every so often. At a very high level, you can shift from deep / intensive reading to wide / extensive reading. You can find ways to write and speak that will help you ‘activate’ the words and topics that you’re learning.\nThe primary benefit of reading is that it increases your vocabulary. After you’ve done a few years of Arabic study, you will probably realise that the way the language works (particularly the way the verb system supports how words are generated / derived) means you really need to work on your vocabulary if you’re going to emerge into the advanced levels of comprehension. Moreover, the size of this challenge — the number of words, that is — is such that whatever tricks or skills that got you this far aren’t going to be useful or efficient in surmounting the problem.\nThis is where reading comes in. If you can split the words you know into active and passive categories, reading is immediately helpful in increasing your passive vocabulary. (Active vocabulary are the words that you can produce easily in writing or in conversation, while passive vocabulary are the words that you can recognise when you see them in Arabic, but couldn’t necessarily produce or use them yourself). When you read books at the right level, you can figure out the meaning from the context. (This is how we learn new words and figure out meaning in our native tongues). This will rapidly increase your passive understanding of texts (and, to a lesser extent, things you hear). There is some transfer of these words into your active vocabulary, but that transition is not guaranteed and it usually takes a bit of extra work. This work usually involves writing of some kind.\nAt its core, the work of increasing your vocabulary comes down to how many times you are exposed to a particular word (or words). The more you are exposed to it, and the more contexts in which this happens, the better that will be for your ability to feel comfortable with the word. For example, if you hear the word for dish in a kitchen, a cafeteria, or in a restaurant, you will remember it when you have to order a meal at a restaurant.\nFrom this perspective, reading is cost-effective and time-efficient. It is much faster to pick up a book and read it than to have to schedule a lesson, or distract someone from what they’re doing. You are also much more likely to find a varied stream of content by using written materials than if you solely rely on conversations or videos.\nBeing able to read is a valuable skill. Numerous studies have shown that your ability to be professionally useful in a language benefits far more from reading skills than spoken ability. Many people tend to discount reading from their skill set because they feel like it will take them too much time to learn how to read. And even though this book discusses listening and reading practices, reading is key to intermediate Arabic study. By not reading Arabic you are missing out on a great opportunity and a great way to distinguish yourself from your peers. Think what you could do and add to your work, career or discipline if your reading ability in Arabic was as good as that in English.\nReading connects you to other people, generations and eras. It’s the easiest way to mind-read and time-travel on the market! If you’re stuck in a country outside the Arab-speaking world, this is how you can get in touch with people, their culture, their history, their entertainment and all sorts of other dimensions. In terms of efficiency, reading is at the top of the list in this sense.\nReading is a badge of competence, both in how peers see you but how Arabic-speakers perceive you. If you are able to confidently navigate the written word, you’ll have more opportunities, people will take you more seriously and you’ll have a generally better quality of engagement as a result. Given that the way to reach that is enjoyable and — all things being relative — not too time-consuming, it’s harder to see reasons why you wouldn’t want to be reading more.\nIf a command of dialect / spoken Arabic is important to you, or your writing, perhaps, reading offers a break from that skill work and affords great opportunity for cross-training. The words you learn while reading can and do transfer over to other domains.\nArabic written by professional writers (whether fiction or non-fiction) often has the unfortunate tendency to be very verbose, using multiple synonyms for a similar meaning. In fact, finding an obscure yet beautiful adjective is almost a badge of achievement for many writers, particularly in older generations. This makes taking the dive into authentic reading materials for language students much harder. Luckily, publishers have been busy at this and a large number of intermediate- to advanced-level readers are being released. This is good news as you can now easily bridge the gap between basic textbook sentences and authentic material, whereas this wasn’t as possible even a few years ago. (It wasn’t the case when I first studied Arabic, for example). There are also a number of useful technologically-enabled services which allow you to ramp up and gradually increase this difficulty level. Moreover, you can follow your interests much more, whether you’re interested in fiction, politics or history, there are appropriate materials available. When you don’t read Arabic, you’re actually missing out on this huge conversation and discourse which is happening right this moment - whether its on Twitter or in op-ed columns - and which has been happening for as long as there have been Arabic speakers and writers.\nReading is an incredibly flexible skill to practice. You can do it almost anywhere, using physical books or digital materials. It works for short as well as long time slots, and, best of all, it’s fun! Particularly when you’re working on extensive reading, you can really lose track of time when reading a good novel or following whatever blog covers the topic that really interests you.\n[This is the first in a series of posts on the importance of reading in learning Arabic. The next post will summarise an academic study of the role of reading in bringing students up to the highest levels of achievement in their Arabic proficiency.]"
  },
  {
    "objectID": "personal/2017-04-29-guest-post-can-robots-be-language-coaches.html",
    "href": "personal/2017-04-29-guest-post-can-robots-be-language-coaches.html",
    "title": "Guest Post: Can robots be language coaches?",
    "section": "",
    "text": "I wrote a guest post over on the Learner Coaching ELT blog:\n\nI read From English Teacher to Learner Coach only recently. I received a glowing recommendation from a fellow language coach, Lýdia Machová, and the more I read, the more enthusiastic I became. The methods described in the book mesh pretty well with those I developed through my own studies of various languages. This was an approach suited not only to those learning English, but rather something that could (and should!) be adopted by the thousands of native English-speakers who struggle to learn a second language each year.\n\n\nIndeed, the general message of the book – one I interpreted as ‘we don’t have to wait to be taught; we can learn for ourselves’ – is something that I believe has important implications for how we go about education in the coming decades. (I was especially pleased to learn a new word – heutagogy – for this somewhat hard-to-define practice of self-education). Seth Godin has written about the demise of the system of factory education and others are tackling this from various angles.\n\nRead more…"
  },
  {
    "objectID": "personal/2017-05-24-meaningful-leisure.html",
    "href": "personal/2017-05-24-meaningful-leisure.html",
    "title": "Language Learner’s Journal: Meaningful Leisure",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and* coaching work together with me. For other posts in the series, click here.*]\nIf the first phase of my Arabic study in Jordan was intensive textbook fusha and the second was track-switching ammiya classes, this third and current could be called meaningful leisure, or, hanging out around town a lot and making friends.\nWhen I went to Bombay for an extended stay in 2010, a journalism colleague gave me a piece of advice: “Take everyone up on their offer to hang out with you.” It may sound “duh,” but over the years living abroad, I’ve seen how foreigners spend their free time in ways that often diverge from how residents in a given city do so. When we, as gringos in Rio, may have wanted to go to foreign film festivals or paragilding over the beach, many of our Brazilian peers would be going to baby showers, a classmate’s thesis defense, or Outback Steakhouse. All of those activities are great ones, and I think the spirit of my colleague’s advice was: If you want to get to know a culture, let your host take the lead and show you how they spend their free time.\nThat means over the past few weeks, I’ve sat on the sidewalk in front of a gift shop with a delightful young sculptor and a store clerk, my partners in very unstructured language exchanges that break when one of them needs to pop into the shop to attend a client. I went for a 6:30 a.m. workout with two of the fastest runners in Amman, a pair of brothers I met at a sunset race in Wadi Rum as we waited in the dunes watching for headlamps of other runners finishing. I went to a capoeira performance at Jadal cafe that was held in commemoration of the nakba; I was pleased with how accessible the discussion after the performance was for me, particularly when an older man in the audience vigorously questioned the capoeristas as to why they needed to do someone else’s sport when they could do dabke.\nAlex often talks about “islands” of vocabulary, and I thought about that as I spent more time with the same people and can make good guesses about the words they’re using. (As I crossed the finished line at the race, other runners asked me ايش كان مركزك؟ though I certainly hadn’t run fast enough to place. It was satisfying, though, to deduce what they were saying.) The store clerk and I talk often about money and salaries, since she hustles to work two jobs to help her family out.\nI could be more purist; I speak plenty of English in these interactions. I’m still searching for the point of equilibrium between taking advantage of each opportunity I get to speak in Arabic while (of course!) having genuine friendships with peers with whom I share interests (running, yoga, current events, feminism, vegetarianism, pets). Plenty of the vocabulary and references regarding those topics are in English, not to mention the people who are interested in them often read and speak in English about them. I don’t believe every friendship needs to be instrumentalized for one’s language-learning goals (though I believe even more strongly that such an attitude should not be a lofty cover for native English speakers kicking back and relaxing). When I told Alex about my happy sidewalk sessions, which qualify more as bilingual shooting-the-shit than a proper language exchange, he said: You’re doing the real thing, rather than practicing for it.\nSome working notes, now, on practice:\nI’ve been happy with my second time around testing out language exchanges; I’ve used the website Conversation Exchange, which I had suspected could be out of use by its retro web design but is actually popping. I’m pretty strict about where I meet the person, i.e., it needs to be as quiet as possible (a first exchange at Indoor cafe across from the University of Jordan was really hard to decipher and, from my point of view, turned into disjointed monologues rather than a conversation because I couldn’t hear her well).\nI think the exchanges, for my current level, are less experimental zones and more consolidation ones. That is to say, I don’t risk and try to reach for vocabulary I’m shaky on but work with what I know decently. That’s why I like coupling the exchanges with private classes, which I go to twice a week and are a better place for reaching and experimenting. I also think that in a language exchange it is useful to ask my partner “is the way I said that correct?” but not productive to ask “why?” I save those questions for my teacher.\nAlex encouraged me to discover certain transition phrases (على فكرة… على كل حال… بالرغم من) and put them into practice in my speech, which give the impression of being more fluent and conversant than I am. This has been a fun exercise with my private teacher, since I take the English phrases I want and try to describe to her a situation that I might use them.\nI’m on board with the many lines of criticism telling us that we need to make an active effort to start unplugging our lives before we turn into cyborgs; that said, having a round of friends here I chat with on Facebook or Whatsapp has indeed been great practice for seeing spelled out how people are saying what I hear each day. In conversations, I still feel like I rarely could repeat back word-for-word what someone has said to me, even if I usually get the message through key words and context.\nI bought Diwan Baladna, an ammiya vocabulary book organized by subject matter. I really like it – my hope is that it will help me turn a lot of passive vocabulary into active vocabulary. I have a quibble with the audio component (read too fast in long audio files that make it tedious to isolate the word I want. And having sample sentences is far better than English translations!).\nAnd finally, as per Alex’s encouragement, I continue to avoid dictionaries and translation apps. I make ample use of Reverso Context, but only after I’ve read a message or passage several times through, and usually I’m using it to confirm my guess of a word’s meaning is true. Especially when it comes to Whatsapp and chatting, the majority of messages I am receiving are ones that involve words I know well (Want to meet at this time? How far did you run today? I have foul and rice my mom made, want some? It’s veg.)"
  },
  {
    "objectID": "personal/2017-06-19-big-long-book.html",
    "href": "personal/2017-06-19-big-long-book.html",
    "title": "Robert Caro’s Big Long Book",
    "section": "",
    "text": "One million words. More.\nThe Power Broker is the kind of book you see in a bookshop, pick it up, put it back down again because its weight is a physical assault on your wrist.\nIt isn’t available as a digital edition for Kindle, so you’re stuck with lugging the physical copy around with you. I started this book in February, setting up a Beeminder goal to keep me on track. The pages are large and the type small, so even a goal of reading twenty pages a day took 30-45 minutes.\nI persevered, despite some weeks where I was travelling and therefore separated from the 1.3 kilogramme tome.\nIt was worth it. At over a million words, Caro obviously has the space to include a good deal of detail.\nRobert Moses, if you’re still reading by this point, is the subject of this biography. Moses worked in city planning, transportation and huge building projects in the New York state area (and some other big ones elsewhere in the United States). Caro’s book details his life story, showing how Moses’ drive ‘got things done’ in and around the city. Whether it was building roads or huge buildings, Moses was a force to be reckoned with in political as well as administrative terms.\nCaro approaches his subject from a number of different perspectives. He appears to have read a huge amount of the raw primary source material in the public domain as well as interviewed 522 individuals close to or involved in Moses’ work. That he managed to keep the book so readable is almost a miracle in and of itself. Every page draws you in, details and stories and glimpses that make Moses and the times in which he live come to life.\nThere are so many amazing details and sections to this book that picking any one would detract from the whole. If you have the time (and the stamina), give this book a read, even if you have no prior interest in urban planning. Highly recommended."
  },
  {
    "objectID": "personal/2017-06-26-the-exile-review.html",
    "href": "personal/2017-06-26-the-exile-review.html",
    "title": "The book you need to read on bin Laden’s life post-2001",
    "section": "",
    "text": "There is no shortage of books on the September 11 attacks and their aftermath. Some stand back to analyse it in terms of trends and networks, seeking to explain the ‘why’ through abstractions. Others have written participant accounts or their histories from the sidelines. The Exile offers a fulsome corrective to this trend towards abstraction. Curious what life was like for bin Laden, his commanders and their families? Cathy Scott-Clark and Adrian Levy deliver in spades.\nThe beating heart of this book are the stories of bin Laden’s wives, their children and their life in ‘exile’. The authors seem to have managed to achieve as yet unparalleled access to the wives and some other family members of Osama bin Laden, and their tale is both gripping and believable. The second important contribution that the book makes is to reveal Iran’s role in hosting the bin Laden families (and commanders) post-2001. The rich detail goes a long way to giving the reader a sense of the day-to-day frustrations of their lives in Tehran (and other places) as well as their interactions with Iranian military and government officials. The book would be worth its price just for these sections alone.\nChapter Eleven tells the story of the night bin Laden died, to a large extent from the perspective of his wives and family members. They also weave in accounts from US soldiers participating in the raid, but this is a perspective we have been denied till now and I think it is an important one. Indeed, the trauma faced by the children on that night (and throughout the years prior, for the most part unable to leave their home) is one of the understated but crucial themes that stand out from ‘The Exile’.\nEvery account of bin Laden’s time post-2001 has to grapple with the question of Pakistan’s role. The authors take a smart position throughout the book, which is to abstain from abstraction and a strong analytic voice. There are some claims of Pakistani ISI involvement and meetings here and there, but they step back a little before charging the government or senior officials with a state-level conspiracy. Whatever happened, this account holds, was much more an affair of bit players.\nThe book had the feeling of being rushed to press. It must have cost a lot to research the book, so perhaps the authors simply ran out of funds, but it seemed like there were so many other lines of enquiry that could have been started. The hardcover copy I read still had a fairly large number of typos, and it’s a shame that the authors use the word “Afghanis” to refer to Afghans.\nThe book is extremely readable – it kept me up until two in the morning as I finished it – and this is in large part because of the use of dialogue and building up narrative tension through conversations. Unfortunately, the handling of some of these conversations – reported through interviews with participants – strains credulity. Study of memory and oral history has shown how these kinds of memories degrade or get reshaped with each telling, and I wish there were more caveats throughout the book that what we’re reading is an approximation of what happened in order to better imaginatively enter the situation.\nAll in all, though, The Exile is an important book, an engrossing read and hopefully the beginning of more enquiries as others follow up on leads and side-stories raised in the telling. I took copious notes and will be digesting its various contents for a while, I think. It seems that scholars of September 11 and its aftermath are doomed to eternally reading and retelling the same events in slightly different contortions as new facts and witnesses emerge. If all the books were as good as this one, I wouldn’t mind so so much."
  },
  {
    "objectID": "personal/2017-08-13-phone-calls-timed-breaks.html",
    "href": "personal/2017-08-13-phone-calls-timed-breaks.html",
    "title": "Language Learner’s Journal: Phone Calls and Timed Breaks",
    "section": "",
    "text": "[This is a continuation of Taylor’s blog series where she details some of the week-in-week-out lessons that she learns through her Arabic studies and* coaching worktogether with me. For other posts in the series,click here.*]\nI haven’t written in this space in a bit because I’ve been taking breaks, both extended ones and daily small ones. At first I was anxious about “wasting” time by taking a sort of staycation holiday, but I’m seeing now it did me well.\nTwo friends came to visit me around Ramadan, and I threw a lot of my daily habits to the wind to enjoy having them here. One stayed an entire month – having a friend from Rio de Janeiro to pound pavement around this city with me during the unique time which is Ramadan was a fine way to enjoy what is otherwise a rather quiet and still time. I think together we made as many Jordanian friends as I had otherwise made in the six months before she came. We went to multiple yoga classes, including the Indian embassy’s “International Day of Yoga,” took a makeup shopping spree with two young women we met at Amman’s Roman amphitheatre and waded through Wadi Mujib with a guide who seemed happy to have any clients during a slow month. There’s something to be said for the “reset” button you can push when you have the novelty and joy of seeing a place through a visitor’s eyes.\nEven as I wasn’t studying formally and took a month off from classes, I still had useful, spontaneous ways to put my language to use. I got a speeding ticket on the way back from the Baptism site, which taught me some useful new vocabulary words (مخالفة غرامة، رخص). I even tried to argue with the officer that we couldn’t see the sign because it was covered by a tree, but I was not successful (that said, the ticket was only 20JOD). During a week-long trip to Greece, a friend who took me to Athens’ central fish market told me one of the men working at a booth was Syrian, so I went and introduced myself. It is a motivation super-charger to see how someone lights up to hear their own language when everything else around them sounds like Greek.\nWhen my trips were finished, I sat down with Alex’s “Mastering Arabic” to review and refresh my study methods. I’m doing decently with motivation, a topic Alex covers at length. Learning Arabic has always been a project stemming from intrinsic motivation, something I chose to do for and by myself. I have no hoops to jump through to please an employer or scholarship committee – for now, I define my own success, which usually is satisfaction at being able to make a phone call or recalling a precise and useful word I’ve studied and putting it to use in the real world.\n“Mastering Arabic” did prompt me to think more about time organization methods, which I had let slide into just some vague sort of maximalism (“I have the evening free and will study as much as possible.”) Two methods that have been useful are to divide tasks into discrete parts and then “reward” myself by taking a break afterward. That may mean: Read one story in حكايتهن from the UNRWA and then take a break by chatting on Whatsapp, or make Anki cards for 45 minutes then go do pushups while listening to Despacito.\nI’ve also made more active use of a language notebook. In the past, I took notebooks to class, but wasn’t proactive enough to take it to stores or pull it out as soon as a friend and I part ways. One of my favorite exercises now is to try to recall new words or types of sentences I heard the soonest possible after I finish an activity – that recently included زبيب and شمندر after trips buying groceries, and اخبرني عنك and شو خطة الليل؟ when I listened in on phone conversations. I heard a friend say …كنت رح اشتغل في محل بس كان عندي مشكلة and jotted it down. I’ve been feeling that what I lack is many basic speaking forms and daily vocabulary, and as simple as many of these things are, I didn’t know them beforehand – I wouldn’t have know how to say “I was going to do X” and that it was such a simple construction. I’ve long been a podcast fan, and now, when circumstances permit, I listen with a notebook beside me to jot down anything new. (My favorite new podcast discovery is عيب، which is in Jordanian colloquial Arabic and on subjects very accessible to me.) I’ve returned to private classes twice a week with a fabulous new teacher, so I bring her the questions that have accumulated in the days between our meetings.\nAlex is a big proponent of making phone calls, which for me is one of the final frontiers for language learning – it’s hard to have no visual cues and introduce yourself coherently when you are a disembodied foreign voice over the phone and ask for things like interviews or deliveries. Thankfully, each successful call boosts my confidence a bit more, though I can’t say I’m great – I stutter and stammer a lot, and I speak much more simply than I know how to speak if you gave me a quiet space and time to compose my thoughts. Still, to my delight, I was able to speak several times with a business whose employees I sought to interview, to my landlady to complain that she’s been locking our veranda door unnecessarily, and to order food deliveries and try to explain where I live (“between the liquor store and the dukan of fulan”). My comprehension rate is variable – I understand nothing my building’s haris tells me on the phone other than a general sense that he is granting or denying my request, but, thankfully, he seems to understand me.\nA final reflection on taking breaks: When I was hosting visitors, I had moments of stress about how behind I was getting. I could do some calculation for you about how many words I “lost” by kicking back and speaking more English, Portuguese, and Spanish on a daily basis than Arabic.\nThat said, the break served me well. One, it was nice to reflect on what language skills are – not a one-time performance but a world of abilities we keep in our pockets and pull out often at unexpected times. I haven’t been in a Spanish class in about ten years, but I still am making friends and enjoying pop music in the language. Many of Alex’s techniques are about how to maintain language abilities, i.e., not how to just do well on a test but recall vocabulary even several years from now.\nTwo, athletic comparisons have always made sense to me when it comes to tackling this difficult project which is learning Arabic. My Rio de Janeiro guest added a new layer to that analogy when she began to show me what she was learning in her advanced yoga and acrobatics classes back home. My mode of exercise had long been about having time to daydream while I threw my body into motion, muscles and mind unattached while I ran ten, fifteen, twenty miles with little effort and thought. A pushup, on the other hand, or playing a competitive strategy-based sport was always difficult to me. The difference, my guest said, is attention to form and control and knowing that it won’t “be natural” and effortless in the beginning. We practiced yoga chaturangas and headstands, two exercises that require total concentration and engagement rather than “zoning out.” We also talked about rest. Nobody will run well if they run every day; yoga always ends with a savasana “corpse pose.” The rest makes us more energetic and more engaged the next time we work out, pushing forward the frontiers of what one day will become natural for us.\nP.S. – This blog writing was also a timed activity. I went a little over my allotted hour-time block, but still wrote swiftly. Now, I’m off to do some pushups."
  },
  {
    "objectID": "personal/2018-01-17-tabula-extracting-data-pdfs.html",
    "href": "personal/2018-01-17-tabula-extracting-data-pdfs.html",
    "title": "Tabula for extracting table data from PDFs",
    "section": "",
    "text": "Have you ever come across a PDF filled with useful data, but wanted to play around with that data yourself? In the past if I had that problem, I’d type the table out manually. This has some disadvantages:\n\nit is extremely boring\nit’s likely that mistakes will get made, especially if the table is long and extends over several pages\nit takes a long time\n\nI recently discovered a tool that solves this problem: Tabula. It works on Windows and Mac and is very easy and intuitive to use. Simply take your page of data:\n[caption id=“” align=“alignnone” width=“2040”] A page listing Kandahar’s provincial council election polling stations from a few years back. Note the use of English and Dari scripts. Tabula handles all this without problems. [/caption]\nThen import the file into Tabula’s web interface. It’s surprisingly good at autodetecting where tables and table borders are, but you can do it manually if need be:\n\nThen check that the data has been correctly scraped, select formats for export (from CSV to JSON etc):\n\nAnd there you have it, all your data in a CSV file ready for use in R or Python or just a simple Excel spreadsheet:\n\nNote that even though the interface runs through a browser, none of your data touches external servers. All the processing and stripping of data from PDFs is done on your computer, and isn’t sent for processing to cloud servers. This is a really nice feature and I’m glad they wrote the software this way.\nI haven’t had any problems using Tabula so far. It’s a great time saver. Highly recommended."
  },
  {
    "objectID": "personal/2018-01-24-postgresql-install-mac.html",
    "href": "personal/2018-01-24-postgresql-install-mac.html",
    "title": "Installing PostgreSQL on a Mac",
    "section": "",
    "text": "PostgreSQL is a SQL-type database system. It has been around for a while, and is in the middle of a sort of revival. Installing Postgres on your own system can be a little difficult. Last time I tried, I was helped through the process while doing the Udacity Intro to Programming Nanodegree.\nRecently I had to reinstall Postgres, and there were some useful improvements to the process when guided through it in my Dataquest lessons.\nPostgres.app is an application you can install on your Mac which simplifies a lot of the legwork, particularly when setting up new databases, servers and so on.\nWhen you want to install a commonly used Python library for interfacing with Postgres, psycopg2 is a good option. You can do this easily with Anaconda:\nconda install psycopg2"
  },
  {
    "objectID": "personal/2018-01-31-fuzzy-search.html",
    "href": "personal/2018-01-31-fuzzy-search.html",
    "title": "Fuzzy Searching and Foreign Name Recognition",
    "section": "",
    "text": "Here’s something that happens fairly often: I’ll be reading something in a book and someone’s name is mentioned. I’ll think to myself that it’d be useful at this point to get a bit of extra information before I continue reading. I hop over to DevonThink to do a full-text search over all my databases. I let the search compute for a short while, but nothing comes up. I tweak the name slightly to see if a slightly different spelling brings more results. That works a bit better, but I have to tweak the spelling several times until I can really claim the search has been exhaustively performed.\nAnyone who’s done work in and on a place where a lot of material is generated without fixed spellings for transliteration. In Afghanistan, this ranges from people’s names – Muhammad, Mohammad, Muhammed, Mohammed etc – to place and province names – Kunduz, Konduz, Kondoz, Qonduz, Qhunduz etc.\nDevonThink actually has a ‘fuzzy search’ option that you can toggle but it isn’t clear to me how it works or whether it’s reliable as a replacement for a more systematic approach.\nAs I’m currently doing more and more work using Python, I was considering what my options would be for making my own fuzzy search emulator.\nMy first thought was to be prescriptive about the various rules and transformations that happen when people make different spelling choices. The Kunduz example from above reveals that vowels are a key point of contention: the ‘u’ can also be spelt ‘o’. The ‘K’ at the beginning could also, in certain circumstances, become ‘Q’ or ‘Qh’. These various rules could then be coded in a system that would collect all the possible spelling variations of a particular string and then search the database for all the different variations.\nFollowing a bit of duckduckgo-ing around, I’ve since learnt that there are quite extensive discussions of this problem as well as approaches to solution that have been proposed. One, commonly referenced, is a Python package called ‘FuzzyWuzzy’; it uses a mathematical metric called the Levenshtein distance to measure how similar or not two strings are. I imagine that there are many other possible metrics that one could use to detect how much two strings resemble one another.\nI imagine the most accurate solution is a mixture of both approaches. You want something that is agnostic about content in the case of situations where you don’t have domain knowledge. (I happen to have read a lot of the materials relating to Afghanistan, so I know that these variations of names exist and that there is a single entity that unites the various spellings of Kunduz, for example). But you probably want to code in some common rules for things which come up often. (See this article, for example, on the confusion over spellings of Muslim names and how this leads to law enforcement mistakes).\nI may end up coding up a version that has high accuracy on Afghan names because it’s a scenario in which I often find myself, but I’ll have to explore the other more mathematically-driven options to see if I can find a happy medium."
  },
  {
    "objectID": "personal/2018-06-04-ml-with-weka.html",
    "href": "personal/2018-06-04-ml-with-weka.html",
    "title": "Machine Learning with Weka",
    "section": "",
    "text": "Learning to program is an infinite process. The field is as open and wide as you can imagine, and you are mostly constrained by your imagination.\nI spent much of May getting my mind around Go. I took Todd McLeod’s Go course on Greater Commons and learned a great deal. The course was somewhat short on practical implementation, however, and I’m eager to do things with what I learned. More on that in due course.\nA parallel strand of my studies has been in statistics and more advanced applications of statistical methods i.e. machine learning. I had done a bit of this in the past, but my poor foundation in basic statistics didn’t serve me well. I am now rectifying that through Andy Field’s excellent textbook.\nFor machine learning I decided to take a step back from the programming and use a graphical interface to start with. There are great APIs / tools available for this in most languages you can think of but I wanted more of a solid foundation in workflows around machine learning and the kinds of analysis that get done.\nI read through a good deal of Jason Brownlee’s blog(https://machinelearningmastery.com/) as well as his book on Weka and he made a good case for why Weka is a good place to start.\nI have noted a number of steps to move through in sequence, at the same time recognising that data analysis is often unsequential. I expect this to expand and/or redefine this over time.\nKaggle is one of the major hubs for machine learning practice (and learning) and I wanted to reengage there. The first data set they usually have you work on comes from the Titanic disaster. You take the full roster of people who boarded, including data points like their economic class, where they were staying on board the ship and their age/gender etc and use anything and everything in terms of tools to predict who survived and who didn’t. I had used this data set in the past when I was studying ML with Python.\nMy initial idea, therefore, was to take the .csv files from the Kagge competition and use them in Weka to come up with some predictions. Unfortunately, there are some idiosyncracies about the .csv file that make this difficult. Some of the attributes / columns in the data (like names) use punctuation marks which make parsing the csv data non-trivial. Weka uses ARFF files as standard but has the option to parse CSV data. It ran into quite a few errors when trying to crunch through the Titanic data, and no amount of basic fiddling would fix it.\nReading around a little, it seems that others have noted this problem in the past. One blog post tackled the problem head on but the solution didn’t really help me much in the short term. I’m now somewhat stuck, knowing that the fix to the problem is to use another language (Python, perhaps) to range over the data and process it in a form that will be more palatable for Weka. Alternatively, I could use it as an opportunity to build a short Go programme that could perform the same function.\nFor the moment, i’ve decided to do neither. I’m going to find an alternative data set which doesn’t require wrangling and fiddling. I know wrangling and fiddling is an important skill to master, but it’s not the skill I’m trying to focus on right now. Luckily, between the UCI Machine Learning repository and various other places, I’m not exactly lacking for examples / other data sets. Today I’ll work with the Pima Indians Diabetes data set which came built-in with Weka."
  },
  {
    "objectID": "personal/2018-06-13-ai-to-brahms.html",
    "href": "personal/2018-06-13-ai-to-brahms.html",
    "title": "From AI to to Brahms",
    "section": "",
    "text": "The past few days have been consumed by a flurry of meetups, conferences and other engagements.\nI attended the inaugural PyLondinium conference at the Bloomberg office in London. Lots of interesting speakers. Some went over my head but overall I was pleased at how much was accessible. It has been a while since I’ve done much Python coding, but apparently it hasn’t been completely forgotten. The day before the conference started I did a day-long hackathon with Trans*Code, an organisation that sets up these kinds of events in support of trans community issues in the UK.\nMuch of Tuesday was spent at CogX, a conference devoted to all things AI. There was a strong emphasis on the commercial application, and it had a distinctly different feeling to PyLondinium a couple of days earlier. Everywhere you looked at CogX, people were pitching, networking or sitting on the sidelines making pitch slide decks on their laptops, it seemed. I went to some great talks:\n\nHaiyan Zhang talking about innovation at Microsoft’s Research lab in Cambridge\nSarah Gold talking about safety and accountability in ‘learned systems’\nZavain Dar got philosophical and talked about how AI and machine learning breakthroughs were reconfiguring how we talk and think about empiricism\na really great panel on the intersection between AI and education, with a memorable set of contributions from Priya Lakhani.\n\nThere were many others. Then I wandered over to the ExCel exhibition centre for TechXLR8 which was quite disappointing.\n(This week is Tech Week in London, if you haven’t noticed, so there are many more events and meetups than normal, it seems).\nIn the evening I went to see the Belcea Quartet perform Mozart’s String Quintet (K515), Shostakovich’s Eighth Quartet and Brahms’ Second String Quintet. I haven’t heard the Belcea Quartet before, but they’re really something. The lead violinist and cellist have a great rapport, and judging from the sound seem to be playing really wonderful sonorous instruments. It was a warmingly affirmative reminder — after all the talk of machines and technology — that humans still have something to contribute."
  },
  {
    "objectID": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html",
    "href": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html",
    "title": "Diagnosing Diabetes with Weka & Machine Learning",
    "section": "",
    "text": "[I mentioned two weeks ago that I was working to dive into the practical uses of machine learning algorithms. This is the first of a series of posts where I show what I’ve been working on.]\nThe Pima Indians dataset is well-known among beginners to machine learning because it is a binary classification problem and has nice, clean data. The simplicity made it an attractive option. In what follows I’ll be mostly following a process outlined by Jason Brownlee on his blog.\nThe Pima Indian population are based near Phoenix, Arizona (USA). They have been heavily studied since 1965 on account of high rates of diabetes. This dataset contains measurements for 768 female subjects, all aged 21 years and above. The attributes are as follows, and I list them here since they weren’t explicitly stated in the version of the data that came with Weka and I only found them after a bit of digging online:\nThis video gives a bit of helpful context to the data and the test subjects:\nhttps://www.youtube.com/watch?v=pN4HqWRybwk\nI also came across a book by David H. DeJong called “Stealing the Gila: The Pima Agricultural Economy and Water Deprivation, 1848-1921” which describes how the diverting of water and other policies “reduced [the Pima] to cycles of poverty, their lives destroyed by greed and disrespect for the law, as well as legal decisions made for personal gain.” It looks like a really interesting read."
  },
  {
    "objectID": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#the-problem",
    "href": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#the-problem",
    "title": "Diagnosing Diabetes with Weka & Machine Learning",
    "section": "The Problem",
    "text": "The Problem\nThe idea with this data set is to take the attributes listed above, combine them with the labelling (i.e. we know who has been diagnosed with diabetes and who hasn’t) and figure out the pattern as much as we can. Can we figure out if someone is likely to have diabetes just by taking a few of these measurements?\nThe promise of machine learning and other related statistical tools is that we can learn from the data that we have to make testing more useful. Perhaps we only need your height, genetic risk factor and skin thickness to make such a prediction? (Unlikely, but still, perhaps…). If we emerge from our study with a statistical model, how well does it perform? How much can we generalise from the data? What would be an acceptable error rate in the medical context? Is it 80% or is it 99.99%? The former would save millions of dollars in test costs but would throw lots of errors; the latter would be highly accurate but it might be expensive to calculate the model.\nThe use case for this specific case would maybe be to identify at-risk individuals who are on the way to a diagnosis of diabetes and intervene somehow. Our motivation here is clear: people don’t want to be diabetic, so how early can we catch this transition? It would save governments money, expose fewer people to unnecessary tests and improve their quality of life.\nI’m not a doctor, but to solve this problem manually would seem to require monitoring of blood tests (glucose and insulin levels), perhaps looking at exercise and diet, and also weight. At scale across the population of an entire country, for example, this seems like it might get expensive and/or too much for one person to process in their head. The data isn’t too large or complex, but it still seems to be useful you’d want to automate it to some extent.\nThere are some potential ethical issues around the data. Everything offered as part of the table of data is anonymised, but there are some outliers (see below) that I have to believe wouldn’t be too hard to find. The applicability of whatever model comes from this data will likely only have a limited application — the data is drawn only from women, after all. I also noticed that while the data is no longer available on the UCI Machine Learning Repository website, it still comes packaged with Weka. There was a notice on the UCI site (which I can no longer seem to be able to locate) stating that the permission to host the data had expired. It is unclear to me what’s going on with the permission there."
  },
  {
    "objectID": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#data-preparation",
    "href": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#data-preparation",
    "title": "Diagnosing Diabetes with Weka & Machine Learning",
    "section": "Data Preparation",
    "text": "Data Preparation\nExploring the data using Weka’s explorer tool plus the attribute list above we can see that we have some blood test data, some non-blood body measurements and this genetic marker (presumably achieved through either blood tests or interview questions about family history). As I was working to understand the various attributes, it occurred to me that for this to be really useful, we’d want our model to work on data that wasn’t derived from blood tests; they’re expensive and they’re invasive. I didn’t get round to doing that for this round of exploration but it’d be high up on my wishlist next time I return to this data.\nThere are only 768 instances, so it’s still quite a small data set, especially in the context of machine learning examples. This is probably explained by the fact that it’s real medical data (so there are consent issues) plus the fact that it is several decades old and the processing power available then didn’t lend itself to processing mega-huge sets.\nThinking about what attributes might be removed to make a simpler model, I first thought that maybe the number-of-pregnancies might be dispensable, but then I thought to the number of hormonal and other changes that happen and I guess actually it is probably quite important.\nThere were some outliers in the data that I identified as needing further consideration / processing before we get our model trained:\n\nThere were some women who had been pregnant 16 or 17 times. They were on the far edge of the long tail, but I ended up leaving them in for the model rather than deleting them completely.\nThere were 5 people who had 0 as their result for ‘plus’, which seems to be an error. I decided to remove these.\nThere were 35 people who had 0 as their blood pressure, which seems to be an error.\nThere were 227 people with 0mm skin thickness. This is possible, but I think it’s more likely that no measurement was taken, at least for a lot of them.\nThere were 11 people who are listed as weighing 0kg. That seems to be an error.\n\nAfter I’d identified these various outliers I decided to make a series of transformations to the whole set. From this I’d emerge with three broad versions of the data:\n\nthe baseline dataset, with nothing removed or changed\nthe outliers removed completely and replaced with NaN values\nthe outliers replaced with mean averages for each particular attribute\n\nFor each of these broad versions, moreover, I prepared three separate versions:\n\nall values normalised (ranges and values for all attributes transformed to being from 0-1 instead of being in their original ranges. i.e. maximum weight as 1 and minimum weight as 0 etc)\nall values standardised (set the mean for the data as being zero and the standard deviation to 1)\nall values normalised and standardised (i.e. both transformations applied)\n\nProducing these various versions of the data was something I learned from Brownlee’s book, “Machine Learning Mastery With Weka”. It turned out to be somewhat fiddly to do in Weka. In particular, every time you want to open up a file to apply transformations the default folder it remembers is often several folders down in the folder hierarchy. By the ninth transformation (there were nine sets in total, by the end of this process) I was ready for a more functional / automated approach to these data conversions!\nWeka does offer some nice tools for the initial exploration of the data. Here you can see two charts that are generated in the ‘explorer’ application. First we have a series of simple bar charts visualising all the individual attributes. Then we have a plot matrix showing how all the various attributes correlate to each other (or not, as was mostly the case for this data set).\n[caption id=“” align=“alignnone” width=“2168”] Visualisation of Pima Indians dataset attributes (auto-generated in Weka) [/caption] [caption id=“” align=“alignnone” width=“1444”] Plot matrix showing visualisation of correlations between all attributes (auto-generated using Weka) [/caption]"
  },
  {
    "objectID": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#choosing-algorithms-and-training-the-model",
    "href": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#choosing-algorithms-and-training-the-model",
    "title": "Diagnosing Diabetes with Weka & Machine Learning",
    "section": "Choosing Algorithms and Training the Model",
    "text": "Choosing Algorithms and Training the Model\nGiven that I’m very much at the beginning of my machine learning journey, I don’t have any strong sense of which algorithms might be more appropriate or not for this particular data set. I knew that this is a classification problem and not regression (i.e. we’re trying to decide whether people have diabetes or not — two categories — instead of predicting where people fall on a scale / spectrum) so that ruled out a few options, but really the field was wide open.\nJason Brownlee advises taking a sample of around ten different algorithm families to get an initial sense of whether there are any clear outliers (either over-performers or under-performers). Once I have a better sense of the overall space, I can then tweak things, or double down on a particular algorithm family to select a more limited feature set perhaps.\nFor this algorithmic spot-check I chose 12 algorithms, sampling from all the main families as I currently understand them. Running this set, I immediately came across an error message: Weka was telling me that the function.LinearRegression algorithm doesn’t function for classification algorithms. I removed that and reran the tests.\nWhen doing this kind of test, it helps to have a baseline accuracy figure against which you can compare how much these fancy algorithms are improving predictions. In Weka, this is called a ZeroR algorithm and I think it basically says that everyone has no diabetes. For this dataset, it got 65.11% accuracy, which isn’t bad all things considered!\n(Note that everything here is being run through k-fold cross-validation where training and test data are kept separately, and then this is repeated ten times. The final results are averaged out between them. Weka does this all with great ease, making it pleasant to conform to best practices when it comes to data science).\n\nThis figure shows how logistic regression was the best performing algorithm out of the box at 77.47% accuracy. I read somewhere that it often performs well on binary classification problems, so this didn’t surprise me. Support Vector Machines (listed as SMO in the Weka GUI) are also supposedly quite good for binary problems and it was only two-thirds of a percent behind logistic regression. Using Weka’s tools for statistical analysis of the result, I came to the conclusion that LMT, logistic regression, SGD and SMO were all worth further exploration and tinkering.\nFor example, I tried the following with the Support Vector Machines algorithm:\n\ntweaking the value of c (complexity) to see if 0.25 performed better than 0.75, for example. It turned out that 0.5 was the sweet spot for the c value.\ntrying different kernels - I tried most of the options listed in Weka and they all performed pretty poorly. In particular, RBF (radial basis) was really poor.\n\nNone of my tweaks really seemed to improve the accuracy of the model. I imagine that some of the algorithms function better with more data, but I am not in a position to generate more.\nThe next step was to try some ensemble methods where the predictions made by multiple models are combined. In particular, bagging/bootstrap, boosting and voting were all recommended as options to try out.\n\nYou can see here that ultimately none of those outperformed logistic regression, which was surprising to me. I’m not at a place where my statistical understanding can explain why that’s the case — ensemble methods seemed to offer the best of many worlds — but I can’t really argue against the results.\nFinally, I tried the MultiLayerPerceptron to throw one possible implementation of Deep Learning at the problem. This performed pretty poorly as per default configuration."
  },
  {
    "objectID": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#findings-conclusions",
    "href": "personal/2018-06-18-diagnosing-diabetes-with-weka-machine-learning.html#findings-conclusions",
    "title": "Diagnosing Diabetes with Weka & Machine Learning",
    "section": "Findings / Conclusions",
    "text": "Findings / Conclusions\nThe best accuracy I was able to achieve on this data set was using a logistic regression model. This performed with 77.47% accuracy (standard deviation of 4.39%). We can restate this as an accuracy of between 68.96% and 86.25% accuracy on unseen data. This is slightly disappointing since it isn’t that much better than the ZeroR algorithm.\nTowards the latter stages of my work on this problem, I came across a blog post by someone who used a neural network to reach results of 95% accuracy on this same data set, showing that there are models that bring dramatically improved performance. I don’t understand neural networks enough to be able to evaluate what he did (i.e. to know whether this is simply overfitting or actually a performant / real improvement on my results). Nevertheless, it seems like a significant improvement.\nAs my first big push to work on a real data set using machine learning tools, this process was instructional in the following ways:\n\nWeka is easy to use and it makes some of the best practices in data science no-brainers to implement\nConstructing the various data sets, implementing the experiments to compare the algorithms and so on was made slightly tedious by the GUI interface. If I wanted to run through many more variations it would have been prohibitively tiresome to have to manually click through all the options.\nWeka is slow (or maybe my linux laptop is slow). Some of the algorithm sets I tried (Support Vector Machines, for example, or ensemble methods using SVMs) took 20+ minutes to run. The data set wasn’t huge at all, so I have to imagine that a real ‘big data’ set would make this kind of quick incremental exploration and iteration difficult to practice. Weka is, of course, a Java app and I’m running that on my Mac. I suspect that if I were to run similar algorithms through Python (or even better, C) on my Mac I’d get significant performance improvements.\nI have very little sense of the variation between the various algorithms, what each one does and where the strengths and weaknesses lie. I want to tackle this from two directions: improving my baseline understanding of statistics and also just getting more experience implementing them for practical problems such as this one.\n\nThe next problem I want to tackle is that of the UCI soybean dataset. Each instance describes properties of a crop of soybeans and the task is to predict which of the 19 diseases the crop suffers. Again, the dataset isn’t huge but it is a multivariate classification problem so there are new challenges to be tackled there."
  },
  {
    "objectID": "personal/2018-06-23-real-world-go-with-referenced-functions.html",
    "href": "personal/2018-06-23-real-world-go-with-referenced-functions.html",
    "title": "Real-World Go with Referenced Functions",
    "section": "",
    "text": "Writing small test scenarios and packages in the Go Playground, or in a single main.go file is all good and well, but in the real world things are more complex. (Or so I heard).\nToday I wanted to get a brief sense of how that all works. I had some sense of how to make this all work when working through the GreaterCommons course, but it proved harder when the training wheels were taken away.\nThis is what I emerged with at the end of my experiments. It is a main.go file that references extended.go. This ended up being dependent on knowing a bunch of setup quirks and techniques that aren’t particularly well signposted for beginners. I ended up finding a tremendously helpful document on the main Go site entitled “How to Write Go Code”. They don’t signpost it much, especially when compared to ‘Effective Go’ or the general specification.\nI worked my way through the official examples. Luckily they were simple enough that I could follow all of the logic. Everything was incrementally constructed, slowly becoming more complex. I learnt the following:\n\nit’s best to setup your workspace in $GOPATH/src/github.com/YOURUSERNAMEGOESHERE\nevery time I initialise a GitHub repository on my computer (rather than creating it at GitHub.com and then simply cloning it down to my computer) I have to relearn all the little commands relevant to making that work when pushing up to Github. (It’s sufficiently fiddly and reminiscent of using PGP. Lots of small moving parts and room for error. Big space for improving that user experience…).\nif you use the path listed above as your workspace, you can reference your other folders fairly easily. See my code for examples of that.\nThe packages you’re referencing (i.e. not in the central main.go file) can be called anything you’d like, as can the file names. If you want to make the functions in those packages globally visible, however, make sure the functions start with a capital letter. (In my example, I have extended.Extended).\n\nI am already demonstrating some sloppy habits with my documentation, tests, benchmarks and examples, I realise. For these kinds of toy demonstration examples it is less mission-critical, but there feels like a vague principle at stake.\nMy next Go exploration project will probably be the final version of an Exercism exercise I’ve been working on for a while."
  },
  {
    "objectID": "personal/2018-06-24-python-virtual-environments-testing-environments-and-markdown-strikethrough.html",
    "href": "personal/2018-06-24-python-virtual-environments-testing-environments-and-markdown-strikethrough.html",
    "title": "Python Virtual Environments, Testing Environments and Markdown Strikethrough",
    "section": "",
    "text": "I spent part of this afternoon fixing things in my PDF splitter code.\n\nI learnt about virtual environments and the various choices available in Python. This was the most useful overview. I ended up choosing pipenv which is also outlined here. It installs a Pipfile in your directory which is an equivalent to the old requirements.txt that was previously used. This means that whenever you use pip to install a new package, it’ll remember and update the file accordingly.\nFor testing, I ended up holding off for the moment. It wasn’t immediately apparent which of the various testing suites I should be using and the examples given in places like this used strange syntax. I’ll have to tackle this later, but for now I’m putting it on hold.\nI learnt that you can make some text strikethrough (EXAMPLE) in Markdown by enclosing the text in two tildes (~~).\nI read about application layouts / structures and made some initial decisions about what files to include in my project. Some of this is overkill for where I am currently, but soon enough this project will expand and I’ll need a standard structure format.\n\nTomorrow I want to start working on my regex search-and-rename function. I’ll start by figuring out the right regex string to use for my search, then I’ll figure out how to add in re.search into my script."
  },
  {
    "objectID": "personal/2018-11-05-pashto-podcasts.html",
    "href": "personal/2018-11-05-pashto-podcasts.html",
    "title": "Learn Pashto with Podcasts",
    "section": "",
    "text": "It’s been several years since I’ve actively studied Pashto. I’m returning to a somewhat stale vocabulary base. In these cases, listening is one of the more useful ways to reactivate words I once knew, or to learn new words from scratch in context. Podcasts are usually a really useful way to get that active language practice. For most big languages you’re usually spoiled for choice (think French, Mandarin, or even Arabic) but Pashto has fewer options despite around 50 million people speaking it worldwide (source for estimates here and here).\nHardly any of these are directly available via a search on a good podcast client like Overcast so you’ll have to manually add whatever you like via the RSS feed.\nThere are a few options that I’ve managed to find so for the few souls who are also looking, perhaps this might be useful:\n\nCELCAR Pashto Language Learning Podcasts (RSS)\n\nThis is no longer updated but you can find a bunch of audio excerpts read out. They’re intended as language study materials so they’re a bit more user-friendly for the intermediate language learner.\n\nSBS Pashto\n\nThis is an Australian broadcaster that offers news and cultural reports regularly in podcast format. You can find it on Overcast by searching for ‘SBS Pashto’. Episodes are sometimes very local (i.e. tailored to Australian news) but it’s a regular source of input nonetheless.\n\nVoice of America Pashto Evening News\n\nThis podcast / broadcast is released in the evening (Afghan time) every day. Episodes last 30 minutes and are made up of various small news reports and sometimes interviews. It’s a good way to stay up to date with the news and also keep your formal media Pashto up to scratch. The reports and stories are usually pretty interesting and engagingly told.\n\nDa Pulay Poray Drama\n\nThis is a serialised drama produced by PACT Radio out of Jalalabad. Unfortunately they don’t offer episodes in RSS format so you are forced to either visit them at their Facebook page (ugh) or their own website and manually click to listen. The quality of the material and storytelling is excellent, however, so I usually try to listen in despite the shortcomings in the delivery method.\n\nRFE/RL Azadi Radio (RSS)\n\nRadio Freedom - Radio Liberty offers a wide variety of their programmes for subscription as podcasts (via RSS). Some are more regularly updated than others but this is a useful and interesting set of materials to complement VOA’s more straightforward news broadcasts.\nI will update this page as and when I discover more. One of the big missing pieces is BBC Pashto. I can’t seem to find any of their broadcasts / materials online in a subscribable format."
  },
  {
    "objectID": "personal/2018-11-13-in-afghanistan-watching-someone-learn-to-code-for-the-first-time.html",
    "href": "personal/2018-11-13-in-afghanistan-watching-someone-learn-to-code-for-the-first-time.html",
    "title": "In Afghanistan, watching someone learn to code for the first time",
    "section": "",
    "text": "I recently had the opportunity to observe how a total newcomer to coding / computer programming starts their learning journey. She was embarking on this journey in Kabul, which brings some special considerations that I thought I’d outline here.\nFirst off, coding in Afghanistan? I actually happen to think it’s not the worst thing someone could do with their time. Power outages and internet costs, while better than they were in previous years, still make the prospect of spending hundreds of hours learning skills a precarious proposition. (Outside Kabul, just to give some context, even big cities like Kandahar are starved of electricity. When I was down there recently we had a couple of hours per day at best.) But all that to one side, there’s a need for skilled professionals in the computer science / engineering world and there seems to be a decent amount of room for remote work. (Note that there’s some debate about the extent of the shortage but I don’t think it’s getting ahead of myself to suggest that proficiency in computer science will be a valuable skill in the years to come.)\nIf you are going through university in Afghanistan you’re teaching yourself anyway, to some extent. The quality of the tuition — even in some of the private universities — isn’t great and the job market doesn’t reward degree certificates; it rewards competency, something which you aren’t necessarily going to get from a degree / certificate. To use some of the free resources available online would seem like a smart option. Worst case, you make yourself a little more employable because you took some initiative and have skills to contribute to a new employer. Another scenario is that you have some skills you can market around the world, all from the convenience of your home.\nAll of this requires English. It’s an unfortunate fact that there really is no way round learning English if you want to teach yourself computer science skills. There are ways round it, especially at the beginning, but if you’re going to progress you’ll really have to have a decent level of competency at understanding technical documents / tutorials and the like. I wish it was otherwise, but that isn’t changing any time soon.\nI wanted to recommend my friend learn via some resource that was at least partly in her native language, Dari / Persian. After all the fanfare of recent years about how young people are learning how to code around the world in a variety of languages, I was a bit disappointed at the options available. For Dari, there were basically no options. One site, code.org, had some initial courses that had been translated into Persian, but it was the Iranian version / dialect and this caused some problems in comprehending the instructions later on. Worse still, the courses were only half-translated. Videos in between units were in fast idiomatic English, with scenes and situations that seemed representative of some alien universe. Yes, it’s nice that Bill Gates was there to explain how variables work, but first let’s slow down and explain what a variable is, preferably without flashy graphics that don’t actually explain the topic.\nThere were various assumptions made about students’ experience and skills in mathematics. Early units in the code.org syllabus expect you to understand basic geometry and some algebra. I guess you have to start somewhere, but I wish there were ways of teaching this starting from an earlier baseline of understanding.\nI had initially thought that a good way to get my friend thinking about some of the bigger issues would be to have her use a Raspberry Pi and handle some basic Linux administration but that ended up being too many new things at one time. Some of the much-vaunted tropes about learning to code really don’t make much sense.\nOne other practical problem turned out to be that code.org is a data-hog. My friend ended up having to spend a non-trivial amount of money recharging her 4G phone (from which to tether her laptop) because code.org used fancy javascript animations and who knows what else. I feel like this is a serious symptom of a blind spot in Silicon Valley’s efforts at some kind of cultural imperialism. They can’t imagine themselves into the shoes of a nascent coder, so they produce these bloated websites that mean that while someone can maybe code for an hour or two using fancy javascript animations, maybe they won’t be able to afford the hundreds of hours that it takes to become competent.\nI feel like better skills to teach at this level are more fundamental: how do you figure out the answer to a problem? how do you solve problems in general? This is actually the real skill that you want someone to master, not whether they can get a turtle to draw a circle in 4 colours, and this skill is not taught at all in these courses. (A corollary: how do you find good / worthwhile problems? or how do you figure out what things are worth working on? None of this is taught at all).\nThe kind of education provided by sites like code.org gets it backward. In this way of seeing the world, you can get busy ‘learning to code’ but it won’t necessarily fit into your life and you can’t really make real progress without other external guidance and support. The dream of universal education for a young person in somewhere like Afghanistan, delivered by these kinds of programmes, seems more smoke and mirrors than a real ladder towards education and real skills."
  },
  {
    "objectID": "personal/2018-12-31-raspberrylpic-new-series-setup-steps.html",
    "href": "personal/2018-12-31-raspberrylpic-new-series-setup-steps.html",
    "title": "RaspberryLPIC: A New Series & Setup Steps",
    "section": "",
    "text": "I mentioned in my last post that I hoped to move on to the LPIC-1 exam in the coming weeks. I’m going through a bit of flux in terms of my stable laptop setup at the moment and I wanted a bit of stability as I work my way through the course. The idea suggested itself to me: what if I work through the syllabus using a Raspberry Pi?\nI have a few Raspberry Pi 3 and Zeros lying around the house, so I’ve chosen the latest model I have — a model B version 1.2. I can SSH into the device over wifi regardless of whatever laptop I’m using at the time.\nI’m choosing to use a Raspberry Pi for a few reasons:\n\nI want something that feels (and is) ‘disposable’ — if I make some mistake in my settings, I can install everything from scratch fairly trivially\nI didn’t want to do it on a virtual machine because sometimes this can behave idiosyncratically and I wanted something as close to an ‘authentic’ Linux experience as possible.\nI didn’t want to use a server from a cloud hosting provider since a dedicated server running online is probably overkill for what I need. You can get cheaper if you’re just using part of a server (via some virtualised service etc) but that seemed likely to provide non-standard output.\nI wanted to plug and play bits of hardware as part of my studies. That’s obviously not possible on a cloud server, and can provide non-standard responses when done through a virtual machine.\nI didn’t just want to install Linux on a spare laptop since I don’t have one of those lying around where I currently am. If I break anything, moreover, the installation / reformatting process and so on takes much longer than just flashing a SD card for a Raspberry Pi.\n\nThe hardware is pretty decent on the model I’m using, at least for the purposes of the LPIC-1 exam. This seems like an ideal use case.\nOnce again, I’m following through using the Linux Academy’s video lectures. As far as I understand things, the LPIC-1 exam requires more than just passing familiarity with a few commands. For that reason, I’m using a few supplementary books. Once I’ve gone through both books and videos I’ll be testing myself with practice exams.\nYesterday I spent a few hours trying to get my base setup installed on the Raspberry Pi. I started with an ambitious plan to install the version of Arch developed for use on a Raspberry Pi 3 (i.e. this version of an ARM chip) but it ended up being somewhat non-trivial. I ended up breaking Pacman (the Arch package installer) and unable to install any new software or update the system.\nI realised that Arch probably wasn’t the ideal setup for this experiment in any case. The default Raspbian distro, based on Debian Wheezy, seemed a better option. Flashing that onto my SD card and getting a headless copy up and running was easy.\nI might take a short detour before diving into the LPIC course proper by working my way through the Linux From Scratch series. I figure I’ll learn some useful things in that process of building my own custom kernel / distribution that I can then build on through the LPIC-1 syllabus. But I haven’t fully decided on that path yet."
  },
  {
    "objectID": "personal/2019-02-13-ruby-zip.html",
    "href": "personal/2019-02-13-ruby-zip.html",
    "title": "Using Ruby’s .zip method to combine arrays",
    "section": "",
    "text": "In Ruby, zip is a way to combine elements from two different arrays, albeit in a way that is slightly difficult to understand at first glance.\nThe documentation is a bit opaque, at least to my eyes, and the examples given take a bit of time to get your head around.\nLet’s say you had an array of fruits that you wanted to distribute to your friends. You’re organised, so you have a list of your friends as well.\nfruits = [\"mango\", \"orange\", \"pomegranate\"]\nfriends = [\"Rob\", \"Mary\", \"Holly\"]\nUsing multiple methods and loops, it’d be fairly trivial to conjure up something to combine these two into a new array, but luckily for us .zip exists to save the day.\nfriends.zip(fruits)\nwill return:\n[[\"Rob\", \"mango\"], [\"Mary\", \"orange\"], [\"Holly\", \"pomegranate\"]]\nThis way everyone will know what fruit they’re getting.\nNote that if one of the two arrays is longer / shorter than the other, the missing space(s) will be filled with nil values."
  },
  {
    "objectID": "personal/2019-02-25-using-rubys-digits-method.html",
    "href": "personal/2019-02-25-using-rubys-digits-method.html",
    "title": "Using Ruby’s .digits method",
    "section": "",
    "text": "I discovered the .digits method in Ruby the other day. As a quick illustration, it extracts the digits of a method into an array, reverse sorted.\n12345.digits #=&gt; [5, 4, 3, 2, 1]\nYou can optionally specify what base you’d like it to use to calculate the digits using, i.e. the same calculation as above but in base 100 would give you the following:\n12345.digits(100) #=&gt; [45, 23, 1]\nReading around a little bit, it seems that if you’re trying to get hold of the digits of a number, simply doing a .digits.reverse is perhaps an ok solution if the number is small, but at a certain point it starts to get slow. This is because .digits isn’t just ‘splitting’ the number.\nFor that reason, perhaps using .to_s.chars might be a better alternative. You can then use a .map function to convert the characters into integers:\n12345.to_s.chars.map { |digit| digit.to_i }\nI’m not entirely sure what .digits is actually used / useful for, given the speed issues."
  },
  {
    "objectID": "personal/2019-03-21-mastery-based-learning-with-launch-school.html",
    "href": "personal/2019-03-21-mastery-based-learning-with-launch-school.html",
    "title": "Mastery-based Learning with Launch School",
    "section": "",
    "text": "It’s a new week, so we have a new podcast episode for you. Matt and I spoke with Chris Lee of Launch School about his approach to online education. We discuss the different tradeoffs and considerations that come into play when a curriculum is being put together.\nTo my mind, mastery-based learning — where you don’t advance to the next stage until you’ve really mastered the topic at hand — really shines for things where you have some kind of longer-term goal. Just because it’s a good approach doesn’t mean it’s easy, though. In Chris’ words:\n\nWe started this to try to figure out education. It was not a money making endeavor. So to us, teaching became the engineering problem to solve. I was not a proponent of Mastery Based Learning before Launch School. Mastery Based Learning or Competency Based Learning is not unique to Launch School, it’s a well known pedagogy in academic papers. But it’s really hard to implement.\nThink about a physical classroom. Mastery Based Learning means that a student gets to occupy a seat in that classroom for an indefinite amount of time. That’s a really hard promise to make when our schools are tasked to usher through students. It’s not about training students and making sure they understand every topic, but getting people through.\n\nYou can download and listen to the episode over on the main Sources & Methods website here."
  },
  {
    "objectID": "personal/2020-01-29-how-the-internet-works.html",
    "href": "personal/2020-01-29-how-the-internet-works.html",
    "title": "How the Internet Works",
    "section": "",
    "text": "The internet. It just works. Understanding exactly how is a bit more complicated than many pieces of engineering. The more you examine the different aspects and parts that make it up, the more you see complexity concealed under the surface.\nVisiting this website, for instance: it feels like a trivial thing to do, but there are many different parts making that happen, from the parts that actually transport the bits of data across the physical infrastructure, to the pieces that serve it all to you on a secure connection (ensuring that what I’ve written hasn’t been altered by a third-party).\nI’ve just finished Launch School’s LS170 module which takes you a decent way down in the weeds to explain exactly how all of these pieces fit together to make up ‘the internet’. So today I thought I’d retransmit some of that as a way of cementing it in my own mind.\nAt a very abstract level, the internet can be thought of as a network of networks. A network itself is a set of two or more computers which are able to communicate between each other. This could be the computers attached to a home network, or the computers that connect through a central server to a particular Internet Service Provider or ISP.\nThe internet makes use of a series of ‘protocols’, shared rules and understandings which have been developed or accreted over time. These protocols allow a computer on the other side of the planet to communicate in a mutually comprehensible manner. (If these shared sets of rules didn’t exist, communicating with strangers or sending messages from one server to another would be a lot more difficult).\nSo once we have this top-down understanding of the internet as a bunch of networks that interact with each other, what, then, is the process by which a web browser in the United Kingdom communicates with a web server in China? Or in other words, if I want to access a website hosted on a Chinese webserver, how does that series of communication steps work to make that happen?\nAt this point, it’s useful to make use of another abstraction: communication across the internet happens across a series of layers. There are several different models for these various layers. Two of the more common models — the OSI model and the TCP/IP model — are represented below:\nAt the top level — “Application” — you have your website or whatever the user comes into contact with that is being served up to your web browser, let’s say. All the layers below that are progressively more and more specialised, which is another way of saying that they become progressively less comprehensible if you were to eavesdrop on the data as it were passing over the wire or through the fibre optic cable.\nLet’s move through the big pieces of how information is communicated, then, starting at the bottom. (I’ll mostly follow the TCP/IP model since it’s a bit less granular and allows me to split things up in a way that make sense). This chart will help keep all the pieces in your mind:\nNote that each layer has something known as a ‘protocol data unit’ or PDU. A PDU is usually made up of a combination of a header, payload or chunk of data and an optional footer or trailer. The header and footer contain metadata which allows for the appropriate transmission / decoding etc of the data payload.\nThe PDU of one layer is used by the layer below or above it to make up its own separate PDU. See the following diagram as an illustration:"
  },
  {
    "objectID": "personal/2020-01-29-how-the-internet-works.html#physical-layer",
    "href": "personal/2020-01-29-how-the-internet-works.html#physical-layer",
    "title": "How the Internet Works",
    "section": "Physical Layer",
    "text": "Physical Layer\nBefore we get into the realm of protocols, it’s worth remembering and reminding ourselves that there is a physical layer on which all the subsequent layers rely. There are some constraints relating to the speed or latency with which data can be transmitted over a network which relate to fixed laws of physics. The most notable of those constraints is the fact of the speed of light."
  },
  {
    "objectID": "personal/2020-01-29-how-the-internet-works.html#link-layer-ethernet",
    "href": "personal/2020-01-29-how-the-internet-works.html#link-layer-ethernet",
    "title": "How the Internet Works",
    "section": "Link Layer — Ethernet",
    "text": "Link Layer — Ethernet\nEthernet is the protocol that enables communication between devices on a single network. (These devices are also known as ‘nodes’). The link layer is the interface between the physical network (i.e. the cables and routers) and the more logical layers above it.\nThe protocols for this layer are mostly concerned with identifying devices on the network, and moving the data among those devices. On this layer, devices are identified by something called a MAC (Media Access Control) address, which is a permanent address burned into every device at the time of manufacturing.\nThe PDU of the Ethernet layer is known as a ‘frame’. Each frame contains a header (made up of a source address and a destination address), a payload of data, and a footer."
  },
  {
    "objectID": "personal/2020-01-29-how-the-internet-works.html#internet-layer-the-internet-protocol-ipv4-or-ipv6",
    "href": "personal/2020-01-29-how-the-internet-works.html#internet-layer-the-internet-protocol-ipv4-or-ipv6",
    "title": "How the Internet Works",
    "section": "Internet Layer — The Internet Protocol (IPv4 or IPv6)",
    "text": "Internet Layer — The Internet Protocol (IPv4 or IPv6)\nMoving up a layer, part of the Ethernet frame is what becomes the PDU for the internet or network layer, i.e. a packet.\nThis internet layer uses something known as the internet protocol which facilitates communication between hosts (i.e. different computers) on different networks. The two main versions of this protocol are known as IPv4 and IPv6. They handle routing of data via IP addressing, and the encapsulation of data into packets.\nIPv4 was the de facto standard for addresses on the internet until relatively recently. There are around 4.3 billion possible addresses using this protocol, but we are close to having used up all those addresses now. IPv6 was created for this reason and it allows (through the use of 128-bit addresses) for a massive 340 undecillion (billion billion billion billion) different addresses.\nAdoption of IPv6 is increasing, but still slow.\nThere is a complex system of how data makes its way from one end node on the network, through several other networks, and then on to the destination node. When the data is first transmitted, a full plan of how to reach that destination is not formulated before starting the journey. Rather, the journey is constructed ad hoc as it progresses."
  },
  {
    "objectID": "personal/2020-01-29-how-the-internet-works.html#transport-layer-tcpudp",
    "href": "personal/2020-01-29-how-the-internet-works.html#transport-layer-tcpudp",
    "title": "How the Internet Works",
    "section": "Transport Layer — TCP/UDP",
    "text": "Transport Layer — TCP/UDP\nThere are a number of different problems that the transport layer exists to solve. Primarily, we want to make sure our data is passed reliably and speedily from one node to another through the network.\nTCP and UDP are two protocols which are good at different kinds of communication. If the reliability of data transmission is important to us and we need to make sure that every piece of information is transmitted, then TCP (Transmission Control Protocol) is a good choice. If we don’t care about every single piece of information — in the case of streaming a video call, perhaps, or watching a film on Netflix — but rather about the speed and the ability to continuously keep that data stream going, then UDP (User Datagram Protocol) is a better choice.\nThere are differences between the protocols beyond simply their functionality. We can distinguish between so-called ‘connection-oriented’ and ‘connectionless’ protocols. For connection-oriented protocols, a dedicated connection is created for each process or strand of communication. The receiving node or computer listens with its undivided attention. With a connectionless protocol, a single port listens to all incoming communication and has do disambiguate between all the incoming conversations.\nTCP is a connection-oriented protocol. It first sends a three-way handshake to establish the connection, then sends the data, and sends a four-way handshake to end the connection. The overhead of having to make these handshakes at the beginning and at the end, it’s a fairly costly process in terms of performance, but in many parts of internet communication we really do need all the pieces of information. Just think about an email, for example: it wouldn’t be acceptable to receive only 70% of the words, would it?\nUDP is a connectionless protocol. It is in some ways a simpler protocol compared to TCP, and this simplicity gives it speed and flexibility; you don’t need to make a handshake to start transmitting data. On the negative side, though, it doesn’t guarantee message delivery, or provide any kind of congestion avoidance or flow control to stop your receiver from being overwhelmed by the data that’s being transmitted."
  },
  {
    "objectID": "personal/2020-01-29-how-the-internet-works.html#application-layer-http",
    "href": "personal/2020-01-29-how-the-internet-works.html#application-layer-http",
    "title": "How the Internet Works",
    "section": "Application Layer — HTTP",
    "text": "Application Layer — HTTP\nHTTP is the primary communication protocol used on the internet. At the application layer, HTTP provides communication of information to applications. This protocol focuses on the structure of the data rather than just how to deliver it. HTTP has its own syntax rules, where you enclose elements in tags using the &lt; data-preserve-html-node=“true” and &gt; symbols.\nCommunication using HTTP takes the form of response and request pairs. A client will make a ‘request’ and it’ll receive (barring some communication barrier) a ‘response’. HTTP is known as a ‘stateless’ protocol in that each request and response is completely independent of the previous one. Web applications have many tricks up their sleeve to make it seem like the web is stateful, but actually the underlying infrastructure is stateless.\nWhen you make an HTTP request, you must supply a path (e.g. the location of the thing or resource you want to request / access) and a request method. Two of the most common request methods are GET and POST, for requesting and amending things from/on the server respectively. You can also send optional request ‘headers’ which are bits of meta-data which allow for more complicated requests.\nThe server is obliged to send a HTTP status code in reply. This code tells you whether the request was completed as expected, or if there were any errors along the way. You’ll likely have come across a so-called ‘404’ page. This is referring to the 404 status code indicating that a resource or page wasn’t found on the server. If the request was successful, then the response may have a payload or body of data (perhaps a chunk of HTML website text, or an image) alongside some other response headers.\nNote that all this information is sent as unencrypted plain text. When you’re browsing a vanilla http:// website, all the data sent back and forth is just plain text such that anyone (or any government) can read it. This wasn’t such a big issue in the early days of the internet, perhaps, but quite soon it became more of a problem, especially when it came to buying things online, or communicating securely. This is where TLS comes in.\nTLS or Transport Layer Security is sometimes also known as SSL. It provides a way to exchange messages securely over an unsecured channel. We can conceptually think of it as occupying the space between the TCP and HTTP protocols (at the session layer of the OSI framework above). TLS offers:\n\nencryption (encoding a message so only authorised people can decode it)\nauthentication (verifying the identity of a message sender)\nintegrity (checking whether a message has been interfered with)\n\nNot all three are necessarily needed or used at any one time. We’re currently on version 1.3 of TLS.\n\nWhew! That was a lot. There are some really good videos which make the topic slightly less dry. Each of these separate sections are extremely complex, but having a broad overview is useful to be able to disambiguate what’s going on when you use the internet."
  },
  {
    "objectID": "personal/2020-06-11-dil-dhadakne-do.html",
    "href": "personal/2020-06-11-dil-dhadakne-do.html",
    "title": "Dil Dhadakne Do",
    "section": "",
    "text": "A family travels around the Turkish coast on a cruise ship, while romance and past transgressions erupt among them. Some catchy dance moments in this film, but the plot felt too scripted, too unnatural. Too much like Titanic, or perhaps Midsummer Night’s Dream at times. Fun, but forgettable."
  },
  {
    "objectID": "personal/2020-06-14-sholay.html",
    "href": "personal/2020-06-14-sholay.html",
    "title": "Sholay",
    "section": "",
    "text": "**Some spoilers ahead**\nSholay was not what I was expecting. Somehow the posters or some imagery that I’d seen had led me to expect a film that was about grand politics, perhaps vast armies fighting in the desert.. Instead, and this is all I have to compare it to, some kind of western?\nI enjoyed the plot twists. I wasn’t expecting the various deaths in the film, especially towards the end. The songs were all pretty good. One felt almost modern somehow.\nSome scenes I felt went on quite a bit longer than they needed, particularly early on. I am told that the action scenes were praised at the time, though they too felt a little too long for my tastes.\nAll in all, some strong character portraits, and I feel some precedents set for future films. I half wonder if there’s a sequel where Gabbar escapes from the surely-minimum-security prison to which he gets sent."
  },
  {
    "objectID": "personal/2020-10-13-what-is-lexical-scope.html",
    "href": "personal/2020-10-13-what-is-lexical-scope.html",
    "title": "What is Lexical Scope?",
    "section": "",
    "text": "Scope defines what functions / variables and values are available at any point in a programme. Lexical scope is when you can derive the scope from looking at the code, or by looking at its position within the code. Lexical scope is also known as static scope.\nThe source code of our JavaScript code — in my example — defines the scope. It particularly relates to functions and nested functions. In those cases, we can create a hierarchy of scopes.\nThe inner functions have access to everything outside them, but the outer functions don’t have access to everything from the inner functions.\nEvery function creates a new local variable scope. Every block also creates a new local variable scope. The code doesn’t have to be executed for these scoping rules to be true, for these inner and outer scopes to exist.\nThis example below will log ‘hello from inner scope’ but will then give a ReferenceError since the variable secret is only available / accessible inside the inner scope function:\nfunction outerScope() {\n  function innerScope() {\n    let secret = 'this is in inner scope';\n    console.log('hello from inner scope'); \n  }\n\n  innerScope();\n  console.log(secret);\n}\n\nouterScope();"
  },
  {
    "objectID": "personal/2020-11-10-turning-array-like-objects-into-arrays-with-javascript.html",
    "href": "personal/2020-11-10-turning-array-like-objects-into-arrays-with-javascript.html",
    "title": "Turning array-like objects into arrays with JavaScript",
    "section": "",
    "text": "I’ve long been wondering how the following piece of code works:\n// assuming a website with a bunch of elements with 'h1' tags\nlet liveCollection = document.querySelectorAll('h1');\nlet arrayVersion = Array.prototype.slice.call(liveCollection); // returns that live collection of elements in the form of an array\nSo I thought I’d write up a little on my understanding of how it works.\nArray.prototype.slice is simple. The slice() function is stored inside the prototype object property on the Array constructor. That’s all as you might expect given how JavaScript handles things with the prototypal chain.\nBut why are we using call here, and how does that work when you pass in liveCollection as the object within which you want to invoke slice()?\nNormally when we have an array that we want to call slice on, we have to do that using slice as a method. Arrays have the slice method available to them through the prototypal chain (also in the Array.prototype object). But our array-like object (i.e. the live collection in the example above) don’t have those methods available to them.\nUnder the hood, when the slice method is invoked, what it does is iterates over the array as part of its more special functionality (which I’ll ignore for now). If we have an array-like object with a length and with elements that you can sequentially iterate over, then we can run something like slice on that array-like object.\nSo how do we bring these two pieces together? We use call. call is a way of using a function inside a different execution context. In our case, we want to use the execution context (i.e. what this is set to) of the live collection, but still have it use the functionality defined in slice. call and apply are both ways we can do this.\n(For more on this, there are some very useful explanations in this stackoverflow post.)"
  },
  {
    "objectID": "personal/2020-11-14-different-ways-of-accessing-the-text-contents-of-dom-nodes-in-javascript.html",
    "href": "personal/2020-11-14-different-ways-of-accessing-the-text-contents-of-dom-nodes-in-javascript.html",
    "title": "Different ways of accessing the text contents of DOM nodes in JavaScript",
    "section": "",
    "text": "For a while now while studying the DOM and JavaScript’s interaction with the web browser, I’ve been wondering about various properties available on nodes that relate to their contents or text values.\nI spent a bit of time the other day unpacking what each of those do. This is a bit of code you can play around with to evaluate how they all work:\n&lt;!doctype html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;title&gt;title&lt;/title&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n  &lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div&gt;\n      &lt;h1&gt;First bit of text.&lt;/h1&gt;\n      &lt;p&gt;Some &lt;span&gt;old&lt;/span&gt; text&lt;/p&gt;\n      &lt;textArea&gt;pre-written&lt;/textArea&gt;\n    &lt;/div&gt;\n  &lt;script&gt;\n    console.log(document.querySelector('div').innerHTML);\n  &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n.textContent\n\nThis property concatenates the text of the element plus all child elements including any whitespace (from the HTML markup itself).\n\n.data\n\nThis is the text content of a text node.\n\n.nodeValue\n\nFor text nodes, this is the text content of that text node (i.e the same as .data). For comments, it is the comment content. For element nodes (and most other types), it is null.\n\n.value\n\nThis is either an attribute property on an element (in which case the value is whatever was assigned to that property), or it is the contents of a textArea element.\n\n.innerText\n\nThis is the rendered text (i.e. as displayed on the browser page) of the node and child nodes. Note, if it is not being rendered, then this will be identical to the .textContent property.\n\n.innerHTML\n\nThis is all the HTML markup contained within the element (including nested elements and text (and whitespace). Note that sometimes this will just be plain text. Note too, that this can be used to create HTML inside an element as well."
  },
  {
    "objectID": "personal/2020-12-03-how-events-drive-programming-for-the-web.html",
    "href": "personal/2020-12-03-how-events-drive-programming-for-the-web.html",
    "title": "How events drive programming for the web",
    "section": "",
    "text": "As part of my studies for Launch School, last month I was introduced to the idea of ‘events’ and how we can use the DOM and JavaScript to cause things to happen without having to force a full reload of the page. This is sometimes known as AJAX requests, which stands for Asynchronous JavaScript And XML.\nIn this mental model, we can think of the HTML and CSS code loaded by the browser (when you visit a website, for example) as the part which we can interact with. Absent any JavaScript code we include, this is somewhat static.\nIf we add in some JavaScript, we can first make sure that the DOM is fully loaded. For this we add an event listener for the DOMContentLoaded event.\nThen after that, the sky is the limit. We can add event listeners to basically anything in our document (or just the whole document object itself). This is the list of events that we can watch for.\nFrom reading outside the course, I know that companies’ monitoring of user behaviour via JavaScript events triggering is a really big source of privacy violations or issues. I am not sure how the current model of how the web works serves to limit these violations. For instance, do people know that their mouse movements and clicks and patterns of mouse behaviour can all be monitored while they’re on a site. So even if you don’t click on an object, the website can still track that you’re hovering over a particular element. That, to my mind, is a step too far, but it’s the world we live in. You can disable JavaScript completely, but then you break a majority of sites that you’re visiting.\nOf course, if you’re browsing in something like emacs’ eww, or in brow.sh, then you bypass this problem somewhat.\nBack to the JavaScript, though, the most interesting thing I found was the sheer amount of events that the browser was built to encounter. I’d love to see a chart showing how and when all these events were added to the browser’s capabilities."
  },
  {
    "objectID": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html",
    "href": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html",
    "title": "A basic overview of how to use Handlebars",
    "section": "",
    "text": "Handlebars is a simple templating language that you can use with your JavaScript code. It came somewhat late to the game; Python and Ruby and others had their own templating options for a while.\nThe problem it is trying to solve is the case where you have a lot of HTML code that you need to create in your JavaScript files, but you don’t want to leave your .js files completely bursting to the seams with HTML.\nAn example is probably good at this point:\nand here’s the associated JavaScript file:\nA few points to note from this (simple) example:\nNote also how we have managed to keep our JavaScript file mostly free of HTML code. This allows us to keep the layout and templating work to the HTML file, and our logic for our JavaScript file."
  },
  {
    "objectID": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#logic-conditionals",
    "href": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#logic-conditionals",
    "title": "A basic overview of how to use Handlebars",
    "section": "Logic: Conditionals",
    "text": "Logic: Conditionals\nHandlebars does allow us to include some logic in our templates, however. I can add a few lines to my template:\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/handlebars@latest/dist/handlebars.js\"&gt;&lt;/script&gt;\n    &lt;script src='handlebarsApp.js'&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Hello, World&lt;/h1&gt;\n    &lt;script id='handlebars1' type='text/x-handlebars'&gt;\n      &lt;div&gt;\n        {{{name}}}\n        &lt;br&gt;\n        {{date}}\n        &lt;br&gt;&lt;/br&gt;\n        {{#if homework}}\n        I love homework.\n        {{else}}\n        -- No homework today\n        {{/if}}\n      &lt;/div&gt;\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nIn this example I added a conditional if/else statement. I checked there was a homework key on the object that we compiled our HTML with. There wasn’t, so the ‘no homework today’ string was output.\n(A Handlebars conditional is false if the property value is false, undefined, null, '', 0 or an empty array.)\nNote that we start the vast majority of these ‘logic’ parts of Handlebars with a # character. {else} is apparently an exception and you shouldn’t include a # in front of that when using it."
  },
  {
    "objectID": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#logic-iteration",
    "href": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#logic-iteration",
    "title": "A basic overview of how to use Handlebars",
    "section": "Logic: Iteration",
    "text": "Logic: Iteration\nWe can also include iteration as part of our Handlebars templates using the {#each} keyword. Here’s the HTML:\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/handlebars@latest/dist/handlebars.js\"&gt;&lt;/script&gt;\n    &lt;script src='handlebarsApp.js'&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Hello, World&lt;/h1&gt;\n    &lt;script id='handlebars1' type='text/x-handlebars'&gt;\n      &lt;div&gt;\n        {{{name}}}\n        &lt;br&gt;\n        {{date}}\n        &lt;br&gt;&lt;/br&gt;\n        {{#if homework}}\n        I love homework.\n        {{else}}\n        -- No homework today\n        {{/if}}\n        &lt;br&gt;&lt;br&gt;\n        &lt;h3&gt;List of people&lt;/h3&gt;\n        {{#each people}}\n        &lt;p&gt;{{this}}&lt;/p&gt;\n        {{/each}}\n      &lt;/div&gt;\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nAnd here’s the associated JavaScript file, updated to include an array as part of the context object:\ndocument.addEventListener('DOMContentLoaded', () =&gt; {\n  let script1 = document.querySelector('#handlebars1').innerHTML;\n\n  let obj = {\n    \"name\": \"&lt;h2&gt;Henry&lt;/h2&gt;\",\n    \"date\": \"&lt;i&gt;2020-12-10&lt;/i&gt;\",\n    \"people\": [\n      \"Alex Strick\",\n      \"Hope Riley\",\n    ],\n  };\n\n  let templateScript = Handlebars.compile(script1); // returns a function\n  document.body.innerHTML = templateScript(obj);\n});\nThings to note from this example:\n\nWe start the iteration with {#each people}, passing in the name of the object property that is iterable\nWe end the iteration with a closing {/each}\nthe this inside our iteration refers to the individual element that we’re currently on as part of our iteration.\n\nIf we want to deal with nested values, the as keyword along with | pipes allows us to do what we need. Our HTML runs as follows:\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/handlebars@latest/dist/handlebars.js\"&gt;&lt;/script&gt;\n    &lt;script src='handlebarsApp.js'&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Hello, World&lt;/h1&gt;\n    &lt;script id='handlebars1' type='text/x-handlebars'&gt;\n      &lt;div&gt;\n        {{{name}}}\n        &lt;br&gt;\n        {{date}}\n        &lt;br&gt;&lt;/br&gt;\n        {{#if homework}}\n        I love homework.\n        {{else}}\n        -- No homework today\n        {{/if}}\n        &lt;br&gt;&lt;br&gt;\n\n        &lt;h3&gt;List of people&lt;/h3&gt;\n        {{#each people}}\n        &lt;p&gt;{{this}}&lt;/p&gt;\n        {{/each}}\n\n        &lt;h3&gt;Agenda&lt;/h3&gt;\n        {{#each agenda as |agendaItem name|}}\n        &lt;p&gt;{{name}}: {{agendaItem.hobby}} // {{agendaItem.work}}&lt;/p&gt;\n        {{/each}}\n\n      &lt;/div&gt;\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\nAnd our updated JavaScript to include a nested object:\ndocument.addEventListener('DOMContentLoaded', () =&gt; {\n  let script1 = document.querySelector('#handlebars1').innerHTML;\n\n  let obj = {\n    \"name\": \"&lt;h2&gt;Henry&lt;/h2&gt;\",\n    \"date\": \"&lt;i&gt;2020-12-10&lt;/i&gt;\",\n    \"people\": [\n      \"Alex Strick\",\n      \"Hope Riley\",\n    ],\n    'agenda': {\n      'alex': {\n        'hobby': 'swimming',\n        'work': 'reading',\n      },\n      'hope': {\n        'hobby': 'dancing',\n        'work': 'walking',\n      },\n    },\n  };\n\n  let templateScript = Handlebars.compile(script1); // returns a function\n  document.body.innerHTML = templateScript(obj);\n});\nNote that after using the as keyword and the pipes operators, we specify first the variable name to denote the bottom-level object (agendaItem), and then a variable name to denote the two named keys (i.e. alex and hope). This allows us to use both in the pattern we code in our template."
  },
  {
    "objectID": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#logic-iteration-and-the-with-keyword",
    "href": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#logic-iteration-and-the-with-keyword",
    "title": "A basic overview of how to use Handlebars",
    "section": "Logic: Iteration and the with keyword",
    "text": "Logic: Iteration and the with keyword\nWe can use the with keyword to gain access to object properties. Instead of using the as syntax shown above, we can use with so we don’t need the dot syntax to access those properties. The example is a bit contrived, but you get the idea. The relevant additional section of our HTML file would be:\n&lt;h3&gt;Agenda 2&lt;/h3&gt;\n{{#each agenda as |__ name|}}\n{{#with this}}\n&lt;p&gt;{{name}}: {{hobby}} // {{work}}&lt;/p&gt;\n{{/with}}\n{{/each}}\nI use the as syntax here so that we can refer to each individual object’s key, but we wouldn’t always necessarily need to do this."
  },
  {
    "objectID": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#partials-for-more-complex-templating",
    "href": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#partials-for-more-complex-templating",
    "title": "A basic overview of how to use Handlebars",
    "section": "Partials For More Complex Templating",
    "text": "Partials For More Complex Templating\nWe can use partials to include templates within templates. The syntax to include a partial inside a template is to use the &gt; symbol inside a pair of curly braces. For example, our HTML might look like this in a simple case:\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/handlebars@latest/dist/handlebars.js\"&gt;&lt;/script&gt;\n    &lt;script src='handlebarsApp.js'&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;h1&gt;Title of our Page&lt;/h1&gt;\n    &lt;script id='handlebars1' type='text/x-handlebars'&gt;\n      &lt;div&gt;\n        {{&gt; partialTemplate}}\n        &lt;ol&gt;\n        {{#each names}}\n        &lt;li&gt;{{this}}&lt;/li&gt;\n        {{/each}}\n        &lt;/ol&gt;\n      &lt;/div&gt;\n    &lt;/script&gt;\n\n    &lt;script id='partial' type='text/x-handlebars'&gt;\n      &lt;h2&gt;Job: {{job}}&lt;/h2&gt;\n      &lt;h2&gt;Year: {{year}}&lt;/h2&gt;\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n…and the associated JavaScript would look like this:\ndocument.addEventListener('DOMContentLoaded', () =&gt; {\n  let script1 = document.querySelector('#handlebars1').innerHTML;\n  let script2 = document.querySelector('#partial').innerHTML;\n\n  let obj = {\n    \"names\": [\n      \"Alex Strick\",\n      \"Hope Riley\",\n    ],\n    'job': 'fencing',\n    'year': 2021,\n  };\n\n  let templateScript1 = Handlebars.compile(script1);\n\n  Handlebars.registerPartial('partialTemplate', script2);\n\n  let newNode1 = document.createElement('div');\n  newNode1.innerHTML = templateScript1(obj);\n\n  document.body.appendChild(newNode1);\n});\nSome points worth mentioning here about the two chunks of code above:\n\nOur partial looks like any other template when we define it in our HTML file. We might want to distinguish it by giving it a certain id, but otherwise it looks like any other handlebars template, with the same type applied to it.\nIn order for our partial to be usable as a template-within-a-template, it needs to have a variable name that we can use to reference it. We register this name by using Handlebars.registerPartial(). This method takes two arguments: the name that we’d like to register it under, and the template HTML.\nThe rest of our code runs as in previous examples mentioned above, with the exception that when it reaches the line of code {{&gt; partialTemplate}}`, this is then converted and passed in to replace the placeholder at this point.\n\nYou can also define partials in the JavaScript code itself. This website offers an example of that:\nHandlebars.registerPartial(\n  'partialTemplate',\n  '{{language}} is {{adjective}}. You are reading this article on {{website}}.'\n);\n\nvar context={\n  \"language\" : \"Handlebars\",\n  \"adjective\": \"awesome\"\n}\nThis is coupled with the following defined in the HTML file:\n{{&gt; partialTemplate website=\"sitepoint\"}} &lt;br&gt;\n{{&gt; partialTemplate website=\"www.sitepoint.com\"}}\nYou can play around with that example on CodePen."
  },
  {
    "objectID": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#closing-thoughts",
    "href": "personal/2020-12-10-a-basic-overview-of-how-to-use-handlebars.html#closing-thoughts",
    "title": "A basic overview of how to use Handlebars",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nYou can play around with Handlebars here. I like how it gives you options as to where you want to house logic for a particular website. There are obviously tradeoffs to consider, particularly whether code should be running server-side or client-side and how that might affect speed and loading times.\nI will need more practical experience to really unpack the implications of using Handlebars, but I am looking forward to getting to use it with a better sense of control now that I understand the fundamentals."
  },
  {
    "objectID": "personal/2021-05-23-deep-learning-best-in-show.html",
    "href": "personal/2021-05-23-deep-learning-best-in-show.html",
    "title": "Deep Learning: Best in Show?",
    "section": "",
    "text": "Deep Learning is an incredibly powerful technology and there are a number of (focused / specific) areas where it already surpasses human-level abilities. Here are some examples:\n\nTranslation: If you haven’t been watching closely, the quality of Google Translate translations has really been improved in recent years. This 2016 story is a little dated, but it explains how they made a big push a few years back and it continues to improve as the technology improves.\nX-ray interpretation: In a matter of a few years, the performance of Deep Learning in reading and making diagnoses from x-rays has surpassed top radiology practitioners. See how DeepMind raised the bar on identifying breast cancer.\nPlaying Go: Watch the AlphaGo documentary if you haven’t already.\nProtein Folding: Check out AlphaFold from last November, where DeepMind blasted through a notoriously complicated problem in biology.\nColourising images: A former fast.ai student, Jason Antic, made great progress with his work on DeOldify.\n\nThe really great thing about the fastai course is how it successfully has managed to democratise Deep Learning as a technology. I always enjoy reading about niche areas where specific burning problems were solved because someone took the opportunity to educate themselves."
  },
  {
    "objectID": "personal/2021-05-23-pdp-a-precursor-to-modern-neural-networks.html",
    "href": "personal/2021-05-23-pdp-a-precursor-to-modern-neural-networks.html",
    "title": "PDP: a precursor to modern neural networks?",
    "section": "",
    "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, a multi-volume publication by David Rumelhart, James McClelland and the PDP Research Group, was released in 1968 and is recognised as one of the most important works relating to neural networks.\n\n\n\nPDP (1968\n\n\nThey lay out eight features necessary to perform what they called ‘parallel distributed processing’ (which I suppose you can think of as a sort of precursor to modern-day deep learning):\n\nprocessing units\na state of activation\nan output function for each processing unit\na pattern of connectivity among units\na propagation rule (for propagating what is learned through the network)\nan activation rule\na learning rule (where ‘patterns of connectivity are modified by experience’)\nan environment in which the system operates\n\nI haven’t read the book, and I don’t fully understand all these different pieces, but it isn’t particularly hard to see the pattern of what would later come to be handled by modern-day neural networks in these features. The vocabulary used to describe it is slightly different, but you have the connectivity between neurons, and you have a process through which you update the layers…\nThis feels like a book that would reward returning to for a proper in-depth read later on in my studies."
  },
  {
    "objectID": "personal/2021-05-23-rosenblatts-mark-i-perceptron.html",
    "href": "personal/2021-05-23-rosenblatts-mark-i-perceptron.html",
    "title": "Rosenblatt’s Mark I Perceptron",
    "section": "",
    "text": "I’ve now read a little about Rosenblatt’s Perceptron in two different places: in the Howard/Gugger Deep Learning book, and also in Cade Metz’ Genius Makers.\n\n\n\nThe Mark I Perceptron\n\n\nBuilt in 1958, it is usually described as the first machine which was based on the principle of the artificial neutron. It used a single layer in this initial configuration, and even in that simple way you could already see glimpses of where it might go.\nUnfortunately, Marvin Minsky and Seymour Papert’s apparently perceptive but also damning assessment of the perceptron as a technology without a future ushered in the first of the so-called ‘AI winters’, and the idea of using neural networks was buried for several years.\nThankfully, some ignored the herd and stuck with it."
  },
  {
    "objectID": "personal/2021-05-26-arabic-language-plateau-principles.html",
    "href": "personal/2021-05-26-arabic-language-plateau-principles.html",
    "title": "Getting Out of the Intermediate Language Plateau: Arabic Edition / Principles",
    "section": "",
    "text": "[This is part of a series on getting out of a language-learning plateau at the intermediate-advanced level. Check out the other parts here.]\nSeasoned language learners are familiar with the concept of the ‘language plateau’. If you’re learning a second language for the first time, you will inevitably reach a point in your studies where your progress seems to flatten. You will find this place and period extremely frustrating.\nWhen you are in your plateau, it’s hard to improve because you’re already at a point of (some kind of) self-sufficiency. You can express yourself. You understand most of what is going on in a conversation or TV series you watch. You can write things and people will understand what you’re saying. You could (and many do) stop your studies at this point and still be ‘functional’ in the language.\nGetting out of this flat, dead zone is what I want to talk about today. It’s hard, but it’s by no means impossible, and making this kind of progress is possibly the most valuable work you’ll do in your language studies, because all of it will be specifically tailored to your needs.\nThe starting point, though, is to identify your current status. What can you do? You don’t (necessarily) need to take a formal language certification test to get a grade, though that can sometimes be useful. The kind of measurements you want to take are more subjective. You want to take stock of your capacity in certain situations, what level you are able to achieve in different contexts (your skills in reading will be different from writing vs listening or speaking, for example) and you want also to assess your experience on the cultural level as well – i.e. how much experience do you have navigating all the unspoken parts of culture, whether that is body language, or behaviours and so on."
  },
  {
    "objectID": "personal/2021-05-26-arabic-language-plateau-principles.html#principles-of-skill-acquisition",
    "href": "personal/2021-05-26-arabic-language-plateau-principles.html#principles-of-skill-acquisition",
    "title": "Getting Out of the Intermediate Language Plateau: Arabic Edition / Principles",
    "section": "Principles of Skill Acquisition",
    "text": "Principles of Skill Acquisition\nNow a slight detour into some more general principles of skill acquisition. Some of this is derived from my own personal experience, other parts from interviews with experts in this field (such as my conversation with K. Anders Ericsson, who more or less invented the field of expert performance studies), and other parts still from reading a bunch of books on the subject.\nThree things are relevant here:\n\n1) Stretch\nWhen you’re learning a new skill, you want to step outside your comfort zone. This is usually difficult work, and work that is mentally (and possibly emotionally) taxing. Thus, if you want to get better at speaking in Arabic, you’ll need to speak more, but at the beginning this practice (i.e. talking with other people) will feel pretty horrible, simply because you’re not used to doing it. It’s a paradox that you need to do the thing to get better at doing the thing. It is this difficulty, pushing yourself a little past what you’re capable of doing, that allows for personal growth. (I wrote about this in an entirely different context a few weeks ago with respect to my attempts to get better at climbing.)\n\n\n2) Lots of practice coupled with speedy feedback\nThese two parts (practice and feedback) go together. It isn’t practice alone that will allow you to improve, but rather the combination of making efforts to use new skills alongside getting some kind of feedback that tells you when you’re getting it wrong vs when you’re not. An implication of this, too, is the reality that this kind of practice is going to involve you making lots of mistakes. This can feel crappy, especially when you’re getting immediate feedback on exactly when this is happening. You need to adopt a flexible mindset, if possible, in which you see the mistakes as indicators of growth rather than as any kind of personal or intellectual failures on your part.\n\n\n3) Know what you’re practicing and focus on that\nThis is basically Ericsson’s principle of “deliberate practice”:\n\n“Rather than chilling out in the comfort of skills you’ve already acquired, as an expert-to-be, you’re relentless about heading to the frontier of your abilities. The practice shouldn’t be so difficult that it overwhelms you—that would be depressingly demotivating, but not so easy that you’re unconsciously languishing. In other words, you’re arranging for flow, that space where you’re right at the boundary of your abilities.”\n\nSee also this summary of the routines that ‘experts’ tend to have around deliberate practice:\n\nThey can only engage in practice without rest for around an hour.\nThey practice in the morning with a fresh mind.\nThey practice the same amount every day, including on weekends.\nThey only have four to five hours of deliberate practice a day.\nIf they don’t get enough rest, they can get overtraining injuries or burnout.\n\nIf you’re hoping that ‘using the language’ in a general and non-specific way will get you out of your plateau, you’ll be disappointed. It’s perfectly possible to exist in the plateau zone without improvement ad infinitum. If you want to improve at a certain skill, you’ll need to isolate that element and focus on it in a targeted way. This can be vocabulary, or speaking about a certain topic, or even something as small as ‘using conditional sentences’. Whatever it is, you’ll only get better if you concentrate your efforts."
  },
  {
    "objectID": "personal/2021-05-26-arabic-language-plateau-principles.html#customisation-your-individual-needs",
    "href": "personal/2021-05-26-arabic-language-plateau-principles.html#customisation-your-individual-needs",
    "title": "Getting Out of the Intermediate Language Plateau: Arabic Edition / Principles",
    "section": "Customisation & Your Individual Needs",
    "text": "Customisation & Your Individual Needs\nLearning languages at the post-intermediate level will be a different experience from what you are used to in the early stages. Early on, you’re doing a great deal of necessary-but-boring work to learn basic patterns, vocabulary and grammar.\nOnce you have mastered that, and you can explain yourself in most basic contexts, you reach the point where you have to customise. There’s a great deal of science and research behind this claim. Check out this talk, by the always stimulating Alexander Arguelles, for an overview of some of that research.\nYou’ll need to pick which areas you’re most interested in. This is the hard work of advanced language studies – you pick one area or context, conquer it, and then pick another area and repeat. This fulfils the princicle of focus that I mentioned above.\nTo give an example from my own studies. My current big push for Arabic is to be able to read serious fiction (i.e. short stories and novels written for native speakers). I’ve written previously that this was a personal goal, but various realities of how modern literature is written really make it hard to take the leap into complex native-reader-level fiction (especially novels). Arab writers like to use many synonyms (for poetic effect, or perhaps as an attempt at pretension?) for words, so when reading I often find myself stuck referring to dictionaries the whole time. Fortunately, a new textbook offering graded literature at just that ‘stretch’ level was released recently, which is allowing me an entry point into that world. None of the texts are simplified, and the language is hard and the number of unknown words is pretty large, but it’s not too far down the scale of difficulty."
  },
  {
    "objectID": "personal/2021-05-26-arabic-language-plateau-principles.html#on-making-a-self-study-plan",
    "href": "personal/2021-05-26-arabic-language-plateau-principles.html#on-making-a-self-study-plan",
    "title": "Getting Out of the Intermediate Language Plateau: Arabic Edition / Principles",
    "section": "On Making a Self-Study Plan",
    "text": "On Making a Self-Study Plan\nMy next post will cover and offer a host of suggestions for resources you can use to get out of this plateau / dead zone. Before you start reading through and diving into things that seem interesting, I’d strongly advise you take the time to figure out your specific goals. “Improve my Arabic” is not a useful goal. It’s too unspecific. Even “improve my spoken Arabic” may not be particularly useful at the intermediate-advanced level. Once you figure out your goal, write it down somewhere. Maybe stick it to your wall or on the inside of your notebook. It’s good to be reminded why we’re doing the work.\nOnce you have your goal, then you want to set yourself small targeted bursts or challenges to push out into your stretch zone. You don’t want these challenges to feel like you’re straining against the limits of what you are capable. You want it to be just challenging enough that you feel uncomfortable, but not so much that you are constantly questioning yourself and your abilities in any kind of fundamental sense.\nThe scale of these challenges will be pretty variable, so examples will span a range of tasks from taking a week to learn and read deeply in a niche topic, to something more longer-term (over six months, perhaps) like my modern literature challenge. The characteristic that you need to look for, however, is that you’ll be able to tell when you’re finished with the challenge. Part of defining the goal is finding a specific (and somewhat measurable) definition of what it means to have achieved what you want.\nThen the rest of the trick is basically keeping moving, tracking your progress and achievements along the way. There are various ways of doing this, some of which will depend on what else you have done in this regard. You can add in things like Beeminder to encourage compliance and regularity, or you can do that in other ways.\nWhen I work with people 1-on-1 to learn a language, a lot of what we do is figuring out this kind of ongoing goal setting and progress assessment. (If you want to learn more about this, click here and read through what I offer).\nThe next posts will offer a roadmap to the different resources available to the intermediate student of Arabic and some of the ways you can utilise these resources. It won’t be exhaustive, but I’m pretty sure that most will find something of use in them. Feel free to get in touch if you have specific things you want me to tackle in terms of skill development in Arabic."
  },
  {
    "objectID": "personal/2021-05-26-telling-cats-from-dogs.html",
    "href": "personal/2021-05-26-telling-cats-from-dogs.html",
    "title": "Telling Cats from Dogs",
    "section": "",
    "text": "One of the main ways that using neural networks to train models is different from traditional (imperative) programming can be illustrated with a specific task: let’s say you want to use a computer to tell you whether any particular photo you give it is a cat or a dog.\nAn imperative approach might be to make a mega list of certain kinds of features that cats and dogs have, and try to encode the differences into some kind of list of logical features. But even knowing how to tell the computer how it should recognise those features is a potentially massive project. How would you even go about that?\nInstead, with neural network layers, we turn that pattern on its side. Instead of this:\n\nWe have something closer to this:\n\nSo the neural networks are learning on the basis of data provided to it — you give it a bunch of images which you’ve pre-labelled to say ‘this one is a cat and that one is a dog’ and so on.\nIf you use transfer learning, too, you even can use a pretrained model (which is already pretty good at recognising features from images). You can then fine-tune that model to get really good at the specific task you need it to do. (Note, that’s exactly what you do in the first chapter of Howard/Gugger’s Deep Learning for Coders)."
  },
  {
    "objectID": "personal/2021-05-28-on-the-interpretability-of-models.html",
    "href": "personal/2021-05-28-on-the-interpretability-of-models.html",
    "title": "On the interpretability of models",
    "section": "",
    "text": "A common criticism of deep learning models is that they are ‘black boxes’. You put data in one end as your inputs, the argument goes, and you get some predictions or results out the other end, but you have no idea why the model gave your those predictions.\n\n\n\nWays of interpreting learning in computer vision models - credit https://thedatascientist.com/what-deep-learning-is-and-isnt/\n\n\nThis has something to do with how neural networks work: you often have many layers that are busy with the ‘learning’, and each successive layer may be able to interpret or recognise more features or greater levels of abstraction. In the above image, you can get a sense of how the earlier layers (on the left) are learning basic contour features and then these get abstracted together in more general face features and so on.\nSome of this also has to do with the fact that when you train your model, you do so assuming that the model will be used on data that the model hasn’t seen. In this (common) use case, it becomes a bit harder to say exactly why a certain prediction was made, though there are a lot of ways we can start to open up the black box."
  },
  {
    "objectID": "personal/2023-02-19-vermeer-at-the-rijksmuseum.html",
    "href": "personal/2023-02-19-vermeer-at-the-rijksmuseum.html",
    "title": "Vermeer at the Rijksmuseum",
    "section": "",
    "text": "[caption id=“” align=“alignnone” width=“768”] Not by Vermeer. This generated by my friend Stable Diffusion. [/caption]\nI visited the bumper Vermeer exhibition today in Amsterdam. I had previously seen many of the works separately in different galleries in the USA and here in the Netherlands, but there’s certainly a raw power to bringing them all together in the same space.\nAbove all, it was the still city landscapes from Delft that left the biggest impression. These small scenes captured so much and seemed to pull you in as if weighted down from within.\nI’m glad I managed to get tickets to the exhibition, but will enjoy revisiting the two Delft paintings when they return to the Hague later in the year."
  },
  {
    "objectID": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html",
    "href": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html",
    "title": "All the things I wish I knew about studying at school",
    "section": "",
    "text": "My niece reached out to me a few days back asking about tips for studying at school. She was specifically interested in any ideas I had about how to excel in her maths studies. I wrote up my thoughts for her and it occurred to me yesterday that there might be some benefit from putting these notes online as well.\nWithout further ado, therefore…"
  },
  {
    "objectID": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html#learning-to-understand",
    "href": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html#learning-to-understand",
    "title": "All the things I wish I knew about studying at school",
    "section": "1. Learning to Understand",
    "text": "1. Learning to Understand\n\nSomething you just have to ‘learn’\nFirst off, you’ll want to learn the small tiny ‘fact-like’ things first. Some of this you just have to put in some work and learn them. It’ll help you a LOT when it comes to getting a grasp on the bigger-picture concepts. To give an example, if there are certain equations or definitions of concepts, it can help to take a bit of time and just learn those by heart (in whatever way usually works for you for that). Note that this is limited to just small fact-like things. For sciences, maybe there are some facts or properties of something derived from the periodic table, etc, that are important. In the specific case of quadratics that you asked about, it might be simple things like:\n\nthe usual form of a quadratic equation\nthe quadratic formula\n‘what is a discriminant’\nwhat does the completed square form look like, and how do you solve it?\nwhat does this or that quadratic look like when you graph it out?\n\nThe key thing is to isolated and master those tiny morsels of information / facts early on, since a) it needs to be done anyway and b) it’ll help you with the big-picture stuff. Sometimes textbooks help out by highlighting or bolding those key concepts. Just make sure you’re not just trying to learn EVERYTHING by rote, since that advice is as good as no advice in my opinion!\n\n\nLearning for understanding: foundations\nSomething else I’ll say early on is that there are some core foundations that will serve you well:\n\nstart with a / the problem — it will help to try to wrestle with the kinds of things you’re expected to do with this particular topic you’re studying. It can actually help you to do this at the very beginning of starting a new topic: before you even study things, take a look at the kinds of questions that you’ll be tested on and expected to answer. Maybe even try to solve them. You’ll learn a lot about the subject area just by rolling around in the mud and struggling a bit.\ncuriosity — this almost goes without saying, but you should try to find a way to be curious about whatever you’re meant to be studying. It’ll help a LOT with your motivation and interest, which will in turn help you keep moving forward and give you energy at moments when things get hard. Sometimes the trick to this is just trying to ask questions about whatever it is you’re studying: why is this important? how does it connect to topic X or Y we already studied? what is most surprising about this topic? Sometimes it can also help to know a tiny bit about the history of a topic. Calculus is a bit down the road in terms of your studies, but back in the day (late 19th century, early 20th century) there were really epic debates between the various people who were developing this new area of maths. Lots of drama and falling out between people. So sometimes just knowing a bit about the personalities behind things can help.\ngrowth mindset — I wrote a blog on this a while back but the core thing is just to believe that you have it in you to master this thing. Once you have that, and if you start from that place, then you’ll have much of what you need to keep moving forward. And as one teacher once told me, as long as you never give up then you’ll eventually master it. Sometimes you just have to be a bit patient. Most things come if you give them enough time :)\n\n\n\nRetrieval Practice\nThis is sort of the core of most learning, in my opinion, and I have a lot of thoughts and practice around this. (Have read a LOT about what research recommends in this area, but I figure you’re interested mostly in whatever is practical so I’ll keep it mostly at that level.)\nThe core: if you want to learn something / master it, you have to retrieve it from your memory somehow. (Retrieve can mean lost of things: for German vocabulary it might mean knowing the gender or translation of a particular word. For Maths it might mean a particular formula, or even a high level understanding of how one concept relates to another.)\nThe important thing about this retrieval is that it will be and should be hard to do. (This is one reason why people don’t necessarily enjoy doing this, and even fewer actually do it.) It’s hard to struggle to put together a coherent explanation of topic x or y, but that struggle is what helps create neural pathways that cement the understanding going forward.\nOne possible way you could do this is to write short summaries of what you understood of a particular topic, then check your notes to see if you were correct. IMPORTANT: the key thing is to do this from memory / without notes. Otherwise you’re not actually reinforcing materials in your head.\nAnother way of testing things (e.g. in your specific case of quadratics) is to do practice examples. Your school or textbook probably gives you some practice examples, but you shouldn’t confine yourself just to doing those. Make up your own examples, or go online and find more examples (use Khan Academy or whatever).\nIt can also help to make distilled summary sheets at some point during your studies which gather together your understanding on a single piece of paper for the entire topic. Here actually you can see the one I made digitally for my own study of quadratics a few months ago:\n\nThe key thing with retrieval practice is to get a lot of it, and to try to make it the ‘hard’ / ‘difficult’ kind of retrieval. Mostly this starts with a blank sheet of paper and then you try to write down what you know about a topic, or a concept, or whatever specific thing you’re trying to understand. Writing things down will help you realise (quickly! painfully!) which parts you don’t actually understand. So it’s as much to reveal to you which parts you need to work on as it is for anything else.\n(There’s a much-praised technique named after a well-known American scientist, Richard Feynman, and you can do something like this, too:\n\nWrite the title of a topic that you want to study / test yourself on\nWrite or map out an explanation of that subject intelligible / appropriate to a non-specialist. Do this from memory.\nIdentify any gaps in your explanation / understanding.\nRelearn / restudy / interrogate to fill in the gaps.\n\nYou can use narrative / diagrams to condense and clarify your explanation. It’s basically the same idea. And yes, bullet points or spider diagrams are all possible ways of doing this.)\n\n\nDeveloping mental models\nThere’s this idea that the whole thing you’re doing when you learn something is developing ‘mental models’, which I personally find a bit hard to wrap my head around, but it is a thing… It’s maybe the next layer up in what’s happening when you try to learn something.\nMental models are, for me, about making a topic your own somehow. They’re also about making the concepts of that topic manipulable somehow.\nThe ‘making it your own’ part has a lot to do with confidence, somehow, but it’s also just feeling familiar and effective with the concepts that you feel comfortable solving problems in that area. If you see a problem, e.g., you know which techniques (or which subset of techniques) are needed to solve it. If there are multiple possible ways to solve something, you’ll have a good feel for the tradeoffs: i.e. why this way is better than that way etc. In the case of quadratics, for example, we know that there is this amazing thing which is the quadratic formula, but you probably don’t want to use that formula the whole time because it’s easy to make a mistake with it and it’s a bit cumbersome. Instead, we often use other simpler techniques that work for many (if not totally 100%) of the problems that you’ll be exposed to.\nOne way to help develop mental models is to try to explain the topic to someone else. You already did a bit of this in the retrieval practice above: trying to explain it on paper is already some of this. But trying to explain a topic to people at different levels of understanding can be really clarifying. I.e. if you had to explain quadratics to a 5-year old it’s probably different to how you’d explain it to a 40-year old. (Along with this, you can test out this approach by chatting with a chatbot about the topic. I’m sure you’ve heard of ChatGPT, but Claude is also another good option, esp for things like maths. The key thing is to start the conversation by saying something like “I would like to have a conversation about quadratics. I’ve been studying it and I’d like to test out my explanations of some core concepts with you. I would like you to tell me if things feel unclear about what I’m saying, or if you notice that there are some areas where I could improve my understanding.”)\n(While we’re here, using things like ChatGPT to develop mental models can be useful. I will often have conversations that begin with something like “What is a good way to think about the discriminant in relation to quadratic equations? Please make your explanation simple to follow and use some concrete items in your reply, like only items that you’d find in a kitchen.”)\nMaking mental models is hard! But the work you do to solidify things and make them your own is really worth it!\nAnyway, the big point here is to reflect on what you’re studying. Make sure to also give some time to connecting it to other things either in your maths studies or outside, or even life in general. It’s not the best to just view everything completely isolated and disconnected from the other topics, so try to take a step back from time to time! (Unfortunately, most schools aren’t built to encourage that process much, but it’s important!)\n\n\nSpecific contexts\nThere are some other specific contexts that require different / more targeted advice, but you didn’t mention them so I’ll ignore them a bit. But language learning is one of them, and learning some kind of ‘motor skill’ is another (i.e. that requires coordination or physical movement like playing golf or the yoyo or whatever).\n\n\nIn Practice: Understanding Quadratics\nTo summarise the practical points listed above:\n\nlearn the small fact-sized pieces early on\nget lots of retrieval practice (a mixture of examples of doing whatever the skill requires of you, and/or writing or explaining the topic at various levels)\ndevelop mental models where you can."
  },
  {
    "objectID": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html#studying-for-exams",
    "href": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html#studying-for-exams",
    "title": "All the things I wish I knew about studying at school",
    "section": "2. Studying for Exams",
    "text": "2. Studying for Exams\nI’ll take it for granted that you agree that you can’t study for something without properly understanding it, so somehow the things in part 1 are sort of a prerequisite for this section; you can’t get ready for an exam if you don’t understand what’s going on.\nThat said, there are some tactical things you can do to help your chances of success once you do have an understanding of a particular topic. Note that for all of this, it’s a bit of a question of picking which parts seem doable / manageable. It’s probably unwise / counterproductive to necessarily try to do EVERYTHING :)\n\nGround rules\nYou should understand the requirements of the exam. Take a bit of time to read through some previous exam papers. I’m sure your teachers have also given you clear guidelines on what kinds of things to expect. That will give you a map for how to prepare, so be sure to do this.\n\n\nFoundations: Exam Study\nThere are some basic foundations here which for various reasons get forgotten when you’re under exam pressure, but it’s good to remind yourself of these, since if you neglect these it’ll negatively affect your ability to study etc.\n\nsleep\neating things that nourish your body instead of just feeding cravings\ntaking breaks (every hour, ideally, get up and walk around for a minute or two)\nminimise distractions (put your phone in airplane mode or in a lock box while studying)\nmovement in general / going out of the house for walks a few times a day is a good minimum.\n‘managing your energy’ — this one’s a bit hard to quantify / explain, but I’d say it’s worth trying to embody the principle that you should only study as much today as allows you to keep studying tomorrow. I.e. if you overdo it and you study a lot today, but it’s a bit too much and tomorrow then you can’t do any study etc, then that was counterproductive. (Hope that was clear!)\n\n\n\nMnemonic / memory tricks\nThere are a TON of memory tricks out in the world. All of them are useful, but not all of them are equally useful for every situation :)\nThings like the major system, the link system and the peg system are all useful, but they require a bit of time and probably also someone who knows how they work to explain them to you.\nIf you already have a bit of experience with these things, then I’d encourage you to use them in your studies, but if you don’t have much experience then I’d say probably that it’s not going to be the difference between an A and a B grade so probably it’s a waste of your time to try to get into that in the run-up to exams.\nThat said, it would TOTALLY be a really useful investment to learn a few of these during summer holidays in a non-stress / fun way. You can play around with learning the order of decks of cards etc — I can explain all this if you’re interested — and then you’ll have that skill available to you if you need it next year or throughout your life.\nThere are some general memory principles that you can rely on in general terms:\n\nwhen trying to remember something, make it memorable in your mind! so maybe try to imagine the concepts as characters in some kind of image in your mind, and use all your senses and bring in some shock or drama etc etc. (LMK if you want more of these kinds of advice. I have a lot, but not sure how useful it is for you right now.)\n\n\n\nSpaced Repetition\nThis is a really useful tool, but it requires a bit of upfront (time) investment and unless you’re feeling super comfortable / not stressed at all, I might suggest to add it to the list of ‘things to learn about over summer / winter (?) holidays’.\nBasically this means testing yourself with (digital) flashcards, but the twist is that you only get shown the flashcard at exactly the optimum time / day when you need to be tested on it. (There’s a whole science to this which I won’t go into, but there’s a TON of backing to the fact that this is the way to make things get into your memory.)\nThe best option for this is a piece of software called Anki. It runs on your laptop and phone etc, but it has a bit of a steep learning curve mainly because the defaults it comes with aren’t great. So if you were interested I could help you set that up, but the key thing to know is that using this requires a bit of extra work.\nThe main idea is that you create (digital) flashcards for all the things you need to know, and then every day you check in with Anki to review whatever cards it says you need to know. There’s an algorithm that calculates which cards you should review. It should mesh well with your intuitive sense of how memory works: i.e. over time you slowly forget things, so Anki will prompt you to recall a particular concept just at the point before you forget it, since that exact moment is the best time to review it. When you review it at that moment and you get it right, it’ll really strengthen your memory for that thing. If you don’t remember it, then it’ll reset the status of that card and it’ll know to show it more often for a few days etc.\nThere are more manual ways to get the same effect, but (for a lot of reasons) they’re not as effective since humans don’t work / behave like computers so really using a digital tool is the only way to go.\nFor quadratics, to use my experience, I have a bunch of cards relating to that that I get tested on. Writing a ‘good flashcard’ is a bit of an art, and we can get into that if you’re interested, but I’ll just lay it here as an option for now.\n\n\nInterleaving\nThis is a fancy word for saying: ‘don’t study just one topic on its own’. When you’re testing yourself on things that you’ll need to know for exams, make sure to switch things up a lot. This means doing one problem from quadratics, then another relating to trigonometry, and another relating to topic z etc etc.\nThere is again quite a bit of evidence that this makes you much stronger in your understanding / learning, even though (or maybe because!) it’s a bit harder to do.\nIf at least part of your review of topics / facts are handled by Anki it’ll take care of giving you random flashcards anyway, so this more relates to things like solving maths problems by hand.\nSo don’t just do 50 iterations of the same maths problem, in other words. Make sure you’re switching topics etc."
  },
  {
    "objectID": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html#next-steps",
    "href": "personal/2023-08-06-all-the-things-i-wish-i-knew-about-studying-at-school.html#next-steps",
    "title": "All the things I wish I knew about studying at school",
    "section": "3. Next Steps",
    "text": "3. Next Steps\n\nCheck the practical suggestions above\nlet me know if anything’s unclear / or you want to know more about how to do thing x or y\ngather some problems to solve so you can make sure you’re practicing the things you need to study\nget into some good habits around retrieval practice (i.e. writing things down to test whether you know them or not)"
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html",
    "title": "First stitches: on learning to knit",
    "section": "",
    "text": "I previously learned how to crochet. I never made anything too fancy or intricate but it was an easy way to make beautiful presents for special occasions or individuals. This past christmas we decided that presents could only be exchanged if they were handmade. Cue: hours of double crochet to make a jumper, a snood and some embroidery of Ramallah tatreez patterns. Along the way I discovered Elise’s Purls Before Thyme channel over on YouTube, devoured all the videos and got the idea that maybe I might want to try knitting.\nWhat follows is mostly some self-reflection on some of the first practice swatches I made as part of The Knitting Guild Association (TKGA)’s Learn to Knit correspondence course. I also want a bit of a record of my progress since Ravelry isn’t really a place for logging swatches."
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#knits-and-purls",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#knits-and-purls",
    "title": "First stitches: on learning to knit",
    "section": "Knits and Purls",
    "text": "Knits and Purls\nThe first thing I did was learn knit stitches and purls. With this swatch I just put in a bunch of repetition. I didn’t have any context beyond the specific stitches, in other words how these stitches fit together or what they were called.\n\nAlong the way there were a few places, especially at the beginning, where I accidentally increased or decreased the number of stitches. I was quite happy with how it turned out, this being basically the first thing I’ve ever knit.\nIt took me ages to summon up the courage to figure out how to purl, but once I’d done it a few times it wasn’t too hard."
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#garter-stockinette",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#garter-stockinette",
    "title": "First stitches: on learning to knit",
    "section": "Garter & stockinette",
    "text": "Garter & stockinette\nI did two small swatches of garter stitch and stockinette:\n\nNote how the garter stitch doesn’t curl at the edges. That’s a characteristic of the pattern, as I learned! (And vice versa for the stockinette swatch).\nThe yellow-ish yarn I used for this was Lana Grossa Cool Merino which is bound or woven (?) in a sort of chain as its internal structure. This means it doesn’t split, but it does offer many small holes that you might accidentally end up passing your needle through. Lesson learned!"
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#single-and-double-rib",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#single-and-double-rib",
    "title": "First stitches: on learning to knit",
    "section": "Single and Double Rib",
    "text": "Single and Double Rib\nI switched to Drops Karisma for the remaining swatches. Much easier, if fairly slippery on my metal (circular) needles. Both single and double ribs came together really easily and felt satisfying to produce.\n\nYou can see that the bottom, where I cast on, is a bit uneven. I found previous swatches extremely tightly bound so I just used two needles together when casting on. This loosened things up considerably but seems to have had the unwanted side effect of making this wonky first row / edge.\nI spoke with a friend (thanks Naheed!) and she suggested I try the old Norwegian cast on. This is AKA the twisted German cast on. It’s a variation of the long-tail cast on that I’d previously been using. It took a while to get the hang of but in the end it was not so complicated in the end. It’s what I’ve been using since then.\nThe TGKA course requires me to use the long-tail cast on so I’ll have to figure it out. The other alternative suggestion that people often have when your cast on is tight is to use a needle size one larger for the cast on. My interchangeable needles require a different base / connector for the next size up, so I’m not exactly sure in my head how this will all work.\n\nWith the double ribs you can see that the base / cast on row is so much neater. Though now I’m starting to look at the right and left edges and wondering how those undulations can be reduced (or if it’s meant to be this way at all?)."
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#seed-stitch",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#seed-stitch",
    "title": "First stitches: on learning to knit",
    "section": "Seed stitch",
    "text": "Seed stitch\nThis seed stitch swatch proved trickier than expected although I learned some lessons along the way.\n\nAs you can see below, part of the swatch has what looks like some descending stockinette (or I’m not exactly sure what to call it). While doing the work I got confused as to whether I had to knit or to purl. The instruction was to do the opposite of whatever was on the stitch I was at (so knit if I saw a purl bump and so on), but this meant that I somehow ended up perpetuating mistakes I’d made on a previous row. In the end the only way to get out of it was to just stick to something regardless of what I’d done in the previous row.\nI also found myself often getting into extremely tough and tight spots, where the tension was almost unmanageable. At first I was a bit confused as to the source of the problem, but I think (working hypothesis) that it is how loose my hands are and how loose the feeding or working yarn is. When I keep my hands loose and I actively work to keep the feeding yarn free, that seemed to improve things. But it often felt like I was being haunted by the ghosts of decisions past, where some tight knitting of a previous row would resurface and I’d have to deal with it in the next row even if my hands were much looser."
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#increases-decreases-and-yarnovers",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#increases-decreases-and-yarnovers",
    "title": "First stitches: on learning to knit",
    "section": "Increases, decreases and yarnovers",
    "text": "Increases, decreases and yarnovers\nThe final swatch includes multiple different techniques and somehow it’s the least obviously successful of the lot.\n\nWith this, I started off with some stockinette, then increased the number of stitches on the row with the bar increase and three kinds of Make One increase (M1-L, M1-R and M1-O). At that point I did some more stockinette and then some decreases (k2tog, ssk and SKP).\nThen there were some yarn overs. It’s unclear to me why yarn overs are taught as a whole separate category or section from increases, since functionally they both are adding stitches. Also at this point in the pattern it seemed like I had fewer stitches on the needle than the pattern or instructions were expecting, but it was time to bind off.\nThis swatch looks somehow the least ‘nice’ of all of them. I’m not quite sure why. I also struggled quite a lot with tension (i.e. extremely tight stitches to handle), but towards the end I got the end of making it looser. Clearly there’s a balance to be had: tighter makes for neat and tidy, but at a certain point it’s unworkable. Conversely, by the end I was much looser in my hands but that’s where you can see that things look a lot less uniform."
  },
  {
    "objectID": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#next-steps",
    "href": "personal/2025-01-05-first-stitches-on-learning-to-knit.html#next-steps",
    "title": "First stitches: on learning to knit",
    "section": "Next steps",
    "text": "Next steps\nThe next (and final) sections in the learning part of the course are weaving in loose ends as well as blocking. I’ll try to block the last swatch to see if that makes things clearer (and just for the experience, not having blocked anything before).\nOnce that’s all done (tonight, if I’m lucky!), it’ll then be time to put everything I’ve learned into practice by making some larger swatches. These swatches can then be submitted to TKGA for feedback (the correspondence part of correspondence course) which I’m looking forward to.\nDown the line, and depending on how the feedback from my first swatches goes, I’d like to maybe try some other courses and perhaps even the Master Hand Knitting certification. I’ll also be dipping into the Fearless Knitting Workbook which is also swatch-based, a format I’ve been really enjoying as it allows me to get fast feedback and improve my skills."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "",
    "text": "I am working my way through the fastai course as part of an online meetup group I host.1\nThis week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’).\nJeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course emphasise repeatedly.)\nI decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#the-problem-domain-image-redaction",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#the-problem-domain-image-redaction",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "The Problem Domain: Image Redaction",
    "text": "The Problem Domain: Image Redaction\nUnder the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here.\nQuite often, however, these images are censored or redacted.\n\nKnowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case.\nIt could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless.\nIn the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#getting-the-data",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#getting-the-data",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Getting the Data",
    "text": "Getting the Data\nThe first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training.\nNote that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end.\nAt the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#labelling-the-images-with-prodigy",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#labelling-the-images-with-prodigy",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Labelling the images with Prodigy",
    "text": "Labelling the images with Prodigy\nI had used Explosion.ai’s Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly.\n\nIt took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not:\n\nFrom that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted:\nimport json_lines\nfrom PIL import Image\nfrom pathlib import Path\n\ndef save_resized_image_file(location_path):\n    basewidth = 800\n    img = Image.open(record['image'])\n    wpercent = (basewidth / float(img.size[0]))\n    hsize = int((float(img.size[1]) * float(wpercent)))\n    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n    img.save(location_path)\n\npath = '/my_projects_directory/redaction-model'\n\nredacted_path = path + \"/redaction_training_data/\" + \"redacted\"\nunredacted_path = path + \"/redaction_training_data/\" + \"unredacted\"\n\nwith open(path + \"/\" + \"annotations.jsonl\", \"rb\") as f:\n    for record in json_lines.reader(f):\n        if record[\"answer\"] == \"accept\":\n            save_resized_image_file(Path(redacted_path + \"/\" + record['meta']['file']))\n        else:\n            save_resized_image_file(Path(unredacted_path + \"/\" + record['meta']['file']))"
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#transferring-the-data-to-paperspace-with-magic-wormhole",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#transferring-the-data-to-paperspace-with-magic-wormhole",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Transferring the data to Paperspace with magic-wormhole",
    "text": "Transferring the data to Paperspace with magic-wormhole\nOnce I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer.\nI used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data.\nAgain, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#using-the-labelled-data-in-our-training",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#using-the-labelled-data-in-our-training",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Using the labelled data in our training",
    "text": "Using the labelled data in our training\nThe process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the DataBlock class instead of ImageDataLoaders for extra flexibility.\npath = Path('redaction_training_data')\n\nfoia_documents = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(224))\n\ndls = foia_documents.dataloaders(path)\n\nfoia_documents = foia_documents.new(\n    item_tfms=Resize(224, method='pad', pad_mode='reflection'),\n    batch_tfms=aug_transforms(max_zoom=1))\ndls = foia_documents.dataloaders(path)\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(10)\nThe images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate.\nI train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-augmentations",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-augmentations",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Experimenting with augmentations",
    "text": "Experimenting with augmentations\nInitially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found.\nIn the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of the documentation here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in.\nI chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-different-architectures",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-different-architectures",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Experimenting with different architectures",
    "text": "Experimenting with different architectures\nThe course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison.\nThe danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#hosting-the-model-with-mybinder",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#hosting-the-model-with-mybinder",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Hosting the model with MyBinder",
    "text": "Hosting the model with MyBinder\nChapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online.\nUsing MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you’ll see an interface where you should first upload an image — i.e. a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#next-steps",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#next-steps",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Next steps",
    "text": "Next steps\nI’m at the point in the course where I know enough to be dangerous (i.e. train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy:\n\nbetter augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied.\nmore labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me.\ndifferent redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong).\n\nOtherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted.\nI will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#footnotes",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#footnotes",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can find our thread in the fastai forum here.↩︎\nOther countries have variations of this law, like this from the United Kingdom.↩︎\nI realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator.↩︎"
  },
  {
    "objectID": "posts/2021-09-09-auto-reload-external-libraries.html",
    "href": "posts/2021-09-09-auto-reload-external-libraries.html",
    "title": "How to set a Jupyter notebook to auto-reload external libraries",
    "section": "",
    "text": "The code to insert somewhere into your Jupyter notebook is pretty simple:\n%load_ext autoreload\n%autoreload 2\nWhen you’re working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html",
    "href": "posts/2021-09-11-tfx-paper.html",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "",
    "text": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you’d need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It’s worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale)."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#a-tensorflow-based-general-purpose-machine-learning-platform",
    "href": "posts/2021-09-11-tfx-paper.html#a-tensorflow-based-general-purpose-machine-learning-platform",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "‘A TensorFlow-based general-purpose machine learning platform’",
    "text": "‘A TensorFlow-based general-purpose machine learning platform’\nThe engineers wanted a general-purpose tool, one that could serve many different use cases. I haven’t yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed.\nThe problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn’t anticipated, specifically sequence-to-sequence language models used in machine translation)."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#an-end-to-end-platform",
    "href": "posts/2021-09-11-tfx-paper.html#an-end-to-end-platform",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "An end-to-end platform",
    "text": "An end-to-end platform\nThe ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I’d say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‘best practices’, but certainly there hasn’t been uniform acceptance of these.\nI imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it’s easier to be fast and iterate and do all the things we expect of a more agile process."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#continuous-training-and-serving",
    "href": "posts/2021-09-11-tfx-paper.html#continuous-training-and-serving",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Continuous training and serving",
    "text": "Continuous training and serving\nTFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams.\nIn this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it’s a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#reliable-serving-models-at-scale",
    "href": "posts/2021-09-11-tfx-paper.html#reliable-serving-models-at-scale",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Reliable serving models at scale",
    "text": "Reliable serving models at scale\nIf you’re Google, you need to make sure that you aren’t serving garbage models to your users, or that inconsistencies in the input data aren’t polluting your retraining processes. At scale, even small mistakes compound really easily.\nIn the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn’t entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models — all of which had to happen in a reliable manner — was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#fast-retraining-with-warm-starting",
    "href": "posts/2021-09-11-tfx-paper.html#fast-retraining-with-warm-starting",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Fast retraining with ‘warm-starting’",
    "text": "Fast retraining with ‘warm-starting’\nFor the specific challenge of retraining models with streaming data, engineers were finding that they couldn’t retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‘warm-starting’) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#missing-pieces",
    "href": "posts/2021-09-11-tfx-paper.html#missing-pieces",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Missing pieces",
    "text": "Missing pieces\nThere are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area.\nSimilarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you’d want to care about.\nAs a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you’re operating at serious scale. If you’re a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes.\nA couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I’d be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries.\nAgain on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually.\nI wasn’t active in the field in 2017, so it’s hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn’t seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google."
  },
  {
    "objectID": "posts/2021-09-18-reading-python.html",
    "href": "posts/2021-09-18-reading-python.html",
    "title": "Reading Python Code",
    "section": "",
    "text": "It’s a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you’re going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote.\nOne way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you’re reading.\nI’ll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it’ll be useful for someone else as well."
  },
  {
    "objectID": "posts/2021-09-18-reading-python.html#good-quality-python-code",
    "href": "posts/2021-09-18-reading-python.html#good-quality-python-code",
    "title": "Reading Python Code",
    "section": "Good Quality Python Code",
    "text": "Good Quality Python Code\n\njinja — a templating engine written in Python (and see the recommendations for supplemental reading and watching for jinja here)\nhowdoi — a search tool for coding answers via the command line\nflask — a micro-web framework for Python\nFastAPI — another web framework that’s a bit larger than flask\ndiamond — a Python daemon that collects and publishes system metrics\nwerkzeug — a web server gateway library\nrequests — an HTTP library, now part of the Python standard library\ntablib — library for Pythonic way to work with tabular datasets\nclick — a Python package for creating command line interfaces\npathlib — part of the Python standard library; a module to handle filesystem paths (also the corresponding PEP proposal #428)\ndataclasses — a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding PEP proposal #557)\njoblib — a library to support lightweight pipelining in Python"
  },
  {
    "objectID": "posts/2021-09-18-reading-python.html#other-resources",
    "href": "posts/2021-09-18-reading-python.html#other-resources",
    "title": "Reading Python Code",
    "section": "Other Resources",
    "text": "Other Resources\n\n500 Lines or Less — a book in which specific small open-source projects are profiled to understand how they approached their particular challenge.\nThe Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks — examination of the structure of the software of some open-source software applications.\nThe Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks — the second volume in the series."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html",
    "href": "posts/2021-10-25-debugging.html",
    "title": "Some things I learned about debugging",
    "section": "",
    "text": "I’ve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it’d be useful to put down some thoughts about things that I’ve learned along the way."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#logging-printing",
    "href": "posts/2021-10-25-debugging.html#logging-printing",
    "title": "Some things I learned about debugging",
    "section": "Logging & Printing",
    "text": "Logging & Printing\nThese are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them.\nThere are some scenarios where simple print calls aren’t enough. If you’re running code through a series of tests, then the test harness will often consume all output to stdout so you won’t see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers.\nOnce you can see what’s happening at a particular moment, you can see if what you expected to happen at that moment is actually happening."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#breakpoint-your-way-to-infinity",
    "href": "posts/2021-10-25-debugging.html#breakpoint-your-way-to-infinity",
    "title": "Some things I learned about debugging",
    "section": "Breakpoint your way to infinity!",
    "text": "Breakpoint your way to infinity!\nThe breakpoint() function comes built-in with Python. It’s a convenience wrapper around some pdb magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment.\nI wish I had known about this earlier on. It’s extremely useful for understanding exactly how a function or piece of code is being executed."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#come-with-hypotheses",
    "href": "posts/2021-10-25-debugging.html#come-with-hypotheses",
    "title": "Some things I learned about debugging",
    "section": "Come with hypotheses",
    "text": "Come with hypotheses\nIf you don’t have a sense of what you expect to happen, it’s going to be hard to determine if what you’re doing is having any effect or not.\nI’ve been lucky to do some pairing sessions with people as they work through bugs and problems, and I’ve had this ‘come with a hypothesis’ behaviour modelled really well for me.\nIt’s not a panacea; there’s still a lot of work to be done around this, but it’s sort of the foundation, particularly for non-trivial bugs."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#leave-your-assumptions-at-the-door",
    "href": "posts/2021-10-25-debugging.html#leave-your-assumptions-at-the-door",
    "title": "Some things I learned about debugging",
    "section": "Leave your assumptions at the door",
    "text": "Leave your assumptions at the door\nDon’t assume what’s written is what’s actually working. This applies to the code you’re working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place.\nThe rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you’re importing as well. Of course, it’s probably more likely that you’re misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#follow-the-thread-wherever-it-leads",
    "href": "posts/2021-10-25-debugging.html#follow-the-thread-wherever-it-leads",
    "title": "Some things I learned about debugging",
    "section": "Follow the thread wherever it leads",
    "text": "Follow the thread wherever it leads\nThis is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#be-systematic",
    "href": "posts/2021-10-25-debugging.html#be-systematic",
    "title": "Some things I learned about debugging",
    "section": "Be systematic",
    "text": "Be systematic\nI’ve found a few times now, that there are certain moments where I notice I’m far far down the road. I’ll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I’ve made in order to reach this point.\nI’ll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I’m travelling down. I’ll specifically write down all the conditions that need to be present for this bug to present (as far as I know them).\nQuite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn’t, it’s extremely useful in re-grounding myself and reminding me of why I’m going down rabbit hole x or y."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#know-when-to-stop",
    "href": "posts/2021-10-25-debugging.html#know-when-to-stop",
    "title": "Some things I learned about debugging",
    "section": "Know when to stop",
    "text": "Know when to stop\nIn an ideal world you’d get to follow every windy road and to figure out everything that doesn’t make sense. But — and this is again especially true for fast-moving startups — you might not always have time to do that.\nThis is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you’d planned on spending on a particular bug. If you’re finding that it’s taking far longer than expected, and you have other things you’re committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#remember-this-is-the-work",
    "href": "posts/2021-10-25-debugging.html#remember-this-is-the-work",
    "title": "Some things I learned about debugging",
    "section": "Remember: this is the work",
    "text": "Remember: this is the work\nSometimes when I’m fixing bugs I have the feeling that I’m wasting my time somehow, or that I should be doing something more productive. It’s often the case, though, that this is the work. I’m low on experience, but proxy experience that I’ve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#know-when-to-ask-for-help",
    "href": "posts/2021-10-25-debugging.html#know-when-to-ask-for-help",
    "title": "Some things I learned about debugging",
    "section": "Know when to ask for help",
    "text": "Know when to ask for help\nSometimes there are bugs which turn out to be bigger than you’re able to handle. It’s certainly worth pushing back against that feeling the first few times you feel it. Early on it’s often going to feel like the bug is unsolvable.\nBut some times there are pieces of context you don’t have, which a quick overview of what you’ve done and tried might alert someone more season to the fact that you’re going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice."
  },
  {
    "objectID": "posts/2021-11-25-entr-reruns-tests.html",
    "href": "posts/2021-11-25-entr-reruns-tests.html",
    "title": "entr: a tool to run commands when files change",
    "section": "",
    "text": "It’s a fairly common pattern that you have some code that you’re repeatedly running. Perhaps you’re fixing a failing test, and you just have to keep running it every time you make a fix.\nEnter entr. This handy little tool reruns a particular command whenever changes are detected in a particular set of files.\nLet’s take the example I mentioned above: you have a failing test that you’re debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in src and you’re using pytest, then you could use something like the following:\nls src/*.py | entr -c pytest test.py::test_some_feature\nSo now, any time you change any Python file inside the src folder, it’ll rerun your test. The -c flag will clear the terminal every time the test runs.\n[Many thanks to calmcode for continuing to make these really useful videos.]"
  },
  {
    "objectID": "posts/2021-11-27-pipeline-conversations.html",
    "href": "posts/2021-11-27-pipeline-conversations.html",
    "title": "Launching a podcast about MLOps",
    "section": "",
    "text": "I’ll be co-hosting a new podcast about MLOps, with new episodes out every fortnight. Pipeline Conversations: A Machine Learning Podcast by ZenML is the new podcast from the company where I work. (We build an open-source tool for data scientists to empower them to take control of how their models live in production.)\nOur first episode gets into some of the background for why ZenML exists in the first place. Upcoming episodes will be discussions with guests from the data science and MLOps space.\nI’m excited to get the opportunity to talk with so many interesting and smart people."
  },
  {
    "objectID": "posts/2021-11-29-prodigy-object-detection-training.html",
    "href": "posts/2021-11-29-prodigy-object-detection-training.html",
    "title": "How to annotate image data for object detection with Prodigy",
    "section": "",
    "text": "I’m back to working on the redaction model, though this time with a slightly more focused objective: object detection.\nObject detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify — for an arbitrary image — which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted.\nFor this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example.\nI showed in an earlier post how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn’t enough to offer a binary ‘yes’ or ‘no’ for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction.\nIn terms of the final output of the annotations, there are two main ways that this could go. I could either:\n\nget x and y coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point\nget the four coordinates for each of the corners of the bounding box.\n\nThe COCO dataset format will eventually want datasets in the second format, but Prodigy has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a custom recipe which will save the data in exactly the format that I want. For now, it’s good enough.\nInstalling Prodigy into your development environment is a breeze now that you can do it with pip:\npip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy # where the XXXs are your license code\nGetting going with the image training was as easy as the following CLI command:\nprodigy image.manual redaction-object-detection /path/to/image/data --label CONTENT,REDACTION --remove-base64\nNote that the --remove-base64 is to ensure that Prodigy doesn’t store the raw binary image data inside the database alongside the annotations. Prodigy (and their sister tool Spacy) is a little more focused on textual data, where storing the original data alongside the annotation doesn’t pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database.\nYou get a local URL to go visit and you see an interface where you can make the necessary annotations:\n\nYou can see that I am distinguishing between two different classes: redactions and content. Redactions are what we’ve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I’d want a final amount closer to 100% for that image rather than the 50% I’d get if I just went with the total percentage of redacted pixels on the whole image file.\nDoing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this:\n\nThe whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction?\nAnd for the following image, what is the right way to think about how to make the annotation?\n\nThis redaction encompasses multiple lines, so to some extent it doesn’t make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)?\n{% include info.html text=“As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important.” %}\nOnce we’re done with our annotations, we can easily export our data to a jsonl file with the following CLI command:\nprodigy db-out redaction-object-detection &gt; ./redaction-object-detection-annotations.jsonl\nThis gives us a file containing all our annotations. A sample for one image gives the idea:\n{\n  \"image\": \"sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg\",\n  \"text\": \"04-F-0269_Global_Screening_Guidance-03\",\n  \"meta\": { \"file\": \"04-F-0269_Global_Screening_Guidance-03.jpg\" },\n  \"path\": \"sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg\",\n  \"_is_binary\": false,\n  \"_input_hash\": 1413334570,\n  \"_task_hash\": 1588323116,\n  \"_view_id\": \"image_manual\",\n  \"width\": 800,\n  \"height\": 1035,\n  \"spans\": [\n    {\n      \"id\": \"0ef6ccd0-4a79-471d-9aa1-9c903c83801e\",\n      \"label\": \"CONTENT\",\n      \"color\": \"yellow\",\n      \"x\": 76.5,\n      \"y\": 112.5,\n      \"height\": 786.1,\n      \"width\": 587.6,\n      \"center\": [370.3, 505.55],\n      \"type\": \"rect\",\n      \"points\": [\n        [76.5, 112.5],\n        [76.5, 898.6],\n        [664.1, 898.6],\n        [664.1, 112.5]\n      ]\n    },\n    {\n      \"id\": \"cd05d521-8efb-416b-87df-4624f16ca7f3\",\n      \"label\": \"REDACTION\",\n      \"color\": \"cyan\",\n      \"x\": 80.3,\n      \"y\": 786.2,\n      \"height\": 20.2,\n      \"width\": 428.4,\n      \"center\": [294.5, 796.3],\n      \"type\": \"rect\",\n      \"points\": [\n        [80.3, 786.2],\n        [80.3, 806.4],\n        [508.7, 806.4],\n        [508.7, 786.2]\n      ]\n    },\n    {\n      \"id\": \"3e268e33-4eba-457d-8d17-8271a79ee589\",\n      \"label\": \"REDACTION\",\n      \"color\": \"magenta\",\n      \"x\": 108.1,\n      \"y\": 772.3,\n      \"height\": 15.1,\n      \"width\": 400.6,\n      \"center\": [308.4, 779.85],\n      \"type\": \"rect\",\n      \"points\": [\n        [108.1, 772.3],\n        [108.1, 787.4],\n        [508.7, 787.4],\n        [508.7, 772.3]\n      ]\n    }\n  ],\n  \"answer\": \"accept\",\n  \"_timestamp\": 1638214078\n}\nEverything we’re interested in is inside the spans attribute, and it actually contains both kinds of the annotation that I mentioned above.\nAs you can see, annotating images in this way is fairly painless, and it brings you in closer contact with your raw data which is an added bonus."
  },
  {
    "objectID": "posts/2021-12-11-redaction-progress-week-one.html",
    "href": "posts/2021-12-11-redaction-progress-week-one.html",
    "title": "73% accuracy for redaction object detection",
    "section": "",
    "text": "Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all.\nOnce I had my redactions, I needed to convert the files from a Prodigy format into a .coco annotation format. I am using IceVision, a really useful computer vision library, for which it is easier if I pass in the annotations in the .coco format.\nFrom that point, it was fairly easy to follow the steps of the object detection tutorial outlined in the IceVision documentation. I ran into some problems with Paperspace Gradient not easily installing and importing IceVision. For some reason files don’t get unzipped on Paperspace, but it’s possible to just do this manually:\n\nDo the basic install, including the import of icevision.all. Wait for the error to get raised, then open up a terminal and enter:\n\ncd /root/.icevision/mmdetection_configs/\nrm v2.16.0.zip\nwget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip\nunzip v2.16.0.zip\nThen run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal):\njupyter nbextension enable --py widgetsnbextension\nThis enables ipywidgets in the notebook, I think.\nOnce through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected VFNet as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%:\n\nIf we look at some of the results (using model_type.show_results()) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect:\n\nI was surprised that something like this worked as well as it did:\n\nIt wasn’t perfect, but I don’t remember having annotated too many of this specific redaction type, so I’m fairly happy with how it worked out. You can see it still makes a number of mistakes and isn’t always precise about where the boxes should go. I hope that’ll improve as I add more examples of this type of redaction.\nMy next steps for this project include the following:\n\ncreate synthetic data. The redactions are probably easy enough to mimic where we’ll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It’ll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy.\npotentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates).\nthink through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training.\nadd in experiment tracking. I’ve never used something like Weights & Biases, so I’m excited to try that out and have a real process for tracking my progress throughout this project.\ncleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It’s starting to get a bit unwieldy and I’m worried I’ll start to forget the order things were done and some of those small details."
  },
  {
    "objectID": "posts/2021-12-29-j-language.html",
    "href": "posts/2021-12-29-j-language.html",
    "title": "Exploring J, an array programming language",
    "section": "",
    "text": "I’ve long wanted to explore the J programming language. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He’s since spoken about it in other places.\nIt is part of the family of programming languages that includes APL, K and Q. These can broadly be categorised as array-programming languages, where arrays are generally the core data structure and mental model to keep in mind. They used to be extremely popular in the 1970s and 1980s, particularly among institutions or businesses with a requirement for performant calculation / computation. One of these, Q, continues to live on (as a closed-source language) in the world of finance and trading. (Q is popular alongside the proprietary database kdb+).\nYou’re probably wondering why someone would want to use this fairly specialised and niche language. When you look at examples of J code — like the ones here, for example — it’s easy to simply dismiss it as an unreadable (‘write-only’) language. Indeed, many do dismiss it for this reason. Code is often compact, with single letters or symbols doing all the work. Defenders of J hold this up as a feature, not a problem. The compactness of the language means that you can fit the entirety of the solution (space) of a complex problem on a single screen, whereas in many (most?) other languages you would have to be scrolling up and down through dozens or even hundreds of lines of code.\nThe array languages seem to come at solving problems from a particular perspective. The symbols and letters that transform the arrays in J function as a pattern language. For a simple example, think of what you have to do when you want to find the count of a particular element from within an array/list. The array language paradigm argues that you don’t want to waste your time and screen space writing out boilerplate code to carry out this calculation, when it’s a common pattern that you can just use from the language itself. When problem-solving, therefore, spend your time thinking about the problem and not messing around with syntax or repeating yourself.\nJ and its cousins are extremely efficient. It is written in C, and I recently heard someone quote one of the early J pioneers as having said that “it is not theoretically possible to write J code that is more performant than C, but it often ends up being so”. For some math- or statistics-heavy domains (think the world of finance), it is extremely helpful to have this highly abstracted language that works performantly on large datasets. Moreover, it seems to be even more helpful when you have a hard problem to work on that isn’t fully understood.\nKenneth Iverson’s wrote a paper (“Notation as a Tool of Thought”) that is a classic in computer science and gets into some of the above arguments. (It is written using APL, but it also applies to J). I will probably return to that at a future date, because it often comes up and is recommended as a particularly rich document worth taking time to explore in depth.\nVery much as a project to indulge my curiosity, I will be exploring J over the coming months. I have been listening to the back catalogue of The Array Cast podcast, and I will be slowly working my way through some of the resources listed on the official J site. Let me know if you have experience working with J!"
  },
  {
    "objectID": "posts/2021-12-30-robust-python-2.html",
    "href": "posts/2021-12-30-robust-python-2.html",
    "title": "What’s special about types in Python?",
    "section": "",
    "text": "The first section of Robust Python dives into types. We begin by taking a step back to think about what exactly types are being used for, and what they might bring us. Python was not (until v3.5) a language with which you could easily use typing. I remember going to the Pylondinium conference in London in 2018 and going to a talk by Bernat Gabor about type hints in Python. Back then I didn’t have much of a sense of how new they were to many people, but even now I don’t get the feeling that they’ve been universally adopted. Hence Patrick’s book, I suppose…\nA type is defined in the book as being “a communication method”, both to / for computers (“mechanical representation”) as well as for humans (“semantic representation”). For the computer, when a variable is of a certain type this determines what methods can be called on that particular object. As such, though I’m straying into territory I don’t fully understand, I believe it also helps with compilation efficiency. (Python is a dynamically-typed language so any errors or type mismatches will only become apparent at runtime, however).\nFor humans, types can help signal intent. This connects with my previous chapter summary from this book where I stated that code should communicate intent well to be considered ‘robust’. Take the following simple code snippet:\ndates = [...]\n\ndef process_date(input):\n  date = extract_date(input)\n  dates.append(date)\n  return date\nWe have an extract_date function (defined elsewhere in the code), but we have no real sense of what this input parameter would be. Are we taking in strings as input? Are we taking in datetime.datetime objects? Does the extract_date function accept both, or do we need to ensure that we are only taking a specific type? All these questions could be cleared up with a simple type hint as part of the function definition, like so:\ndates = [...]\n\ndef process_date(input: datetime.datetime):\n    date = extract_date(input)\n  dates.append(date)\n  return date\nNow we know what the input should be, and we can also add a type hint to the extract_date function as well which will help communicate our intent.\nWe also learn how Python is more towards the ‘strongly-typed’ side of things on the language spectrum. If you try to concatenate a list with a dict in Python using the + operator, Python will throw a TypeError and fail. If you try to do the same in Javascript you get two different answers depending on the order of the two operands:\n&gt;&gt;&gt; [] + {}\n\"[object Object]\"\n\n&gt;&gt;&gt; {} + []\n0\nFor our purposes, using Python, we can use the strong typing to our advantage.\nPython is dynamically typed, though, which takes a bit more caution to handle in a robust manner. Any type mismatches will only be found at runtime — at least using just the vanilla install of the language without any extra imports or modules.\nThe chapter ends with a brief discussion of duck typing, defined as “the ability to use objects and entities in a programming language as long as they adhere to some interface”. We gain a lot in terms of increased composability, but if you rely on this feature of the language too much then it can become a hindrance in terms of communicating intent.\nThis chapter didn’t add too many new concepts or skills to my current understanding of the benefits of types, but it was useful to have this concept of ‘communicating intent’ to be reiterated. When I think back to how I’ve heard types mentioned in the past, they often get cast in a technical sense, whereas thinking about communication between developers I think is a more motivating framing."
  },
  {
    "objectID": "posts/2022-01-03-robust-python-3.html",
    "href": "posts/2022-01-03-robust-python-3.html",
    "title": "Getting practical with type annotations and mypy",
    "section": "",
    "text": "The third chapter of ‘Robust Python’ offers a quick introduction to the practicalities of type annotations in Python. We also see tools like mypy being used to catch places where the reality of your code doesn’t necessarily match the type annotations that you’ve stated.\nFor the first, a quick example can suffice:\nname: str = \"alex\"\n\ndef some_function(some_number: int, some_text: str = \"some text\") -&gt; str:\n    # your code goes here\n    return \"\" # returns a string\nYou can see the different places that type annotations might appear. You can annotate variables in your code. I’ve seen this one less often, but it’s possible. Then you can have type annotations for the parameters when defining functions (some even with default values assigned). You can also have type annotations for the return value of those functions.\n\nNote that type hints are not used at runtime, so in that sense they are completely optional and don’t affect how your code runs when it’s passed through the Python interpreter. (Type hints were introduced in Python 3.5, though there is a way to achieve the same effect using comments and a standard way of listing type annotations that way if you are stuck with a 2.7 codebase, for example.)\nWith some type annotations added to our code, we can use a typechecker like mypy to see whether things are really as we imagine. In Viafore’s own words:\n\n“type checkers are what allow the type annotations to transcend from communication method to a safety net. It is a form of static analysis.”\n\nIf your codebase uses type annotations to communicate intent, and you’re using mypy to catch any of those type errors, remember that typecheckers only catch this certain type of errors. You still need to be doing testing and all the other best practices to help catch the rest.\nOne forward-looking benefit covered by this chapter was how having code covered with type annotations and type checking could give you the confidence to change things in the codebase that otherwise you would have hesitated to even approach. There are, of course, also some tradeoffs and disadvantages to adding this in: particularly around speed of iteration and possibly flexibility, but the book makes a strong case for why most large Python codebases could probably use type checking as part of their arsenal."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html",
    "href": "posts/2022-01-08-robust-python-4.html",
    "title": "Different ways to constrain types in Python",
    "section": "",
    "text": "The fourth chapter of ‘Robust Python’ continues on from where we left off last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal.\nNote that you can assign all of these type assignments to variables (‘type aliases’), which might just make your code that much more readable."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#optional-to-catch-none-references",
    "href": "posts/2022-01-08-robust-python-4.html#optional-to-catch-none-references",
    "title": "Different ways to constrain types in Python",
    "section": "Optional to catch None references",
    "text": "Optional to catch None references\nOptional as a type annotation is where you want to allow a specific type or None to be passed in to a particular function:\nfrom typing import Optional\n\ndef some_function(value: Optional[int]) -&gt; int:\n    # your code goes here\nNote that you’ll probably want (and mypy will remind you if you forget) to handle what happens in both those cases inside your function. (You may need to specifically pass in the —strict-optional flag to catch this when using mypy.)"
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#union-to-group-types-together",
    "href": "posts/2022-01-08-robust-python-4.html#union-to-group-types-together",
    "title": "Different ways to constrain types in Python",
    "section": "Union to group types together",
    "text": "Union to group types together\nThis is used when multiple different types can be used for the same variable:\nfrom typing import Union\n\ndef returns_the_input(input: Union[str, int]) -&gt; Union[str, int]:\n    return input\nThis function doesn’t really do anything, but you get the idea. Note, too, that Optional[int] is really a version of Union[int, None]. (The book gets into exactly why we might care about reducing the number of possible options by way of a little detour into set theory.)"
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#literal-to-include-only-specific-values",
    "href": "posts/2022-01-08-robust-python-4.html#literal-to-include-only-specific-values",
    "title": "Different ways to constrain types in Python",
    "section": "Literal to include only specific values",
    "text": "Literal to include only specific values\nA little like what I believe enumerations do, we also have the Literal type. It restricts you to whatever specific values are defined:\nfrom typing import Literal\n\ndef some_function(input: Literal[1, 2, 3]) -&gt; int:\n    return input\nHere the function is restricted to inputs that are either 1, 2 or 3. Note that these are a feature that applies to Python 3.8 and above."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#annotated-for-more-complicated-restrictions",
    "href": "posts/2022-01-08-robust-python-4.html#annotated-for-more-complicated-restrictions",
    "title": "Different ways to constrain types in Python",
    "section": "Annotated for more complicated restrictions",
    "text": "Annotated for more complicated restrictions\nThese are available, but not really useful since they only function as a communication method. You can specify specific restrictions such as the following (example is taken from the book, p. 56:\nfrom typing import Annotated\n\nx: Annotated[int, ValueRange(3,5)]\ny: Annotated[str, MatchesRegex('[abc]{2}')\nRead more about it here. The book doesn’t spend much time on it and it seems like it’s probably best left alone for the moment."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#newtype-to-cover-different-contexts-applied-to-the-same-type",
    "href": "posts/2022-01-08-robust-python-4.html#newtype-to-cover-different-contexts-applied-to-the-same-type",
    "title": "Different ways to constrain types in Python",
    "section": "NewType to cover different contexts applied to the same type",
    "text": "NewType to cover different contexts applied to the same type\nNewType, on the other hand, is quite useful. You can create new types which are identical to some other type, and those new values made with the new type will have access to all the methods and properties as the original type.\nfrom typing import NewType\n\nclass Book:\n    # you implement the class here\n    \nNewBook = NewType(\"NewBook\", Book)\n\ndef process_new_book(book: NewBook):\n    # here you handle what happens to the new book\nYou can achieve something like the same thing with classes and inheritance, I believe, but this is a lightweight version which might be useful to achieve the same end goal."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#final-to-prevent-reassignment-rebinding",
    "href": "posts/2022-01-08-robust-python-4.html#final-to-prevent-reassignment-rebinding",
    "title": "Different ways to constrain types in Python",
    "section": "Final to prevent reassignment / rebinding",
    "text": "Final to prevent reassignment / rebinding\nYou can specify that a particular variable should have a single value and that value only. (Note that mutations of an object etc are all still possible, but reassignment to a new memory address is not possible.\nfrom typing import Final\n\nNAME: Final = \"Alex\"\nIf you tried to subsequently change this to a different name, mypy would catch that you’d tried to do this. This can be valuable across very large codebases, where the potential for someone to reassign a variable might be not insignificant.\nSo there you have it: a bunch of different ways to handle combinations of types and/or more complicated annotation scenarios. The next chapter will cover what happens when we throw collections into the mix, and what type annotation challenges are raised."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html",
    "href": "posts/2022-01-18-robust-python-5.html",
    "title": "Using type annotation with collections in Python",
    "section": "",
    "text": "The fifth chapter of ‘Robust Python’ continues on from where we left off last time. We saw how to apply type annotations when simple things like strings, integers and floats were involved. This chapter deals with the different ways you annotate your types when collections get involved.\nWe start with the context for why this is even something that requires a separate chapter to deal with. This involves the difference between homogenous and heterogeneous types. For a Python list, we could say it had homogenous types if all the items were of the same type (strings, e.g.). If this list contains multiple different types (a mix of strings and integers, e.g.) then we’d have to say it contained heterogenous types. This is of importance given that the presence of multiple types in a single list is going to require you to handle the types differently. Even in the most trivial of examples (as with strings and integers being together), the interfaces for both are different. Try adding a string to an integer in Python and see what happens.\nSo it’s actually not quite true to say that a collection of homogenous types have to all be exactly the same type, but they must share common interfaces and ideally be handled using the same logic. If you think about it, in the real world heterogenous types are pretty common occurrences. There are often situations where, for example, you have to handle the output of API calls or data that doesn’t derive from code that’s in yous control and then you’ll perhaps be dealing with a dictionary that contains all sorts of types.\nIn Python we do have the typing.Any annotation, but it’s pretty clear — and the book emphasises this — that isn’t really useful in the vast majority of cases. You might as well not bother with type annotations if you’re going to liberally be using Any."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#the-first-of-our-collection-type-helpers-typeddict",
    "href": "posts/2022-01-18-robust-python-5.html#the-first-of-our-collection-type-helpers-typeddict",
    "title": "Using type annotation with collections in Python",
    "section": "The first of our collection type helpers: TypedDict",
    "text": "The first of our collection type helpers: TypedDict\nTypedDict was introduced in Python 3.8 and allows you to communicate intent when it comes to the types that are being passed through your code. Note that, as with a lot of what we’re talking about here, this is all information that’s useful for a type checker and isn’t something that is dynamically checked.\nYou can use TypedDict to define structures that specify the types of fields of your dictionary in a way that is easier to parse as a human reader than just using dict. See this example, adapted from one in the book:\nfrom typing import TypedDict\n\nclass Range(TypedDict):\n    min: float\n    max: float\n\nclass Stats(TypedDict):\n    value: int\n    unit: str\n    confidenceRange: Range\n\nour_stats = Stats(value=3, unit=\"some_name\", confidenceRange=Range(min=1.3, max=5.5))\nprint(our_stats) # returns {'value': 3, 'unit': 'some_name', 'confidenceRange': {'min': 1.3, 'max': 5.5}}\nIf TypedDict doesn’t do everything you need it to, we have some other options."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#custom-collections-with-typevar",
    "href": "posts/2022-01-18-robust-python-5.html#custom-collections-with-typevar",
    "title": "Using type annotation with collections in Python",
    "section": "Custom Collections with TypeVar",
    "text": "Custom Collections with TypeVar\nTypeVar in Python is how you can implement generics. Generics, as I learned while reading, are ways of representing things that are the same, like when you don’t care what specific type is being used. Take this example from the book, where you want to reverse items in a list, but only if the items are all of the same type. You could write the following:\nfrom typing import TypeVar\nT = TypeVar('T')\ndef reverse(coll: list[T]) -&gt; list[T]:\n    return coll[::-1]\nYou can use generics in other ways to create new kinds of collections or groupings. For example, again this one is adapted from the book, if you were writing a series of methods that returned either something useful or a particular error message:\ndef get_weather_data(location: str) -&gt; Union[WeatherData, APIError]:\n    # …\n\ndef get_financial_data(transaction: str) -&gt; Union[FinancialData, APIError]:\n    # …\n…and so on, you could use generics as a way of simplifying how this gets presented:\nT = TypeVar('T')\nAPIResponse = Union[T, APIError]\n\ndef get_weather_data(location: str) -&gt; APIResponse[WeatherData]:\n    # …\n\ndef get_financial_data(transaction: str) -&gt; APIResponse[FinancialData]:\n    # …\nThat looks and feels so much cleaner!"
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#tweaking-existing-functionality-with-collections",
    "href": "posts/2022-01-18-robust-python-5.html#tweaking-existing-functionality-with-collections",
    "title": "Using type annotation with collections in Python",
    "section": "Tweaking existing functionality with collections",
    "text": "Tweaking existing functionality with collections\nIf you’re just making slight changes to the behaviour of collections, instead of subclassing dictionaries or lists or whatever, it’s better to override the methods of collections.UserDict, collections.UserString and/or collections.UserList.\nYou’ll run into fewer problems when you actually implement this. Of course, there is a slight performance cost to importing these collections, so it’s worth making sure this cost isn’t too high.\nYou’ll maybe have noticed that there isn’t a collections.UserSet in the list above. For sets we’ll have to use abstract base classes which are found in collections.abc. The big difference between the User* pattern of classes, there is no built-in storage for the abc classes. You have to provide your own storage if you need it. So for sets, we’d use collections.abc.Set and then implement whatever group of methods are required for that particular class.\nIn the set example, we have to implement __contains__, __iter__ and __len__, and then the other set operations will automatically work. There are currently (as of Python 3.10.2) 25 different ABCs available to use. I definitely will be exploring those as they seem really useful.\nEven though this chapter got into the weeds of collections a little, I learned a lot and I’m already finding places in the ZenML codebase where all of this is being used."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#typeguard",
    "href": "posts/2022-01-18-robust-python-5.html#typeguard",
    "title": "Using type annotation with collections in Python",
    "section": "Typeguard",
    "text": "Typeguard\nBefore I leave, since we’re still thinking about types, I wanted to share this little package I discovered the other day: typeguard. You can use it in a bunch of different ways, but a useful short video from calmcode.io showed how a simple decorator can simplify code and catch type errors.\nConsider the following example code:\ndef calculate_risk(risk_factor: float) -&gt; str:\n    \"\"\"Calculates how much risk you took\"\"\"\n    return risk_factor * 3 # arbitrary return value :)\nWhat if someone passes in a wrong type into this function? It’ll fail. So maybe we want to handle that particular situation:\ndef calculate_risk(risk_factor: float) -&gt; str:\n    \"\"\"Calculates how much risk you took\"\"\"\n    if not isinstance(risk_factor, float):\n        raise ValueError(\"Wrong type for risk_factor\")\n    return risk_factor * 3\nIf you have lots of parameters in your function and you have to handle them all, this could get messy quite quickly. Instead, we can pip install typeguard and do the following:\nfrom type guard import typechecked\n\n@typechecked\ndef calculate_risk(risk_factor: float) -&gt; str:\n    \"\"\"Calculates how much risk you took\"\"\"\n    return risk_factor * 3\nNow that’s a handy little decorator! It’ll handle all the raising of appropriate errors above based on whether you passed in the right type or not. It works for classes as well. You’re welcome, and thanks Vincent for making the introductory video!"
  },
  {
    "objectID": "posts/2022-01-30-robust-python-8.html",
    "href": "posts/2022-01-30-robust-python-8.html",
    "title": "How and where to use enums in Python",
    "section": "",
    "text": "The second part of Viafore’s ‘Robust Python’ is all about user-created types. We start simple in chapter eight and consider the Enum type as a way of defining a particular restricted set of values. An example might help get us started:\nfrom enum import Enum\n\nclass TrafficLightsState(Enum):\n  RED = \"red\"\n  YELLOW = \"yellow\"\n  GREEN = \"green\"\n  OFF = \"off\"\n\ncurrent_state = TrafficLightsState.GREEN\nprint(current_state.value) # prints 'green'\nWe subclass off Enum and define the pairings of values that belong together. I hope you can see already that this is a readable way to define these values and show that they are part of the same semantic grouping.\nIf we’re using these definitions not because we care about the values themselves but because we want to be able to evaluate whether the state of one particular traffic light is the same as a different traffic light, we can use auto to automatically assign values (ascending integers, by default) in the following way:\nfrom enum import Enum, auto\n\nclass TrafficLightsState(Enum):\n  RED = auto()\n  YELLOW = auto()\n  GREEN = auto()\n  OFF = auto()\n\ncurrent_state = TrafficLightsState.GREEN\nprint(current_state.value) # prints 3\nYou can iterate through your enums or get their length just as if it was a list, too.\nWhile writing the above text, I realised that I was getting confused about the difference between types and classes in Python. It turns out that whatever differences once existed, they aren’t much of a thing any more and to all intents and purposes they’re practically the same thing.\nA lot of the enum-related definitions at work are defined in this file. You can see that we tend not to use auto, though I’m not really sure why. (We don’t ever seem to compare against actual values.)\nIf you want to make sure that the actual values assigned to these grouped constants are unique, you can add the @unique decorator which will enforce that you aren’t duplicating values.\nBetter still for the readability of your code, you can use this collective type in your type annotations. For sure the difference between these two options should be clear:\ndef get_status(some_input: str) -&gt; str:\n    # code goes here\n\ndef get_status(some_input: str) -&gt; TrafficLightsState:\n    # code goes here\nIn the first case, it is far less clear what’s going on.\nNote that if you’re purely looking for a way to restrict the assignation to a particular variable, you can also use the Literal type, introduced in Python 3.8, though remember that it doesn’t help with iteration, runtime checking or map values from name to value. For all that, you’ll want to be using Enum.”\nIf you want a way to combine Enums together, you can subclass from enum.Flag. Consider the case of when you have a list of enums for days of the week, but you want to represent the weekend as a pairing of Saturday and Sunday (if you were in Europe, e.g.). You could do the following:\nfrom enum import Flag, auto\nclass Weekday(Flag):\n    MONDAY = auto()\n    TUESDAY = auto()\n    WEDNESDAY = auto()\n    THURSDAY = auto()\n    FRIDAY = auto()\n    SATURDAY = auto()\n    SUNDAY = auto()\n    \nweekend = Weekday.SATURDAY | Weekday.SUNDAY\nYou can perform bitwise operations on these combined groupings, but note that the values must support bitwise operations. (Strings don’t support them, while integers do.)\nFinally, the chapter covers the special case of IntEnum and IntFlag which allows for the conversion of integer values. This can be confusing and lead to non-robust behaviours, so the book discourages this particular usage.\nNext up is Data Classes, something I’m extremely interested in getting to grips with as it comes up in our codebase at work a decent amount."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html",
    "href": "posts/2022-02-08-robust-python-10.html",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "",
    "text": "We’ve read about enumerations and we’ve read about data classes. Now it’s the turn of classes. Chapter 10 of Patrick Viafore’s excellent book, ‘Robust Python’, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I’ve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps.\nFirst off, for someone who has just learned about data classes, how would you explain what is new or distinct when it comes to classes? They’re slightly different syntactically, with classes requiring you to write a bit more boilerplate. Compare the following:\nYou can note how it seems like the data class version is much more readable and involves less boilerplate to achieve the same effect, and for a simple example like this you’re probably right. The difference, and where classes make sense and shine, is when you have a conceptual grouping or type that includes some notion of invariants."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#what-is-an-invariant",
    "href": "posts/2022-02-08-robust-python-10.html#what-is-an-invariant",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "What is an invariant?",
    "text": "What is an invariant?\nMost of this chapter is about invariants and how they relate to classes, and I’ll admit I had never heard of the concept before reading in this book. An invariant is defined as “a property of an entity that remains unchanged throughout the lifetime of that entity.” You can think of it as some kind of context or a property about that particular type that you need to encode and that won’t change.\nThe book gives a pizza example (where a Pizza object could encode that in its list of toppings, the cheese could only be the final topping (i.e. on top) of the pizza). An alternative might be some kind of rule relating to an ID number, where either it must be unique to some kind of specification, or where the ID must conform to some kind of specification.\nEven with this rudimentary definition, you can see how there might be some advantages to being able to account for these rules and properties of the object type. (With data classes, you don’t have as much flexibility to specify all these nuances.) So what happens when you’re instantiating a class and you hit one of those scenarios where your contextual rules dictate that something can’t happen? (i.e. someone tries to create a Pizza object that has cheese as the bottom-layer topping) The book offers up two options:\n\nThrow an exception — this will break you out of the code flow and prevent the object from being constructed\nDo something to make the data fit — you can perform some kind of transformation which sees the cheese ingredient as being forced onto the top layer of the pizza toppings (or whatever is the equivalent for your specific scenario)\n\nNote that the kinds of restrictions posed by these invariants are things that can’t fully be captured by the typing system. We’ve covered type hints and how they can help make your code more robust, but types don’t help much when it comes to the order of a list, for example."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#why-code-around-invariants",
    "href": "posts/2022-02-08-robust-python-10.html#why-code-around-invariants",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Why code around invariants?",
    "text": "Why code around invariants?\nSo why go to all of this trouble in the first place? How does it benefit to code with the invariants in mind? To start with, it’ll probably help you think through edge cases and exceptions that you could do well to be wary of. The invariants alert you to the fact that arguments passed into functions and methods will not always be in the form that you would ideally like. (As a side note, this might also encourage you to add unit tests.)\nIt will help you keep the code that handles the invariants together instead of mixing it in with the code that instantiates the objects. In general, it will enhance your ability to reason about the code and the concepts that your code reflects. This is important not only for the implementation in code, but for how you think about any particular part and how it relates to the rest of your code base.\nThe goal for all of this: fewer bugs and a more robust system. Yes, it takes a bit more effort to think whether there are implicit or explicit invariants, but doing so makes your code and your system more reliable. In Viafore’s words:\n\n“You’re making an easier API for people to think about, and you reduce the risk of people using your objects incorrectly. […] You never want someone to be surprised when using your code.” (p. 141)"
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#invariants-and-class-consumers",
    "href": "posts/2022-02-08-robust-python-10.html#invariants-and-class-consumers",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Invariants and class consumers",
    "text": "Invariants and class consumers\nThe rest of the chapter is about the implementation consequences of thinking about classes in this invariants-first way. For consumers of the class, how should you ensure that the invariants handled are clear? Aside from the implementation itself (in the constructor), docstrings and code comments are suggested as a means to this end. Of course, README files and documentation in general can serve the same purpose, but it’s best if the context and information about invariants is as close to the code as possible."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#invariants-and-class-maintainers",
    "href": "posts/2022-02-08-robust-python-10.html#invariants-and-class-maintainers",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Invariants and class maintainers",
    "text": "Invariants and class maintainers\nFor (future) maintainers of the class, unit tests are the way to go. Make sure that the relevant scenarios and invariants are covered by testing code and you will have extra confidence that your object instantiation really does do what you intend. Your code should already be doing the checking for invariants on the instantiation side, but unit tests are a way of ensuring that this is actually the case (and also that these invariants remain covered as the code base continues to evolve.\n(The book offers one way of doing such tests for invariants with contextlib.contextmanager on page 145.)"
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#encapsulation-and-classes",
    "href": "posts/2022-02-08-robust-python-10.html#encapsulation-and-classes",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Encapsulation and classes",
    "text": "Encapsulation and classes\nAs the final chunk of the chapter, we learn about private, protected and public access to the properties and methods of a class, and how they relate to the maintenance of invariants.\nThis is an important part of the story. As users interface with your class and API, encapsulation is a way to ensure that they update and interact with the these properties in a way that is under your control. For example, even if at instantiation you enforce the Pizza object having cheese as the top-layer topping, what do we have in place to ensure that the user doesn’t just amend the toppings property such that the cheese is the bottom-layer topping (i.e. AFTER instantiation)? Encapsulation — having an entity hide or restrict access to certain properties and actions — is how you handle that.\nThe book goes into a fair amount of detail on the uses of these different levels of access, and introduces the idea of ‘accessors’ and ‘mutators’ as an alternative to the more commonly-used ‘getters’ and ‘setters’.\nRemember, “you use invariants to allow users to reason about your objects and reduce cognitive load.” (p. 151)"
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#so-what-am-i-supposed-to-use",
    "href": "posts/2022-02-08-robust-python-10.html#so-what-am-i-supposed-to-use",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "So what am I supposed to use?",
    "text": "So what am I supposed to use?\n\nThe end of the chapter offers this really helpful flowchart diagram which summarises the choices that we’ve covered during the previous three chapters. I really want to highlight that this chapter helped me think about classes in a way I hadn’t, despite having been through courses, having read numerous articles and of course coded in this class-oriented fashion for several years.\nThe next few chapters continue onwards by thinking about how to design your interfaces such that they make sense for your users and allow your code base to grow with as few headaches as possible."
  },
  {
    "objectID": "posts/2022-02-27-python-parsers.html",
    "href": "posts/2022-02-27-python-parsers.html",
    "title": "Three Python Helpers for Parsing Inputs",
    "section": "",
    "text": "I continue to slowly work my way through the calmcode back catalogue. This week I learned about three tiny utility packages that make certain parsing tasks less painful.\nparse (introduced here) is a way of turning simple text patterns into restructured data. Take the following example as an illustration:\nfrom parse import parse\n\nurl = \"https://github.com/strickvl/some-repo/\"\n\nparse(\"https://github.com/{owner}/{repo}/\", url).named\n\n# returns {'owner': 'strickvl', 'repo': 'some-repo'}\nAs Vincent explains, it’s sort of the inverse or opposite operation to what happens with an f-string.\nFor URLs of various kinds that you want to decompose easily, yarl (introduced here) is a great way to approach that in Python.\nFor dates stored in some kind of a string format, you might want to try datefinder (introduced here), an elegant if not always perfect way for converting date strings into datetime.datetime objects."
  },
  {
    "objectID": "posts/2022-03-12-fiftyone-computervision.html",
    "href": "posts/2022-03-12-fiftyone-computervision.html",
    "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nSo you’ve trained a computer vision model, but you think it could do better. What do you do next? This is a common scenario, especially for computer vision problems where fine-tuning someone else’s pre-trained model is a pretty normal initial step that gets taken. You emerge with a decent score on whatever metric you care about, but it also isn’t great.\nOne part of the solution is certainly ‘more data’. This approach was recently highlighted by Boris Dayma on Twitter:\n{% twitter https://twitter.com/borisdayma/status/1502317249423679495 %}\nIn my case, I currently have a little over 1200 images that have been annotated, but of those some 600 of them don’t contain any redactions at all (i.e. they just have content boxes). I did mention that I was using a similar approach early on, where I’d use the model to help pre-annotate images, but I haven’t been using that recently.\nI’m realising that more important than pure volume of data is to annotate types of images that are the hardest for the model to learn. So what I really want to know at this point is where I should place my focus when it comes to supplementing the training data. My images aren’t currently divided into separate classes, but I have a proxy (the filename) which will be really helpful once I’ve identified which types I need to supplement.\nWhen seeking to improve computer vision models with error analysis, some kind of visual inspection is essential. fastai had a number of utility methods that helped in the interpretation of where a model was underperforming, but for object detection I think you do need something that was built to purpose, where you can really dive into the specific ways each object was or wasn’t detected.\nEnter FiftyOne.\nFiftyOne is an open-source tool built specifically to support the curation and creation of datasets for computer vision models. It is almost two years old in its open-source incarnation, and (or but?) it feels very solid and robust in its implementation. Voxel51, the company behind it, has taken great pains to write excellent documentation and guides, and they have a supportive community behind the scenes, too."
  },
  {
    "objectID": "posts/2022-03-12-fiftyone-computervision.html#viewing-only-high-confidence-predictions",
    "href": "posts/2022-03-12-fiftyone-computervision.html#viewing-only-high-confidence-predictions",
    "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of",
    "section": "Viewing only high-confidence predictions",
    "text": "Viewing only high-confidence predictions\nNot all predictions are created equal, too, so it would be useful to view only those predictions where the confidence was higher than 75%. FiftyOne makes this kind of conditional view easy. You can do it in code, as in the following snippet, or you can do it via the GUI inside the app.\nfrom fiftyone import ViewField as F\n\n# Only contains detections with confidence &gt;= 0.75\n# `dataset` is the FiftyOne core object that was created before\nhigh_conf_view = dataset.filter_labels(\"prediction\", F(\"confidence\") &gt; 0.75)"
  },
  {
    "objectID": "posts/2022-03-12-fiftyone-computervision.html#patches-detailed-views-for-detected-objects",
    "href": "posts/2022-03-12-fiftyone-computervision.html#patches-detailed-views-for-detected-objects",
    "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of",
    "section": "‘Patches’: detailed views for detected objects",
    "text": "‘Patches’: detailed views for detected objects\nFor a more fine-grained understanding on the ways our model is predicting redactions, we can create what are called ‘patches’ to view and scroll through prediction-by-prediction.\n\nThis is an excellent way to view things through the eyes of your model. These are all the objects it considers to be redactions. We’ll get to finding the ones where it doesn’t do as well in a bit, but this view allows us to immerse ourselves in the reality of how our model is predicting redaction boxes. We can see that certain types of boxes are well-represented in our dataset: coloured or shaded rectangles in particular."
  },
  {
    "objectID": "posts/2022-03-25-paperspace-docker-icevision.html",
    "href": "posts/2022-03-25-paperspace-docker-icevision.html",
    "title": "Building my own image to use IceVision with Paperspace",
    "section": "",
    "text": "I’ve been using Paperspace right to fuel my ML/Deep Learning experimentation since more or less the beginning. It was one of the recommended platforms that offered GPUs for the fastai course and when I started working on my redaction project I chose to keep going since I had little reason to change.\nFast-forward a few months, and I’ve had a few issues along the way. Paperspace works by provisioning a Docker image, connecting it to a fixed filesystem / storage backend and then serving this up to you in a web interface as a Jupyter notebook. I found that sometimes there were issues with dependencies breaking, or special pip install magic I had to include in my notebook so that things would work again.\nIncluded in this is the reality that a full install of IceVision — an amazing library for computer vision that handles a lot of the pain around integrating various libraries and use cases — simply takes a while as it has to download and setup some pretty hefty dependencies. I had found that going from zero to working on the day’s specific issue took around 20 minutes when you factored in all the setup, updates from the Github repo, syncing data and so on.\nInspired by my reading and study of Docker — and with a tip from a Paperspace engineer about how I could get started — I set out to build a custom image that handled most of the setup upfront and automatically updated with the latest changes and data.\nAmazingly, it worked more or less immediately! I created a new Dockerfile based of the original suggestion and the core additions were the following:\nRUN wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh && bash icevision_install.sh cuda11 && rm icevision_install.sh\n\nRUN pip install torchtext==0.11.0 --upgrade\nRUN pip install opencv-python ipywidgets icevision-dashboards\nRUN apt update && apt install -y libsm6 libxext6\nRUN apt-get install -y libxrender-dev\n\nCMD make lfs && git lfs pull\nIn order to set this up with Paperspace, you first have to go to your notebooks inside a project and click to create a new Paperspace notebook.\n\nOnce there, you can ignore the suggestion to “select a runtime”, but rather select your machine from the available GPUs. I usually choose the RTX5000 and set it up for an auto-shutdown after 6 hours.\n\nThen you want to click the ‘Advanced Options’ toggle so you can add in all the details of the image being used.\n\nThis is what worked for me. In order to use JupyterLab, the container command should be:\njupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin='*' --ServerApp.allow_credentials=True\nI enter my private GitHub repo (along with my username and a custom token generated to allow Paperspace to download the repo) in the ‘Workspace’ section.\nThen when I click ‘Start Notebook’, it works! No more hanging around for IceVision to install. My Docker image already has this!\nI realise that I’m probably a little late to the party in terms of using Docker and seeing how it can bring some real improvements in terms of reproducibility of environments as well as these little quality-of-life perks like not hanging around to install everything each time you want to use it. This was a really useful experience for me to learn from, and I’ll certainly be using this going forward in other projects I work on."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html",
    "href": "posts/2022-04-06-synthetic-data-results.html",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nA clean and focused dataset is probably at the top of the list of things that would be nice to have when starting to tackle a machine learning problem. For object detection, there are some useful starting points, but for many use cases you’re probably going to have to start from scratch. This is what I’ve been doing for the past few months: working to bootstrap my way into a dataset that allows me to get decent performance training a model that can recognise redactions made on documents.\nAs part of that journey so far, some of the big things that I’ve taken time to do include:\nAt the end of my synthetic data creation blogpost, I mentioned that the next step would be to test the effect of adding in the new synthetic examples. Well… the results are in!"
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#a-failed-attempt-to-train-with-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#a-failed-attempt-to-train-with-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "A failed attempt to train with synthetic data",
    "text": "A failed attempt to train with synthetic data\nI wasn’t sure exactly how much synthetic data would be appropriate or performant to use, so created a loose experiment where I started with 20% of the total images and increasing up until I reached 50%. (I figured that more than 50% synthetic data probably wasn’t a great idea and would probably not help my model perform out in the real world.)\n\nAs you can see above: my initial experiment did not show great results. In fact, in several places, if I added synthetic data my model actually performed worse. This was a strong repudiation of my intuition of what would happen. After all, the whole point of adding the synthetic data was to get the model more of a chance to learn / train and thus improve its ability to recognise redaction object in documents.\nI dug into the data that I’d generated and the data I’d been using to train, and discovered a nasty bug which was tanking the performance. A week of debugging mislabelled bounding boxes in evenings after work and I was back with results that finally made sense."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#performance-boosts-after-adding-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#performance-boosts-after-adding-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "Performance boosts after adding synthetic data",
    "text": "Performance boosts after adding synthetic data\n\nIn this chart, at the bottom you can see how training the model without the synthetic data (no-synthetic-batch16) performed. Ok, not great. Then the next best performing (combined-75real-25synthetic-randomsplit)was when 25% of the total number of images was synthetic, and the rest were real manually annotated images. At the top, with around an 81% COCO score, was the model where I used 50% synthetic and 50% real images. This seemed to fit what my intuition said would happen.\nMore synthetic data helped. I guessed that if I had millions of labelled images then the synthetic data would perhaps have been less useful, but starting from scratch it was really supporting the process.\nI was curious what would happen when I returned to FiftyOne to carry out some error analysis on the new model’s performance. Even before I had reached those results, I had a hunch that the synthetic images I’d created were perhaps too generic. I think they probably were helping boost some baseline performance of my model, but I knew they weren’t helping with the hard parts of detecting redactions."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#hard-examples-creating-targeted-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#hard-examples-creating-targeted-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "‘Hard examples’: creating targeted synthetic data",
    "text": "‘Hard examples’: creating targeted synthetic data\nAs a reminder, this is the kind of image that is ‘hard’ for my model (or even a human) to be able to identify all the redactions:\n\nThe FiftyOne visualisations of what was and wasn’t working validated my hunch: yes, synthetic data helped somewhat, but the model’s low performance seemed much more vulnerable to misrecognition of the hard examples. Even with a 50/50 split between synthetic data and real manually annotated data, the hard examples were still hard! (And the converse was also true: the model was already pretty good at identifying ‘easy’ redactions (e.g. of the black box type).\nIf we look back at the example of a ‘hard’ redaction above, two things stood out:\n\nThey’re hard, even for a human! This was borne out in the way I needed to take special care not to forget or mislabel when I was adding manual annotations.\nThere are lots of redactions on a single page/image.\n\nThe second point was probably important, not only in the sense that there were more chances of getting something wrong on a single page, but also in the sense that the redactions were (relatively) small. The detection of small objects is almost its own field in the world of computer vision and I don’t know too much about it, but I do know it’s somewhat an unsolved problem. That said, finding a way to boost the performance of the models on these ‘hard’ examples (there were a few other types of hard image) seemed like it might tackle a significant shortcoming of my model.\nI decided to try creating a separate batch of synthetic image data, this time fully tailored to tackling some of the hardness mentioned above: it would have many small redactions on a single page, they would all be white boxes and there might also be things like tables with white box-like shapes coexisting next to redactions.\nLuckily, the work I’d done previously on creating synthetic data helped me get started quickly. I returned to borb, an open-source tool for quickly creating PDF documents that allows for a pretty flexible prototyping of layouts with all sorts of bells and whistles added. These were some of the documents I generated:\n\nThe hard images were hard, and I had created some synthetic chimeras that (I believed) approximated some of the features of the original hard images. I did not want to overbalance my training data, however, and took care not to create too many of this type of image.\nMy script — as with the previous synthetic data — also required me to create the annotation files at the same time as creating the document. With borb it was relatively trivial to get the bounding box data for objects created, and there was even in-built functionality to create and apply redactions onto a document. (I’m moving fairly quickly over the mechanics of how this all worked, but it’s not too far distant from how I described it in my previous post so I’d refer you there for more details).\nOnce the images were created and added to my datasets, it was time to retrain the model and see what benefit it brought.\n\nAs you can see, the model jumped up from around 80.5 to 84% when I aded the hard synthetic examples in. That’s a pretty nice jump as far as I’m concerned, especially given that I only added in 300 images to the training data. I still had a little over a thousand of the original basic synthetic images that I was using, but this result showed me that tackling the badly performing parts of the model head-on seemed to have a positive outcome.\nAt this point, I did some more experiments around the edges, applying other things I knew would probably boost the performance even more, notably first checking what would happen if I increased the image size from 512 to 640. I got up to an 86% COCO score with that improvement alone.\nIn a final twist, I second-guessed myself and wondered whether the original synthetic data was even helping at all… I removed the thousand or so ‘basic’ synthetic images from the data and retrained the model. To my surprise, I achieved more or less the same COCO score as I had with the basic synthetic images. I’m taking this as a strong suggestion that my basic synthetic images aren’t actually helping as much as I’d thought, and that probably a smaller number of them as a % of the total would be beneficial."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#reflections-on-experimenting-with-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#reflections-on-experimenting-with-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "Reflections on experimenting with synthetic data",
    "text": "Reflections on experimenting with synthetic data\nSo, what can I conclude from this whole excursion into the world of synthetic image creation as a way of boosting model performance?\n\nadding synthetic data really can help!\nthe world of synthetic data creation is a huge rabbit hole and potentially you can get lost trying to create the perfect synthetic versions of your original data. (I mean this both in the sense of ‘there’s lots to learn’ as well as ‘you can spend or lose a ton of time here’.)\nTargeted synthetic data designed to clear up issues where the model has been identified as underperforming is probably best. (Conversely, and I’ll be careful how much I generalise here, middle-of-the-road synthetic data that doesn’t resemble the original dataset may not be worth your time.)\nKnowing your original data and domain really well helps. A lot. My intuition about what things the model would stumble on was fuelled by this knowledge of the documents and the domain, as well as by the experience of having done manual annotations for many hours.\n\nThere are probably many (many) more things I can do to continually tinker away at this model to improve it:\n\ncontinue down the path of more error analysis, which would fuel more targeted addition of annotations, and so on.\ncreate better versions of synthetic data with more variation to encompass the various kinds of documents out in the real world.\nmore self-training with the model in the loop to fuel my manual annotation process.\nfurther increases to the image size (perhaps in conjunction with progressive resizing).\nincreasing the backbone from resnet50 to resnet101.\n\nIn general, improving the quality of the data used to train my model seems to have been (by far) the best way to improve my model performance. Hyper-parameter tuning of the sort that is often referenced in courses or in blog posts does not seem to have had much of a benefit.\nIt is probably (mostly) good enough for my use case and for where I want to be heading with this project. There are other things that need addressing around the edges, notably parts of the project that could be made more robust and ‘production-ready’. More about that in due course, but for now please do comment below if you have suggestions for things that I haven’t thought of that might improve my model performance!"
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nIn the first part of this series, I made the case for why you might want to include some kind of data validation if you’re working on training a model in general, and if your working on object detection in specific. There are many things that can go wrong with your data inputs and you ought to have some kind of safeguards in place to prevent some tricky failures and bugs."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#tldr-for-data-validation-with-great-expectations",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#tldr-for-data-validation-with-great-expectations",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "TL;DR for data validation with Great Expectations",
    "text": "TL;DR for data validation with Great Expectations\n\n👀 Data validation helps give you confidence in the raw ingredients that feed into your models, especially in scenarios where you retrain or fine-tune regularly.\n✅ For object detection problems, there are many ways your data can fail in some silent way. You should want to be aware of when your training data isn’t meeting your assumptions of what it should look like.\n🛠 Great Expectations is a general purpose data validation tool that goes a long way to restoring trust in your data, and their automatic profiling feature is really useful when getting started.\n💪 In this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of Great Expectations to get you started with increasing your confidence in your object detection annotations. I will show you a concrete example where I created some validation rules for my manually-annotated dataset. I then applied those rules to my synthetic dataset in order to validate it."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#initial-notebook-based-setup",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#initial-notebook-based-setup",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "Initial notebook-based setup",
    "text": "Initial notebook-based setup\nIn the last post I showed how you can easily use the Great Expectations library directly on a Pandas DataFrame, manually specifying values you expect to be the case for your data. For example, perhaps your data should always have certain columns, or the values of a certain column should always be a certain type or mostly range between certain values. You can define all these fairly easily, leveraging your domain knowledge of the data.\nIf you know you’re going to want to use Great Expectations as a more fully-fledged part of your pipeline or workflow, you’ll probably want to go through the more extensive setup stages and create a dedicated ‘context’ which can be longer-lasting than just length of your script runtime. Think of the ‘context’ as somewhere all your expectations and configuration of how to access your data is stored.\nFull instructions on how to set all this up can be found in the docs, but for the most part it’s a matter of pip installing Great Expectations, running great_expectations init , and then great_expectations datasource new.\nThat final command will take you through an interactive setup that has you fill in and amend Jupyter notebooks. (I’m not fully sold on the prominence of this workflow that has you spinning up a Jupyter runtime, dynamically editing notebooks and so on, but I found doing it for my project wasn’t as inconvenient as I’d expected. Plus, there are non-interactive and pure Pythonic ways to get everything configured if you need or prefer that.)\nOnce you have your context created and your data sources connected, you can move on to the main course: using the Profiler."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#using-the-great-expectations-profiler",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#using-the-great-expectations-profiler",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "Using the Great Expectations Profiler",
    "text": "Using the Great Expectations Profiler\nSetting up your validations (i.e. your ‘expectations’) for your data can be done in a number of different ways. We saw last time how you can define these manually, but in this post I want to show how you can follow another recommended workflow by allowing the profiler to review your data and to make an initial set of assumptions about the boundaries and patterns embedded in those values.\nNote, as the docs mention, the expectations that are automatically generated from your dataset are “deliberately over-fitted on your data”. This means that if your DataFrame has 10,321 rows, one of the expectations generated will be that datasets due for validation with this suite of expectations will also have exactly 10,321 rows:\n\n“The intention is for this Expectation Suite to be edited and updated to better suit your specific use case - it is not specifically intended to be used as is.” (source)\n\nYou’ll want and have to do a decent amount of manual checking through, amending and updating any expectations that get created during this process. That said, I am finding that it makes a lot of sense to start with some kind of initial baseline of assumptions that can be corrected versus starting from complete zero and building things up purely based on your domain knowledge of the data.\nNeedless to say, this whole process assumes you have a decent grasp on the domain context and have explored your data already. You probably wouldn’t go to the trouble of setting up Great Expectations if you were doing something that required only a quick solution, but it bears repeating that the expectations you define are only as good as your understanding of the limits and underlying realities of your data. This is probably why something like Great Expectations lends itself quite well to a data-centric approach.\nGetting the profiler to work requires a few interlocking abstractions to be created or instantiated:\nexpectation_suite_name = \"redaction_annotations_suite\"\n\nmain_batch_request = RuntimeBatchRequest(\n    datasource_name=\"redaction_data\",\n    data_connector_name=\"default_runtime_data_connector_name\",\n    data_asset_name=\"main_annotations_df\",  # This can be anything that identifies this data_asset for you\n    runtime_parameters={\"batch_data\": main_annotations_df},  # df is your dataframe\n    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n)\n\ncontext.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name, overwrite_existing=True # toggle this as appropriate\n)\nvalidator = context.get_validator(\n    batch_request=main_batch_request, expectation_suite_name=expectation_suite_name\n)\n\nprofiler = UserConfigurableProfiler(profile_dataset=validator)\nsuite = profiler.build_suite()\ncontext.save_expectation_suite(suite) # use this to save your suite in the context for reuse\nThe above code perhaps seems like a lot, but really all you’re doing is getting your data, making the relevant connections between Great Expectations and your context, and then running the profiler so it can work its magic.\n\nYou can’t yet see the specific values that were imputed from your data, but even this high-level output shows you some of the expectations that it’s thinking would be useful to create.\nAt this stage, you’ll want to take some time to review the specific expectations. You’ll want to:\n\nensure that they make sense for your dataset\nremove any of the really rigid expectations (e.g. that any dataset must have exactly the same number of rows)\nuse the inputed expectations as a springboard for any other ideas that might come to mind\n\nNote that this is an essential step to complete before moving forward. You could use the unedited auto-generated expectations suite as your data validation, but it would almost certainly have little use or value for you. The auto-generated suite is a starting place that you need to amend and tailor to your specific situation.\nIn my case, I was able to amend some of the min / max values to more suitable defaults. (You amend these expectations in the .json file that was created inside the expectations subfolder within your context.) I also included some other domain-driven expectations that the profiler couldn’t have known to include. For example, I know from having immersed myself in this data for several months now that most annotations should have a ‘horizontal’ or ‘square’ orientation. Great Expectations doesn’t create this expectation automatically, so I add it to the list of basic assumptions already generated."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#viewing-data-docs-reports-on-validated-data",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#viewing-data-docs-reports-on-validated-data",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "Viewing Data Docs reports on validated data",
    "text": "Viewing Data Docs reports on validated data\nOnce you have a suite of expectations set up to your liking, you can run a checkpoint against your original data just to make sure you haven’t introduced or amended something that doesn’t match up with the original data. You should get no errors at this point.\ncheckpoint_config = {\n    \"name\": \"my_checkpoint\",\n    \"config_version\": 1,\n    \"class_name\": \"SimpleCheckpoint\",\n    \"validations\": [\n        {\n            \"batch_request\": {\n                \"datasource_name\": \"redaction_data\",\n                \"data_connector_name\": \"default_runtime_data_connector_name\",\n                \"data_asset_name\": \"main_annotations_df\",\n            },\n            \"expectation_suite_name\": expectation_suite_name,\n        }\n    ],\n}\ncontext.add_checkpoint(**checkpoint_config)\n\nresults = context.run_checkpoint(\n    checkpoint_name=\"my_checkpoint\",\n    batch_request={\n        \"runtime_parameters\": {\"batch_data\": main_annotations_df},\n        \"batch_identifiers\": {\n            \"default_identifier_name\": \"default_identifier\"\n        },\n    },\n)\n\ncontext.build_data_docs() # builds data docs to inspect the results\nWhat you really want, however, is to run your expectations suite against new data. That’s the real value of what Great Expectations brings, i.e. to check that incoming data due to be added to your larger base dataset conforms to the broad realities of that base dataset.\nIn my case, the first thing I was interested to check was whether the synthetic images I created would match the expectations suite I’d created based off my core hand-annotated data. (Quick context if you haven’t been following the project so far: I have a core dataset which is manually annotated for the objects inside images. I also created two sets of synthetic data to supplement the manual annotations, which boosted my model performance considerably.)\n\nThe web UI is where you can go to get a visual overview of where your data is passing and failing to meet your (great) expectations. You will want (and I will need) to configure your expectations suite to meet the core assumptions you make about your data derived from your particular domain.\nFor my case, some expectations I will add that are specific to my use case:\n\nredaction annotations should mostly be of horizontal orientation\ncontent annotations should mostly be of portrait orientation\nmost images should have only one content annotation\nannotations shouldn’t be larger than the associated image, or positioned outside the boundaries of that image. (Because of how you define them, in reality this is several expectations, but conceptually it’s just one or two).\nthe area taken up by most annotations should be less than half that taken up by the total image\n\n…and so on. I hope it’s clear now how Great Expectations can be a tremendous asset that can give you confidence in your data.\nIn the next and final post of the series, I will explore some other tools that you can consider when performing these kinds of validation. I will also offer my take on when each tool would be appropriate, as well as where they would be appropriate to use within the machine learning workflow and lifecycle."
  },
  {
    "objectID": "posts/2022-05-02-pet-cat-image-classifier-fastai.html",
    "href": "posts/2022-05-02-pet-cat-image-classifier-fastai.html",
    "title": "How my pet cat taught me a lesson about validation data for image classification",
    "section": "",
    "text": "I’m participating in the latest iteration of the fastai course as taught by Jeremy Howard. This past week we got a very high-level overview of some of the ways deep learning is proving very powerful in solving problems as well as how we can use its techniques to fairly quickly get great results on image classification problems.\nI’ve done the earlier parts of the course before, so some of these demonstrations were less mind-blowing than the first time I saw them. For this iteration of the course, Jeremy showcased a Kaggle notebook which trains a model to distinguish whether an image is of a bird or not.\nLast time I did the course, I trained an image classifier model to distinguish whether an image was redacted or not to around 95% accuracy. (This actually was the genesis of my larger redaction object detection project that I’ve been blogging about for the past few months.)\n\nThe key ingredients: what goes into a model?\nThe course teaches things top-down, so we start off with both the practical experience of training state-of-the-art models as well as the overall context to what goes into these high-level functions. These pieces include:\n\nyour input data — this style of programming differs from traditional software engineering where your functions take data in order to ‘learn’ how to make their predictions\nthe ‘weights’ — when we’re using pre-trained models, you can think of these as an initial set of variables that are already pretty useful in that configuration and can do a lot of things.\nyour model — this is what you’re training and, once trained, you can think of it as a function in and of itself that takes in inputs and outputs predictions.\nthe predictions — these are the guesses that your model makes, based on whatever you pass in as inputs. So if you pass in an image of a cat to a model (see below), the prediction could be whether that cat is one particular kind or another.\nyour ‘loss’ — this is a measure of checking how well your model is doing as it trains.\na means of updating your weights — depending on how well (or badly) the training goes, you’ll want a way of updating the weights so that each time it gets a bit better at optimising for whatever you’ve set up your model to do. In lesson one we learn about stochastic gradient descent, a way of optimising and updating these weights automatically.\nyour labels — these are the ground truth assertions that get used to determine how well the model is doing as it trains.\ntransformations & augmentations — more on this will come in lesson two, but these allow you to squeeze more value out of your data. This is especially valuable when you’re fine-tuning a model and don’t have massive amounts of data to use for training.\n\nRepresented in code, the classic fastai example where you train a model to distinguish between cats and dogs is as follows:\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\nThis small code snippet contains all the various parts just mentioned. The high-level API and abstractions that fastai provides allows you to work with these concepts in a way that is fast and flexible, though if you need to dive into the details you can do so as well.\n\n\nImage classification isn’t just about images\nOne of the parts of the first chapter I enjoy the most is the examples of projects where image classification was applied to problems or scenarios where it doesn’t first appear that the problem has anything to do with computer vision.\nWe see malware converted into images and distinguished using classification. We see sounds in an urban environment converted into images and classified with fastai. In the study group I host for some student on the course, one of our members presented an initial proof of concept of using images of music to distinguish genre:\n{% twitter https://twitter.com/kurianbenoy2/status/1520470393760272384?cxt=HHwWgMCi-Y3x5ZkqAAAA %}\nI like the creativity needed to think of how to turn problems and data into a form such that they can become computer vision problems.\n\n\nMy own efforts: classifying my cat\nTrue story: a few years ago my cat escaped from the vet and a reward was mentioned for anyone who found our cute ginger cat. Throughout the course of the day, the vets were perplexed to see people coming in with random ginger cats that they’d found in the neighborhood, but none of them were ours! With this iteration of the course, therefore, I was curious to try out this simple but slightly silly example and see how well a deep learning model could do at recognising distinguishing Mr Blupus — don’t ask! — from other random photos of ginger cats.\nTraining the model was pretty easy. Like any cat owner, I have thousands of photos of our cat so an initial dataset to use was quick to assemble. I downloaded a few hundred random ginger cat photos via DuckDuckGo using some code Jeremy had used in his bird vs forest Kaggle notebook. A few minutes and ten epochs later, I had achieved 96.5% accuracy on my validation data after fine-tuning resnet50!\n{% twitter https://twitter.com/strickvl/status/1520405802091175936 %}\nAfter the initial excitement died down, I realised that the result was probably an illusion. Our cat is an indoor cat and we have a relatively small house. Couple that with the fact that the backdrops to the photos of Mr Blupus are relatively distinctive (particular kinds of sheets or carpets) and it seems pretty clear that the model wasn’t learning how to identify our cat, but rather it was learning how to distinguish photos of our house or our carpets.\n☹️\nLuckily, chapter one gets into exactly this problem, showing an example of how exactly this validation issue can give you a false sense of confidence in your model. When I evaluated my model on the validation data it wasn’t a fair test, since in all likeliness may model had already seen a similar backdrop to whatever was found inside the validation set.\nI discussed this when I presented this to those at the study group / meetup yesterday and we agreed that it’d be best if I held out some settings or locations from the training entirely. I took 30 minutes to do that in the evening and had a third ‘test’ dataset which consisted of 118 images of our cat in certain locations that the model wasn’t trained on and thus couldn’t use to cheat. I added a few more photos to the training data so that there were enough examples from which to learn.\n\nI was supposedly getting 98% accuracy now, but I knew that number to be false. I then needed to figure out how to get the accuracy for my held-out test set. With a lot of help from Francesco and a really useful blogpost on doing batch inference with fastai, I first got the predictions for my test data:\ntest_files = [fn for fn in sorted((Path(\"/path/to/test_set_blupus_photos\")).glob('**/*')) if fn.is_file()]\ntest_dl = learn.dls.test_dl(test_files)\npreds, _ = learn.get_preds(dl=test_dl)\nI then created a tensor with the ground truth predictions for my test set and compared them with what my model had predicted:\ngts = torch.tensor([0 for _ in range(118)])\naccuracy = (gts == preds.argmax(dim=1))\nAt this point, getting the final accuracy was as simple as getting the proportion of correct guesses:\nsum([1 for item in accuracy if item]) / len(preds)\nThis gave me an accuracy on my held-out test set of 93.2% which was surprisingly good.\nI half wonder whether there is still some cheating going on somehow, some quality of the photos or the iPhone camera I used to take them that is being used to distinguish the photos of my cat vs other ginger cats.\nNevertheless, this was a useful lesson for me to learn. I realised while working with the tensors in the final step above that I’m not at all comfortable manipulating data with PyTorch so luckily that’ll get covered in future lessons.\nUPDATE:\nFollowing some discussion in the fastai forums, it was suggested that I take a look at Grad-CAM in chapter 18. This is a technique to visualise the activations which allows you to see which parts of the image it is paying the most attention to (sort of). I ran the code using a sample Blupus image and this was the result. I don’t understand how most (any?) of this works, but it was really cool to have a working result of sorts nonetheless!"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html",
    "href": "posts/2022-05-24-data-versioning-dvc.html",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nIf you’ve been following along as I train an object detection model to detect redactions, you’ll know that there have been a few iterations in how I go about doing this. For the most part, though, the dataset has remained relatively static. I downloaded a huge tranche of publicly-released government documents right at the beginning and aside from my experiments in synthetic data creation I haven’t really been adding to this data.\nWhen it comes to turning this model into something that can work in production, it won’t be enough to have a big bucket of image files that I train on. I’ll need to have a bit more control and fine-grained segmentation of the ways this data is being used. In short, if I want to be able to reproduce my workflows then I need some kind of data versioning."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#tldr-data-versioning-for-computer-vision",
    "href": "posts/2022-05-24-data-versioning-dvc.html#tldr-data-versioning-for-computer-vision",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🚦 TL;DR: Data Versioning for Computer Vision",
    "text": "🚦 TL;DR: Data Versioning for Computer Vision\n\n⛽️ We version our data because it is the fuel for our model development and experimentation process.\n💻 Data versioning tools like DVC allow you to apply the same mental model you have for git to your data.\n⋔ The ability to ‘branch’ off your data gives you the flexibility to experiment just as the same is true for branching off your code to try out some new behaviour.\nDVC is probably the leading tool that allows you to version your data and flexibly access all the previous ‘commits’ and checkpoints you make along the way."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#why-do-we-need-data-versioning-isnt-git-enough",
    "href": "posts/2022-05-24-data-versioning-dvc.html#why-do-we-need-data-versioning-isnt-git-enough",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🤔 Why do we need data versioning? Isn’t git enough?",
    "text": "🤔 Why do we need data versioning? Isn’t git enough?\nIf the lifeblood of traditional software engineering is code then the equivalent for machine learning is data. We solve the problem of checkpointing what our code looked like at a particular moment with git and online hubs like Github. Until recently there weren’t many equivalent options for data. We’re trying to solve the problem that often occurs if you’re asked to reproduce the data that was used to train a particular iteration of a model from some point in the past. Without some kind of data version control this is more or less impossible, particularly if your data is constantly changing.\nEven in my case for this redaction project, I wasn’t ingesting new data all the time but I was removing bad annotations or updating those annotations as I conducted error analysis or used tools like FiftyOne to understand why my model wasn’t performing as well as I’d have liked.\nLuckily there’s a pretty great tool in this space that seems to be more or less unchallenged for what it does in the data versioning domain: Data Version Control or DVC."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#dvc-use-cases",
    "href": "posts/2022-05-24-data-versioning-dvc.html#dvc-use-cases",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "👩‍💻 DVC Use Cases",
    "text": "👩‍💻 DVC Use Cases\nDVC does many things, but for our purposes at this moment its core value is that it helps us version our data. It also handles the case where we have large files or a dataset that changes a lot and where we might end up having problems with storing all the versions of this data.\nThe core behaviour we want to use with a data versioning tool is to access our data at one particular moment. Just like you incrementally annotate your code updates using git, with sometimes atomic progressions as you do your work, so it is with DVC that you can checkpoint your data as you make changes.\nAt the beginning this was a slight mental adjustment for me. When working on a project it is now second nature to regularly make git commits along the way, but I wasn’t in the habit of making regular data commits as a second step. In the long-run, this requires a bit of a mental shift but this is exactly what will enable the benefits that using a tool like DVC brings.\nIn particular, being able to experiment with data in a way that you can always roll-back from feels pretty liberating once you’ve covered your back with DVC. Just as you can use create git branches for your code, so you can create branches for your versioned data. Checking out the precise data used for some zany experiment you did is pretty painless. If you realise that the experiment is a dead-end and it’s not helping you move forward, just rewind and reset your data back to a useable state from before you had that crazy idea to create a million synthetic images :)\nOne other thing: DVC is built on top of git and it follows many of the mental models you might have about how versioning works. In this way, DVC luckily is smart about how it allows you to make incremental changes to your data. When it calculates the diff of your dataset before and after, it really is able to do some atomic updates and logging of what changed rather than just storing all the files multiple times over. This helps prevent you building up a really huge data cache and it helps the whole process be efficient.\n{% include info.html text=“I’ve mentioned this for other tools like Evidently before so I should also note that the DVC online community (https://dvc.org/community) is a pretty friendly and helpful place to hang out and learn about data versioning or to troubleshoot your problems. Nobody will tell you to RTFM here and their community events are generally beginner-friendly in my experience. This makes a big difference so they should be commended for the efforts they take to foster this kind community atmosphere. ❤️” %}"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#how-to-get-started-with-dvc",
    "href": "posts/2022-05-24-data-versioning-dvc.html#how-to-get-started-with-dvc",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🚀 How to get started with DVC",
    "text": "🚀 How to get started with DVC\nThe basics are mostly similar to how you’d use a tool like git:\n\nYou init your repository. This add some DVC superpowers on top of what you already have with git.\nYou specify which files you want to have DVC manage and track. It would make a lot of sense, for example, to have DVC handle tracking your models, your image files and your data annotations (if those exist as separate files).\nYou can optionally also specify a remote location where you want these files to be stored. (DVC supports several types of remote storage: local file system, SSH, Amazon S3, Google Cloud Storage, HTTP, HDFS, among others.)\n\n(To get a taste of the full workflow when using DVC for data tracking I’d recommend something like the basic tutorial they have here. They also recently added a three-part tutorial specific to computer vision that you might want to check out.)\nIf you want to use DVC programmatically using their Python API, you can get some information on this in their docs here. Unfortunately, these docs are incomplete and you’ll have to experiment a bit if you want to do anything beyond the simple functionality they themselves list. I’m told it behaves very similarly to how a tool like GitPython works, where you can just use the equivalent add() or checkout() function call that corresponds to a DVC CLI command, but given the lack of documentation it’s a bit harder to get a full sense of what is possible.\n{% include alert.html text=“DVC includes a lot of extra functionality around experiment tracking and pipelining of your code. You can safely ignore all that and just use DVC for data versioning. No shame in that :)” %}"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#when-to-use-dvc",
    "href": "posts/2022-05-24-data-versioning-dvc.html#when-to-use-dvc",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🛠 When to use DVC",
    "text": "🛠 When to use DVC\nIt probably is a good practice to use something like DVC from the start of most projects. If you know you’re never going to need to update the data you use, or if you will only ever generate one model, then maybe you have no need for data versioning. But realistically, when are you going to do that? Generally speaking you’ll be iterating a lot and you’ll be trying things out, so perhaps just start using DVC at the start of any new project: a git init can just as easily be followed by a dvc init…\nDVC will thrive in long-lived projects where you go down certain rabbit-holes, trying out different approaches and techniques. If you have a decent amount of data — and you probably do if you’re bringing deep learning to the table — then you can leverage how DVC makes it easy to store your data in the remote infrastructure of your choice with dvc remote."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#how-im-using-dvc-in-my-redaction-project",
    "href": "posts/2022-05-24-data-versioning-dvc.html#how-im-using-dvc-in-my-redaction-project",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "📄 How I’m using DVC in my redaction project",
    "text": "📄 How I’m using DVC in my redaction project\nFor my purposes, the things I’m tracking with DVC include:\n\nthe models I train\nthe PDF documents I downloaded from public sources that form the basis of the data in this project\nthe images that I extracted from the PDF documents\nthe annotations I make on the images using the standard COCO Dataset format.\n\nThis covers the core data that I expect to be working with for this project. I keep all this data synced to a remote Amazon S3 bucket which allows me to easily get set up on a new remote machine if needed.\nI’ll next be writing about how to move towards a ‘production-ready’ system in the coming weeks, but one thing I’ll hope to be adding to the current way I do things is to add some kind of ‘data cards’. I think a combination of manual comments and annotations alongside some auto-generated data profiles would be a useful thing to get a sense of for every checkpoint we make, particularly as the data grows and is augmented.\nLet me know if you’re using DVC to version your data for computer vision projects! I’m curious if there are any tricks I’m missing out…"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#appendix-how-to-switch-from-git-lfs-to-dvc",
    "href": "posts/2022-05-24-data-versioning-dvc.html#appendix-how-to-switch-from-git-lfs-to-dvc",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🏃 Appendix: How to switch from git-lfs to DVC",
    "text": "🏃 Appendix: How to switch from git-lfs to DVC\nWhen I first started this project, git-lfs or Git Large File Storage seemed the best option that didn’t constrain my choices. It allowed me to store any large files I had inside my repository and allowed for some sort of versioning. Over time this ended up being less robust, especially in the context of an ML workflow, so I decided to switch to using DVC backed by an Amazon S3 bucket.\nI didn’t find any useful information on the DVC website or forums on how to make this switch so I’m including my notes on how I switched over myself.\n{% include alert.html text=“Lots of caution is advised when doing this for your own project or work. I hit some major roadblocks along the way while doing this owing to some quirks of how I’d set ‘git-lfs’ up in the beginning. Please take all necessary backups and snapshots of your data in case something goes wrong along the way!” %}\nSome resources I consulted to understand how to do this:\n\nThis Stackoverflow thread\nA Github issue on the same topic\nA super useful Gist I reached via the previous Github issue that ended up guiding me most of the way\n\nThis is what I did, step by step:\n\nCommit and push everything to Git / Github / git-lfs\nCreate a branch, something like fix/remove-lfs\nRemove the hooks using git las uninstall\nGo into the .gitattributes file and delete whatever tracking you don’t want git-lfs to handle from now on. For me, this involved removing lines referring to .pth (model) files and .jpg (image) files.\nGet a list of all the files that git-lfs currently is storing using the following command: git lfs ls-files &gt; files.txt\nModify the file to remove the beginnings of each line that we don’t need. At the end we want our files.txt to contain just a series of paths to our files. I did it with a simple Python script:\n\nwith open(\"filenames.txt\", \"w\") as f:\n    f.write(\"\")\n\nwith open(\"files.txt\", \"r\") as f2:\n    for line in f2:\n        with open(\"filenames.txt\", \"a\") as f:\n            f.write(line.split(\" \")[-1])\n\nRun run git rm --cached for each file that git-lfs is storing. I did this with a simple bash command that uses the file I’d created in the previous step:\n\nwhile read line; do git rm --cached \"$line\"; done &lt; files.txt\n\nInitialise the DVC repository with dvc init\nAdd whatever data sources you want tracked (dvc add FOLDERNAME)\nAllow for autostaging with DVC with the dvc config core.autostage true command\nCommit everything\nCheck that no git-lfs files are left with the git lfs ls-files command. Whatever you uncached in previous steps should not show up any more.\nRemove any lfs with rm -rf .git/lfs\nMerge your branch into main\n\n(if you’re using git-lfs as a team, now is probably the time when other collaborators can uninstall git-lfs as specified above)\n\nIf needed, add your DVC remote storage with dvc remote add … (consult the docs for exactly how to set this up for your specific needs)\ndvc push to get your files synced with your remote storage\n\nAt this point you should be fully transitioned over. As I mentioned above, there are a ton of weird edge cases and quirks to this process and you probably shouldn’t follow this list blindly. I’m mainly writing this up for my own records as much as anything else, so perhaps it’s helpful for someone else seeking to transition but maybe it should be taken less as a direct list of instructions than an inspiration or general template. (I wish DVC would provide some official-ish guidance on this process through their documentation. I imagine that it’s a fairly common path for someone to outgrow git-lfs and want to get going with DVC but currently there are no instructions for how to think this through.)\nUPDATE: I originally made reference to ‘continuous training’ in the title of this blogpost but I didn’t actually get into this specific use case in what I covered, so I took that out of the title and we’ll save the specifics for a subsequent post!"
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html",
    "href": "posts/2022-09-07-serialisation.html",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nSerialisation and deserialisation. I ran headfirst into these two words on my first day in my new job. From the way my colleagues discussed them, it seemed like this was something I should have learned from a computer science degree; foundational concepts with practical applications throughout most places that computers touched.\nA few months in, I’ve come to appreciate a little more about what the underlying concept is about as well as some of the reasons why it remains both relevant and something that pops up regularly. I’ll begin by setting out some of this context before showing an example of where I encountered it recently in my own project. By the end, you’ll understand why this is such an important (and practical) concept and why you’ll encounter it a lot while doing machine learning."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#the-basics",
    "href": "posts/2022-09-07-serialisation.html#the-basics",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🔢 The Basics",
    "text": "🔢 The Basics\nIn the common definition, serialisation is the process by which you convert something into a sequence of bytes, and deserialisation is when you convert the other way (i.e. from bytes). In some domains it is also known as marshalling or pickling.\nThis commonly is encountered when you need to store some data on disk (i.e. not or no longer in memory). Perhaps you need some kind of permanent storage of that data, or you need to make the data available to another process. The process through which you transform the data (from something that is comprehensible to whatever environment or language you’re working on) is serialisation.\nTo give another example, in a language like Python we often think in and deal through a series of ‘objects’: think dictionaries or even classes in an OOP context. In order to save this to disk, we have to convert it to some other format that firstly is in some format that is stable when saved as a file. We might want to send that data across the network, or have it opened by a different process or a programme running in a different language. Serialisation is the process by which something context and perhaps language-specific gets transformed into this universal substrate (i.e. a sequence of bytes)."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#common-ways-to-serialise-data-in-python",
    "href": "posts/2022-09-07-serialisation.html#common-ways-to-serialise-data-in-python",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🍏 Common ways to serialise data in Python",
    "text": "🍏 Common ways to serialise data in Python\nIn the past, pickle was a commonly-used way of making this conversion. It has a lot of shortcomings, two of which sit at the top of the list:\n\nthere isn’t (as far as I’m aware) much interoperability for objects that are serialised with pickle. If you want to load an object that has been ‘pickled’, the entity doing the ‘unpickling’ will have to be running the exact same version of Python as the one that did the pickling. (If I’m not mistaken, there might even be some cross platform interoperability issues as well.)\nsecurity concerns are serious when it comes to pickle: when you load(...) some pickled object, this will run whatever code is inside with the assumption that it is ‘trusted’. As such, it is unsuitable for use with untrusted data and generally people tend to turn their nose at pickle. (If you do have to interact with some pickled data, pickletools is a handy tool that allows you to inspect and interact with the file without running the arbitrary code packaged inside. While we’re at the library recommendations, it’s also worth checking out fickling which overlaps in functionality somewhat.)\n\nJSON has become a commonly-used format for serialising data (or its cousin JSONL, for too-much-to-load-into-memory-at-once data). This is a common format with many uses, but it does come with a serious shortcoming which is that it only supports certain data types. If you’re saving some custom object of your own creation, you’ll first need to convert that into a format that can be transformed into a JSON object/file. If you don’t, then your object will not be able to be rehydrated from the on-disk representation.\nNote that the Python pickle module serialises data into a binary format, whereas the json module converts it into a text format (i.e. readable and comprehensible to someone browsing files or displaying their contents with something like cat). Moreover, pickle does handle many (most?) objects and types that you can throw at it, though with all the caveats mentioned above.\nI haven’t explored it at all, but while reading a bit about this area I was consistently pointed to Google’s Protobuf format / library which is another way to serialise structured data. I am unable to properly evaluate the extent to which this is an improvement on existing protocols."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#serialisation-and-deserialisation-in-machine-learning",
    "href": "posts/2022-09-07-serialisation.html#serialisation-and-deserialisation-in-machine-learning",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🔐 Serialisation and deserialisation in Machine Learning",
    "text": "🔐 Serialisation and deserialisation in Machine Learning\nI mentioned earlier that this concept and operation was something that I confronted more or less on my first day working in my new job. (We build an open-source framework that supports someone working to build and deploy machine learning models.) In order to understand why this is so important, a small detour showing a basic example of a ZenML pipeline is necessary. What follows is an extremely simple example showcasing how pipelines are composed of steps, and how those are in turn run:\nfrom zenml.steps import step\nfrom zenml.pipelines import pipeline\n\n@step\ndef read_integer() -&gt; int:\n    return 3\n\n@pipeline\ndef basic_pipeline(read_integer) -&gt; None:\n    read_integer()\n\nbasic_pipeline(read_integer=read_integer()).run()\nPipelines are constructed out of a series of steps. The steps are defined with an @step decorator, and pipeline definitions are composed in a similar way. Finally, at the end we specify which steps correspond to which parts of the pipeline definition and then call the run() method to execute our pipeline.\nYou’ll also note the presence of some type annotations as part of how we define our step and pipeline. These are required, and while they may seem simplistic and unnecessary at the moment, later on they will make things much clearer.\nOur pipeline isn’t doing much at the moment, you might think. Behind the scenes, however, ZenML is doing a lot of legwork:\n\nstoring the outputs (and inputs, though there aren’t any in this basic example) of all steps\ncaching those output values or objects, such that if the code doesn’t change then we should just retrieve the cached value.\nvalidating and checking the types of values that get returned so that we can be sure our code is returning what we hope / think it should be returning.\n\nMoreover, it does all this in a way that all this intermediary state is stored on disk and versioned. If you update your pipeline steps then rerun it, ZenML will save the new outputs such that you can go back and inspect where data came from and so on.\nIn order to save all these objects on disk, however, and to bring this story full-circle, ZenML serialises the data when saving the artifacts from pipeline runs, and deserialises that data when those artifacts are needed (by the cache, for example, or when you want to access a step output once your pipeline has completed its run). We call this part of the process ‘materialisation’. (There’s more in our docs on materialisation here, and if you’re searching, be sure to search with a ‘z’ and not an ‘s’, coz America.)"
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#a-basic-custom-materializer",
    "href": "posts/2022-09-07-serialisation.html#a-basic-custom-materializer",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🛠 A basic custom materializer",
    "text": "🛠 A basic custom materializer\nFor most kinds of ‘normal’ Python objects, this is no problem at all. But as we saw above, if we’re going to be able to reconstruct and rehydrate an object from a static sequence of bytes, we’re going to need to do a bit more to make this happen. Within ZenML this means that if you have some special kind of object or type, you’ll need to define a ‘custom materialiser’; this is code that defines how ZenML should serialise and deserialise the objects that you want to be stored as state on disk.\nTo give you a sense of what this will look like, here’s our code from above but updated a little to fit this new scenario:\nimport os\nfrom typing import Type\n\nfrom zenml.artifacts import DataArtifact\nfrom zenml.io import fileio\nfrom zenml.materializers.base_materializer import BaseMaterializer\nfrom zenml.pipelines import pipeline\nfrom zenml.steps import step\n\nclass MyCustomObject:\n    def __init__(self, name):\n        self.name = name\n\nclass MyCustomMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (MyCustomObject,)\n    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n\n    def handle_input(self, data_type: Type[MyCustomObject]) -&gt; MyCustomObject:\n        \"\"\"Read from artifact store\"\"\"\n        super().handle_input(data_type)\n        with fileio.open(os.path.join(self.artifact.uri, \"data.txt\"), \"r\") as f:\n            name = f.read()\n        return MyCustomObject(name=name)\n\n    def handle_return(self, my_obj: MyCustomObject) -&gt; None:\n        \"\"\"Write to artifact store\"\"\"\n        super().handle_return(my_obj)\n        with fileio.open(os.path.join(self.artifact.uri, \"data.txt\"), \"w\") as f:\n            f.write(my_obj.name)\n\n@step\ndef read_custom_object() -&gt; MyCustomObject:\n    return MyCustomObject(\"aria\")\n\n@pipeline\ndef basic_pipeline(read_custom_object) -&gt; None:\n    read_custom_object()\n\nbasic_pipeline(\n    read_custom_object=read_custom_object().with_return_materializers(\n        MyCustomMaterializer\n    )\n).run()\nYou’ll notice a new piece of code which defines the MyCustomMaterializer class. This is subclassed off our BaseMaterializer class and we just have to define two methods, one that handles how to serialise or save the data to disk, and the other that handles how to deserialise or rehydrate the objects/data from disk. We add a special .with_return_materializers call when we run the pipeline; this lets ZenML that when we encounter a weird type of object, it can go ahead and use our custom defined materialiser to handle it.\nI hope you’ll agree that this stuff isn’t too hard to grok, and while the precise steps of how you implement all this might take a bit of getting used to, it’s conceptually not too hard once you understand the foundations of what you’re doing. It took me longer than I’m proud to admit to really understand the elegance of this way of doing things, but all these little pieces add up and you can then go off and use them in your real-life projects."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#materialisation-in-practice-icevision-and-custom-objects",
    "href": "posts/2022-09-07-serialisation.html#materialisation-in-practice-icevision-and-custom-objects",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🕵️ Materialisation in practice: IceVision and Custom Objects",
    "text": "🕵️ Materialisation in practice: IceVision and Custom Objects\nCase in point: my object detection pipeline. I took a bit of a break over the summer, but now I’m back and working to get my pipeline production-ready. Defining the basic steps of my pipeline were fairly easy; I’ve already described that in my last blog post.\nThe moment I started defining my pipeline in code, I immediately hit a whole array of non-standard objects. My data loading steps returned IceVision-specific parsers custom to COCO BBoxes and my training step returned a collection of various custom objects combining code with the trained model parameters. (Note: for some common use cases like training with raw PyTorch or Tensorflow etc, ZenML has defined many standard materialisers already to get you going quickly.) I realised that I’d have to define custom materialisers to handle these different inputs and outputs.\nSome of this wasn’t trivial to implement. Sometimes you might get lucky and the library you work with has implemented some handy features to help with serialisation and deserialisation. From what I can tell, this seems to be the case when saving models with PyTorch, for example. But for the rest it’s often less clear what need to happen and why code works in the way it does. To save the IceVision RecordCollection object, for example, I had to jump through some hoops, converting several sub levels of custom objects along the way, to make sure that my objects were serialisable.\nHere’s the custom materialiser code responsible for handling those conversions and serialisation for the RecordCollection. (Think of RecordCollection just as a type of stored data, parsed and ready to use for model training.)\nimport os\nimport pathlib\nfrom typing import Any, Dict, List, Type\n\nfrom icevision.all import *\nimport srsly\nfrom zenml.artifacts import DataArtifact\nfrom zenml.io import fileio\nfrom zenml.materializers.base_materializer import BaseMaterializer\n\nclass COCOMaterializerParser(Parser):\n    def __init__(self, template_record, records: List[Dict[str, Any]]):\n        super().__init__(template_record=self.template_record())\n\n        self.records = records\n        self.class_map = ClassMap(records[0][\"common\"][\"classes\"])\n        print(self.class_map)\n\n    def __iter__(self) -&gt; Any:\n        yield from self.records\n\n    def __len__(self) -&gt; int:\n        return len(self.records)\n\n    def record_id(self, o: Any) -&gt; Hashable:\n        return o[\"common\"][\"filepath\"]\n\n    def template_record(self) -&gt; BaseRecord:\n        return BaseRecord(\n            (\n                FilepathRecordComponent(),\n                InstancesLabelsRecordComponent(),\n                AreasRecordComponent(),\n                IsCrowdsRecordComponent(),\n                BBoxesRecordComponent(),\n            )\n        )\n\n    def filepath(self, o) -&gt; Path:\n        return pathlib.Path(o[\"common\"][\"filepath\"])\n\n    def img_size(self, o) -&gt; ImgSize:\n        return ImgSize(width=o[\"common\"][\"width\"], height=o[\"common\"][\"height\"])\n\n    def labels_ids(self, o) -&gt; List[Hashable]:\n        return o[\"detection\"][\"label_ids\"]\n\n    def areas(self, o) -&gt; List[float]:\n        return o[\"detection\"][\"areas\"]\n\n    def iscrowds(self, o) -&gt; List[bool]:\n        return o[\"detection\"][\"iscrowds\"]\n\n    def bboxes(self, o) -&gt; List[BBox]:\n        boxes = []\n        for bbox in o[\"detection\"][\"bboxes\"]:\n            a, b, c, d = bbox\n            new_bbox = BBox.from_xyxy(a, b, c, d)\n            boxes.append(new_bbox)\n        return boxes\n\n    def parse_fields(self, o: Any, record: BaseRecord, is_new: bool):\n        if is_new:\n            record.set_filepath(self.filepath(o))\n            record.set_img_size(self.img_size(o))\n\n        record.detection.set_class_map(self.class_map)\n        record.detection.add_areas(self.areas(o))\n        record.detection.add_iscrowds(self.iscrowds(o))\n        record.detection.add_bboxes(self.bboxes(o))\n        record.detection.add_labels(o[\"detection\"][\"labels\"])\n\n\ndef detection_record_collection_to_json(rcoll: RecordCollection) -&gt; str:\n    indexes = list(rcoll._records)\n    records = [rcoll._records[index] for index in indexes]\n    classes = rcoll[0].detection.class_map.get_classes()\n    dict_records = [record.as_dict() for record in records]\n    for record in dict_records:\n        record[\"common\"][\"filepath\"] = str(record[\"common\"][\"filepath\"])\n        bboxes = record[\"detection\"][\"bboxes\"]\n        new_bboxes = []\n        for bbox in bboxes:\n            a, b, c, d = bbox.xyxy\n            new_bbox = [a, b, c, d]\n            new_bboxes.append(new_bbox)\n        record[\"detection\"][\"bboxes\"] = new_bboxes\n        record[\"common\"][\"classes\"] = classes\n    return srsly.json_dumps(dict_records)\n\n\ndef detection_json_str_to_record_collection(records: str) -&gt; RecordCollection:\n    r = srsly.json_loads(records)\n    template_record = ObjectDetectionRecord()\n    parser = COCOMaterializerParser(template_record, r)\n    parsed_records, *_ = parser.parse(data_splitter=SingleSplitSplitter())\n    return parsed_records\n\n\nclass COCOBBoxRecordCollectionMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (RecordCollection,)\n    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n\n    def handle_input(self, data_type: Type[RecordCollection]) -&gt; RecordCollection:\n        \"\"\"Read from artifact store\"\"\"\n        super().handle_input(data_type)\n        with fileio.open(\n            os.path.join(self.artifact.uri, DEFAULT_RECORD_COLLECTION), \"r\"\n        ) as f:\n            return detection_json_str_to_record_collection(f.read())\n\n    def handle_return(self, record_collection_obj: RecordCollection) -&gt; None:\n        \"\"\"Write to artifact store\"\"\"\n        super().handle_return(record_collection_obj)\n\n        json_string = detection_record_collection_to_json(record_collection_obj)\n        with fileio.open(\n            os.path.join(self.artifact.uri, DEFAULT_RECORD_COLLECTION), \"w\"\n        ) as f:\n            f.write(json_string)\nAs you can see, there’s a decent amount going on here. In my custom materialiser, I have a detection_record_collection_to_json method that constructs the JSON representation of my custom RecordCollection object. I use Explosion’s handy srsly package for their forks + bundling together of various Python serialisation libraries. For the rest, that requires a bit more knowledge of how IceVision handles things like BBox objects and COCO Records under the hood, but you can get the idea that it’s not completely trivial."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#serialisation-is-for-everyone",
    "href": "posts/2022-09-07-serialisation.html#serialisation-is-for-everyone",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🥳 Serialisation is for Everyone!",
    "text": "🥳 Serialisation is for Everyone!\nIt’s also not completely impossible to implement either, though, lest you feel like I’m leaving you without hope. My aim with this article was to guide you to the point where you feel you can understand why serialisation is important and to know why you might well encounter it during your data science journey. The moment you need to do something just slightly longer-lasting than an ephemeral training run that is tracked nowhere and just lives in a Colab notebook, that’s when you’ll hit serialisation.\nMoreover, I showed how you can incrementally build up your pipelines with a tool like ZenML to handle lots of parts of the complexity that come with your modelling work.\n[Image credit: Photo by fabio on Unsplash]"
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "",
    "text": "(This is part of a series of blog posts relating to and responding to the live FastAI course (part 2) being taught October-December 2022. To read others, see the ones listed for the ‘parttwo’ tag.)\nMuch awaited and anticipated, the second part of the FastAI course is being taught live again. If part one is about getting solid foundations and learning how to get going in a practical/useful way, part two is about approaching things from the foundations but with research or ‘impractical’ questions kept in mind. The backdrop of the current iteration is the breakthroughs happening in the world of generative computer vision models like Stable Diffusion, which we’ll explore and deconstruct (and reconstruct?!) over the coming weeks.\nDiving into the details of how things work means that along the way we’re much more likely to encounter (legitimate) specialist vocabulary and techniques as well as a decent dose of jargon. Whereas bringing up the intricacies of particular algorithms, architectures or mathematical methods was unnecessary during part one, it seems like part two is a little bit more of a venue for that kind of material. I will use these blogs as a way of reviewing materials and concepts introduced during the lectures as well as keeping track of the big questions I have.\nIn this blog, in particular, I’ll keep a glossary at the bottom for some new terms which were introduced. I may repeat this for subsequent blog reviews, depending on what’s covered in those lessons. I’ll also keep a section containing new mathematical symbols that are introduced. (This blog mainly relates to the core lecture given during week 1. I’ll update it later with some small extras that came up from the 9A and 9B videos, or expand those into separate posts on their own.)\nStable Diffusion isn’t, in itself, a model that I’m especially interested in, except insofar as it teaches me fundamental principles about the craft of deep learning or about doing research in this field. As such, my plan and current intention is to stick to documenting core mental models or bigger-picture lessons that I’m taking away from the lessons rather than each individual step that Jeremy made along the way. (This seems to be the motivation behind including it in the course at all. Stable Diffusion touches so many topics (big and small) and getting to grips with this one thing will help understand many other things about machine learning and the world of research.)"
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#fundamentals-still-count",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#fundamentals-still-count",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "Fundamentals still count",
    "text": "Fundamentals still count\nEven though there are a hundred and one small innovations and technologies which make something like Stable Diffusion possible, in the end we’re still dealing with Deep Learning and we’re still dealing with finding ways of converting things into numbers which can be used by machines to update weights by way of evaluating loss functions. So many of the individual pieces that make up how you build something like Stable Diffusion amount to:\n\nfigure out how to get this non-number-like thing into a numeric representation (ideally a vector of some kind)\ndo all the usual deep learning things that we’ve done a thousand times and that we know work\nat the end, maybe find a way to convert the numeric representation that our model learned into some kind of form that is useful to us\n\nObviously the details are important and nobody is creating magical generative art with this very high-level hand-wavy explanation, but for someone at the earlier end of their journey into deep learning it is reassuring that the fundamentals continue to have relevance and that those mental models remain useful as a way of thinking about new developments."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#the-tricks-are-the-way",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#the-tricks-are-the-way",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "The tricks are the way",
    "text": "The tricks are the way\nThe other pleasant surprise was the enduring relevance of ‘tricks’. In chapter one of the FastAI book, Jeremy & Sylvain showcase a number of examples where clever approaches are taken to solve problems with Deep Learning:\n\na malware classification program is made by converting malware code into an image which is used to train a model\na fraud detection algorithm is trained by converting images of computer mouse movements\n…and so on\n\nEven amongst the Delft FastAI study group, Kurian trained a classifier to detect genre in music samples using a similar method (i.e. using images as an intermediary form for the samples which were used in training). The book emphasises:\n\n“In general, you’ll find that a small number of general approaches in deep learning can go a long way, if you’re a bit creative in how you represent your data! You shouldn’t think of approaches like the ones described here as”hacky workarounds,” because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.”\n\nMost of these ‘tricks’ seem to relate to either performance improvements (i.e. how can we get this training to happen faster, or with fewer compute needs) or ways of getting your problem domain into a form that we can use deep learning techniques on them. In the case of Stable Diffusion, one of the problems we have to address is how to work in this multi-modal manner, where text is used to represent a particular idea (which in turn needs a vector/numeric representation) but where we also want to represent that same idea in image form.\nAt the same time, we have the whole autoencoder part of the story — whereby we use an encoder to turn a large image into a (smaller-sized) latent representation which can be used in training, and then we use a decoder to turn a noisy latent into a full-sized image — which seems to mainly be about making the training process more efficient.\nEach of these techniques come with their own complexities and histories, but it’s just notable to me how the story of the development of machine learning techniques seems somehow to be a succession of these small incremental innovations that progressively accrue. That’s not to say that there aren’t big breakthroughs in either understanding why things work the way they do, or in the more tactical method space, but it just seemed very apparent in the unpacking of Stable Diffusion that a great deal of creative stitching together of ideas had taken place.\nThe historian in me is fascinated by the different pathways that the field has explored, or the reasons why certain techniques emerged when they did, or how hardware improvements gave tried-and-rejected techniques a new lease of life, but I’m guessing that probably doesn’t help much with the work of research."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-train-the-diffusion-model",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-train-the-diffusion-model",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "💪 What happens when we train the diffusion model",
    "text": "💪 What happens when we train the diffusion model\nA diffusion model is a neural network that we train. The way it works is that it removes noise from an image (passed in as input along with a text prompt) such that the output more closely resembles the prompt. When we are training our network, we pass in the vectorised words along with the latent forms of the images (since those are much smaller file sizes and thus faster / more efficient to train). We use the encoder to get a latent representation of the image that we use for training.\nFor the text caption, we want a way to represent the association of images with text captions in vector space. In other words, if there are various phrases that all represent more or less the same image if you were to translate those phrases into an image, then those should be similar when represented as a vector. The technique or trick for this is to use ‘contrastive loss’, a particular kind of loss function which allows us to calculate the relative similarity of two vectors. This contrastive loss is what gives us the first two letters of ‘CLIP’, a neural network developed by OpenAI.\nThe CLIP model takes some text and outputs an embedding, i.e. some features in vector form that our unet can use for training along with the images in their latent representation form."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-generate-an-image",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-generate-an-image",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "🎨 What happens when we generate an image",
    "text": "🎨 What happens when we generate an image\nWhen generating our image we can use the neural network we trained to progressively remove noise from our candidate image. We start off with a more or less completely noisy image, then apply the unet to it and it returns the noise that it calculates is sitting on top of a latent representation that approximates the vectorised version of our prompt. We take a fraction of that, remove it, and repeat a few times. (Currently that can take as many as 50 iterations before we reach a really impressive image, but new techniques are in review which would dramatically reduce the need for so many iterations.)\nNote that it is during the inference stage where we need the decoder part of our (VAE) encoder to turn a latent tensor representation of an image into a fully-fledged large picture."
  },
  {
    "objectID": "posts/2023-04-28-removing-git-commits.html",
    "href": "posts/2023-04-28-removing-git-commits.html",
    "title": "How to remove a commit (or two) from your git branch",
    "section": "",
    "text": "I ran into a problem where a tool I was using auto-updated my git branch, forcing dozens of changes. The changes were relatively innocuous, but they were irrelevant to the work I was doing which would make reviewing the Pull Request pretty hard going.\nThis is what I did to remove the traces of that commit from my git branch logs:\nFirst, make sure you are on the branch where the commit you want to remove exists. If not, switch to that branch using:\ngit checkout &lt;branch_name&gt;\nUse git log to find the commit hash (the unique identifier) of the commit you want to remove. The commit hash will be a long string of characters and numbers, e.g., ab12cd34ef56gh78ij90kl12mn34op56qr78st90.\nUse git rebase -i (interactive rebase) to remove the commit. This command will open an editor where you can manipulate the commit history. Use the commit hash that’s one before the one you want to remove:\ngit rebase -i &lt;parent_commit_hash&gt;\nIn the editor that opens, you will see a list of commits starting from the parent commit hash you provided. Find the line with the commit you want to remove. Change the word pick at the beginning of that line to drop or simply delete the entire line. Save and close the editor. (Alternatively, if you’re using VS Code, a sort of UI interface will open which will allow you to select from drop-down pickers which options you want for each of the downstream commits. There is a button at the bottom to switch to the pure text interface if you prefer.)\nGit will perform the rebase, removing the specified commit from the commit tree.\nIf you’re satisfied with the changes, push your branch to the remote repository:\ngit push -f origin &lt;branch_name&gt;\nImportant Note: Using git push -f (force push) can be dangerous, as it overwrites the remote branch with your local one. Make sure you are confident about your changes before using this command. If you’re working on a shared branch with others it’s important to communicate with your team to ensure that no one else is working on the same branch to avoid losing any work.\nThe commits I’d made after the one I wanted to remove didn’t overlap or relate to each other, so I didn’t face any conflicts during the rebase process. However, if you do encounter conflicts, you’ll need to resolve them manually and continue with the rebase using git rebase --continue.\nRemember that rebasing and force pushing can rewrite the commit history, so always be cautious when performing these operations."
  },
  {
    "objectID": "posts/2023-05-02-logarithms-exponents-mu123.html",
    "href": "posts/2023-05-02-logarithms-exponents-mu123.html",
    "title": "Exponents and Logarithms: a MU123 review",
    "section": "",
    "text": "I just finished a unit of my Open University Maths degree that’s all about exponents and logarithms (unit 13 of MU123). This is the penultimate unit and these final units are where the difficulty has started to stack up for me.\nThe 2020 COVID pandemic was when all of us had lots of exposure to the idea and reality of exponential curves. In fact, it seemed early on like the people who really understood what exponential growth can do were the most worried. Cancer’s another thing where something that keeps on doubling or increasing can cause real havoc when it compounds. The book talks a bit about Moore’s law, though I recall watching a talk at JuliaCon in 2018 where Sophie Wilson spoke about all the ways where we may be edging up at various physical realities these days that might slow things down.\nLogarithms were the other big part of the module, and these were fascinating to work with. I had familiarity with the idea of them, mainly just as a button on a calculator, but it was really interesting to start to play around with all the ways they could be useful. Prior to the widespread availability of calculators they were the de facto way of doing big calculations, since using something like logarithm tables or a slide rule would give you a practically useful approximation of your answer in a way that didn’t require you to do complicated calculations in your head.\nI got hold of both an old copy of a book of logarithm tables as well as a slide rule — old tech ftw — and look forward to becoming more familiar with them once the unit has come to an end in a few weeks.\nThe previous unit (trigonometry) touched on working with radians and (doing some exercises) it was immediately clear why someone would want to use radians instead of degrees as the unit when working with a certain kind of problem. Similarly, for this exponents & logarithms unit, we came across e, Euler’s number, and had some exposure to how there were quite a few places where it made sense to work with e as the base of our logarithms instead of base 10. I really enjoy these parts of mathematics, where abstract concepts or things that people come up with allow us to manipulate ideas and symbols and objects that in turn allow us to solve problems, or think about things in new ways. In many ways these are the things I enjoy the most while studying the course materials.\nWe came up on a few places where we were asked to prove something using various identities that we’d previously explored. For example, we prove that for any base b and any positive numbers x and y, that:\n\\[\\log_b x - \\log_b y = \\log_b(\\frac{x}{y})\\]\nWe’re not given a great deal of guidance on this task aside from an analogous example. I know the idea of ‘proof’ is a big thing in higher-level mathematics so I’m looking forward to getting a bit more experience with this.\nAs with most of the previous units, the bigger picture is somewhat elusive. I know a lot more about exponents and logarithms than I knew a few weeks ago, but I’m still unsure of how it connects to everything else around it. I’m hoping that comes with time, but in the meanwhile I’m trying to write these blogposts to at least take a step back and think through those bigger connections.\nIn the end, logarithms and exponents are another trick, another tool in the box, and another example of how mental models or abstractions can allow for other new thoughts to be had. We end up using some of the index laws that we learned a few months back to allow us to manipulate these new symbols. Manipulating symbols allows us to either simplify things (so we can think at a different level) or move further into something more complex (so we can see what’s going on, through whatever lens we’ve brought to look through).\nAs a project and as a path to be explored, it’s exciting to take these steps even if sometimes you just have to trust that everything will come together in the end. Next up: “Mathematics Everywhere”, the final unit which is all about practical applications using the things we’ve learned earlier in the module, with a serving of abstract mathematics thrown in for good measure!"
  },
  {
    "objectID": "posts/2023-05-21-balochi-language-modelling.html",
    "href": "posts/2023-05-21-balochi-language-modelling.html",
    "title": "Low-resource language models: making a start with Balochi",
    "section": "",
    "text": "Large Language Models are all the rage, but what do you do when the language you want to model is essentially unrepresented in the public datasets used for training? I have a few months before the start of my next maths module and I’d like to use the time in part to dive into the ins and outs of training your own language models from scratch.\nThe language I’m working with is Balochi, in particular the dialect or subset of Balochi that is spoken in southeastern Iran. The parent subgroup of ‘Balochi’ is spoken by 8-10 million people, but those break down into a few varieties which is in turn driven to a large extent by geography. The kind of Balochi used in Pakistan is subject to different linguistic influences than the one I’m interested in, for example.\nDespite the existence of millions of people speaking this language (family), it is more or less unrepresented in benchmarks and so-called breakthroughs in language modelling. Even the raw data to represent the language is absent. Common Crawl, one popular source of data for training language models, doesn’t even include Balochi as one of the languages represented in its corpus. Moreover, there’s no Balochi Wikipedia or anything really like it, so anyone hoping to work on language models in Balochi is first going to have to put together a corpus of data.\nThere’s nothing particularly novel about this problem. Languages with very few resources are known as low-resource languages and there’s a whole field of research (and some practice) busy trying to find ways to better serve these smaller language communities. I view the work as valuable, not only in that it seeks to preserve what might otherwise be lost, but also in terms of the disproportionately large (potential) impact it can have.\nI have personally experienced this issue at a distance, for the most part, having studied and worked with languages for which there are few study materials. The two languages I was passed down by my parents — English and, to a lesser extent, Dutch — are well-represented in the work and time spent by researchers and practitioners thus far. I have learned languages from neighbouring families, both geographically and linguistically, and always wanted to study Balochi myself. My hope is that it will be a gateway for me into the language and its community of speakers.\nI’m quite conscious of wanting to go about this project in a way that is ethical. Work of this kind is too often predicated on a principle of ‘take now, ask later’ so I’ll be writing more about this as I go as well as (hopefully) working with Balochi speakers and researchers to augment the work that is already being done. My initial survey of what has been done so far leads me to think that there is much remaining in the way of low-hanging fruit. I haven’t yet come across a Balochi tokenizer, for example, or embeddings or many other things that you would take for granted if you were working with the English language.\nMy somewhat distant goal — one for which I’m unsure how unrealistic I’m being — for all of this would be develop models and materials that can aid non-native speakers of Balochi to learn the language through what is known as comprehensible input. All of which is to say: I don’t know much Balochi as I start this work, but I hope to develop my fluency over time."
  },
  {
    "objectID": "posts/2023-05-29-balochi-language-dataset.html",
    "href": "posts/2023-05-29-balochi-language-dataset.html",
    "title": "Building a Balochi Language Dataset for NLP Applications",
    "section": "",
    "text": "I’m working on building out some language models and utilities for the Balochi language. (Read previous posts in this series for the full context.) Even though there are some 8-10 million estimated speakers, it certainly falls into the category of being a ‘low-resource’ language. Many (most?) things that you’d take for granted when working with English-language models are either non-existent or bare bones for Balochi.\nThe experimentation phase of a project like this rewards a fast iteration speed, so I’m looking for ways to keep moving forward. I don’t need to spend days running a single experiment to validate my ideas; I’m sufficiently green that small datasets and these frequent tweaks to what I’m doing will hopefully reward me.\nI did an initial survey of materials and resources that already exist, collecting a mix of more general language materials alongside some prior work that exists in the NLP space for Balochi. In particular, there are some small datasets on GitHub as well as some more targeted projects for Named Entity Recognition (NER). Since it’s my repository, I also threw in some blog posts that inspired me to get started in the first place (from Lj Miranda and Kurian Benoy, among others).\n\nThe awesome-balochi-nlp repository is my first effort at gathering a list of resources. I’ll be keeping it up to date as I continue.\nFor my work gathering the dataset together, I had my eyes on three potential sources of authentic Balochi texts:\n\nSina Ahmadi’s PersoArabicLID project (language classification for a series of low-resource languages that share a common script) includes (labelled) datasets as part of the repository\nBaask.com — a website that’s been posting Balochi content for around a decade and that I had come across in the past\nKissah.org — a project by Junaid Qadir that collates Balochi stories\n\nThe mechanics of gathering the texts from these sources was straightforward (a few scripts using beautifulsoup and the requests module), but I’ll admit that the experience felt a little uncomfortable. The content from these sources may technically be ‘fair game’ but I’ll admit to a certain queasiness about how easy it was to put together my promo-dataset of Balochi language in an evening. (For that reason, I’m probably not going to open up the dataset until I’ve figured out a way to do that properly; the ideal end-goal is to have datasets like this available publicly on the Huggingface Hub and so on.)\nSo now I have a dataset containing some 2.6 million words of Balochi text. This feels like it’s enough to do some experiments at least, and we’ll see how far I get with it. The first order of business is to look into tokenisation or the process of splitting those texts up into pieces that can be used and processed by the machine learning machinery. Surprise surprise: there aren’t any pre-existing tokenisers for Balochi and while there are language-agnostic tokenisation processes I want to understand the tradeoffs around the different algorithms and approaches they take."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html",
    "href": "posts/2023-06-22-input-variables-terraform.html",
    "title": "Terraform Input Variables",
    "section": "",
    "text": "When working with Terraform code, there are ways to take in user input at the time when you are applying whatever you’ve defined. To take a perhaps needlessly simple example, you might write a definition that allows you to deploy a new S3 bucket but you probably wouldn’t want to hardcode the name of the new bucket; instead, you’d rather take that name at the point of deployment. So if we think of the terraform process as a big function call, our input variables are the inputs we pass into this function application.\nThere are three main ways that you’ll use and encounter input variables in HCL code.\nThe first two are fairly commonly used, especially during development / testing, but are not really a great idea if your aim is a production-grade setup. Let’s walk through them one by one, but first let’s look at the variable block itself in HCL."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variable-blocks",
    "href": "posts/2023-06-22-input-variables-terraform.html#variable-blocks",
    "title": "Terraform Input Variables",
    "section": "Variable blocks",
    "text": "Variable blocks\nA variable block can look like this:\nvariable \"some_variable\" {\n  description = \"This is where you describe the variable\"\n  type        = string\n  default     = \"ginger_cat\"\n  nullable    = false\n  sensitive   = false\n}\nMost of this is self-explanatory. You can also specify a validation value in which you can determine the appropriate or accepted values for the variable. If you set sensitive to true, then terraform will (mostly) keep the value out of any output printed in the terminal. The name for the variable should be unique within the module and the value of this variable must be a literal value; it cannot be the result of an expression. If you don’t specify a default value then on running terraform apply or plan you will be asked what value you want to use.\n(There are also a limited handful of key words that are forbidden for use as names for the variable; you can look them up in the docs.)"
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-at-the-command-line",
    "href": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-at-the-command-line",
    "title": "Terraform Input Variables",
    "section": "Variables passed in at the command line",
    "text": "Variables passed in at the command line\nInstead of just waiting for the terraform CLI to ask you what values you want to assign to variables, you can pass them in at the command line. Your variables might live in a variables.tf file (or be located across your .tf files) but when you run terraform apply you can do so in the following way:\nterraform apply -var=\"some_variable=black_cat\"\nIf you have multiple variables (as you are likely to have) then you can use the -var option multiple times."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-with-files",
    "href": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-with-files",
    "title": "Terraform Input Variables",
    "section": "Variables passed in with files",
    "text": "Variables passed in with files\nAs you might imagine, this rapidly gets unwieldy so there is also a way to populate a whole file with the variable:value pairs. You can either explicitly specify a filename to use for those variables, or there are certain filenames that will be automatically be loaded in and parsed.\n# passing a file name explicitly\nterraform apply -var-file=\"my_vals.tfvars\"\nA file called terraform.tfvars, or terraform.tfvars.json, or any file ending in .auto.tfvars or auto.tfvars.json will all be loaded automatically without the need to explicitly pass them in at the command line."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-using-environment-variables",
    "href": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-using-environment-variables",
    "title": "Terraform Input Variables",
    "section": "Variables passed in using environment variables",
    "text": "Variables passed in using environment variables\nSome situations require different approaches than populating a file or passing them in via the command line so terraform also checks any environment variable with the TF_VAR_ prefix and imports those automatically. For example, we could run:\nexport TF_VAR_some_variable=white_cat\nAnd the some_variable variable would be populated with the white_cat value."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#using-variables",
    "href": "posts/2023-06-22-input-variables-terraform.html#using-variables",
    "title": "Terraform Input Variables",
    "section": "Using variables",
    "text": "Using variables\nYou can use variables (to get their values) by using the var. prefix. For example, if I was defining an AWS S3 bucket and I wanted to use the value of the some_variable value for that bucket name, I could do something like this in my HCL definition:\nresource \"aws_s3_bucket\" \"my_first_bucket\" {\n  bucket        = var.some_variable\n}"
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#local-variables",
    "href": "posts/2023-06-22-input-variables-terraform.html#local-variables",
    "title": "Terraform Input Variables",
    "section": "Local variables",
    "text": "Local variables\nSometimes you want to define local variables that don’t need exposing as configurable parameters (as the input variables described above are). We have local variables for this, and you define them in their own block:\nlocals {\n  my_cat = \"aria\"\n}\nThe useful thing about local variables is that you can assign expressions as the values for these variables, or you can combine or otherwise transform things.\nThese local variables are accessible elsewhere with the local. prefix (e.g. in the above example, as local.my_cat)\nYou might want to use local variables where you have some particular expression that’s used multiple times in your file so you want to keep it DRY so you use a local variable. Similarly, if something is likely to be changed, you might want to make the change only in one place instead of multiple places. These are only available for use within the module where it is declared and can’t be overridden from the outside."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variable-precedence",
    "href": "posts/2023-06-22-input-variables-terraform.html#variable-precedence",
    "title": "Terraform Input Variables",
    "section": "Variable Precedence",
    "text": "Variable Precedence\nWith all these different ways to pass in values for variables, how do you know which one gets chosen? There are precedence rules that determine which value is used. Variables are loaded in this order, and the last one seen is the one that is used:\n\nenvironment variables\nthe terraform.tfvars file\nthe terraform.tfvars.json file\nany ….auto.tfvars (or the .json equivalent) files\nany -var and/or -var-file values set via the command line\n\nNothing about using these variables is particularly difficult to understand, but it’s useful to have a sense of those precedence rules nonetheless, especially if you have the same variable defined in multiple places."
  },
  {
    "objectID": "posts/2023-07-24-database-backups-tarsnap.html",
    "href": "posts/2023-07-24-database-backups-tarsnap.html",
    "title": "Automating database backups with Tarsnap",
    "section": "",
    "text": "Yesterday I wrote about my MathsPrompt tool which serves up questions to help me practice new skills I’m learning as part of my mathematics degree. Today I realised that all the data (both autogenerated and copy-pasted) is being stored in my database and that I hadn’t really given much thought yet to ensuring a long life for that data.\nI put together a shell script that backs up my data to Tarsnap, my preferred cloud backup service for things like this. It has the tagline “online backups for the truly paranoid” and while it perhaps isn’t the best choice for rich media like photos and videos, it’s really great for compressible text data. I use it on all my personal cloud machines to painlessly back up critical files.\nSo my backup script goes as follows:\n#!/bin/sh\npg_dump -U my_username -F t mathsprompt &gt; /tmp/mathsprompt_backup.tar || exit 1\n/usr/local/bin/tarsnap -c \\\n    -f \"$(uname -n)-$(date +%Y-%m-%d_%H-%M-%S)-mathsprompt-db\" \\\n    /tmp/mathsprompt_backup.tar \nrm /tmp/mathsprompt_backup.tar\nThen I set up a cronjob to handle running this script every day at 5pm. Adding that was as simple as adding the following line (via crontab -e) to my previously configured jobs:\n0 17 * * * path/to/tarsnap-db-backup.sh\nThere are fancier things I could do to backup this database, for instance configuring a streaming backup (a useful suggestion I learned about here), but that’d be overkill for my particular scenario, I think.\nTarsnap is also sufficiently cheap that I don’t need to worry about costs or using up too much space, but in the future I might consider doing some regular cleanup of really old database backups."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "",
    "text": "If you’re reading this blog, you’ve probably visited the Huggingface website and you’ve almost certainly tried out one of their ‘Spaces’. These are deployed mini-applications hosted on Huggingface infrastructure. I’ve created spaces of my own, and at work I added a way for people to quickly deploy a ZenML server as a ‘Space’. I love browsing all the spaces that exist and they’re really a testament to the creativity, smartness and elbow-grease contributed by the thriving open-source community that Huggingface facilitates through their platform.\nI’ve been working on my Terraform skills for a while and recently I thought up a little project that I hope will help deepen my skill-building as well as be useful for others. My goals were to build something that would:\nThis blogpost will describe my process and some of the things I learned along the way. A special thanks to Sean Kane at SuperOrbital for writing an extremely useful blogpost that guided me on this journey (alongside the official Hashicorp documentation). (TL;DR: check out the finished provider here!)"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#the-terraform-golden-path",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#the-terraform-golden-path",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "The Terraform Golden Path",
    "text": "The Terraform Golden Path\nWhat I wanted to be able to do was define my resource in something like the following manner:\nterraform {\n  required_providers {\n    huggingface-spaces = {\n      source = \"strickvl/huggingface-spaces\"\n    }\n  }\n}\n\nprovider \"huggingface-spaces\" {\n  apikey = var.huggingface_apikey\n}\n\nresource \"huggingface-spaces_space\" \"zenml_server\" {\n    name     = \"test-zenml-space\"\n    private  = false\n    template = zenml/zenml\n}\nThis really simple interface would allow people to spin up Huggingface spaces without needing to click through buttons on the Huggingface website. You can get a sense of how it works if you were to do it via that web interface here:\n\n\n\nWeb interface to deploy Huggingface Spaces as of November 2023"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-http-requests-to-deploy-spaces",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-http-requests-to-deploy-spaces",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Using HTTP Request(s) to Deploy Spaces",
    "text": "Using HTTP Request(s) to Deploy Spaces\nInternally, this web interface will just be using API calls (through some layer of abstraction) to make the deployment, so we can take a look at the Huggingface documentation and experiment a bit to find out how we can make such deployments using the raw underlying HTTP.\nIt’s important to have a sense of the HTTP API because this is what we’re going to ultimately be using when we write the Terraform provider. For the provider, we’ll have to use Go to define how that works so in the interests of clarity I’ll just show the HTTP requests on their own first.\nThere’s a useful Space by Enzo that allows you to play around with the Hub API, and the Huggingface docs cover the endpoints available for the Hub API.\nFor our purposes, we’re interested in the POST /api/repos/create endpoint. Note that we’re not making our query to the api/spaces endpoint, but rather to the repos endpoint. When you create a space, what you’re actually doing is creating a repository with certain custom features that Huggingface then knows to instantiate and deploy as a ‘Space’.\nSo as a simple HTTP request we can run the following in the terminal:\nhttp POST \"https://huggingface.co/api/repos/create\" \\\n  Authorization:\"Bearer YOUR_TOKEN_GOES_HERE\" \\\n  type=space \\\n  name=test-hf-api \\\n  private=false sdk=docker template=zenml/zenml\nYou’ll need to add in your authorisation token that you create on the Huggingface site and you can change the specific template you’re creating and the name as you see fit. Running this command will do the same as clicking through the various buttons on the web interface. (I’m using the httpie CLI tool to make the request, but you could just as well use something like curl if you prefer.)\nOnce we’ve confirmed that the basic creation of a Space based on the template works, we can try some other commands. If we wanted to rename the space, we can ‘move’ it. (Think how renaming folders in the UNIX terminal is also accomplished with the mv command.)\nhttp POST \"https://huggingface.co/api/repos/move\" \\\n  Authorization:\"Bearer YOUR_TOKEN_GOES_HERE\" \\\n  fromRepo=\"strickvl/test-hf-api\" toRepo=\"strickvl/my-renamed-space\" type=\"space\"\nAnd then to delete the space we can do the following:\nhttp DELETE \"https://huggingface.co/api/repos/delete\" \\\n  Authorization:\"Bearer YOUR_TOKEN_GOES_HERE\" \\\n  type=space \\\n  name=my-renamed-space \\\n  type=space\nSo the API allows us to interact with the Space without using the website as expected. So far so good!"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#terraform-provider-basics",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#terraform-provider-basics",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Terraform Provider Basics",
    "text": "Terraform Provider Basics\nAt the end I want this provider to be available on the Terraform Registry for others to use, so in that case the repository needs to be named in the format terraform-provider-THE_NAME_GOES_HERE. I’ve chosen terraform-provider-huggingface-spaces.\nTerraform released a framework for creating these Terraform providers which is a new (v2) way of doing this. It’s worth noting that it’s fairly new and many (most?) of the community providers you see on the Terraform registry are using the old way. There’s lots of support and even a repository you can use as a template scaffold for your own efforts. That’s what we’ll be using as well."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#implementing-the-provider",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#implementing-the-provider",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Implementing the Provider",
    "text": "Implementing the Provider\nIt’s all implemented in Go, and while the template / framework got me started, there was a fair amount of boilerplate code to be written. Between Claude, GPT-4 (when Claude cut me off for making too many queries) and a bit of elbow grease, I got it all implemented and working. You can view the finished provider up on the Terraform Registry here."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-the-provider",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-the-provider",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Using the provider",
    "text": "Using the provider\nSo now you can use code like this:\nterraform {\n  required_providers {\n    huggingface-spaces = {\n      source = \"strickvl/huggingface-spaces\"\n    }\n  }\n}\n\nprovider \"huggingface-spaces\" {\n  token = var.huggingface_token\n}\n\nvariable \"huggingface_token\" {\n  type        = string\n  description = \"The Hugging Face API token.\"\n  sensitive   = true\n}\n\nresource \"huggingface-spaces_space\" \"test_space\" {\n  name     = \"test-hf-api-${formatdate(\"YYYYMMDD\", timestamp())}\"\n  private  = true\n  sdk      = \"docker\"\n  template = \"zenml/zenml\"\n}\n\ndata \"huggingface-spaces_space\" \"test_space_data\" {\n  id = huggingface-spaces_space.test_space.id\n}\n\noutput \"test_space_id\" {\n  value = huggingface-spaces_space.test_space.id\n}\n\noutput \"test_space_name\" {\n  value = data.huggingface-spaces_space.test_space_data.name\n}\n\noutput \"test_space_author\" {\n  value = data.huggingface-spaces_space.test_space_data.author\n}\n\noutput \"test_space_last_modified\" {\n  value = data.huggingface-spaces_space.test_space_data.last_modified\n}\n\noutput \"test_space_likes\" {\n  value = data.huggingface-spaces_space.test_space_data.likes\n}\n\noutput \"test_space_private\" {\n  value = data.huggingface-spaces_space.test_space_data.private\n}\n\noutput \"test_space_sdk\" {\n  value = data.huggingface-spaces_space.test_space_data.sdk\n}\nAnd this will create a Hugging Face Space using the ZenML Docker template. You can even specific specific hardware that you want your space to be provisioned with, or state that you want persistent storage to be added, or add secrets and/or environment variables as part of the deployment. I would encourage you to visit and read the documentation for the provider for more information.\nUnfortunately at some point towards the end of my development I seem to have triggered the Hugging Face API’s rate limiter and now I’m in some purgatory where I can’t do anything on the Hugging Face Hub for the next 23+ hours. Not sure what that means for my ability to test this functionality in our CI, but I’ll be exploring this next."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-this-unlocks",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-this-unlocks",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "What this unlocks",
    "text": "What this unlocks\nI’m pretty excited that I finally got this working. This unlocks a few things for me.\nFirstly, I’m one of the lead engineers responsible for maintaining mlstacks, a Python package and library that allows you to quickly spin up MLOps infrastructure. We started off with Kubernetes-based implementations of all of the possible infrastructure combinations you might need, but it’s clear that for many use cases that’s too heavy a footprint. With this provider, we can easily add in the ability to spin up an Argilla annotation instance, for example, or any of the many other options.\n\nHugging Face technically allow you to create any kind of repository, so potentially this opens up a way to deploy any kind of Docker-backed image to the Hugging Face Hub. Given how simply and easy these are to deploy and configure, I’m personally excited by this idea and how it makes various parts of MLOps more approachable and accessible to others.\nI also think that this provider might be used by others. Technically you could use the Python SDK for the huggingface_hub, or perhaps raw HTTP requests using curl, but I find this interface useful and more maintainable. (I have a bit more work to do on the provider to add more robust testing and CI in that regard.) The newly-established mlinfra tool might also find this provider useful as part of its parallel attempt to allow for the deployment of MLOps infrastructure."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-i-learned",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-i-learned",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "What I learned",
    "text": "What I learned\nI’ve recently been reminded quite frequently that projects that seem daunting are often just a case of keeping moving forward, putting one step in front of another. I first thought of the idea for this provider a year ago, yet it only took me a weekend to implement the functionality that I’ve now released.\nI found it quite hard to understand the precise endpoints of the Hugging Face API that were available. Some documentation can be found, but it’s split over various places and still is incomplete. The actual source code for the Hub’s API is closed-source so it’s not even possible just to go to the code implementation. Once I’m done with the testing for the provider I’ll maybe see if I can go back and make a PR to improve the Hugging Face documentation.\nWorking with Go code is pretty pleasant. There were some new parts of Go I hadn’t previously encountered, notably modules. Using the new Terraform provider plugin template / system means you have to use modules, but it wasn’t too hard to figure out. Go is certainly easier to grok as a non-expert user than Rust.\nA final ‘as always’ lesson was that my work was really helped by taking some time away from code to think through exactly how the provider would work, what pieces of information would be needed where and so on. I don’t always remember this lesson — it’s really easy to just start typing on a keyboard — but I appreciated having a game plan and some notes to keep me on track when I had choices to make.\nSo give the provider a try and let me know if you found it useful! (Also let me know if you’d like some other functionality adding and I’ll see what I can do!)"
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "",
    "text": "I previously experimented with one-click LLM finetuning providers and now is a good time to return to the core of the matter: evaluating how well all these fine-tuned models and experiments are faring. I have a gut feeling that my fine-tuned models did pretty well, but we’re not in the business of gut feeling so I’m hoping to be able to put some real numbers down to either prove or disprove this hypothesis.\nAs a quick reminder if you didn’t read any of the previous posts in the series, I’m building a model that can take a press release text like this:\n…and then turn it into structured data (i.e. a JSON object) like this:\nI’ve now fine-tuned several models and I want to get a sense of how good these are. I showcased some initial baseline evaluations using OpenAI’s gpt-4-turbo but I want to pit model against model now.\nI’m also interested in teasing out some of the edge cases where I know I struggled as a human annotator. (I released the dataset for this project publicly on the Hugging Face Hub and also was responsible for annotating every single item so I know the data intimately.) I can even consider using the hard examples to generate some synthetic data to boost performance on those edge cases, but that’s a task for much later on.\nThis blogpost is a prose overview of some of the evaluations I’m adding to my suite of tests (and why I’m adding them). I learned a lot from Hamel Husain’s “Your AI Product Needs Evals” blogpost and if you’re interested in this I’d recommend reading it and then actually implementing his suggestions."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#core-evaluations-for-accuracy",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#core-evaluations-for-accuracy",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Core evaluations for accuracy",
    "text": "Core evaluations for accuracy\nThe most important measurement to start with is just a pure “did the LLM make a correct prediction or not?” If I was doing all these evaluations manually myself, I’d take a look at the example above, for example, and ask myself “was the start date of the event mentioned in the blogpost really ‘2011-11-07’ as predicted by the model?” and “did the event take place in Badakhshan province?” and “were the Haqqanis the group targeted?”\nIt’s fairly straightforward to make these determinations when comparing each property one by one. I can then repeat this over every example in my test slice of my dataset and take an average if I want a single aggregate figure, or I can get individual figures for dates, provinces, target groups and so on (to know if maybe there’s one part of the prediction it struggles with most)."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#out-of-domain-data",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#out-of-domain-data",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Out of domain data",
    "text": "Out of domain data\nThe ISAF mission has come to an end, so I don’t have to worry too much about new data and having to adapt to a continuously changing world, but it is possible that some smaller groups weren’t well represented in the training data (for predicting the target group, for example) so I want to know how well my model does with data it hasn’t seen.\nMy prompt passes in the schema for the data and I encourage it to follow the schema in its response, but if there’s a new group will it add the new group to the schema? I can write an evaluation to test this.\nAnother edge case is the possibility that a press release doesn’t follow the standard format. Having read them all, I know that the vast majority are pretty formulaic, but sometimes there is a special event or incident which caused the author of the press release to depart from the standard formula. I want to know that my model will:\n\nnot just make something up so as to have some kind of JSON response even if the press release is about someone’s birthday party\neven better, produce some sort of error code or blank response when this happens.\n\nI can use examples of this out of domain data to see what happens, and put a value to how often my model will just hallucinate something out of nothing. This will be important since the name of the game for this model is accuracy."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#gradations-of-some-a-few-many",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#gradations-of-some-a-few-many",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Gradations of ‘some’, ‘a few’, ‘many’",
    "text": "Gradations of ‘some’, ‘a few’, ‘many’\nThe press releases try to give some information without actually giving too much. Indeed, when I published my report on the press releases back in 2011, ISAF even issued a press release (!) in which they stated that:\n\n“Release of information in insurgent warfare is not always made public, so studies based on the use of press releases can be both incomplete and problematic. […] Authoritative research cannot be conducted through mere analysis of press releases, since the release of information through such releases is, by design, incomplete.”\n\nSo reading the press releases is very much an exercise in reading between the lines. In the press release cited earlier, all the numbers are specific (“a facilitator”, “a male”, “two insurgents”) so it’s easy to put numbers to how many were killed or captured. In many of the press releases, particularly during times where raids were being conducted at a very high tempo, you have to just take assumptions about what their words mean and assign minimum values to those words.\nSo ‘a couple’ meant at least two, but ‘a few’ meant 3 or more. Similarly ‘dozens’ means multiple dozens so that meant a minimum value of at least 24. From the original report:\n\n“If a press release said that ‘insurgents’ were detained, without further details, we assigned that incident as having a minimum number of 2 detained (since we could not be sure of more). ‘A couple’ we took to mean 2. ‘Several’ we took to mean at least 3, even though on other occasions ‘several’ was used to refer to 7 or 8. Other terms we classified as denoting at least 3 included: ‘a few’, ‘some’, ‘a group’, ‘a small group’ and ‘multiple’; these terms sometimes were used to refer to far larger numbers but we chose the smaller number (if no other information was available in the press release) in order to come up with a minimally acceptable figure. ‘Numerous’ and ‘a handful’ we took to mean at least 4, and ‘a large number’ at least 5.”\n\nThe reports mostly referred to events that had taken place that day or the day before, but occasionally they’d refer to events that took place “last Thursday” or “last week” and so then you’d have to know what day the press release was issued and then make calculations accordingly. For this backwards-referring time assignations, I’m particularly interested (read: concerned!) to know how well my LLMs did. Whatever score we get, it’s probably fixable with a bit of manual parsing and logic, but we need to know if there’s a problem or not first.\nGenerally speaking there were province names assigned to incidents (all but 23, to be precise) but when they weren’t, then the LLM has to work back from a village name, potentially, or just specify that an incident took place in southern Afghanistan or Afghanistan as a whole. On a few occasions, the press release actually made an error, stating that village X or Y was in a particular province, when this was incorrect and it was in a different province. So for this, would we expect the LLM to assign the event to the correct province for that village, or just retain the error in the press release?\nSometimes a press release might refer to an event having taken place “in the provincial capital of X province” but without mentioning that city by name. So the LLM will have to have some knowledge of these things and I want to test how well it performs with this.\nThese might seem like tiny errors to get wrong, but for a project like this (where my report was making some strong accusations and drawing certain conclusions based on the data), it wouldn’t do to get things factually wrong. For an internal-facing LLM-powered chatbot, the price of mistakes is minimal, but for a project like this, there are potentially far more serious consequences which is why I’m putting together such detailed evaluations."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#spelling-variation",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#spelling-variation",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Spelling variation",
    "text": "Spelling variation\nAnother issue with some of the press releases is that they use a variety of spellings for the same locations or names of individuals. For some things — province names, for example — it makes sense to standardise on a fixed naming convention but for others it’s not always clear what to do. So our evaluation should ensure that common variations of certain provinces or designations or names are captured correctly by the LLM output."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#complex-stories",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#complex-stories",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Complex stories",
    "text": "Complex stories\nSome stories are very complicated and there may be no correct way to assign numbers, for example, to the text that was published. In those cases when annotating I often just left the minimum numbers at zero even though we know that something happened. Would the LLM also make the same call? What is the threshold for deciding not to take a chance on making a guess?"
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#next-step",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#next-step",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Next step",
    "text": "Next step\nThe obvious next step is to actually code up these evaluation criteria and run those across our fine-tuned LLMs as well as the API-driven proprietary ones. I’ll be working on that over the coming days. Luckily, I did most of the work to identify all of the above when I first wrote the report so there isn’t much ground that needs to be broken so much as just sitting down and getting it done."
  },
  {
    "objectID": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html",
    "href": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html",
    "title": "Starting to read Prompt Engineering for LLMs",
    "section": "",
    "text": "I’m posting some of my summary notes while reading through John Berryman and Albert Ziegler’s “Prompt Engineering for LLMs”. What follows are my notes from the first two chapters. It was a bit too long for a post to LinkedIn so I’m posting my notes in full here."
  },
  {
    "objectID": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-1-introduction-to-prompt-engineering",
    "href": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-1-introduction-to-prompt-engineering",
    "title": "Starting to read Prompt Engineering for LLMs",
    "section": "Chapter 1: Introduction to Prompt Engineering",
    "text": "Chapter 1: Introduction to Prompt Engineering\nThe opening chapter frames prompt engineering as a comprehensive discipline that extends far beyond just crafting individual prompts. It positions prompt engineering as an integral part of the entire lifecycle of LLM-based applications.\n\nKey Points\n\nThe field of language modeling has seen exponential growth, as evidenced by the GPT series progression from 2018 to 2022:\n\nGPT-1 (2018): 117M parameters\nGPT-2 (2019): 1.5B parameters\nGPT-3 (2020): 175B parameters\nSubsequent models showing continued scaling\n\nPrompt engineering encompasses:\n\nThe structural design of prompts themselves\nStrategic thinking about prompt implementation throughout the application lifecycle\nIntegration of prompts into larger systems and workflows\n\nHistorical Context:\n\nThe chapter provides background on language modeling evolution\nPlaces modern LLMs in the broader context of NLP development\n\n\nThis introductory framework suggests that effective prompt engineering requires both technical skill in prompt construction and strategic understanding of how prompts function within larger systems and applications."
  },
  {
    "objectID": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-2-understanding-llms",
    "href": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-2-understanding-llms",
    "title": "Starting to read Prompt Engineering for LLMs",
    "section": "Chapter 2: Understanding LLMs",
    "text": "Chapter 2: Understanding LLMs\nChapter two tries to peel back the layers of how LLMs produce their output. If you can understand how they work (at least a bit more than ‘it’s magic’), you can better guide them to produce outputs that are valuable for you.\nA very hard chapter to write, I imagine. It is almost certainly a bit too technical for someone ‘non-technical’, but a more experienced user might find some of the analogies too simplistic. I thought the balance was well handled but I probably wouldn’t recommend this to just anyone..\nSome key insights: expect LLMs to respond in a similar way to the training data that went into creating them. (Unfortunately, many model providers are pretty tight-lipped as to the specific composition of that training data, though you can make some guesses…)\n\n“The better you know the training data, the better the intuition you can form about the likely output of an LLM trained on that training data.”\n\nWe then get into a section on tokenization and what that means for how LLMs ‘see’ the world and why this results in certain weaknesses. Most importantly, just remember that LLMs don’t process and interact with text in the same way that humans do. Easy to forget when you’re interacting through a chat interface, but important nonetheless.\n\nI liked this example about capitalization and how tokenization made it hard for the earlier generations of models to do something as ‘simple’ as turning words into upper-case versions.\nEven though this isn’t a problem with more recent models, it reminds you to be cognisant of how much extra work you’re having your model do. The more you can remove extra work, the better responses you’ll get. If you try to have your model do too many things at the same time, you’ll have poorer results.\nThe section on LLMs as auto-regressive models was excellent, though, again, probably not the easiest read for a non-technical reader. Key: LLMs move forward through their text as they ‘read’ the contents. They cannot backtrack, they cannot take things back that they write. They just have to keep moving forward.\nThis can lead to repetitions, getting lost in certain patterns and behaviours. One solution to this: filtering out after the response is given. Another option: playing with temperature and randomness.\nI loved this section on temperatures and how to think about which to choose. Very practical, even amidst a chapter targeted at helping you understand why LLMs behave the way they do.\n\nAlso a useful insight that errors often compound when it comes to temperatures greater than 1. I hadn’t realised that before.\nAfter tokens and temperature we move on to transformers! I found the explanation worked, though the really technical again are bound to be disappointed and the non-technical might find it a bit too hand-wavy. YMMV. Overall enough information was given to understand the key insight around attention:\n\n“Information flows from left to right. Information flows from bottom to top.”\n\nAfter we understand this, we can also understand how processing (‘reading’) text happens much faster than the output: it’s ~ an order of magnitude slower to to output than it is to read the input, even with caching and parallelism in the computation.\nSo the order of the contents of the prompt matters a lot, as does the formulation and the extent to which you make the LLM work hard on the problem at hand or other extraneous tasks.\nA nice illustrative summation:\n\n“Could a human expert who knows all the relevant general knowledge by heart complete the prompt in a single go without backtracking, editing or note-taking?” (if not, then you might find the LLM will struggle with the task or completion)\n\nSo to sum up:\n\nLLMs are completion engines\nLLMs mimic their training data\nLLMs produce one token at a time and can’t backtrack\nLLMs read through the text a single time, from beginning to end\n\nSimple-seeming insights, but ones with large consequences. Tomorrow we move beyond the static models and on to RLHF, the chat models and the differences that come with using the API."
  },
  {
    "objectID": "posts/2025-01-13-assembling-the-prompt-notes-on-prompt-engineering-for-llms-ch-6.html",
    "href": "posts/2025-01-13-assembling-the-prompt-notes-on-prompt-engineering-for-llms-ch-6.html",
    "title": "Assembling the Prompt: Notes on ‘Prompt Engineering for LLMs’ ch 6",
    "section": "",
    "text": "Chapter 6 of “Prompt Engineering for LLMs” is devoted to how to structure the prompt and compose its various elements. We first learn about the different kinds of ‘documents’ that we can mimic with our prompts, then think about how to pick which pieces of context to include, and then think through how we might compose all of this together.\n\nThere’s a great figure to give you an idea of ‘the anatomy of a well-constructed prompt’ early on. The introduction is where you introduce the task, then you have the ‘valley of meh’ (which the LLM can struggle to recall or obey) and finally you have the refocusing and restatement of the task.\nThere are two key tips at this point:\n\nthe closer a piece of information is to the end of the prompt, the more impact it has on the model\nthe model often struggles with the information stuffed in the middle of the prompt\n\nSo craft your prompts accordingly!\nA prompt plus the resulting completion is defined as a ‘document’ in this book, and there are various templates that you can follow: an ‘advice conversation’, an ‘analytic report’ (often formatted with Markdown headers), and a ‘structured document’.\nWe learn that analytic report-type documents seem to offer a lighter ‘cognitive load’ for an LLM since it doesn’t have to handle the intricacies of social interaction that it would in the case of an advice conversation. 🤔\nTwo other tips or possible things to include in the analytic report-style document:\n\na table of contents at the beginning to set the scene\na scratchpad or notebook section for the model to ‘think’ in\n\nI haven’t had much use of either of these myself but I can see why they’d be powerful.\nStructured documents can be really powerful, especially when the model has been trained to expect certain kinds of structure (be it JSON or XML or YAML etc). Also TIL that apparently OpenAI’s models are very strong when dealing with JSON as inputs.\nThe context to be inserted into the prompt (usually dynamically depending on use case or needs) can be large or small depending on what is available in terms of context window or latency requirements. There are different strategies to how to select what goes in.\nI was curious about the idea of what they call ‘elastic snippets’, i.e. dynamic decisions that get taken as to what makes it way into the prompt depending on how much space is available etc.\nAnd even then you have to decide about the:\n\nposition (which order do all the elements appear in the prompt)\nimportance (how much will dropping this element from the prompt effect the response)\ndependency (if you include one element, can you drop another and vice versa…)\n\nIn the end, you have a kind of optimisation problem: given a theoretical unlimited potential prompt length, how to combine all the elements together to get the most value given the space limitations that the LLM dictates.\n\nAnd then what strategy do you use to get rid of elements that your prompt budget cannot afford; we learn about the ‘additive greedy approach’ and the ‘subtractive greedy approach’, all the while bearing in mind that these are all just basic prototypes to play around with.\n\nThe next chapter is all about the completion and how to make sure we receive meaningful and accurate responses from our LLM!"
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "",
    "text": "Had the first of a series of meet-ups I’m organising in which we discuss Chip Huyen’s new book. My notes from reading the chapter follow this, and then I’ll try to summarise what we discussed in the group.\nAt a high-level, I really enjoyed the final part of the chapter where she got into how she was thinking about the practice of ‘AI Engineering’ and how it differs from ML engineering. Also the use of the term ‘model adaptation’ was an interesting way of encompassing all the different things that engineers are doing to get the LLM to better follow their instructions."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#chapter-1-notes",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#chapter-1-notes",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Chapter 1 Notes",
    "text": "Chapter 1 Notes\nThe chapter begins by establishing AI Engineering as the preferred term over alternatives like GenAI Ops or LLM Ops. This preference stems from a fundamental shift in the field, where application development has become increasingly central to working with AI models. The “ops” suffix inadequately captures the breadth and nature of work involved in modern AI applications.\n\nFoundation Models and Language Models\nThe text provides important technical context about different types of language models. A notable comparison shows that while Mistral 7B has a vocabulary of 32,000 tokens, GPT-4 possesses a much larger vocabulary of 100,256 tokens, highlighting the significant variation in model capabilities and design choices.\nTwo primary categories of language models are discussed:\n\nMasked Language Models (like BERT and modern BERT variants)\nAutoregressive Language Models (like those used in ChatGPT)\n\nThe term “foundation model” carries dual significance, referring both to these models’ fundamental importance and their adaptability for various applications. This terminology also marks an important transition from task-specific models to general-purpose ones, especially relevant in the era of multimodal capabilities."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#ai-engineering-vs-traditional-approaches",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#ai-engineering-vs-traditional-approaches",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "AI Engineering vs Traditional Approaches",
    "text": "AI Engineering vs Traditional Approaches\nAI Engineering differs substantially from ML Engineering, warranting its distinct terminology. The key distinction lies in its focus on adapting and evaluating models rather than building them from scratch. Model adaptation techniques fall into two main categories:\n\nPrompt-based techniques (prompt engineering) - These methods adapt models without updating weights\nFine-tuning techniques - These approaches require weight updates\n\nThe shift from ML Engineering to AI Engineering brings new challenges, particularly in handling open-ended outputs. While this flexibility enables a broader range of applications, it also introduces significant complexity in evaluation and implementation of guardrails."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#the-ai-engineering-stack",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#the-ai-engineering-stack",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "The AI Engineering Stack",
    "text": "The AI Engineering Stack\nThe framework consists of three distinct layers:\n\n1. Application Development Layer\n\nFocuses on prompt crafting and context provision\nRequires rigorous evaluation methods\nEmphasizes interface design and user experience\nPrimary responsibilities include evaluation, prompt engineering, and AI interface development\n\n\n\n2. Model Development Layer\n\nProvides tooling for model development\nIncludes frameworks for training, functioning, and inference optimisation\nRequires systematic evaluation approaches\n\n\n\n3. Infrastructure Layer\n\nHandles model serving\nManages underlying technical requirements"
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#planning-ai-applications",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#planning-ai-applications",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Planning AI Applications",
    "text": "Planning AI Applications\nThe chapter outlines a modern approach to AI application development that differs significantly from traditional ML projects. Rather than beginning with data collection and model training, AI engineering often starts with product development, leveraging existing models. This approach allows teams to validate product concepts before investing heavily in data and model development.\nKey planning considerations include:\n\nSetting appropriate expectations\nDetermining user exposure levels\nDeciding between internal and external deployment\nUnderstanding maintenance requirements\n\nA notable insight is the “80/20” development pattern: while reaching 80% functionality can be relatively quick, achieving the final 20% often requires equal or greater effort than the initial development phase."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#evaluation-and-implementation-challenges",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#evaluation-and-implementation-challenges",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Evaluation and Implementation Challenges",
    "text": "Evaluation and Implementation Challenges\nThe chapter emphasises that working with AI models presents unique evaluation challenges compared to traditional ML systems. This complexity stems from:\n\nThe open-ended nature of outputs\nDifficulty in implementing strict guardrails\nChallenges in type enforcement\nThe need for comprehensive evaluation strategies"
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#data-and-model-adaptation",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#data-and-model-adaptation",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Data and Model Adaptation",
    "text": "Data and Model Adaptation\nThe text discusses how data set engineering and inference optimisation, while still relevant, take on different forms in AI engineering compared to traditional ML engineering. The focus shifts from raw data collection and processing to effective model adaptation and deployment strategies."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#modern-development-paradigm",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#modern-development-paradigm",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Modern Development Paradigm",
    "text": "Modern Development Paradigm\nA significant paradigm shift is highlighted in the development approach: unlike traditional ML engineering, which typically begins with data collection and model training, AI engineering enables a product-first approach. This allows teams to validate concepts using existing models before committing to extensive data collection or model development efforts."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#discussion-summary",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#discussion-summary",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Discussion summary",
    "text": "Discussion summary\nThe conversation started with a bit on how AI Engineering represents an interesting shift in the software engineering landscape, potentially opening new career paths for traditional software engineers. While developers may not need deep mathematical knowledge of derivatives and linear algebra upfront, there’s a growing recognition that understanding how AI systems behave - their constraints and opportunities - is becoming increasingly valuable.\nA key tension emerged in the discussion around enterprise adoption. While there’s significant enthusiasm around AI applications, particularly on social media where developers showcase apps with substantial user bases, enterprise companies often maintain their traditional team structures. This creates an interesting dynamic where companies might maintain their existing ML engineering teams while simultaneously forming new “tiger teams” focused on generative AI initiatives, leading to organisational friction.\nThe group discussed how while it’s now possible for software engineers to quickly build AI applications by calling APIs, they often hit limitations that require deeper understanding. This raises questions about whether the “shallow” approach of purely application-level development is sustainable, or whether engineers will inevitably need to develop deeper technical knowledge around model behaviour, evaluation, and fine-tuning.\nA particularly notable challenge discussed was handling the non-deterministic nature of AI systems. Traditional software engineering practices, like unit testing, don’t translate cleanly to systems where outputs can vary even with temperature set to zero. This highlights how AI Engineering requires new patterns and practices beyond traditional software engineering approaches.\nThe discussion also touched on evaluation techniques, including the use of log probabilities to understand model confidence and improve prompts. This represents an emerging area where traditional ML evaluation meets new challenges in assessing large language model outputs."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "",
    "text": "This chapter represents a crucial bridge between academic research and production engineering practice in AI system evaluation. What sets it apart is the Chip’s very balanced perspective - neither succumbing to the prevalent hype in the field nor becoming overly academic. Instead, she melds together practical insights with theoretical foundations, creating a useful framework for evaluation that acknowledges both technical and ethical considerations."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#introduction-and-context",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#introduction-and-context",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Introduction and Context",
    "text": "Introduction and Context\n\nKey Insight: The author’s approach demonstrates that effective AI system evaluation requires a synthesis of academic rigour and practical engineering concerns, much like how traditional software engineering evolved to balance theoretical computer science with practical development methodologies.\n\nThe chapter is structured in three main parts, each building upon the previous to create a complete picture of AI system evaluation:\n\nEvaluation criteria fundamentals\nModel selection and benchmark navigation\nPractical pipeline implementation"
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-1-evaluation-criteria---a-deep-dive",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-1-evaluation-criteria---a-deep-dive",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Part 1: Evaluation Criteria - A Deep Dive",
    "text": "Part 1: Evaluation Criteria - A Deep Dive\n\nThe Evolution of Evaluation-Driven Development\nThe author introduces evaluation-driven development (EDD), a methodological evolution that adapts the principles of test-driven development to the unique challenges of AI systems.\n\nEvaluation-Driven Development: A methodology where AI application development begins with explicit evaluation criteria, similar to how test-driven development starts with test cases. However, EDD encompasses a broader range of metrics and considerations specific to AI systems.\n\nThe fundamental principle here is that AI applications require a more nuanced and multifaceted approach to evaluation than traditional software. Where traditional software might have binary pass/fail criteria, AI systems often operate in a spectrum of performance across multiple dimensions.\n\n\nThe Four Pillars of Evaluation\n\n1. Domain-Specific Capability\nThe author presents domain-specific capability evaluation as the foundational layer of AI system assessment. This approach is particularly innovative in its use of multiple choice evaluation techniques - a method that bridges the gap between human-interpretable results and machine performance metrics.\nFor example, when evaluating code generation capabilities, presenting a model with multiple implementations where only one is functionally correct serves as both a test and a teaching tool. This mimics how human experts often evaluate junior developers’ understanding of coding patterns and best practices.\n\n\n2. Generation Capability\nThe section on generation capability draws parallels with the historical development of Natural Language Generation (NLG) in computational linguistics. This historical context provides valuable insights into how we can approach modern language model evaluation.\nThe author breaks down factual consistency into two crucial dimensions:\n\nLocal Factual Consistency: The internal coherence of generated content and its alignment with the immediate context of the prompt. This is analogous to maintaining logical consistency within a single conversation or document.\nGlobal Factual Consistency: The accuracy of generated content when compared against established knowledge and facts. This represents the model’s ability to maintain truthfulness in a broader context.\n\nThe discussion of hallucination detection is particularly noteworthy, presenting three complementary approaches:\n\nBasic Prompting: Direct detection through carefully crafted prompts\nSelf-Verification: A novel approach using internal consistency checks across multiple generations\nKnowledge-Augmented Verification: Advanced techniques like Google DeepMind’s SAFE paper (search augmented factuality evaluator)\n\nThe knowledge-augmented verification system represents a fascinating approach to fact-checking that mirrors how human experts verify information:\n\nIt breaks down complex statements into atomic claims\nEach claim is independently verified through search\nThe results are synthesised into a final accuracy assessment\n\nSeems pricey, though :)\n\n\n3. Instruction Following Capability\nThe author makes a crucial observation about the bidirectional nature of instruction following evaluation. Poor performance might indicate either model limitations or instruction ambiguity - a distinction that’s often overlooked in practice.\n\nInstruction-Performance Paradox: The quality of instruction following cannot be evaluated in isolation from the quality of the instructions themselves, creating a circular dependency that must be carefully managed in evaluation design.\n\nThe solution proposed is the development of custom benchmarks that specifically target your application’s requirements. This approach ensures that your evaluation criteria align perfectly with your practical needs rather than relying solely on generic benchmarks.\n\n\n4. Cost and Latency Considerations\nThe author introduces the concept of Pareto optimization in the context of AI system evaluation, demonstrating how different performance metrics often involve trade-offs that must be carefully balanced.\n\nPareto Optimization: A multi-objective optimization approach where improvements in one metric cannot be achieved without degrading another, leading to a set of optimal trade-off solutions rather than a single optimal point."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-2-model-selection---a-strategic-approach",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-2-model-selection---a-strategic-approach",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Part 2: Model Selection - A Strategic Approach",
    "text": "Part 2: Model Selection - A Strategic Approach\n\nThe Four-Step Evaluation Workflow\nThe author presents a sophisticated workflow that combines both quantitative and qualitative factors in model selection. This approach is particularly valuable because it acknowledges the complexity of real-world deployment while providing a structured path forward.\n\nInitial Filtering The first step involves filtering based on hard constraints, which might include:\n\nDeployment requirements (on-premise vs. cloud)\nSecurity and privacy considerations\nLicensing restrictions\nResource constraints\n\nPublic Information Assessment This stage involves a systematic review of:\n\nBenchmark performances across relevant tasks\nLeaderboard rankings with context\nPublished latency and cost metrics\n\nThe author emphasises the importance of looking beyond raw numbers to understand the context and limitations of public benchmarks.\nExperimental Evaluation This phase involves hands-on testing with your specific use case, considering:\n\nCustom evaluation metrics\nIntegration requirements\nReal-world performance characteristics\n\nContinuous Monitoring The final step acknowledges that evaluation is an ongoing process, not a one-time event. This involves:\n\nRegular performance monitoring\nFailure detection and analysis\nFeedback collection and incorporation\nContinuous improvement cycles\n\n\n\n\nThe Build vs. Buy Decision Matrix\nThe author provides an analysis of the build vs. buy decision, going beyond simple cost comparisons to consider factors like:\n\nTotal Cost of Ownership (TCO): The complete cost picture including: - Direct costs (API fees, computing resources) - Indirect costs (engineering time, maintenance) - Opportunity costs (time to market, feature development) - Risk costs (security, reliability, vendor lock-in)\n\nThis section particularly shines in its discussion of the often-overlooked aspects of model deployment, such as the hidden costs of maintaining self-hosted models and the true value of vendor-provided updates and improvements."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-3-building-evaluation-pipelines---practical-implementation",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-3-building-evaluation-pipelines---practical-implementation",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Part 3: Building Evaluation Pipelines - Practical Implementation",
    "text": "Part 3: Building Evaluation Pipelines - Practical Implementation\n\nSystem Component Evaluation\nThe author advocates for a dual-track evaluation approach:\n\nEnd-to-end system evaluation\nComponent-level assessment\n\nThis approach allows organisations to:\n\nIdentify bottlenecks and failure points\nUnderstand component interactions\nMake targeted improvements\nMaintain system reliability during updates\n\n\n\nCreating Effective Evaluation Guidelines\nThe author emphasises the importance of creating clear, actionable evaluation guidelines that bridge technical and business metrics. This section introduces the concept of metric alignment - ensuring that technical evaluation metrics directly correspond to business value.\n\nMetric Alignment: The process of mapping technical performance metrics to business outcomes, creating a clear connection between model improvements and business value.\n\n\n\nData Management and Sampling\nChip provides valuable insights into data management for evaluation, including:\n\nData Slicing: The strategic separation of evaluation data into meaningful subsets to: - Identify performance variations across different use cases - Detect potential biases - Enable targeted improvement efforts - Avoid Simpson’s paradox in performance analysis\n\nThe discussion of sample size is particularly practical, providing concrete guidelines based on statistical confidence levels and desired detection thresholds. The author cites OpenAI’s research suggesting that sample sizes between 100 and 1,000 are typically sufficient for most evaluation needs, depending on the required confidence level.\n\n\n\nMeta-Evaluation: Evaluating Your Evaluation\nThe chapter concludes with a crucial discussion of meta-evaluation - the process of assessing and improving your evaluation pipeline itself. This includes considerations of:\n\nSignal quality and reliability\nMetric correlation and redundancy\nResource utilisation and efficiency\nIntegration with development workflows"
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#conclusion",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#conclusion",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Conclusion",
    "text": "Conclusion\nThe author concludes around the inherent limitations of AI system evaluation: no single metric or method can fully capture the complexity of these systems. However, this acknowledgment leads to a constructive approach: combining multiple evaluation methods, maintaining awareness of their limitations, and continuously iterating based on real-world feedback.\nThis chapter ultimately provides a solid framework for AI system evaluation that is both theoretically sound and practically applicable. It serves as a valuable resource for organisations working to implement effective evaluation strategies for their AI systems, while maintaining a clear-eyed view of both the possibilities and limitations of current evaluation methods."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "",
    "text": "I enjoyed chapter 7 on finetuning. It jams a lot of detail into the 50 pages she takes to explain things. Some areas had more detail than you’d expect, and others less, but overall this was a solid summary / review.\nThe chapter’s essential message can be distilled into three key points:\nSo fine-tuning can be incredibly powerful when applied judiciously, but it requires careful consideration of both technical capabilities and organisational readiness."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#chapter-overview-and-context",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#chapter-overview-and-context",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Chapter Overview and Context",
    "text": "Chapter Overview and Context\nThis long chapter (approximately 50 pages, much like the others) was notably one of the most challenging for Chip to write. It presents fine-tuning as an advanced approach that moves beyond basic prompt engineering, covering everything from fundamental concepts to practical implementation strategies.\nThe depth and breadth of the chapter reflect the complexity of fine-tuning as both a technical and organisational challenge, though the things she writes about doesn’t really cover the reality of what it’s like to work on these kinds of initiatives within a team."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#core-decision-when-to-fine-tune",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#core-decision-when-to-fine-tune",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Core Decision: When to Fine-tune",
    "text": "Core Decision: When to Fine-tune\nThe decision to fine-tune should never be taken lightly. While the potential benefits are significant, including improved model quality and task-specific capabilities, the chapter emphasises that fine-tuning should be considered a last resort rather than a default approach.\n\nNotable Case Study: Grammarly achieved remarkable results with their fine-tuned T5 models, which outperformed GPT-3 variants despite being 60 times smaller. This example illustrates how targeted fine-tuning can sometimes achieve better results than using larger, more general models.\n\n\nReasons to Avoid Fine-tuning\nThe chapter presents several compelling reasons why organisations might want to exhaust other options before pursuing fine-tuning:\n\nPerformance Degradation: Fine-tuning can actually degrade model performance on tasks outside the specific target domain\nEngineering Complexity: The process introduces significant technical overhead\nSpecialised Knowledge Requirements: Teams need expertise in model training\nInfrastructure Demands: Self-serving infrastructure becomes necessary\nOngoing Maintenance: Requires dedicated policies and budgets for monitoring and updates\n\n\n\nFine-tuning vs. RAG: A Critical Distinction\nOne of the most important conceptual frameworks presented is the distinction between fine-tuning and RAG:\n\nFine-tuning focuses on form - how the model expresses information\nRAG specialises in facts - what information the model can access and use\n\nThis separation provides a clear decision framework, though the chapter acknowledges there are exceptions to this general rule."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#progressive-implementation-workflow",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#progressive-implementation-workflow",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Progressive Implementation Workflow",
    "text": "Progressive Implementation Workflow\n\nThe chapter outlines a thoughtful progression of implementation strategies, suggesting organisations should:\n\nBegin with prompt engineering optimisation\nExpand to include more examples (up to approximately 50)\nImplement dynamic data source connections through RAG\nConsider advanced RAG methodologies\nExplore fine-tuning only after exhausting other options\nConsider task decomposition if still unsuccessful"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#memory-bottlenecks-and-technical-considerations",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#memory-bottlenecks-and-technical-considerations",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Memory Bottlenecks and Technical Considerations",
    "text": "Memory Bottlenecks and Technical Considerations\n\nCritical Memory Factors\nThe chapter emphasises three key contributors to a model’s memory footprint during fine-tuning:\n\nParameter count\nTrainable parameter count\nNumeric representations\n\n\nTechnical Note: The relationship between trainable parameters and memory requirements becomes a key motivator for PEFT (Parameter Efficient Fine Tuning) approaches.\n\n\n\nQuantisation Strategies\nThe chapter provides a detailed examination of quantisation approaches, particularly noting the distinction between:\n\nPost-Training Quantisation (PTQ)\n\nMost common approach\nParticularly relevant for AI application developers\nSupported by major frameworks with minimal code requirements\n\nTraining Quantisation\n\nEmerging approach gaining traction\nAims to optimise both inference performance and training costs"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#advanced-fine-tuning-techniques",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#advanced-fine-tuning-techniques",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Advanced Fine-tuning Techniques",
    "text": "Advanced Fine-tuning Techniques\n\nPEFT Methodologies\nThe chapter identifies two primary PEFT approaches:\n\nAdapter-based methods (Additive):\n\nLoRA emerges as the most popular implementation\nIncludes variants like Dora and qDora from Anthropic\nInvolves adding new modules to existing model weights\n\nSoft prompt-based methods:\n\nLess common but growing in popularity\nIntroduces trainable tokens for input processing modification\nOffers a middle ground between full fine-tuning and basic prompting, so maybe interesting for teams who don’t really want to go too deep into finetuning (?)\n\n\n\n\nModel Merging and Multitask Considerations\nThe chapter presents model merging as an evolving science, requiring significant expertise. Three primary approaches are discussed:\n\nSumming\nLayer stacking\nConcatenation (generally not recommended due to memory implications)\n\n\nThere’s a lot of detail in this section (much more than I’d expected) but it was interesting to read about something that I haven’t much practical expertise with."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#core-approaches-to-model-merging",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#core-approaches-to-model-merging",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Core Approaches to Model Merging",
    "text": "Core Approaches to Model Merging\nThe chapter outlines three fundamental approaches to model merging, each with its own technical considerations and trade-offs:\n\nTechnical Architecture: The three primary merging strategies\n\nSumming: Direct weight combination\nLayer stacking: Vertical integration of model components\nConcatenation: Horizontal expansion (though notably discouraged due to memory implications)\n\n\nThe relative simplicity of these approaches belies their potential impact on model architecture and performance. Particularly interesting is how these techniques interface with the broader challenge of multitask learning."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#multitask-learning-a-new-paradigm",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#multitask-learning-a-new-paradigm",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Multitask Learning: A New Paradigm",
    "text": "Multitask Learning: A New Paradigm\nTraditional approaches to multitask learning have typically forced practitioners into one of two suboptimal paths:\n\nSimultaneous Training\n\nRequires creation of a comprehensive dataset containing examples for all tasks\nNecessitates careful balancing of task representation\nOften leads to compromise in per-task performance\n\nSequential Training\n\nFine-tunes the model on each task in sequence\nRisks catastrophic forgetting as new tasks overwrite previous learning\nRequires careful orchestration of task order and learning rates\n\n\n\nKey Innovation: Model merging introduces a third path - parallel fine-tuning followed by strategic combination. This approach fundamentally alters the landscape of multitask learning optimisation."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#the-parallel-processing-advantage",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#the-parallel-processing-advantage",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "The Parallel Processing Advantage",
    "text": "The Parallel Processing Advantage\nModel merging enables a particularly elegant solution to the multitask learning challenge through parallel processing:\n\nIndividual models can be fine-tuned for specific tasks independently\nTraining can occur in parallel, optimising computational resource usage\nModels can be merged post-training, preserving task-specific optimisations\n\nThis approach brings several compelling advantages:\n\nStrategic Benefits: - Parallel training efficiency - Independent task optimisation - Flexible deployment options - Reduced risk of inter-task interference"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#practical-implications",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#practical-implications",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Practical Implications",
    "text": "Practical Implications\nWhile the implementation details remain somewhat experimental, the potential applications are significant. Organisations can:\n\nDevelop specialised models in parallel\nOptimise individual task performance without compromise\nMaintain flexibility in deployment architecture\nScale their multitask capabilities more efficiently"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#implementation-pathways",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#implementation-pathways",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Implementation Pathways",
    "text": "Implementation Pathways\nThe chapter concludes with two distinct development approaches:\n\nProgression Path\n\nBegin with the most economical and fastest model\nValidate with a mid-tier model\nPush boundaries with the optimal model\nMap the price-performance frontier\nSelect the most appropriate model based on requirements\n\n\n\nDistillation Path\n\nStart with a small dataset and the strongest affordable model\nGenerate additional training data using the fine-tuned model\nTrain a more cost-effective model using the expanded dataset"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#final-observations",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html#final-observations",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Final Observations",
    "text": "Final Observations\nThe chapter emphasises that while the technical process of fine-tuning isn’t necessarily complex, the surrounding context and implications are highly nuanced. Success requires careful consideration of business priorities, resource availability, and long-term maintenance capabilities. This holistic perspective is crucial for organisations considering fine-tuning as part of their AI strategy."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "",
    "text": "What follows are my notes on chapter 9 of Chip Huyen’s ‘AI Engineering’ book. This chapter was on optimising your inference and I learned a lot while reading it! There are interesting techniques like prompt caching and architectural considerations that I was vaguely aware of but hadn’t fully appreciated how they might work in real inference systems."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#chapter-9-overview",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#chapter-9-overview",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Chapter 9: Overview",
    "text": "Chapter 9: Overview\nMachine learning inference optimization operates across three fundamental domains: model optimization, hardware optimization, and service optimization. While hardware optimization often requires significant investment and may offer limited individual leverage, model and service optimizations provide substantial opportunities for AI engineers to improve performance.\n\nCritical Cost Insight: A 2023 survey revealed that inference can account for up to 90% of machine learning costs in deployed AI systems, often exceeding training costs. This emphasizes why inference optimization isn’t just an engineering challenge - it’s a critical business necessity."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#core-concepts-and-bottlenecks",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#core-concepts-and-bottlenecks",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Core Concepts and Bottlenecks",
    "text": "Core Concepts and Bottlenecks\nUnderstanding inference bottlenecks is essential for effective optimization. Two primary types of computational bottlenecks impact inference performance:\n\nCompute-Bound Bottlenecks: Tasks that are limited by raw computational capacity, typically involving complex mathematical operations that take significant time to complete. These bottlenecks are particularly evident in computationally intensive operations within neural networks.\nMemory Bandwidth-Bound Bottlenecks: Limitations arising from data transfer requirements between system components, particularly between memory and processors. This becomes especially relevant in Large Language Models where significant amounts of data need to be moved between different memory hierarchies.\n\nIn Large Language Models (LLMs), different operations exhibit varying profiles of these bottlenecks. This understanding has led to architectural decisions such as decoupling the prefilling step from the decode step in production environments - a practice that has become increasingly common as organizations optimize their inference pipelines."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-apis-and-service-patterns",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-apis-and-service-patterns",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Inference APIs and Service Patterns",
    "text": "Inference APIs and Service Patterns\nTwo fundamental approaches to inference deployment exist:\n\nOnline Inference APIs\n\nOptimized for minimal latency\nDesigned for real-time responses\nTypically more expensive per inference\nCritical for interactive applications\n\nBatch Inference APIs\n\nOptimized for cost efficiency\nCan tolerate longer processing times (potentially hours)\nAllows providers to optimize resource utilization\nIdeal for bulk processing tasks"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-performance-metrics",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-performance-metrics",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Inference Performance Metrics",
    "text": "Inference Performance Metrics\nSeveral key metrics help quantify inference performance:\n\nLatency Components\n\nTime to First Token\n\nMeasures duration between query submission and initial response\nCritical for user experience in interactive applications\nOften a key optimization target for real-time systems\n\nTime per Output Token\n\nGeneration speed after the first token\nImpacts overall completion time\nCan vary based on model architecture and optimization\n\nInter-token Latency\n\nTime intervals between consecutive tokens\nAffects perceived smoothness of generation\nImportant for streaming applications\n\n\nTotal latency can be expressed as: time_to_first_token + (time_per_token × number_of_tokens)\n\n\nThroughput and Goodput Metrics\n\nThroughput: The number of output tokens per second an inference service can generate across all users and requests. This raw metric provides insight into system capacity.\n\n\nGoodput: The number of requests per second that successfully meet the Service Level Objective (SLO). This metric offers a more realistic view of useful system capacity.\n\n\n\nResource Utilization Metrics\n\nModel FLOPS Utilization (MFU)\n\nRatio of actual to theoretical FLOPS\nIndicates computational efficiency\nKey metric for hardware optimization\n\nModel Bandwidth Utilization (MBU)\n\nPercentage of achievable memory bandwidth utilized\nCritical for memory-intensive operations\nHelps identify memory bottlenecks"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#hardware-considerations-and-ai-accelerators",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#hardware-considerations-and-ai-accelerators",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Hardware Considerations and AI Accelerators",
    "text": "Hardware Considerations and AI Accelerators\nWhile NVIDIA GPUs dominate the market, various specialized chips exist for inference:\n\nPopular AI Accelerators\n\nNVIDIA GPUs (market leader)\nAMD accelerators\nGoogle TPUs\nVarious emerging specialized chips\n\n\nInference vs Training Hardware: Inference-optimized chips prioritize lower precision and faster memory access over large memory capacity, contrasting with training-focused hardware that requires substantial memory capacity.\n\nKey hardware optimization considerations include:\n\nMemory size and bandwidth requirements\nChip architecture specifics\nPower consumption profiles\nPhysical chip architecture variations\nCost-performance ratios"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#model-optimization-techniques",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#model-optimization-techniques",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Model Optimization Techniques",
    "text": "Model Optimization Techniques\n\n\nCore Approaches\n\nQuantization\n\nReduces numerical precision (e.g., 32-bit to 16-bit)\nDecreases memory footprint\nWeight-only quantization is particularly common\nCan halve model size with minimal performance impact\n\nPruning\n\nRemoves non-essential parameters\nPreserves core model behavior\nMultiple techniques available\nRequires careful validation\n\nDistillation\n\nCreates smaller, more efficient models\nMaintains key capabilities\nCovered extensively in Chapter 8\n\n\n\n\nAdvanced Decoding Strategies\n\n\nSpeculative Decoding\nThis approach combines a large model with a smaller, faster model:\n\nSmall model generates rapid initial outputs\nLarge model verifies and corrects as needed\nProvides faster token generation\nEasy to implement\nIntegrated into frameworks like VLLM and LamaCPU\n\n\n\nInference with Reference\n\n\nPerforms mini-RAG operations during decoding\nRetrieves relevant context from input query\nRequires additional memory overhead\nUseful for maintaining context accuracy\n\n\n\nParallel Decoding\nRather than strictly sequential token generation, this method:\n\nGenerates multiple tokens simultaneously\nUses resolution mechanisms to maintain coherence\nImplements look-ahead techniques\nAlgorithmically complex but offers significant speed benefits\nDemonstrated success with look-ahead decoding method\n\n\n\nAttention Optimization\nSeveral strategies exist for optimizing attention mechanisms:\n\nKey-Value Cache Optimization\n\nCritical for large context windows\nRequires substantial memory\nVarious techniques for size reduction\n\nSpecialized Attention Kernels\n\nFlash Attention as leading example\nHardware-specific implementations\nFlash Attention 3 for H100 GPUs"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#service-level-optimization",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#service-level-optimization",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Service-Level Optimization",
    "text": "Service-Level Optimization\n\nBatching Strategies\n\nStatic Batching\n\nProcesses fixed-size batches\nWaits for complete batch (e.g., 100 requests)\nSimple but potentially inefficient\n\nDynamic Batching\n\nUses time windows for batch formation\nProcesses incomplete batches after timeout\nBalances latency and throughput\n\nContinuous Batching\n\nReturns completed responses immediately\nDynamically manages resource utilization\nSimilar to a bus route that continuously picks up new passengers\nOptimizes occupation rate\nBased on Orca paper’s findings\n\n\n\n\nPrefill-Decode Decoupling\n\nSeparates prefill and decode operations\nEssential for large-scale inference providers\nAllows optimal resource allocation\nImproves overall system efficiency\n\n\n\nPrompt Caching\n\n\nStores computations for overlapping text segments\nOffered by providers like Gemini and Anthropic\nMay incur storage costs\nRequires careful cost-benefit analysis\nMust be explicitly enabled\n\n\n\nParallelism Strategies\n\nReplica Parallelism\n\nCreates multiple copies of the model\nDistributes requests across replicas\nSimplest form of parallelism\n\nTensor Parallelism\n\nSplits individual tensors across devices\nEnables processing of larger models\nRequires careful coordination\n\nPipeline Parallelism\n\nDivides model computation into stages\nAssigns stages to different devices\nOptimizes resource utilization\nReduces memory requirements\n\nContext Parallelism\n\nProcesses different parts of input context in parallel\nParticularly useful for long sequences\nCan significantly reduce latency\n\nSequence Parallelism\n\nProcesses multiple sequences simultaneously\nLeverages hardware-specific features\nRequires careful implementation"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#implementation-considerations",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#implementation-considerations",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\nWhen implementing inference optimizations:\n\nMultiple optimization techniques are typically combined in production\nHardware-specific optimizations require careful testing\nService-level optimizations often provide significant gains with minimal model modifications\nOptimization choices depend heavily on specific use cases and requirements"
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html",
    "title": "Starting the Hugging Face Agents course",
    "section": "",
    "text": "I finished the first unit of the Hugging Face Agents course, at least the reading part. I still want to play around with the code a bit more, since I imagine we’ll be doing that more going forward. In the meanwhile I wanted to write up some reflections on the course materials from unit one, in no particular order…"
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#code-agents-prominence",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#code-agents-prominence",
    "title": "Starting the Hugging Face Agents course",
    "section": "Code agents’ prominence",
    "text": "Code agents’ prominence\nThe course materials and smolagents in general places special emphasis on code agents, citing multiple research papers and they seem to make some solid arguments for it but it also seems pretty risk at the same time. Having code agents instead of pre-defined tool use is good because:\n\nComposability: could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?\nObject management: how do you store the output of an action like generate_image in JSON?\nGenerality: code is built to express simply anything you can have a computer do.\nRepresentation in LLM training data: plenty of quality code actions is already included in LLMs’ training data which means they’re already trained for this!\n\nThe thing that gives me pause is that it seems like we moved through the spectrum from highly structured and known workflows (a chain, perhaps, or even something like a DAG) to tool use in a loop (which had some arbitrary or dynamic parts but ultimately was at least a little defined), and all the way out then to code agents where basically anything is possible.\nIf I think about this as an engineer tasked with building a robust, dependable and reliable system, then the last thing I think I want to add into the system is an agent that can basically do any thing under the sun (i.e. code agents). Perhaps I’m misrepresenting the position here of code agents, so I’m looking forward to reading the papers cited above as well as understanding it more from the course authors’ perspective."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#evals-testing",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#evals-testing",
    "title": "Starting the Hugging Face Agents course",
    "section": "Evals & testing",
    "text": "Evals & testing\nFollowing on to my confusion around code agents, I’m very curious how the course will recommend one tests and evaluates these arbitrary code agents. Things I could imagine:\n\ntesting out the specific scenarios that your application or use case requires (i.e. end to end)\ntesting out each component of the system, such as you can break it down into smaller sub-components\nincluding things like linting / unit tests maybe once code is generated by the agent (?) i.e. real-time evaluation of the robustness of the system?\nprobably LLM as a judge somewhere in the mix, though that opens up its own can of worms…\n\nI do hope they talk about that in the later units of the course."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#general-patterns",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#general-patterns",
    "title": "Starting the Hugging Face Agents course",
    "section": "General patterns",
    "text": "General patterns\nThe core loop that came up in unit 1 was:\n\nplan -&gt; act -&gt; feedback/reflection\n\nAnd all of that gets packaged up in a loop and repeated in various forms depending on exactly how you’re using it. And this pattern is related to the ReACT loop which lots of people cite but seems to be a specific version of the general idea mentioned above.\nAnd the fact that all of this works is somehow all powered by the very useful enablement of tool use, which is itself powered by the fact that the model providers finetuned this ability into the models. Crazy, brittle, impressive and many other words for the fact that this ‘hack’ has such power."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#chat-templates",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#chat-templates",
    "title": "Starting the Hugging Face Agents course",
    "section": "Chat templates",
    "text": "Chat templates\nI liked how the unit really impresses on you the impact and importance of chat templates as the real way that LLMs are implemented. You may pass in your requests through a handy Python SDK, passing your tools as a list of function definitions, but in the end this is all being parsed down and out into very precise syntax with many tokens not intended for human consumption."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#points-of-leverage",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#points-of-leverage",
    "title": "Starting the Hugging Face Agents course",
    "section": "Points of leverage",
    "text": "Points of leverage\nAt the end of the unit, I was thinking about all the places where an engineer has leverage over agents. What I could initially think of was:\n\nthe variety and usefulness of tools that you provide to your agent (or perhaps the extent to which you allow your code agent to ‘write’ things out into the world)\nthe discrimination in the volume or choice of a combination of tools or APIs\nhow you chain everything together\n(how robustly you handle failure)\n\nBeyond that there are quite a few things that are somewhat out of your hands unless you decide to custom finetune your own models for a specific use case.\nOverall it was a good start to the course: made me think and also got my hands dirty working on a very simple agent with tools using smolagent and a Gradio demo app in the Hugging Face Hub. I’ll write more after unit two next week."
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "",
    "text": "Large Language Models have transformed how we interact with text, offering capabilities that seemed like science fiction just a few years ago. They can write poetry, generate code, and engage in sophisticated reasoning. Yet surprisingly, one seemingly straightforward task – document translation – remains a significant challenge. This is a challenge I understand intimately, both as a developer and as a historian who has spent years working with multilingual primary sources.\nBefore the era of LLMs, I spent years conducting historical research in Afghanistan, working extensively with documents in Dari, Pashto, and Arabic. This wasn’t just casual reading – it was deep archival work that resulted in publications like “Poetry of the Taliban” and “The Taliban Reader”, projects that required painstaking translation work with teams of skilled translators. The process was time-consuming and resource-intensive, but it was the only way to make these primary sources accessible to a broader audience.\nAs someone who has dedicated significant time to making historical sources more accessible, I’ve watched the rise of LLMs with great interest. These models promise to democratise access to multilingual content, potentially transforming how historians and researchers work with primary sources. However, the reality has proven more complex. Current models, while powerful, often struggle with or outright refuse to translate certain content. This is particularly problematic when working with historical documents about Afghanistan – for instance, a 1984 document discussing the Soviet-Afghan conflict might be flagged or refused translation simply because it contains the word “jihad”, even in a purely historical context. The models’ aggressive content filtering, while well-intentioned, can make them unreliable for serious academic work.\nAfter repeatedly bumping into these limitations in my own work, I built tinbox (shortened from ‘translation in a box’), a tool that approaches document translation through a different lens. What if we had a tool that could handle these sensitive historical texts without balking at their content? What if researchers could quickly get working translations of primary sources, even if they’re not perfect, to accelerate their research process? As a historian, having access to even rough translations of primary source materials would have dramatically accelerated my research process. As a developer, I knew we could build something better than the current solutions.\nThe name “tinbox” is a nod to the simple yet effective nature of the tool – it’s about taking the powerful capabilities of LLMs and packaging them in a way that actually works for real-world document translation needs. Whether you’re a researcher working with historical documents, an academic handling multilingual sources, or anyone needing to translate documents at scale, tinbox aims to provide a more reliable and practical solution."
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#the-hidden-complexity-of-document-translation",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#the-hidden-complexity-of-document-translation",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "The Hidden Complexity of Document Translation",
    "text": "The Hidden Complexity of Document Translation\nThe problem of document translation sits at an interesting intersection of challenges. On the surface, it might seem straightforward – after all, if an LLM can engage in complex dialogue, surely it can translate a document? It can, but there are some edge cases and limitations.\nWhen working with real-world documents, particularly PDFs, we encounter a cascade of complications. First, there’s the issue of model refusal. LLMs frequently decline to translate documents, citing copyright concerns or content sensitivity. This isn’t just an occasional hiccup – it’s a systematic limitation occurring regularly that makes these models unreliable for production use out of the box.\nThen there’s the scale problem. Most documents aren’t just a few paragraphs; they’re often dozens or hundreds of pages long. This runs headlong into the context window limitations of current models. Breaking documents into smaller chunks might seem like an obvious solution, but this introduces its own set of challenges. How do you maintain coherence across chunks? What happens when a sentence spans two pages? How do you handle formatting and structure?\nThe PDF format adds another layer of complexity. Most existing tools rely on Optical Character Recognition (OCR), which introduces its own set of problems. OCR can mangle formatting, struggle with complex layouts, and introduce errors that propagate through to the translation. Even when OCR works perfectly, you’re still left with the challenge of maintaining the document’s original structure and presentation."
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#a-word-about-translations-fidelity-and-accuracy",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#a-word-about-translations-fidelity-and-accuracy",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "A Word About Translations, Fidelity and Accuracy",
    "text": "A Word About Translations, Fidelity and Accuracy\nHaving worked professionally as a translator and worked as an editor for teams of translators, I’m acutely aware of the challenges and limitations of LLM-provided translations. While these models have made remarkable strides, they face several significant hurdles that are worth examining in detail.\nOne of the most prominent issues is consistency. LLMs often struggle to maintain consistent terminology across multiple API calls, which becomes particularly evident in longer documents. Technical terms, product names, and industry-specific jargon might be translated differently each time they appear, creating confusion and reducing the professional quality of the output. This problem extends beyond mere terminology – the writing style and tone can drift significantly between chunks of text, especially when using the chunking approach necessary for longer documents. You might find yourself with a document that switches unexpectedly between formal and informal registers, or that handles technical depth inconsistently across sections.\nEven formatting poses challenges. The way LLMs handle structural elements like bullet points, numbered lists, or text emphasis can vary dramatically across sections. What starts as a consistently formatted document can end up with a patchwork of different styling approaches, requiring additional cleanup work.\nPerhaps more fundamentally, LLMs struggle to find the right balance between literal and fluent translation. Sometimes they produce awkwardly literal translations that technically convey the meaning but lose the natural flow of the target language. Other times, they swing too far in the opposite direction, producing fluid but unfaithful translations that lose important nuances from the source text. This challenge becomes particularly acute when dealing with idioms and cultural references, where literal translation would be meaningless but too free a translation risks losing the author’s intent.\nCultural nuances present another significant challenge. LLMs often miss or mishandle culture-specific references, humour, and wordplay. They struggle with regional variations in language and historical context, potentially stripping away layers of meaning that a human translator would carefully preserve. This limitation becomes even more apparent in specialised fields – medical texts, legal documents, technical manuals, and academic writing all require domain expertise that LLMs don’t consistently demonstrate.\nThe technical limitations of these models add another layer of complexity. The necessity of breaking longer texts into chunks means that broader document context can be lost, making it difficult to maintain coherence across section boundaries. While tools like tinbox attempt to address this through seam repair and sliding window approaches, it remains a significant challenge. Cross-references between different parts of the document might be missed, and maintaining a consistent voice across a long text can prove difficult.\nFormat-specific problems abound as well. Tables and figures might be misinterpreted, special characters can be mangled, and the connections between footnotes or endnotes and their references might be lost. Page layout elements can be corrupted in the translation process, requiring additional post-processing work.\nReliability and trust present another set of concerns. LLMs are prone to hallucination, sometimes adding content that wasn’t present in the original text or filling in perceived gaps with invented information. They might create plausible but incorrect translations or embellish technical details. Moreover, they provide no indication of their confidence in different parts of the translation, no flags for potentially problematic passages, and no highlighting of ambiguous terms or phrases that might benefit from human review.\nWhen it comes to handling source texts, LLMs show particular weakness with poor quality inputs. They struggle with grammatically incorrect text, informal or colloquial language, and dialectal variations. Their handling of abbreviations and acronyms can be inconsistent, potentially introducing errors into technical or specialised documents.\nThe ethical and professional implications of these limitations are significant. There’s often a lack of transparency about the translation process, no clear audit trail for translation decisions, and limited ability to explain why particular choices were made. This raises concerns about professional displacement – not just in terms of jobs, but in terms of the valuable human judgment that professional translators bring to sensitive translations, the opportunity for cultural consultation, and the role of specialist translators in maintaining high standards in their fields.\nThese various limitations underscore an important point: while LLMs are powerful tools for translation, they should be seen as aids to human translators rather than replacements, especially in contexts requiring high accuracy, cultural sensitivity, technical precision, legal compliance, or creative fidelity. The future of translation likely lies in finding ways to combine the efficiency and broad capabilities of LLMs with the nuanced understanding and expertise of human translators.\nSo why build a tool like this given all these problems? I think there’s still a use for something like this in fields where there are few translators and a huge backlog of materials where there’s a benefit to reading them in your own mother tongue, even in a ‘bad’ translation. (That said, having done a decent amount of comparison of outputs for languages like Arabic, Dari and Pashto, I actually don’t find the translations to be terrible, especially for domains like the news or political commentary.) For myself, I am working on a separate tool or system which takes in primary sources and incrementally populates a knowledge database. Having ways to ingest materials written in foreign languages is incredibly important for this, and having a way to do it that doesn’t break the bang (i.e. by using local models) is similarly important."
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#engineering-a-solution",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#engineering-a-solution",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "Engineering a Solution",
    "text": "Engineering a Solution\ntinbox takes a simple approach to solving these issues through two core algorithmic features. The first is what I call “page-by-page with seam repair.” Instead of treating a document as one continuous piece of text, we acknowledge its natural segmentation into pages. Each page is translated independently, but – and this is crucial – we then apply a repair process to the seams between pages.\nThis seam repair is where things get interesting. When a sentence spans a page boundary, we identify the overlap and re-translate that specific section with full context from both pages. This ensures that the translation flows naturally, even across page boundaries. It’s a bit like being a careful tailor, making sure the stitches between pieces of fabric are invisible in the final garment.\nFor continuous text documents (read: a .txt file containing multiple tens of thousands of words), we take a different approach using a sliding window algorithm. Think of it like moving a magnifying glass across the text, where the edges of the glass overlap with the previous and next positions. This overlap is crucial – it provides the context necessary for coherent translation across chunk boundaries.\nThe implementation details matter here. We need to carefully manage memory, handle errors gracefully, and provide progress tracking for long-running translations. The codebase is structured around clear separation of concerns, making it easy to add support for new document types or translation models.\nMoreover, we need to ensure that in the case of failure we’re able to resume without wasting what we spent translating earlier parts of the document."
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#the-engineering-details",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#the-engineering-details",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "The Engineering Details",
    "text": "The Engineering Details\nThe architecture reflects these needs. At its core, tinbox uses a modular design that separates document processing from translation logic. This allows us to handle different document types (PDFs, Word documents, plain text) with specialised processors while maintaining a consistent interface for translation.\nError handling is particularly crucial. Translation is inherently error-prone, and when you’re dealing with large documents, you need robust recovery mechanisms. We implement comprehensive retry logic with exponential backoff, ensuring that temporary failures (like rate limits) don’t derail entire translation jobs.\nFor large documents, we provide checkpointing and progress tracking. This means you can resume interrupted translations and get detailed insights into the translation process. The progress tracking isn’t just about displaying a percentage – it provides granular information about token usage, costs, and potential issues.\n\nPage-by-Page with Seam Repair\nThe page-by-page algorithm handles PDFs by treating each page as a separate unit while ensuring smooth transitions between pages. Pseudocode that can help you understand how this works goes something like this:\ndef translate_with_seam_repair(document, overlap_size=200):\n    translated_pages = []\n    \n    for page_num, page in enumerate(document.pages):\n        # Translate current page\n        current_translation = translate_page(page)\n        \n        if page_num &gt; 0:\n            # Extract and repair the seam between pages\n            previous_end = translated_pages[-1][-overlap_size:]\n            current_start = current_translation[:overlap_size]\n            \n            # Re-translate the overlapping section with full context\n            repaired_seam = translate_with_context(\n                text=current_start,\n                previous_context=previous_end\n            )\n            \n            # Update translations with repaired seam\n            translated_pages[-1] = translated_pages[-1][:-overlap_size] + repaired_seam\n            current_translation = repaired_seam + current_translation[overlap_size:]\n        \n        translated_pages.append(current_translation)\n    \n    return \"\\n\\n\".join(translated_pages)\n\n\nSliding Window for Text Documents\nFor continuous text documents, we use a sliding window approach. Again, pseudocode to help understand how this works goes something like this, though the actual implementation is different:\ndef translate_with_sliding_window(text, window_size=2000, overlap=200):\n    chunks = []\n    position = 0\n    \n    while position &lt; len(text):\n        # Create window with overlap\n        end = min(len(text), position + window_size)\n        window = text[position:end]\n        \n        # Translate window\n        translation = translate_window(window)\n        chunks.append(translation)\n        \n        # Slide window forward, accounting for overlap\n        position = end - overlap\n    \n    return merge_chunks(chunks, overlap)\n\n\nCLI Usage Examples\nThe tool provides a simple command-line interface:\n# Basic translation of a PDF to Spanish\ntinbox --to es document.pdf\n\n# Specify source language and model\ntinbox --from zh --to en --model anthropic:claude-3-5-sonnet-latest chinese_doc.pdf\n\n# Use local model via Ollama for sensitive content\ntinbox --model ollama:mistral-small --to en sensitive_doc.pdf\n\n# Advanced options for large documents\ntinbox --to fr --algorithm sliding-window \\\n       --window-size 3000 --overlap 300 \\\n       large_document.txt"
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#other-notable-features",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#other-notable-features",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "Other notable features",
    "text": "Other notable features\nThe CLI interface for tinbox currently is built on top of litellm so it technically supports most models you might want to use with it, though I’ve only enabled OpenAI, Anthropic, Google/Gemini and Ollama as base providers for now.\nThe Ollama support was one I was keen to offer since translation is such a token-heavy task. I also really worry about the level of sensitivity / monitoring on the cloud APIs and have run into that in the past (particularly with regard to my previous work as a historian working on issues relating to Afghanistan). Ollama-provided local models should solve that issue, perhaps at the expense of access to the very latest and greatest models."
  },
  {
    "objectID": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#things-still-to-be-done",
    "href": "posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html#things-still-to-be-done",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "Things still to be done",
    "text": "Things still to be done\nThere’s lots of improvements still to be made. I’m particularly interested in exploring semantic section detection, which could make the chunking process more intelligent. There’s also work to be done on preserving more complex document formatting and supporting additional output formats.\nCurrently the tool is driven by whatever you tell it to do. Most decisions are in your hands. You have to choose the model to use for translation, notably. I am most interested in using this tool for some other side-projects and for low-resource languages so one of the important things I’ll be doing is to pick sensible defaults depending on the language and input document type you choose.\nFor example, some vision language models like GPT-4o are able to handle translating directly from an image in Urdu to English, the open-source versions (like llama3.2-vision) struggle much more with these kinds of tasks so it’s possible I might even need to insert an intermediary step of transcribe, then translate the transcribed text into English etc. In fact, for highest-fidelity of translation I almost certainly might want to enable that option.\nThe code is available at GitHub, and I welcome contributions and feedback."
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html",
    "href": "posts/2025-03-16-learnings-building-llms.html",
    "title": "Learnings from a week of building with local LLMs",
    "section": "",
    "text": "I took the past week off to work on a little side project. More on that at some point, but at its heart it’s an extension of what I worked on with my translation package tinbox. (The new project uses translated sources to bootstrap a knowledge database.) Building in an environment which has less pressure / deadlines gives you space to experiment, so I both tried out a bunch of new tools and also experimented with different ways of using my tried-and-tested development tools/processes.\nAlong the way, there were a bunch of small insights which occurred to me so I thought I’d write them down. As usual with this blog, I’m mainly writing for my future self but I think there might be parts that are useful for others! Apologies for the somewhat rushed nature of these observations; better I get the blog finished and published than not at all!"
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html#local-models",
    "href": "posts/2025-03-16-learnings-building-llms.html#local-models",
    "title": "Learnings from a week of building with local LLMs",
    "section": "🤖 Local Models",
    "text": "🤖 Local Models\nDuring this project, I experimented with several local models, which continue to impress me with their evolving capabilities. The recent launch of gemma3 was particularly timely - I found myself regularly using the 27B version, which performed admirably across various tasks.\nThere are three or four models I keep returning to. mistral-small stands out as an exceptional model that’s been relatively recently updated and seems a bit underrated / underappreciated. The original mistral model continues to hold up remarkably well, particularly for structured extraction tasks and general writing needs like summarization.\nOne important realization when working with real-world use cases: benchmarks can be deceptive. While helpful as general indicators, each model has its own strengths and quirks. Many newer models are heavily optimized for structured data extraction, but their performance ultimately depends on whether their training documents align with your specific use case. It’s crucial to test models against your actual requirements rather than relying solely on published benchmarks.\nFor robust results with local models, I’ve found that implementing a “reflection, iterate and improve” pattern significantly enhances performance. When you need a model to summarize or analyze content in a particular format, having a secondary model (or even the same model!) review the output against the original prompt requirements is incredibly valuable. This reviewer model can suggest improvements to better fulfill the original request. Running this loop for 2-5 iterations (depending on complexity) can yield results approaching those of proprietary models like Claude or GPT-4, which might achieve similar quality in a single pass. For local deployments, this iterative improvement pattern is essentially non-negotiable.\nI also explored vision models, particularly llava and llama-3.2-vision. These were my primary tools for extracting context from images, generating captions, and analyzing visual content. Their effectiveness varies based on content type and language, but they represent impressive capabilities that can run entirely on local systems.\nA significant portion of my work involved non-English languages, including some relatively rare ones. This is another area where benchmark claims about supporting “hundreds of languages” often don’t align with real-world performance. Models might list impressive language coverage in their specifications, but actual proficiency varies dramatically. It reinforces my earlier point - always verify benchmark claims against your specific use case before committing to a particular model."
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html#prompting-instruction-following",
    "href": "posts/2025-03-16-learnings-building-llms.html#prompting-instruction-following",
    "title": "Learnings from a week of building with local LLMs",
    "section": "💬 Prompting & Instruction Following",
    "text": "💬 Prompting & Instruction Following\nWorking extensively with various models during this project reinforced some fundamental insights about prompting that might seem basic, but prove critical in practical applications. These observations are particularly relevant when working with local models, though they apply to cloud-based systems as well.\nContext matters significantly more than we might assume. While we’ve grown accustomed to proprietary models like Claude or GPT-4o performing admirably with minimal guidance, local models require more deliberate direction. The more relevant context you can provide (within reasonable token limits), the better your results will be. If you would naturally provide certain background information to a human performing the task, make sure to include it in your prompt to the model as well.\nAnother key insight: every model has its unique characteristics. Techniques that work brilliantly with one model might fall flat with another, especially in the local model ecosystem. They each require slightly different prompting approaches, specific phrasing patterns, and tailored guidance. This necessitates running small experiments to understand how different models respond to various prompting styles. It’s still more art than science, but this experimentation phase is crucial when implementing local models effectively.\nPerhaps the most valuable lesson I rediscovered is that breaking complex tasks into smaller components yields superior results compared to using a single comprehensive prompt. This is particularly true with local models. When performing extensive data extraction or when dealing with structured data where the extraction targets differ significantly from each other, don’t expect the model to handle everything in one pass – even a human might struggle with such an approach.\nInstead, break down the task into logical components, create targeted mini-prompts for each aspect, and then recombine the results once all the separate LLM calls are completed. Yes, this approach adds processing time and complexity, but the quality improvement is well worth the trade-off. When accuracy matters more than speed, this decomposition strategy consistently delivers better outcomes."
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html#process-tools",
    "href": "posts/2025-03-16-learnings-building-llms.html#process-tools",
    "title": "Learnings from a week of building with local LLMs",
    "section": "🧰 Process & Tools",
    "text": "🧰 Process & Tools\nMy development environment during this project provided plenty of opportunities to evaluate various tools and workflows. As context, I primarily work on a Mac while maintaining access to a separate (local) machine with GPU capabilities for more intensive tasks. This setup allows me to flexibly experiment with both local and cloud-based models.\nFor managing local models, Ollama continues to be my go-to solution for downloading, running, and interfacing with these models. A recent discovery that significantly improved my workflow is Bolt AI, an excellent Mac interface that provides seamless switching between local Ollama models and cloud-based alternatives. If you’re working in a hybrid model environment, Bolt AI is definitely worth exploring.\nI’ve also recently integrated OpenRouter into my toolkit, which solves the problem of managing countless API keys across different inference providers. OpenRouter not only offers native connections to many cloud providers but also allows you to incorporate your own API keys, streamlining access to a diverse model ecosystem through a unified interface. It also helps with setting spend limits on various models or projects.\nIn terms of development insights, I was impressed by how rapidly front-end development can progress with the assistance of models like Claude 3.7 and OpenAI’s O1-Pro. These models perform exceptionally well when supplemented with documentation (such as an llms.txt file) alongside your prompts. While I can’t speak to their effectiveness with extremely complex applications or massive frontend codebases, they demonstrate remarkable proficiency with small to medium-sized projects.\nA significant portion of my experimentation involved RepoPrompt, a tool that recently transitioned from free beta to a paid license model. RepoPrompt addresses the challenge of getting your codebase into an LLM-friendly format. Unlike standard CLI tools that simply export code to clipboard or text files, RepoPrompt generates a structured XML representation that, when modified by an LLM and pasted back, creates a reviewable diff of the proposed changes. At least, that’s one of the things it allows you to do! It’s actually a bit more powerful / flexible than that and here’s a video so you can see it in action:\n\n\n\nRepoPrompt Demo Video\n\n\nWhile tools like Cursor and Windsurf offer similar functionality, they tend to become less reliable as project complexity increases. RepoPrompt shines when paired with an OpenAI Pro subscription, enabling effective integration of models like O1 Pro and o3-mini-high into your development lifecycle. In my testing, the RepoPrompt + O1 Pro/O3 Mini High combination consistently delivered superior results compared to using Cursor with Claude 3.7 (even with ‘Thinking Mode’ enabled). Despite the occasional pauses while these models process complex problems, the quality improvement justifies the wait.\nAdditionally, I continued working with Claude Code and CodeBuff, both CLI-driven tools focused on code improvement. Of the two, CodeBuff has become my preferred option. Both tools require careful supervision—I typically keep Cursor open to monitor changes in real-time, occasionally needing to revert modifications or redirect the approach. These tools excel when you clearly articulate your objectives and maintain oversight of the implementation process. CodeBuff particularly impresses with larger codebases and demonstrates superior stability overall.\nAn interesting pattern emerged during development: whenever files approached 800-900 lines, it signaled the need to refactor into smaller submodules to maintain LLM comprehension, especially when using agent mode in Cursor. The modular approach significantly improved model performance.\nI was genuinely surprised by the effectiveness of the RepoPrompt and O1 Pro combination. For smaller, targeted modifications, CodeBuff continues to demonstrate remarkable capability. While I didn’t evaluate these tools in conjunction with local models, I suspect such combinations would require more iterative refinement to achieve comparable results."
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html#software-engineering-patterns",
    "href": "posts/2025-03-16-learnings-building-llms.html#software-engineering-patterns",
    "title": "Learnings from a week of building with local LLMs",
    "section": "🧑‍🔬 Software Engineering Patterns",
    "text": "🧑‍🔬 Software Engineering Patterns\nThroughout this experimental project, several software engineering principles proved particularly valuable when working with LLM-assisted development. These patterns aren’t revolutionary, but their importance amplifies in the context of AI-augmented workflows.\nThe principle of simplicity served as a cornerstone approach. Breaking development into the smallest logical next task repeatedly demonstrated its value, especially during the exploratory phases when project architecture was still taking shape. While some engineers might possess the cognitive bandwidth to fully conceptualize complex systems with perfect abstractions from the outset, I’ve found incremental development leads to more robust outcomes. This approach aligns naturally with how most developers actually think through problems and provides clear checkpoints for evaluating progress.\nData visibility emerged as another critical factor. When leveraging LLM-assisted coding, comprehensive logging becomes even more essential than in traditional development. Strategically placed log outputs create a diagnostic trail that proves invaluable when troubleshooting unexpected behaviors. This practice creates a feedback loop that strengthens both your understanding of the system and the LLM’s ability to assist effectively.\nA particularly underappreciated practice I haven’t seen widely discussed is the importance of dead code detection. When working with LLM-assisted development, code cruft tends to accumulate more rapidly than in conventional programming. Tools like deadcode and vulture provide static analysis of Python projects to identify unused functions and variables. Running these tools periodically helps maintain codebase clarity by flagging remnants that might otherwise cause confusion during review. I’m not certain whether newer tools like ruff from Astral include this functionality (particularly for function calls), but the capability is invaluable for maintaining a clean, navigable codebase.\nTaking time to think offline—away from the keyboard—often yields surprising clarity. This deliberate pause creates space to articulate precisely what you need for the next development increment. When you can express your requirements with precision, the LLM’s output improves proportionally. Ambiguous instructions inevitably produce suboptimal results, whereas clarity fosters efficiency.\nA final observation worth emphasizing: having experience as an engineer in the pre-LLM era remains tremendously advantageous. When confronting complex workflows involving chained LLM calls with interdependencies and reflection patterns, traditional debugging skills become indispensable. Knowing when to step away from AI assistance and dive into manual debugging with tools like pdb, stepping through code execution and inspecting variables directly, represents a crucial judgment call.\nLLMs and coding agents often demonstrate a bias toward generating new code rather than methodically analyzing existing problems. Recognizing the moment when direct human intervention becomes more efficient than continually prompting an AI is a skill that comes with experience. Once you’ve manually identified the underlying issue, you can return to the LLM with precisely targeted prompts that yield superior results."
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html#appendix-1-fasthtml",
    "href": "posts/2025-03-16-learnings-building-llms.html#appendix-1-fasthtml",
    "title": "Learnings from a week of building with local LLMs",
    "section": "🌐 Appendix 1: FastHTML",
    "text": "🌐 Appendix 1: FastHTML\nAs a practical addition to my experimentation, I implemented FastHTML for the first time to build a frontend for my knowledge base extraction assistant. The experience was remarkably frictionless, particularly when leveraging their llms.txt file—a markdown-formatted documentation set that integrates seamlessly with your frontend codebase when provided alongside prompts.\nThis approach works exceptionally well with models like O1 Pro or O3 Mini High, creating a development workflow that feels intuitive and responsive. Despite having substantial JavaScript experience from previous roles, I found FastHTML significantly more manageable than complex JavaScript frameworks that dominate the ecosystem today.\nThe reduced cognitive overhead and natural integration with Python-based workflows makes FastHTML a compelling choice for ML practitioners who prefer to minimize context-switching between languages and paradigms. The framework strikes an excellent balance between capability and simplicity that aligns perfectly with rapid prototyping and iterative development cycles common in ML projects. For those building interfaces to ML systems, it’s definitely worth considering as your frontend solution."
  },
  {
    "objectID": "posts/2025-03-16-learnings-building-llms.html#appendix-2-ocr-translation",
    "href": "posts/2025-03-16-learnings-building-llms.html#appendix-2-ocr-translation",
    "title": "Learnings from a week of building with local LLMs",
    "section": "📃 Appendix 2: OCR + Translation",
    "text": "📃 Appendix 2: OCR + Translation\nAnother interesting challenge I tackled involved OCR and translation of handwritten documents in non-English languages—a task that proved impossible to accomplish in a single pass with local models, particularly for less common languages.\nThe solution emerged through methodical problem decomposition:\n\nBreaking down PDFs into individual page images\nSegmenting each page into overlapping image chunks (critical for handwriting where text may slant across traditional line boundaries)\nApplying OCR to extract text in the original source language from each image segment\nUsing translation models to convert the extracted text to English\n\nThis multi-stage pipeline allowed me to overcome the limitations of local models when confronted with the combined complexity of handwriting recognition and translation. Both gemma3 and llama-3.3 performed admirably within this decomposed workflow, demonstrating that even resource-constrained local deployments can achieve impressive results when problems are thoughtfully restructured.\nThis case exemplifies a core principle of effective ML implementation: when dealing with complex, multi-faceted challenges, breaking them into targeted sub-problems often yields better outcomes than attempting end-to-end solutions—especially when working with constrained computational resources. While this approach may increase processing time, the quality improvement justifies the trade-off for many practical applications."
  },
  {
    "objectID": "posts/2025-05-20-how-to-think-about-evals.html",
    "href": "posts/2025-05-20-how-to-think-about-evals.html",
    "title": "How to think about evals",
    "section": "",
    "text": "Today was the first session of Hamel + Shreya’s course, “AI Evals for Engineers and PMs”. The first session was all about mental models for thinking about the topic as a whole, mixed in with some teasers of practical examples and advice.\nI’ll try to keep up with blogging about what I learn as we go. Most of the actual content will go up online at some point in the future, I’m assuming, so not much point writing up super detailed notes. (There is also a book coming, which I assume will be great, and about which you can learn more here.) So in general I’ll try to be doing the following as I blog along:\nToday, fresh out of the first class, I wanted to write about the mental model of the ‘three gulfs’ that they propose, the improvement loop that they suggest is how to measurably improve your applications, and also prompting through the lens of evals. Finally I’ll round off with a bit about what I’ll be exploring this week."
  },
  {
    "objectID": "posts/2025-05-20-how-to-think-about-evals.html#the-three-gulfs-specification-generalization-and-comprehension",
    "href": "posts/2025-05-20-how-to-think-about-evals.html#the-three-gulfs-specification-generalization-and-comprehension",
    "title": "How to think about evals",
    "section": "The Three Gulfs: Specification, Generalization and Comprehension",
    "text": "The Three Gulfs: Specification, Generalization and Comprehension\nSo there’s this image that they shared in the book chapter preview discussion that came up again during the lesson today:\n\n\n\nThe three gulfs of LLM application development\n\n\n(They’ve shared it already in the YouTube discussion + I see it on Twitter being shared so I think I’m not sharing something I ought not to!)\nThe course is very practically focused, especially so for application developers, so this diagram is in that context. The diagram offers up a way of thinking about LLM application development that pinpoints the places where you might do your work, and it’s also a way of thinking through things systematically, too.\nI was especially interested in the differentiation between the gulf of specification and the gulf of generalisation, since these can often feel similar, but actually the way to get out of them is actually slightly different. I’ll go into a bit more detail below, but basically with the gulf of specification you might want to be working on your prompts + how specific you are, whereas with the generalisation gulf you might need things like splitting up your tasks or making sure your system is outputting things in a structured way, etc etc.\nNote also that the world of tooling also doesn’t help you in a specific or targeted way to focus on one aspect of this diagram. Too often the tools try to cover the whole picture and probably also muddy the water by eliding the differences between the different tasks and challenges of each island or the gulf in between. All this is pretty abstract, so let’s go through them one by one.\n\nThe Gulf of Comprehension in Practice\nThis was seen as sort of the starting point for thinking through LLM Application improvement. At this point your big problem is that you’re trying to understand the data that comes your way from your users. You’re trying to understand the inputs to your application (what your users are typing, assuming that text is the medium of communication / input) and you’re trying to understand what the application or LLM is outputting.\nThe challenge comes because you can’t read every single log or morsel of data. You have to filter things down somehow! If this were something more like traditional ML you’d have statistics to help boil down your data, but mostly we’re talking about unstructured text data so it’s much more unwieldy.\nThis challenge means that people often get stuck at this point. This is where POC applications live, breathe and eventually die. You have enough sense that things are ‘kinda’ working, but you don’t really know what the failure modes are, so you don’t know how to improve it. You’ve tried out one or two things in a halfway systematic way, but really you have no idea what’s working well and what’s not.\n\n\n\n\n\n\nTipOn Tools vs Process\n\n\n\nHamel made the good point that it’s probably not so useful to think about tools too much when thinking at this stage. Generally speaking what’s going on is most often actually a process problem and trying to go straight to ‘what tool do I need’ is probably avoiding the real issue.\n\n\n\n\nThe Gulf of Specification in Practice\nThis is the place where you are trying to translate intent into precise instructions that the LLM will follow. You’re trying to be explicit and specific in the hope that the LLM will do what you want it to do, and not do the things that you don’t want it to do.\nThe obvious manifestation of this is people writing bad prompts. It might seem that it’s also present when you try to have an LLM solve one problem when it’s either unsuited for that task or the task needs to be broken up and so on, but that’s the sister gulf of generalisation. Here, we’re focused on how to improve the specificity of your prompts.\nWhen you split things up and highlight the fact that prompts are something that you’ll need to work on and to improve, it becomes clear that it’s something you wouldn’t want to outsource or to skimp time on. Really the prompt writing is the thing that you (at certain moments, and where it’s identified as the thing needing focus / improvement) want to be working on in partnership with domain experts.\nFor small applications, you might be the same person as the domain expert! For bigger projects, you might be working with domain experts. Just be aware that often the domain expert might not necessarily be detached enough to be able to figure out what needs the focus, or where the weaknesses of a prompt are. That’s what the iterative process / error analysis and everything else that’ll be taught in the course is for (see below and see future posts).\nAnother point Hamel made was about why prompts are actually so important: “you have to express your project taste somewhere”. Given that your application might be fully / mostly driven by LLMs, the prompt is actually a really crucial place to express this taste and as such might be thought of as your ‘moat’.\nI know just from having experienced a variety of LLM-driven applications, it’s quite easy to tell the ones where the product team gave their prompts and their specification some real love. It’s the difference between POC junk that will die a slow and lonely death and something that delights and solves real user problems.\n\n\nGulf of Generalisation\nShreya didn’t really get into the details around the generalisation gulf in practical terms in this lesson, but I think this one can be a sort of place of comfort for the technically-minded to make refuge in. It’s one where there’s a ton of tools and technologies and techniques to play with, and vendors also live in this space and try to claim that their particular product or special sauce is the thing to help you and so on."
  },
  {
    "objectID": "posts/2025-05-20-how-to-think-about-evals.html#the-improvement-loop-for-llm-applications",
    "href": "posts/2025-05-20-how-to-think-about-evals.html#the-improvement-loop-for-llm-applications",
    "title": "How to think about evals",
    "section": "The Improvement Loop for LLM Applications",
    "text": "The Improvement Loop for LLM Applications\nWe also got a high-level overview of the loop that allows you to iteratively improve an LLM application:\n\n\n\nThe analyse, measure and improve loop; adapted from an image used in the course\n\n\nThere’s a lot to unpack in all these different stages, and we didn’t really get into the details in the session today but you can see how this offers a really powerful way of thinking through what it means to iteratively improve an LLM application.\nLearning how to implement this in a practical way will be the main thing I want to get good at by the end of this course. The process is made up of a bunch of techniques, but in my experience companies or use cases that struggle with improving what they built also lack the scaffold of this loop to orient themselves."
  },
  {
    "objectID": "posts/2025-05-20-how-to-think-about-evals.html#prompting-through-the-lens-of-evals",
    "href": "posts/2025-05-20-how-to-think-about-evals.html#prompting-through-the-lens-of-evals",
    "title": "How to think about evals",
    "section": "Prompting through the lens of evals",
    "text": "Prompting through the lens of evals\nAs we explored above, prompting is sort of the table stakes of improving your LLM application. In order to get good at prompting, it can help to appreciate what they are good at and what they struggle with. So, as Shreya put it, “leverage their strengths and anticipate their weaknesses” (when prompting).\nAt this point Shreya got into some points around what kinds of things went into a good prompt but I think I’ll write a separate blog on that and I don’t want to just regurgitate what we listened to. Today was more of a high-level introduction, and in any case it was much more about the outer-loop process instead of the inner loop (where tooling + specific techniques play more of a role.)\n\n\n\nA slide from a talk I gave about the inner loop vs the outer loop of GenAI development\n\n\nSo it’s great that the course gets into the weeds (esp in the course materials, which include the draft of the book Hamel & Shreya are writing) but I think the really useful thing they’re doing is situating the tactical improvements and techniques within the strategic patterns and workflows that teams and individuals should be doing to work on these LLM applications.\nAt a high level, what are we talking about:\n\nhow to tease out failure scenarios for these applications and their behaviours\nconversely, how to understand exactly which domains it does well for"
  },
  {
    "objectID": "posts/2025-05-20-how-to-think-about-evals.html#things-i-want-to-think-about-more",
    "href": "posts/2025-05-20-how-to-think-about-evals.html#things-i-want-to-think-about-more",
    "title": "How to think about evals",
    "section": "Things I want to think about more",
    "text": "Things I want to think about more\nThere was a ton of really rich discussion around prompting in the Discord. I’m interested in exploring more:\n\ncross-provider prompting decisions (i.e. how prompting an OpenAI model differs from what you do with a Llama model or whatever)\nprompts that work with reasoning models vs non-reasoning models\nthe tradeoffs of whether you put your instructions in system prompts vs user instructions\n\nIn general there’s been a bunch of noise recently about so-called ‘leaked’ system prompts from a bunch of LLM API providers and I’ve mainly been struck by just how detailed they are. I consider myself pretty good at improving and iterating on prompts, but I’ll admit I’m not writing these multi-thousand word tomes. I’d like to explore which scenarios it makes sense to do so, and how to calculate at what point it makes sense from a cost or latency perspective to do so.\nAs I’m sure you can detect, I’m really enthusiastic about the lesson to come and will work in the meanwhile on some of the readings that have been set as well as the homework task of writing a system prompt for a LLM-powered recipe recommendation application!"
  },
  {
    "objectID": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html",
    "href": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html",
    "title": "Building hinbox: An agentic research tool for historical document analysis",
    "section": "",
    "text": "I’ve been working on a project called hinbox - a flexible entity extraction system designed to help historians and researchers build structured knowledge databases from collections of primary source documents. At its core, hinbox processes historical documents, academic papers, books and news articles to automatically extract and organize information about people, organizations, locations, and events.\nThe tool works by ingesting batches of documents and intelligently identifying entities across sources. What makes it interesting is the iterative improvement aspect: as you feed more documents into the system, entity profiles become richer and more comprehensive. When hinbox encounters a person or organization it’s seen before, it updates their profile with new information rather than creating duplicates. I’ve been testing it extensively with Guantánamo Bay media sources - a domain where I have deep expertise from my previous career as a historian - which allows me to rigorously evaluate the quality of its extractions.\nRight now, hinbox isn’t ready for broader use. The prompt engineering needs significant refinement, and the entity merging logic requires more sophisticated iteration loops. But that’s actually the point - I’ve been participating in Hamel and Shreya’s AI evals course, and I wanted a concrete project where I could apply the systematic evaluation and improvement techniques we’re learning.\nThis project originally came together over a few intense days about two months ago, then sat dormant while work got busy. I’ve recently resurrected it specifically to serve as a practical laboratory for the evals course exercises. There’s something powerful about having a real application with measurable outputs where you can experiment with different approaches to prompt optimization, model selection, and systematic error analysis.\nThe broader vision is creating a tool that could genuinely help researchers working with large document collections - transforming the traditional manual process of reading, noting, and cross-referencing into something more systematic and scalable. But first, it needs to work reliably, which is where the evals work comes in."
  },
  {
    "objectID": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#why-build-this-personal-research-history-meets-the-age-of-agents",
    "href": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#why-build-this-personal-research-history-meets-the-age-of-agents",
    "title": "Building hinbox: An agentic research tool for historical document analysis",
    "section": "Why Build This? Personal Research History Meets the Age of Agents",
    "text": "Why Build This? Personal Research History Meets the Age of Agents\nThis project connects directly to something I’ve done before - but under very different circumstances. In the mid-2000s, I founded and ran a media monitoring startup in Afghanistan (RIP AfghanWire). We had a team of Afghan translators processing daily newspapers and news sources, translating everything into English. Then came my part: reading these translations and manually building what essentially became a structured knowledge database.\nThe process was methodical but exhausting. Each article mentioning a person required checking our existing profiles - did we know this individual? If not, I’d create a new entry and research their background. If yes, I’d update their existing profile with new information. Over time, we developed detailed profiles for hundreds of key figures in Afghan politics, civil society, and security. The more articles we processed, the richer and more interconnected our database became. We were building a living encyclopaedia of contemporary Afghanistan, one translated news story at a time.\nThe startup eventually ran out of funding, but the intellectual framework stuck with me. We’d created something genuinely valuable - contextual intelligence that helped outsiders understand the complex landscape of Afghan media and politics. The manual approach worked, but it was incredibly time-intensive and didn’t scale beyond what a small team could handle.\n\nThe Academic Reality Check\nSince then, I’ve continued working as a researcher (I have a PhD in War Studies from King’s College London and have written several critically-acclaimed books credentials blah blah sorry). This experience has reinforced how common the core challenge actually is across academic and research contexts. Historical research often involves exactly this pattern: you have access to substantial primary source collections - maybe 20,000 newspaper issues covering a decade, or thousands of diplomatic cables, or extensive archival materials - but limited time and resources to systematically extract insights.\nThe traditional academic approach involves months of careful reading, taking notes in physical notebooks, slowly building up understanding through manual cross-referencing. It’s thorough but painfully slow. Most researchers don’t have the luxury of unlimited time to spend four hours daily reading through source materials, even though that’s often what the work requires."
  },
  {
    "objectID": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#beyond-academic-applications",
    "href": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#beyond-academic-applications",
    "title": "Building hinbox: An agentic research tool for historical document analysis",
    "section": "Beyond Academic Applications",
    "text": "Beyond Academic Applications\nThe potential applications extend well beyond historical research. Intelligence analysis, scientific literature review, market research, legal discovery - anywhere you need to build structured knowledge from unstructured document collections. There’s clearly demand for these capabilities, evidenced by the popularity of “second brain” concepts and personal knowledge management tools like Obsidian and Roam.\nBut most existing PKM tools require manual curation. They’re great for organising knowledge you’ve already processed, less effective for bootstrapping that initial extraction from raw sources. What interests me is the hybrid approach: automated extraction that creates draft profiles and connections, which humans can then review, edit, and approve. Not pure automation, but intelligent assistance that handles the tedious first pass.\n\nThe ‘Agentic’ Moment\nWe’re entering what feels like a genuinely different phase of AI capability - the emergence of reliable vertical agents that can handle specific, complex workflows end-to-end. hinbox represents my attempt to explore what this might look like in practice for research applications. Rather than building with heavy agentic frameworks (which I haven’t found necessary yet and which fall in and out of favour too often for my tastes), I’m focusing on the core extraction and synthesis challenge.\nThis feels like the right moment to experiment with these capabilities. The models are sophisticated enough to handle nuanced entity recognition and relationship mapping, but the tooling is still flexible enough that you can build custom solutions for specific domains. It’s an interesting testing ground for understanding both the current state of the art and the practical challenges of deploying AI in knowledge-intensive workflows.\nThe goal isn’t necessarily to “solve” automated research (though that would be nice), but to build something concrete where I can systematically evaluate different approaches to prompt engineering, model selection, and error correction. Sometimes the best way to understand emerging capabilities is to push them against real problems you actually care about solving."
  },
  {
    "objectID": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#what-can-hinbox-do-now",
    "href": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#what-can-hinbox-do-now",
    "title": "Building hinbox: An agentic research tool for historical document analysis",
    "section": "What can hinbox do now?",
    "text": "What can hinbox do now?\nThe system centres around domain-specific configuration - you define the research area you’re interested in through a set of configuration files that specify your extraction targets and prompts. For my testing, I’ve been using Guantánamo Bay historical sources as the test domain since I can rigorously evaluate the quality of extractions in an area where I have deep expertise.\n\n\n\nAn example organisation profile\n\n\nSetting up a new research domain is straightforward: the system generates template configuration files with placeholders for all the necessary prompts. You customise these prompts to focus on the entities most relevant to your research - perhaps emphasising military personnel and legal proceedings for Guantánamo sources, or traders and agricultural cooperatives for Palestinian food history research.\nOnce configured, hinbox processes your document collection article by article, extracting people, organisations, locations, and events according to your specifications. The interesting part is the intelligent merging: rather than creating duplicate entries, the system attempts to recognise when newly extracted entities match existing profiles and updates them accordingly. This iterative enrichment means profiles become more comprehensive as you process additional sources.\n\n\n\nData processing logs\n\n\nThe system supports both cloud-based models (Gemini Flash 2.x has been particularly effective) and local processing through Ollama - crucial for researchers working with sensitive historical materials that can’t be sent to external APIs. Local models like gemma3:27b have proven surprisingly capable for this kind of structured extraction work.\nAfter processing, you get a web-based frontend for exploring the extracted knowledge base. Profiles include source attribution and version history, so you can track how understanding of particular entities evolved as new documents were processed. The entire output can be shared as a self-contained package - useful for collaborative research or creating supplementary materials for publications."
  },
  {
    "objectID": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#how-i-built-it",
    "href": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#how-i-built-it",
    "title": "Building hinbox: An agentic research tool for historical document analysis",
    "section": "How I built it",
    "text": "How I built it\nThis project became a practical testbed for several development tools I’d been wanting to explore seriously. Claude Code and Cursor proved invaluable for rapid iteration - the kind of back-and-forth refinement that complex NLP applications require would have taken significantly longer with traditional development approaches.\nFastHTML deserves particular mention for the frontend work. Building research interfaces without wrestling with JavaScript complexity felt genuinely liberating. The ability to create dynamic, interactive visualisations using primarily Python aligns well with how most researchers already think about data manipulation and presentation.\nThe current data architecture uses Parquet files throughout - a choice that might raise eyebrows but serves the development phase well. Direct file inspection and manipulation proved more valuable than database abstraction during rapid prototyping. Eventually, I’ll likely add SQLite backend options, but the current approach prioritises iteration speed over architectural elegance.\nThe entity merging logic required the most sophistication. The system combines simple string matching with embedding-based similarity search, then uses an LLM as final arbiter when potential matches are identified. A candidate profile gets compared against existing entities first through name similarity, then through vector comparison of full profile text. If similarity exceeds certain thresholds, both profiles are sent to the model with instructions to determine whether they represent the same entity and how to merge them if so.\nThis multi-stage approach handles the nuanced judgment calls that pure algorithmic matching struggles with - distinguishing between John Smith the journalist and John Smith the military contractor, or recognising that “Captain Rodriguez” from one article is the same person as “Maria Rodriguez” from another. The complexity here suggests this merging pipeline will be a primary focus for systematic evaluation and improvement as the project matures."
  },
  {
    "objectID": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#whats-up-next",
    "href": "posts/2025-05-29-hinbox-a-first-draft-of-an-agentic-research-system.html#whats-up-next",
    "title": "Building hinbox: An agentic research tool for historical document analysis",
    "section": "What’s up next?",
    "text": "What’s up next?\nThis blog post represents the softest possible launch - really more of a “here’s what I’m working on” update than any kind of formal announcement. hinbox isn’t ready for broad adoption yet, though I’d certainly welcome contributions and feedback from anyone interested in the problem space.\nThe immediate technical improvements are fairly straightforward. Right now, everything runs synchronously - each article gets processed sequentially to avoid the complexity of concurrent profile updates. Adding parallel processing would require implementing proper queuing or database locking mechanisms. Similarly, moving from Parquet files to a SQLite backend would provide better data management and enable more sophisticated querying patterns. Both changes would improve performance but add architectural complexity I haven’t needed while focusing on core functionality.\nI’m also eager to expand beyond newspaper articles to different document types - academic papers, book chapters, research reports, archival materials. Each format likely requires prompt refinements and possibly different extraction strategies. If this is going to be genuinely useful across research domains, it needs to handle the full spectrum of source materials historians and researchers actually work with.\n\nThe Real Work: Systematic Evaluation and Improvement\nBut the most interesting next phase involves applying systematic evaluation techniques from the AI evals course I mentioned earlier. This is where the project becomes genuinely educational rather than just another NLP application. I’ll be implementing structured approaches to:\n\nError analysis: Understanding exactly where and why entity extraction fails\nPrompt optimization: Systematic testing rather than intuitive iteration\n\nModel comparison: Rigorous evaluation across different architectures and providers\nMerging accuracy: Quantifying the quality of entity deduplication decisions\n\nThe goal is documenting this improvement process in detail through subsequent blog posts. Rather than abstract discussions of evaluation methodology, I want to show concrete examples of how these techniques apply to a real system with measurable outputs. What does systematic prompt engineering actually look like in practice? How do you design effective test suites for complex agentic pipelines? When do local models outperform cloud APIs for specific tasks?\n\n\nContext for Future Technical Content\nHonestly, the main reason for writing this overview wasn’t to launch anything - it was to establish context. I wanted a reference point for future technical posts that dive deep into evaluation methodology and iterative improvement without needing to repeatedly explain what hinbox is or why I’m working on it. The interesting content will be showing how systematic AI development practices apply to concrete research problems.\nThis feels like the right kind of project for exploring these questions: complex enough to surface real challenges, focused enough to enable rigorous evaluation, and personally meaningful enough to sustain the extended iteration cycles that proper system improvement requires. Plus, having worked extensively in the domain I’m testing makes it much easier to distinguish between genuine improvements and superficial metrics optimisation.\nMore technical deep-dives coming soon as the evals work progresses. The real learning happens in the systematic refinement process, not the initial build.\n\nThe hinbox repository is available on GitHub for anyone interested in following along or contributing. All feedback welcome as this evolves from prototype to something genuinely useful for research applications."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-braintrust-and-litellm.html",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-braintrust-and-litellm.html",
    "title": "Testing out instrumenting LLM tracing for litellm with Braintrust and Langfuse",
    "section": "",
    "text": "I previously tried (and failed) to setup LLM tracing for hinbox using Arize Phoenix and litellm. Since this is sort of a priority for being able to follow along with the Hamel / Shreya evals course with my practical application, I’ll take another stab using a tool with which I’m familiar: Braintrust. Let’s start simple and then if it works the way we want we can set things up for hinbox as well."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-braintrust-and-litellm.html#simple-braintrust-tracing-with-litellm-callbacks",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-braintrust-and-litellm.html#simple-braintrust-tracing-with-litellm-callbacks",
    "title": "Testing out instrumenting LLM tracing for litellm with Braintrust and Langfuse",
    "section": "Simple Braintrust tracing with litellm callbacks",
    "text": "Simple Braintrust tracing with litellm callbacks\nCallbacks are listed in the litellm docs as the way to do tracing with Braintrust. So we can do something like this:\nimport litellm\n\nlitellm.callbacks = [\"braintrust\"]\n\ncompletion_response = litellm.completion(\n    model=\"openrouter/google/gemma-3n-e4b-it:free\",\n    messages=[\n        {\n            \"content\": \"What's the capital of China? Just give me the name.\",\n            \"role\": \"user\",\n        }\n    ],\n    metadata={\n        # \"project_id\": \"1235-a70e-4571-abcd-234235\",\n        \"project_name\": \"hinbox\",\n    },\n)\nprint(completion_response.choices[0].message.content)\nYou can pass in a project_id or a project_name and the traces will be routed there. Here’s what it looks like in the Braintrust dashboard:\n\n\n\nOur first trace logged in Braintrust\n\n\nNote how you can’t see which model was used for the LLM call, nor any cost estimates. The docs mention that you can pass metadata into Braintrust using the metadata property:\n\n“braintrust_* - any metadata field starting with braintrust_ will be passed as metadata to the logging request” (link)\n\nThis seems a bit rudimentary, however. If we take a look at the full tracing documentation on the Braintrust docs we can see that they seem to recommend wrapping the OpenAI client object instead:\nimport os\n\nfrom braintrust import init_logger, traced, wrap_openai\nfrom openai import OpenAI\n\nlogger = init_logger(project=\"hinbox\")\nclient = wrap_openai(OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]))\n\n# @traced automatically logs the input (args) and output (return value)\n# of this function to a span. To ensure the span is named `answer_question`,\n# you should name the function `answer_question`.\n@traced\ndef answer_question(body: str) -&gt; str:\n    prompt = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": body},\n    ]\n\n    result = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=prompt,\n    )\n    return result.choices[0].message.content\n\ndef main():\n    input_text = \"What's the capital of China? Just give me the name.\"\n    result = answer_question(input_text)\n    print(result)\n\nif __name__ == \"__main__\":\n    main()\nThis indeed does label the span as answer_question but it doesn’t do much else. Even the model name isn’t captured here. Instrumenting a series of calls to handle ‘deeply nested code’ (as their docs puts it) even didn’t log the things it was supposed to:\n\nimport os\nimport random\n\nfrom braintrust import current_span, init_logger, start_span, traced, wrap_openai\nfrom openai import OpenAI\n\nlogger = init_logger(project=\"hinbox\")\nclient = wrap_openai(OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]))\n\n@traced\ndef run_llm(input):\n    model = \"gpt-4o\" if random.random() &gt; 0.5 else \"gpt-4o-mini\"\n    result = client.chat.completions.create(\n        model=model, messages=[{\"role\": \"user\", \"content\": input}]\n    )\n    current_span().log(metadata={\"randomModel\": model})\n    return result.choices[0].message.content\n\n@traced\ndef some_logic(input):\n    return run_llm(\"You are a magical wizard. Answer the following question: \" + input)\n\ndef simple_handler(input_text: str):\n    with start_span() as span:\n        output = some_logic(input_text)\n        span.log(input=input_text, output=output, metadata=dict(user_id=\"test_user\"))\n        print(output)\n\nif __name__ == \"__main__\":\n    question = \"What's the capital of China? Just give me the name.\"\n    simple_handler(question)\nThis is adapted from the example they pasted in their docs as their one isn’t even a functional code example on its own.\nIt is seeming increasingly clear that Braintrust isn’t going to be the right choice, at least as long as I want to keep using litellm. I know that Langfuse has a very nice integration with litellm, so I think I’ll pivot over to that now."
  },
  {
    "objectID": "posts/2025-06-04-instrumenting-an-agentic-app-with-braintrust-and-litellm.html#basic-tracing-with-langfuse-and-litellm",
    "href": "posts/2025-06-04-instrumenting-an-agentic-app-with-braintrust-and-litellm.html#basic-tracing-with-langfuse-and-litellm",
    "title": "Testing out instrumenting LLM tracing for litellm with Braintrust and Langfuse",
    "section": "Basic tracing with Langfuse and litellm",
    "text": "Basic tracing with Langfuse and litellm\nSimple tracing is easy:\nimport litellm\n\nlitellm.callbacks = [\"langfuse\"]\n\ndef query_llm(prompt: str):\n    completion_response = litellm.completion(\n        model=\"openrouter/google/gemma-3n-e4b-it:free\",\n        messages=[\n            {\n                \"content\": \"What's the capital of China? Just give me the name.\",\n                \"role\": \"user\",\n            }\n        ],\n    )\n    return completion_response.choices[0].message.content\n\ndef my_llm_application():\n    query1 = query_llm(\"What's the capital of China? Just give me the name.\")\n    query2 = query_llm(\"What's the capital of Japan? Just give me the name.\")\n    return (query1, query2)\n\nprint(my_llm_application())\nWe specify langfuse for the callback and each llm call is logged as a separate trace + span. Here you can see what this looks like in the dashboard:\n\n\n\nBasic trace and span in Langfuse dashboard\n\n\nThe litellm docs include information on how to specify custom metadata and grouping instructions for Langfuse. Notably, we can specify (as of June 2025, at least!) things like a session_id, tags, a trace_name and/or trace_id as well as custom trace metadata and so on. So we can get most of what we want to specify in the following way:\nimport litellm\n\nlitellm.callbacks = [\"langfuse\"]\n\ndef query_llm(prompt: str, trace_id: str):\n    completion_response = litellm.completion(\n        model=\"openrouter/google/gemma-3n-e4b-it:free\",\n        messages=[\n            {\n                \"content\": \"What's the capital of China? Just give me the name.\",\n                \"role\": \"user\",\n            }\n        ],\n        metadata={\n            \"trace_id\": trace_id,\n            \"trace_name\": \"my_llm_application\",\n            \"project\": \"hinbox\",\n        },\n    )\n    return completion_response.choices[0].message.content\n\ndef my_llm_application():\n    query1 = query_llm(\n        \"What's the capital of China? Just give me the name.\",\n        \"my_llm_application_run_789\",\n    )\n    query2 = query_llm(\n        \"What's the capital of Japan? Just give me the name.\",\n        \"my_llm_application_run_789\",\n    )\n    return (query1, query2)\n\nif __name__ == \"__main__\":\n    print(my_llm_application())\nThis looks like this in the Langfuse dashboard:\n\n\n\nSpans grouped into traces\n\n\nThis is honestly most of what I’m looking for in terms of my tracing. If I were to use a non-OpenRouter model, moreover, I’d also get full costs in the Langfuse dashboard, e.g.:\n\n\n\nLLM costs in Langfuse dashboard\n\n\nAs such, I can monitor costs from within OpenRouter and have the option to keep track of costs in Langfuse by passing custom metadata should I wish.\nI’ll make a separate blog where I actually go into how I set up + instrumented hinbox for this kind of tracing while continuing to use litellm."
  },
  {
    "objectID": "til/2024-09-16-what-is-the-rust-prelude.html",
    "href": "til/2024-09-16-what-is-the-rust-prelude.html",
    "title": "What is the Rust prelude?",
    "section": "",
    "text": "I’m studying Rust these days on the side and one thing that I keep hearing and seeing is the idea of the ‘prelude’. I thought I’d write a quick blog to cement exactly what’s going on here.\nAt a very high level, the prelude is a bunch of functions, methods and other things that are automatically available to you when you start working on your project without you having to manually or explicitly import them. As the Rust docs state:\n\n“The prelude is the list of things that Rust automatically imports into every Rust program. It’s kept as small as possible, and is focused on things, particularly traits, which are used in almost every single Rust program.”\n\nI thought maybe a good example of this is the classic ‘Hello, World!’ starter when you create a new project using cargo new ...:\nfn main() {\n    println!(\"Hello, World!\")\n}\nSo here we have println! which is actually a macro, and from what I read this is not part of the prelude, though it is available to us by default.\nA better / actual list of some things that are made available would include some types like Option, Result, String and Vec, as well as some traits like Copy, Clone, Eq and so on. For a full list, refer to the official prelude contents as listed in the docs. Note that there are several versions (2015, 2018, 2021 etc) of the prelude. My understanding is that each successive version only adds new things that are exported by default. If that wasn’t the case, then I’m guessing it would be hard to provide those solid backwards-compatibility guarantees.\nSo basically, there are some symbols or imports that were deemed to be used so often that they decided not to force you to have to import them explicitly every time you want to get started writing code."
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html",
    "href": "posts/2021-09-16-ch4-tensors.html",
    "title": "Tensors all the way down",
    "section": "",
    "text": "Code\n#!pip install -Uqq fastbook\n#!pip install fastai\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\nIn chapter 4 of the book, we start to really get into what’s going on under the hood with deep learning. Turns out, tensors are a pretty important piece. We are still in the realm of computer vision, and we are going to work on distinguishing between handwritten digits.\nFirst we use the untar_data function to grab a sample of data from the famous MNIST data set. This function returns the path where that data was stored locally.\npath = untar_data(URLs.MNIST_SAMPLE)\nPath.BASE_PATH = path\npath\n\nPath('.')\nNow we want to briefly inspect the contents of one of our training data folders. This is for the number 7. You can see that it’s just a series of .png image files.\nthrees_dir = (path/'train/3').ls().sorted()\nsevens_dir = (path/'train/7').ls().sorted()\nsevens_dir\n\n(#6265) [Path('train/7/10002.png'),Path('train/7/1001.png'),Path('train/7/10014.png'),Path('train/7/10019.png'),Path('train/7/10039.png'),Path('train/7/10046.png'),Path('train/7/10050.png'),Path('train/7/10063.png'),Path('train/7/10077.png'),Path('train/7/10086.png')...]\nIn order to look at a single image, we can just open it using Image.open which comes from the Python Image Library (PIL).\nim3_path = threes_dir[1]\nim3 = Image.open(im3_path)\nim3\nJupyter knows how to display various files, so we can see that image above. But what exactly is an image made up of? If we turn that image into an array, or to a tensor (the next two cells), slicing them so you aren’t just seeing zeros on the edges, then you can see that these images are made up of a matrix of values from 0 to 255.\nim3_arr = array(im3)[4:10, 4:10]\nim3_arr\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\nim3_tns = tensor(im3)[4:10, 4:10]\nim3_tns\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\nWe can use the show_image function to turn those 0-255 values back into an image, like so:\nshow_image(im3_arr)\nA really nice way of visualising exactly what is going on is to turn this image into a pandas dataframe and then for every individual pixel value, use that value as the background gradient for that cell. Here’s an example of part of an image of a handwritten number 3.\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0\nSo now we have a toolkit of ways to view the pixel values that make up an image. We also have a mental model for how we can think about images and how computers represent those images stored on our machine.\nBut how might we then best go about knowing whether a particular image is a 3, let’s say, or a 7?\nOne naive approach might be just to get the average value for each individual pixel for all of the threes in our training data, and then just compare the difference between our sample image and this average representation.\nLet’s try that now."
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html#getting-the-average-values-for-our-images",
    "href": "posts/2021-09-16-ch4-tensors.html#getting-the-average-values-for-our-images",
    "title": "Tensors all the way down",
    "section": "Getting the average values for our images",
    "text": "Getting the average values for our images\nWe’ll set up two lists with images of the digits converted to tensors. You can see that we have 6131 images in our ‘threes’ list.\n\nthrees_tensors = [tensor(Image.open(i)) for i in threes_dir]\nsevens_tensors = [tensor(Image.open(i)) for i in sevens_dir]\nlen(threes_tensors)\n\n6131\n\n\nWe can view an individual image, as before, with the show_image function:\n\nshow_image(threes_tensors[3])\n\n\n\n\n\n\n\n\nNow in order to get the average values for each pixels, we can use the stack method to handle the first part of this.\nThink of it as basically adding an extra dimension to your data structure, such that you have a ‘stack’ (it’s a useful mental image) of those images.\n\nthrees_stack = torch.stack(threes_tensors)\n\nIf we look at the shape of our Pytorch stack now, we can see we have our 28x28 image, but we have a stack of 6131 of them.\n\nthrees_stack.shape\n\ntorch.Size([6131, 28, 28])\n\n\nEach individual image is still a tensor:\n\na_three = threes_stack[3][4:16, 4:16]\na_three\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0, 104, 253, 253, 253, 255, 253],\n        [  0,   0,   0,   0,   0, 178, 248, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 186, 252, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 186, 252, 243, 172, 172,  39,  39],\n        [  0,   0,   0,   0,   0,  39,  53,  47,   0,   0,   0,  29],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  54, 208],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   3,  41, 253, 252],\n        [  0,   0,   0,   0,   0,   0,   5,  41, 165, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 109, 163, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 186, 252, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 187, 253, 253, 253, 253, 134,  77]], dtype=torch.uint8)\n\n\nGenerally speaking, for some operations (like getting the mean average) we’re going to want to convert the values to floats, and it also makes sense to normalise the values at the same time. Instead of having a range of 0-255, we want a range of 0-1.\n\nthrees_stack[3][4:16, 4:16].float()/255\n\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9725, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9529, 0.6745, 0.6745, 0.1529, 0.1529],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.2078, 0.1843, 0.0000, 0.0000, 0.0000, 0.1137],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8157],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.1608, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.1608, 0.6471, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7333, 0.9922, 0.9922, 0.9922, 0.9922, 0.5255, 0.3020]])\n\n\nNow that we’ve done it for a single image, we can perform the same operations on our whole Pytorch stack.\n\nthrees_stack = torch.stack(threes_tensors).float()/255\nsevens_stack = torch.stack(sevens_tensors).float()/255\nthrees_stack.shape # it's good to keep in touch with the shape of our stack\n\ntorch.Size([6131, 28, 28])\n\n\nNow we’re getting closer to our desired result. We can squash the stack down into just two dimensions with a simple call to .mean(0), where 0 is the index value of the dimension through which we want to calculate the mean. You’ll see now that the shape property of our threes_means variable is simply a 28x28 image.\n\nthrees_means = threes_stack.mean(0)\nthrees_means.shape\n\ntorch.Size([28, 28])\n\n\nWhen we show that image, you’ll see that it’s a sort of blurry ‘ideal’ version of a three\n\nshow_image(threes_means)\n\n\n\n\n\n\n\n\nWe can do the same for the sevens:\n\nsevens_means = sevens_stack.mean(0)\nshow_image(sevens_means)"
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html#validation-comparing-our-average-three-with-a-specific-three",
    "href": "posts/2021-09-16-ch4-tensors.html#validation-comparing-our-average-three-with-a-specific-three",
    "title": "Tensors all the way down",
    "section": "Validation: Comparing our average three with a specific three",
    "text": "Validation: Comparing our average three with a specific three\nNow we have our average values, we want to compare these with a single specific digit image. We’ll get the difference between those values and whichever difference is the smallest will most likely be the best answer.\nOur averaged three is still threes_means and we can get a single three from our validation set like this:\n\nthrees_dir_validation = (path/'valid/3').ls().sorted()\nsevens_dir_validation = (path/'valid/7').ls().sorted()\n\nim3_validation_path = threes_dir_validation[5]\nim3_validation = tensor(Image.open(im3_validation_path)).float()/255\n\nim7_validation_path = sevens_dir_validation[3]\nim7_validation = tensor(Image.open(im7_validation_path)).float()/255\n\nshow_image(im3_validation)\n\n\n\n\n\n\n\n\n\nshow_image(im7_validation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCalculating the difference between two objects\n\n\nWe can use two different measurements of the difference between our mean value and the individual image:\n\nmean absolute difference (calculated by taking the mean of the absolute difference between the two tensor values). Also known as the L1 Norm.\nroot mean squared error (calculated by first squaring the difference between the two tensor values, taking the mean and then square rooting those values). Also known as the L2 Norm.\n\nThe second option, the RMSE, gives a stronger signal, you might say, for the differences because you are taking the averages from the squared values. Squaring the difference also takes care of any negative values you might have.\n\nmean_absolute_difference_3 = (im3_validation - threes_means).abs().mean()\nroot_mean_squared_error_3 = ((im3_validation - threes_means)**2).mean().sqrt()\nmean_absolute_difference_3, root_mean_squared_error_3\n\n(tensor(0.1188), tensor(0.2160))\n\n\n\nmean_absolute_difference_7 = (im7_validation - threes_means).abs().mean()\nroot_mean_squared_error_7 = ((im7_validation - threes_means)**2).mean().sqrt()\nmean_absolute_difference_7, root_mean_squared_error_7\n\n(tensor(0.1702), tensor(0.3053))\n\n\nWe can now see that our individual three image is indeed closer to the threes_means composite image than to the sevens_means composite image. A smaller value at this point is what we’re looking for, and the threes have it.\nIt turns out that there is another way to calculate the difference that’s built in to Pytorch as loss functions:\n\nF.l1_loss(im3_validation, threes_means), F.mse_loss(im3_validation, threes_means).sqrt()\n\n(tensor(0.1188), tensor(0.2160))\n\n\nIt’s a bit more concise, though it does obscure what’s going on under the hood in terms of calculations."
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html#results-of-the-naive-approach",
    "href": "posts/2021-09-16-ch4-tensors.html#results-of-the-naive-approach",
    "title": "Tensors all the way down",
    "section": "Results of the naive approach",
    "text": "Results of the naive approach\nSo this tells us that our single three is closer to an ideal 3 than an ideal 7, which is great since it reflects the ground truth of our problem. But can we get a metric to know how well we perform on average against a large number of threes and sevens from our validation set?\nYes, since we have that dataset ready for use!\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\nNow we can write a helper function that will allow us to calculate the distance between two images. We use the RMSE or L1 Norm for this difference calculation:\n\ndef mnist_distance(a, b):\n    return (a - b).abs().mean((-1, -2))\n\nWe can use this function on our previous example:\n\nmnist_distance(im3_validation, threes_means)\n\ntensor(0.1188)\n\n\nWe can continue onwards by comparing the rank 3 tensor with the rank 2 tensor. This brings a concept called ‘broadcasting’ into play.\nWe are comparing a tensor with 2 dimensions with a tensor with 3 dimensions, so Pytorch behaves as if both tensors have three dimensions, and (without taking extra memory) pretends as if there are multiple copies of the image in 2 dimensions. This effectively makes it as if we’re comparing two 3-dimensional tensors.\nFrom this next calculation, we see returned back a collection of the distances between all of the validation images.\n\nmnist_distance(valid_3_tens, threes_means)\n\ntensor([0.1328, 0.1523, 0.1245,  ..., 0.1383, 0.1280, 0.1138])\n\n\nIn order to check whether an image is a 3, we basically need to know whether the difference for the number 3 is larger than the difference for the number 7.\nWe can write a helper function for that:\n\ndef is_3(img):\n    return mnist_distance(img, threes_means) &lt; mnist_distance(img, sevens_means)\n\nWe can now check our ground truth examples:\n\nis_3(im3_validation), is_3(im7_validation)\n\n(tensor(True), tensor(False))\n\n\nThat’s what we expected to happen. Our 3 image is a 3, and our 7 image is not a 3.\nIf we want to check the distance in general for our validation set, we have to convert them into floats and then get the mean, but it’s really easy. Again, this uses broadcasting:\n\nvalidation_accuracy_3 = is_3(valid_3_tens).float().mean()\nvalidation_accuracy_7 = 1 - is_3(valid_7_tens).float().mean()\nvalidation_accuracy_3, validation_accuracy_7\n\n(tensor(0.9168), tensor(0.9854))\n\n\nOverall, then, we can calculate how good our toy or baseline model is for the entire problem:\n\n(validation_accuracy_3 + validation_accuracy_7) / 2\n\ntensor(0.9511)\n\n\nPretty good!\nThis was of course just a naive way to solve the problem. There are more advanced techniques which we’ll tackle next."
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nIn the previous post I showed a naive approach to calculating the similarity or difference between images, and how that could be used to create a function that did pretty well at estimating whether any particular image was a pullover or a dress.\nChapter 4 of the fastbook then takes us on a journey showing a smarter approach where the computer can make even better estimations and predictions. The broad strokes of this approach are simple to grasp, but of course the individual details are where the nuances of machine learning are to be found."
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html#setup-add-.requires_grad_-to-a-tensor",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html#setup-add-.requires_grad_-to-a-tensor",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "1. Setup: add .requires_grad_() to a tensor",
    "text": "1. Setup: add .requires_grad_() to a tensor\nFor any Tensor where we know we’re going to want to calculate the gradients of values, we call .require_grad() on that Tensor.\n\n# we define a simple function\ndef f(x):\n    return x**2\n\nx_tensor = torch.tensor(3.).requires_grad_()\n\ny_tensor = f(x_tensor)\n\ny_tensor\n\ntensor(9., grad_fn=&lt;PowBackward0&gt;)\n\n\nHere we can see that 3 squared is indeed 9, and we can see the grad_fn as part of the Tensor."
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html#use-.backward-to-calculate-the-gradient",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html#use-.backward-to-calculate-the-gradient",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "2. Use .backward() to calculate the gradient",
    "text": "2. Use .backward() to calculate the gradient\nThis actually refers to backpropagation, something which is explained much later in the book. This step is also known as the ‘backward pass’. Note, that this is again another piece of jargon that we just have to learn. In reality this method might as well have been called .calculate_gradients().\n\ny_tensor.backward()"
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html#access-the-gradient-via-the-.grad-attribute",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html#access-the-gradient-via-the-.grad-attribute",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "3. Access the gradient via the .grad attribute",
    "text": "3. Access the gradient via the .grad attribute\nWe view the gradient by checking this .grad attribute.\n\nx_tensor.grad\n\ntensor(6.)\n\n\nI can’t explain why this is the case, since I’ve never learned how to calculate gradients or derivatives (or anything about calculus, for that matter!) but in any case it’s not really important.\nNote that we can do this whole process over Tensors that are more complex than illustrated in the above simple example:\n\ncomplex_x = tensor([3., 5., 12.]).requires_grad_()\n\ndef f(x):\n    return (x**2).sum()\n\ncomplex_y = f(complex_x)\ncomplex_y.backward()\ncomplex_x.grad\n\ntensor([ 6., 10., 24.])\n\n\nSomething else I discovered while doing this was that gradients can only be calculated on floating point values, so this is why when we create x_tensor and complex_x we create them with floating point values (3. etc) instead of just integers. In reality, I think there will be some kind of normalisation of our values as part of the process, so they would probably already be floats, but it’s worth noting."
  },
  {
    "objectID": "posts/2022-05-14-sgd-fashion-mnist.html",
    "href": "posts/2022-05-14-sgd-fashion-mnist.html",
    "title": "Using the seven-step SGD process for Fashion MNIST",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\nIn the previous post I used the seven-step process to fit to an unknown function. The process as a whole is fairly simple to get your head around, but there are a good few details to keep track of along the way. This will continue to be the case as we get into this walkthrough of how to do the same for the Fashion MNIST pullover vs dress data.\n\nGetting our data into the right format\nThe first thing we need to handle is making sure our data is in the right format, shape and so on. We begin by downloading our data and splitting the data into training and test sets.\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntraining_dresses = [item[0][0] for item in training_data if item[1] == 3]\ntraining_pullovers = [item[0][0] for item in training_data if item[1] == 2]\ntest_dresses = [item[0][0] for item in test_data if item[1] == 3]\ntest_pullovers = [item[0][0] for item in test_data if item[1] == 2]\n\ntraining_dresses_tensor = torch.stack(training_dresses)\ntraining_pullovers_tensor = torch.stack(training_pullovers)\ntest_dresses_tensor = torch.stack(test_dresses)\ntest_pullovers_tensor = torch.stack(test_pullovers)\n\ntraining_dresses_tensor.shape, test_dresses_tensor.shape\n\n(torch.Size([6000, 28, 28]), torch.Size([1000, 28, 28]))\n\n\n\ntrain_x = torch.cat([training_dresses_tensor, training_pullovers_tensor]).view(-1, 28*28)\ntrain_y = torch.cat([torch.ones(len(training_dresses)), torch.zeros(len(training_pullovers))]).unsqueeze(1)\n\nvalid_x = torch.cat([test_dresses_tensor, test_pullovers_tensor]).view(-1, 28*28)\nvalid_y = torch.cat([torch.ones(len(test_dresses)), torch.zeros(len(test_pullovers))]).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12000, 784]), torch.Size([12000, 1]))\n\n\nWe transform our images tensors from matrices into vectors with all the values one after another. We create a train_y vector with our labels which we can use to check how well we did with our predictions.\nWe create datasets out of our tensors. This means that we can feed our data into our training functions in the way that is most convenient (i.e. an image is paired with the correct label).\n\ntrain_dset = list(zip(train_x, train_y))\nvalid_dset = list(zip(valid_x, valid_y))\n\n\n\nInitialising our weights and bias\nAs in the previous times where we’ve done this, we initialise our parameters or weights with random values. This means that for every pixel represented in the images, we’ll start off with purely random values. We initialise our bias to a random number as well.\n\ndef initialise_params(size, std=1.0):\n    return (torch.randn(size) * std).requires_grad_()\n\nweights = initialise_params((28*28, 1))\nbias = initialise_params(1)\n\n\n# calculating a prediction for our first image\n(train_x[0]*weights.T).sum() + bias\n\ntensor([2.8681], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nMatrix multiplication to calculate our predictions\nWe’ll need to make many calculations like the one we just made, and luckily the technique of matrix multiplication helps us with exactly the scenario we have: we want to multiply the values of our image (laid out in a single vector) with the weights and to add the bias.\nIn Python, matrix multiplication is carried out with a simple @ operator, so we can bring all of this together as a function:\n\ndef linear1(x_batch):\n    return x_batch@weights + bias\n\npreds = linear1(train_x)\npreds\n\ntensor([[  2.8681],\n        [ -7.6810],\n        [-17.5719],\n        ...,\n        [ -3.8665],\n        [  2.0646],\n        [ -2.5148]], grad_fn=&lt;AddBackward0&gt;)\n\n\nWe can check our accuracy for these predictions:\n\ncorrects = (preds &gt; 0.0).float() == train_y\ncorrects.float().mean().item()\n\n0.35324999690055847\n\n\nOur accuracy is pretty poor! A lot worse than even 50/50 luck which is what you’d expect to get on average from a random set of initial weights. Apparently we had a bad draw of luck this time round!\n\n\nA loss function to evaluate model performance\nWe now need a loss function which will tell us how well we are doing in our predictions, and that can be used as part of the gradient calculations to let us know (as we iterate) how to update our weights.\nThe problem, especially in the data set we’re working with, is that we have a binary probability: either it’s a dress or a pullover. Zero or one. Unlike in a regression problem, or something similar, we don’t have any smooth selection of contiguous values that get predicted. We have zero or one.\nAt this point we learn about the sigmoid function which is a way to reframe this problem in a way that we can use to our advantage. The sigmoid function when plotted looks like this:\n\ndef sigmoid(x):\n    return 1/(1+torch.exp(-x))\n\nplot_function(torch.sigmoid, title=\"Sigmoid\", min=-5, max=5)\n\n\n\n\n\n\n\n\nThis function, as you can see, takes any input value and squashes it down such that the output value is between 0 and 1. It also has a smooth curve, all headed in the same direction, between those values. This is ideal for our situation. The first thing we must do as part of our loss function, therefore, is to apply the sigmoid function to the inputs.\n\ndef fashion_mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1 - predictions, predictions).mean()\n\nThis torch.where(...) function is a handy way of iterating through all our data, checking whether our target is 1 or not, then outputting the distance from the correct prediction and calculating the mean of these predictions across the entire dataset.\n\n\nDataLoaders and Datasets\nWe’ve already created datasets for our training and validation data. The process of iterating through our data, however, requires some thought as to how we’ll do it. Our options:\n\nwe could iterate through the entire dataset, making the relevant loss and gradient calculations and adjusting the weights but this might make the process quite long, even though we’d benefit from the increased accuracy this would bring since we’d be seeing the entire dataset each iteration.\nwe could do our calculations after just seeing a single image, but then our model would be over-influenced and perturbed by the fluctuations from image to image. This also wouldn’t be what we want.\n\nIn practice, we’ll need to choose something in between. This is where mini-batches or just ‘batches’ come in. These will be need to be large enough (and randomly populated!) that our model can meaningfully learn from them, but not so large that our process takes too long.\nLuckily, we have the abstraction of the DataLoader which will create all our randomly assigned batches for us.\n\ntrain_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\n\n\n\nTraining our model\nNow we can bring the whole process together and train our model:\n\nweights = initialise_params((28*28, 1))\nbias = initialise_params(1)\n\ndef calculate_gradient(x_batch, y_batch, model):\n    preds = model(x_batch)\n    loss = fashion_mnist_loss(preds, y_batch)\n    loss.backward()\n\ndef train_epoch(model, learning_rate, params):\n    # iterate over the training data, batch by batch\n    for x_batch, y_batch in train_dl:\n        # calculate the gradients\n        calculate_gradient(x_batch, y_batch, model)\n        \n        for param in params:\n            param.data -= param.grad * learning_rate\n            # set the gradients to zero\n            param.grad.zero_()\n\ndef batch_accuracy(x_batch, y_batch):\n    preds = x_batch.sigmoid()\n    correct = (preds &gt; 0.5) == y_batch\n    return correct.float().mean()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(x_batch), y_batch) for x_batch, y_batch in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nvalidate_epoch(linear1)\n\n0.2173\n\n\nWe start there, but now we can train and watch our accuracy improving:\n\nlearning_rate = 1.\nparams = weights, bias\n\nfor _ in range(30):\n    train_epoch(linear1, learning_rate, params)\n    print(validate_epoch(linear1), end=\" \")\n\n0.5001 0.5016 0.7994 0.9357 0.9481 0.9523 0.9537 0.9555 0.9572 0.9582 0.9578 0.9604 0.9608 0.9609 0.962 0.9611 0.9622 0.9626 0.9631 0.9625 0.963 0.963 0.9633 0.9638 0.964 0.9631 0.9638 0.9638 0.9643 0.9645 \n\n\nWe had 91% accuracy on our validation dataset last time we tried this with pixel similarity.\nAfter 30 epochs of training with our new process we’ve achieved 96%, but we could still do better! We’ll tackle that in the next post.\n\n\nOptimising with an Optimiser\nEverything that we’ve been doing so far is so common that there is pre-built functionality to handle all of the pieces.\n\nour linear1 function (which calculated predictions based on our weights and biases) can be replaced with PyTorch’s nn.Linear module. Actually, nn.Linear does the same thing as our initialise_params and our linear1 function combined.\n\n\n# initialises our weights and bias, and is our model / function\nlinear_model = nn.Linear(28*28, 1)\n\nOur PyTorch module carries an internal representation of our weights and our biases:\n\nweights, bias = linear_model.parameters()\nweights.shape, bias.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\n\nan optimiser bundles the step functionality and the zero_grad_ functionality. In the book we see how to create our own very basic optimiser, but fastai provides the basic SGD class which we can use that handles these same behaviours.\n\nWe’ll need to amend our training function a little to take this into account:\n\nlinear_model = nn.Linear(28*28, 1)\nopt = SGD(linear_model.parameters(), learning_rate)\n\ndef train_epoch(model):\n    for x_batch, y_batch in train_dl:\n        calculate_gradient(x_batch, y_batch, model)\n        opt.step()\n        opt.zero_grad()\n\ndef train_model(model, epochs):\n    for _ in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=\" \")\n\ntrain_model(linear_model, 30)\n\n0.96 0.9642 0.966 0.9659 0.9663 0.9672 0.9677 0.9668 0.9678 0.9684 0.9681 0.9674 0.9681 0.9671 0.9678 0.9677 0.9684 0.968 0.9687 0.9677 0.9681 0.968 0.9693 0.9684 0.968 0.9686 0.9688 0.9693 0.9698 0.9697 \n\n\n\n\nSome extra fastai abstractions\nfastai handles so much of this for us all, such that the Learner is actually the thing we can use to get all of the above logic built in.\nThe Learner takes all of the pieces that we’ve spent the last few blogs creating:\n\nthe DataLoaders (iterators providing the data in batches, in the right format with paired x and y values)\nthe model itself (our function that we’re trying to optimise)\nthe optimisation function (which receives our weights and bias parameters as well as the learning rate)\nthe loss function\nany optional metrics we want printed\n\n\ndls = DataLoaders(train_dl, valid_dl)\nlearn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=fashion_mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(15, lr = learning_rate)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.063289\n0.045487\n0.963500\n00:00\n\n\n1\n0.044268\n0.042236\n0.964500\n00:00\n\n\n2\n0.037112\n0.040228\n0.965500\n00:00\n\n\n3\n0.034010\n0.038743\n0.967000\n00:00\n\n\n4\n0.032013\n0.038781\n0.966000\n00:00\n\n\n5\n0.030633\n0.037635\n0.966500\n00:00\n\n\n6\n0.030458\n0.037530\n0.967500\n00:00\n\n\n7\n0.029747\n0.036593\n0.967000\n00:00\n\n\n8\n0.029511\n0.036479\n0.967500\n00:00\n\n\n9\n0.029305\n0.035645\n0.967500\n00:00\n\n\n10\n0.028643\n0.035400\n0.966500\n00:00\n\n\n11\n0.028562\n0.035477\n0.966500\n00:00\n\n\n12\n0.028788\n0.035191\n0.968000\n00:00\n\n\n13\n0.028261\n0.034843\n0.968000\n00:00\n\n\n14\n0.028172\n0.034883\n0.968500\n00:00\n\n\n\n\n\nSo there we have it. We learned how to create a linear learner. Obviously 96.8% accuracy is pretty good, but it could be better. Next time we’re going to add the final touches to this process by creating a neural network, adding layers of nonlinearity to ensure our function can fit the complex patterns in our data."
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "",
    "text": "I’ve previously written about my use of fastai’s vision_learner to create a classification model that was pretty good (&gt; 95% accuracy) at detecting whether an image contained redactions or not.\nThis week in the course we switched domains and got to know HuggingFace’s transformers library as a pathway into NLP (natural language processing) which is all about text inputs. I struggled quite a bit trying to think of interesting yet self-contained / small uses of NLP that I could try out this week. A lot of the common uses for simple NLP modelling seem to be in the area of things like ‘sentiment analysis’ where I couldn’t really see something I could build. Also there are a lot of NLP uses cases which feel unethical or creepy (perhaps more so than in the computer vision, it felt to me).\nI emerged at the end of this thought process with the idea to try to pit image classification and text classification against one another: could I train an NLP model that would outperform my image classifier in detecting whether a specific document or page contains a redaction or not?\nOf course, the first thing I had to do was to OCR all the pages in my image dataset and convert this all into a text dataset. When it comes to OCR tools, there are a number of different options available and I’d luckily experimented around with them. (A pretty useful overview of three leading options can be found in this blogpost by Francesco Pochetti.) I went with Tesseract as I knew had pretty good performance and accuracy for English-language documents.\nMy process for converting the documents wasn’t particularly inspired. Essentially I just loop over the image files one by one, run the OCR engine over them to extract the text and then create a new .txt file with the extracted text. At the end, I had two folders with my data, one containing texts whose corresponding images I knew had contained redactions, and one where there were no redactions.\nI had two hunches that I hoped would help my NLP model.\nWhat follows is my attempt to follow steps initially outlined in Jeremy Howard’s Kaggle notebook that the course reviewed this week in the live lesson. My code doesn’t depart from the original notebook much.\nCode\n!pip install datasets transformers tokenizers -Uqq\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nI save my .txt files on the machine and I get a list of all the paths of those files.\npath = Path(\"redaction_texts\")\np = path.glob(\"**/*.txt\")\nfiles = [x for x in p if x.is_file()]\nI iterate through all the paths, making of list of all the redacted texts as strings.\ntexts = []\nfor file_path in files:\n    with open(file_path) as file:\n        texts.append(file.read())\n!ls {path}\n\n\nredacted   unredacted"
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html#converting-text-files-into-a-pandas-dataframe",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html#converting-text-files-into-a-pandas-dataframe",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "Converting text files into a Pandas DataFrame",
    "text": "Converting text files into a Pandas DataFrame\nI needed a way of obtaining the labels for my dataset. These labels were the parent label for each path name. The training process below needed the labels to be floats.\n\ndef is_redacted(path):\n    \"Extracts the label for a specific filepath\"\n    if str(path.parent).split(\"/\")[-1] == \"redacted\":\n        return float(1)\n    else:\n        return float(0)\n\nis_redacted(files[1])\n\n0.0\n\n\nConverting a Python dict into a Pandas DataFrame is pretty simple as long as you provide the data in the right formats. I had to play around with this a little when I was getting this to work.\n\ndata = {\n    \"input\": texts,\n    \"labels\": [is_redacted(path) for path in files],\n}\n\n\ndf = pd.DataFrame(columns=[\"input\", \"labels\"], data=data)\n# df\n\n\ndf.describe(include='object')\n\n\n\n\n\n\n\n\ninput\n\n\n\n\ncount\n3886\n\n\nunique\n3830\n\n\ntop\n\n\n\nfreq\n35\n\n\n\n\n\n\n\nWe now have a DataFrame containing 3886 rows of data. You can see here that 35 rows have no visible text. Potentially something went wrong with the OCR extraction, or the redaction covered the entire image. I didn’t really know or want to fiddle around with that too much, so I left those rows in."
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html#moving-into-hf-transformers-land",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html#moving-into-hf-transformers-land",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "Moving into HF Transformers Land",
    "text": "Moving into HF Transformers Land\nWe create a Dataset object from our DataFrame. It requires that our targets have the column name labels.\n\nfrom datasets import Dataset, DatasetDict\n\nds = Dataset.from_pandas(df)\n\n\nds\n\nDataset({\n    features: ['input', 'labels'],\n    num_rows: 3886\n})\n\n\nWe’re finetuning a pre-trained model here, so I start with the small version of Deberta which will allow me (I hope!) to iterate quickly and come up with an initial baseline and sense of whether this is even a viable approach to solving the problem.\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\nBefore we finetune our model, we have to do two things to our text data in order that it works within our gradient descent powered training process:\n\nwe have to tokenise our text data\nwe have to turn those tokens into numbers so they can be crunched within our GPU as numbers.\n\nTokenisation is the process of splitting our words into shorter stubs of text – there are varying schools of thought and use cases on the extent to which you break the words down. We have to use the same tokenisation process that was used by our pretrained model, so we let transformers grab the original tokenisers that was used with deberta-v3-small.\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\ndef tok_func(x): return tokz(x[\"input\"])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\nWe split our data into training and validation subsets as per usual so that we know how our model is doing while training.\n\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2914\n    })\n    test: Dataset({\n        features: ['input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 972\n    })\n})\n\n\nWe define our metric as Pearson’s r AKA the Pearson correlation coefficient, a metric I don’t feel an immense instinctual understanding for, but suffice it for this blogpost to know that a higher value (up to a maximum of 1) is better.\n\ndef corr(x, y):\n    return np.corrcoef(x, y)[0][1]\n\n\ndef corr_d(eval_pred):\n    return {\"pearson\": corr(*eval_pred)}\n\n\nfrom transformers import TrainingArguments,Trainer\n\nHere we define our batch size, the number of epochs we want to train for as well as the learning rate. The defaults in Jeremy’s NLP notebook were far higher than what you see here. His batch size was 128. When I ran the cells that follow, I hit the infamous “CUDA out of memory” error more or less immediately. I was running on a machine with a 16GB RAM GPU, but this apparently wasn’t enough and the batch size was far too large. I had to reduce it down to 4, as you can see, in order to even be able to train the model. There are tradeoffs to this in terms of how well the model learns, but without spending lots of money on fancy machines this was the compromise I had to make.\n\nbs = 4\nepochs = 5\nlr = 1e-4\n\n\nargs = TrainingArguments(\n    \"outputs\",\n    learning_rate=lr,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=bs * 2,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    report_to=\"none\",\n)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_nm, num_labels=1\n)\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=dds[\"train\"],\n    eval_dataset=dds[\"test\"],\n    tokenizer=tokz,\n    compute_metrics=corr_d,\n)\n\nSome weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing amp half precision backend\n\n\n\ntrainer.train();\n\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 2914\n  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3645\n\n\n\n      \n      \n      [3645/3645 09:15, Epoch 5/5]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\n0.250100\n0.168366\n0.705429\n\n\n2\n0.171600\n0.134761\n0.748499\n\n\n3\n0.118200\n0.114869\n0.784274\n\n\n4\n0.089600\n0.093946\n0.818484\n\n\n5\n0.063100\n0.091717\n0.822977\n\n\n\n\n\n\nSaving model checkpoint to outputs/checkpoint-500\nConfiguration saved in outputs/checkpoint-500/config.json\nModel weights saved in outputs/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-1000\nConfiguration saved in outputs/checkpoint-1000/config.json\nModel weights saved in outputs/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-1500\nConfiguration saved in outputs/checkpoint-1500/config.json\nModel weights saved in outputs/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\nSaving model checkpoint to outputs/checkpoint-2000\nConfiguration saved in outputs/checkpoint-2000/config.json\nModel weights saved in outputs/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-2500\nConfiguration saved in outputs/checkpoint-2500/config.json\nModel weights saved in outputs/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-3000\nConfiguration saved in outputs/checkpoint-3000/config.json\nModel weights saved in outputs/checkpoint-3000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\nSaving model checkpoint to outputs/checkpoint-3500\nConfiguration saved in outputs/checkpoint-3500/config.json\nModel weights saved in outputs/checkpoint-3500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-3500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nAt the end of all this, we have a Pearson’s score of 0.82 on our validation set which doesn’t seem to be as good as our image classifier. I’m not sure how I would go about comparing these two different metrics. I imagine I’d want to ensure that both my metrics were identical to make a like-for-like comparison.\nMy model is available on the Huggingface Hub here."
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html#what-did-i-learn",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html#what-did-i-learn",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "What did I learn?",
    "text": "What did I learn?\n\nTraining NLP models feels like a bit of a different world from that of computer vision. There are different constraints in the process that I wasn’t previously aware of and working with the transformers library exposed me to a bunch of new errors and hoops I had to jump through.\nIt seems that the RAM needed on the GPU is directly correlated with the length of the text documents. Mine were on the long-ish end of the scale (particularly when compared with tweets which was what Jeremy was training on in his notebook). I wonder how people solve this problem, since mine by were by no means incredibly long.\nNLP models take longer to train than computer vision models; at least, the transformer-based models that I was working with.\nIt’s hard to compare two models together that don’t share the same metric or loss function.\nThere are MANY fiddly knobs to twist with NLP, particularly around the pre-processing of text samples, tokenisation strategies and so on. I wonder how much of those will be abstracted away from the high-level fastai abstraction when the library integrates with transformers in the coming months.\nThe end-to-end process is broadly the same, however, and it was good to have the foundation that we’ve been building up over the previous weeks in the course.\n\nThe next model I train hopefully will not be relating to redactions, I promise!\nUPDATE: I read a bit in the new O’Reilly book by the transformers team, Natural Language Processing with Transformers, which seems to address the issue of the same text size:\n\n“Transformer models have a maximum input sequence length that is referred to as the maximum context size. For applications using DistilBERT, the maximum context size is 512 tokens, which amounts to a few paragraphs of text. […] Texts that are longer than a model’s context size need to be truncated, which can lead to a loss in performance if the truncated text contains crucial information.” (pages 28-29 of the paperback edition)\n\nThe book suggests plotting out the number of tokens to get a sense of the distribution of the data by size:\n\nimport matplotlib.pyplot as plt\n\ndf[\"Tokens per document\"] = df[\"input\"].apply(lambda x: len(x.split()))\ndf.boxplot(\n    \"Tokens per document\",\n    by=\"labels\",\n    grid=False,\n    showfliers=False,\n)\nplt.suptitle(\"\")\nplt.xlabel(\"\")\nplt.show()\n\n\n\n\n\n\n\n\nHere we can see that we have a fairly wide distribution, with quite a few texts going all the way up to 800 tokens in length, so that is probably responsible for the large amounts of RAM needed, but perhaps the truncation of texts is also harming our performance.\nWhen I visit the deberta-v3-small model card on Huggingface, I also see reference to a maximum sequence length of 256 which would indeed harm my model and its ability to learn, I reckon."
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html",
    "href": "posts/2023-03-05-stable-eights-adversarial.html",
    "title": "Tricking my digits classifier with diffusion",
    "section": "",
    "text": "In the lesson 9 video of the FastAI course, Jeremy explains how diffusion models work at a very high level. (These initial videos were released to the public early on, though the rest of the course is still ongoing and thus hasn’t yet been released.) Early on we’re introduced to a basic algorithm which would work for generating images. In doing so, we are given a model for how to think about the diffusion process. This mental model itself reminds of how we train models in the standard machine learning workflow.\nThe process goes as follows:\nOn some level, the rest of the complexity around the actual Stable Diffusion models relate to things like being able to combine a text prompt with the image generation, or making the process of training the model more efficient and so on. But for now, I’m choosing to focus on the core process as described above.\nMy choice to do this centered around the digit ‘8’ is arbitrary. We could as well have chosen something like ‘images of a shoe’ or whatever, but that potentially adds more complexity that could distract from the core process."
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#training-an-8-digit-classifier",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#training-an-8-digit-classifier",
    "title": "Tricking my digits classifier with diffusion",
    "section": "1: Training an ‘8’ digit classifier",
    "text": "1: Training an ‘8’ digit classifier\nTraining a model that can output the probability that a digit is a number eight is fairly trivial and is something that the FastAI part 1 course prepares you well to tackle, so I’ll handle that first since it’s a pre-requisite for everything that happens subsequently.\nNeedless to say, I’ll use the MNIST dataset as my source of training data. Potentially I can use Fashion MNIST as a stretch goal later on, but I’ll start with the digit ‘8’ and see where we get.\n\n!pip install -Uqq pip\n!pip install timm fastai torch datasets rich -Uqq\n\n\nfrom fastai.vision.all import *\nimport timm\n\ntorch.set_printoptions(precision=6, sci_mode=False)\n\n\n# dataset patched together from the original MNIST dataset\npath = Path(\"./mnist_8_or_not/training\")\npath.ls()\n\n(#4) [Path('mnist_8_or_not/training/not_8'),Path('mnist_8_or_not/training/8'),Path('mnist_8_or_not/training/.ipynb_checkpoints'),Path('mnist_8_or_not/training/eight_classifier.pkl')]\n\n\n\nfnames = get_image_files(path)\ndef label_func(x): return x.parent.name\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func)\ndls.show_batch()\n\n\n\n\n\n\n\n\nAs you can see, we now have a dataloader with images of handwritten eights and images of handwritten digits that are not eights. This data was taken from the MNIST dataset and reassembled for the purposes of making this model. (Heavy data class imbalance in favour of the not-eights, but putting that issue to one side for now).\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n# only train the model if we have no model already\nmodel_path = Path(\"./eight_classifier.pkl\")\nif not model_path.exists():\n    learn.fine_tune(5)\nlearn = load_learner(\"./eight_classifier.pkl\")\n\n\nan_eight = Path(path / \"8\").ls()[0]\nnot_an_eight = Path(path / \"not_8\").ls()[0]\n\n\nlearn.predict(an_eight), learn.predict(not_an_eight)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(('8', TensorBase(0), TensorBase([1.0000e+00, 5.5913e-07])),\n ('not_8', TensorBase(1), TensorBase([2.2082e-06, 1.0000e+00])))\n\n\nOur model is pretty good at detecting whether digits are eights or not. I only trained it for five epochs, but it was already getting an error rate of almost zero. Good enough for the purposes of this proof-of-concept exploration.\n\nfrom typing import Union\n\ndef get_eight_probability(image_pth: Union[Path, torch.Tensor], learner: Learner) -&gt; torch.Tensor:\n    _, _, probs = learner.predict(image_pth)\n    return probs[0]\n\n\nget_eight_probability(an_eight, learn)\n\n\n\n\n\n\n\n\nTensorBase(0.995113)\n\n\n\nget_eight_probability(not_an_eight, learn)\n\n\n\n\n\n\n\n\nTensorBase(0.000268)\n\n\n\n# export our model so we don't have to retrain it every time from now on\nif not model_path.exists():\n    learn.export(\"eight_classifier.pkl\")\n\n\nloaded_learn = load_learner(\"./eight_classifier.pkl\")\nget_eight_probability(an_eight, loaded_learn)\n\n\n\n\n\n\n\n\nTensorBase(0.999999)"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#create-a-random-noise-image",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#create-a-random-noise-image",
    "title": "Tricking my digits classifier with diffusion",
    "section": "2: Create a random noise image",
    "text": "2: Create a random noise image\nSo we have a way to get the probability that an image is an eight. Now we need to shift to the generation process, and we start by creating an image that is filled with random noise. We’ll shift to raw PyTorch code here probably so that it’s more explicit what’s going on, and so our ‘images’ will be Tensors. Let’s maybe make our images explicitly the same size as our training data, just to eliminate any confusion.\n\nfrom PIL import Image\n\nimg = Image.open(an_eight)\nwidth, height = img.size\n\nprint(f\"Image dimensions: {width} x {height}\")\n\nImage dimensions: 28 x 28\n\n\nNow we can generate a 28x28 image with random noise scattered throughout it.\n\nimport torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Generate a 28x28 tensor filled with random noise\nnoise_tensor = torch.randn(1, 1, 28, 28)\n\n# Convert the tensor to an image\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Grayscale(num_output_channels=1)\n])\nnoise_image = transform(noise_tensor.squeeze())\n\n# Display the image\nplt.imshow(noise_image, cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s turn that into a function so we can use it later on…\n\n# we don't end up using this first function much as we're working with tensors\ndef get_noisy_starter_image() -&gt; Image:\n    # Generate a 28x28 tensor filled with random noise\n    noise_tensor = torch.randn(1, 1, 28, 28)\n\n    # Convert the tensor to an image\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Grayscale(num_output_channels=1)\n    ])\n    return transform(noise_tensor.squeeze())\n\n# we generate a 3x28x28 tensor with random values assigned\n# we ensure that we can use PyTorch's autograd on the values\ndef get_noisy_starter_tensor() -&gt; torch.Tensor:\n    noise_tensor = torch.randn(1, 3, 28, 28, requires_grad = True)\n    return noise_tensor\n\n\nget_noisy_starter_image()\n\n\n\n\n\n\n\n\n\ntype(get_noisy_starter_tensor())\n\ntorch.Tensor"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#get-the-derivatives-for-the-probability-that-its-an-eight",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#get-the-derivatives-for-the-probability-that-its-an-eight",
    "title": "Tricking my digits classifier with diffusion",
    "section": "3: Get the derivatives for the probability that it’s an eight",
    "text": "3: Get the derivatives for the probability that it’s an eight\nOur task now is go through the pixels of this 28x28 image and get the derivatives of each pixel with respect to the probability that the image is a digit ‘8’. We don’t have to do this pixel by pixel (i.e. the ‘finite differencing method’) since we have PyTorch, so we can use ‘analytic derivatives’ to calculate the whole lot in a single sweep.\n(Sidebar: I’m using those terms to reference two ways of calculating gradients from the universe of calculus, but I don’t know anything really about what they signify or how they work. Just wanted to signal that I’m using them just because it came up in lesson 9 and it’s often nice to have a name to put to a technique, even if you don’t necessarily know how it works. If you don’t know how it works too, welcome to the club and know that you don’t always need to know everything at every moment :) )\n\nrandom_sample = get_noisy_starter_tensor()\n\n\nrandom_sample.shape\n\ntorch.Size([1, 3, 28, 28])\n\n\n\nAdding and updating our loss function\nI experimented a bit with getting a loss function to do what I needed and it took several forms. I’m leaving the code here for posterity, but it doesn’t get used anywhere else in this blog. The two helper functions are for displaying a tensor in the notebook, and for turning a tensor into an image that can be displayed.\n\n\nCode\ndef new_eights_loss(preds: torch.Tensor, learner: Learner) -&gt; torch.Tensor:\n    targets = torch.Tensor([[1.0, 0]])\n    return torch.nn.functional.mse_loss(preds, targets)\n\ndef display_tensor(tns: torch.Tensor) -&gt; None:\n    plt.imshow(tns, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef tns_to_img(tns: torch.Tensor) -&gt; Image:\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Grayscale(num_output_channels=1)\n        ])\n    return display_tensor(transform(tns.squeeze()))"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#automating-the-iterations",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#automating-the-iterations",
    "title": "Tricking my digits classifier with diffusion",
    "section": "Automating the iterations",
    "text": "Automating the iterations\nAt this point we should just give it a try. In what follows, we get an image of something that isn’t an eight, then iterate a number of times through the iterate_image function which:\n\nensures that we’re tracking and calculating the gradients on the image\ngets the predictions for our image (i.e. whether it’s an eight or not) and passing them through a softmax\nwe pass our predictions and our targets into a l1_loss function (i.e. the mean absolute error) to get the loss for these predictions.\nwe call backward() on the loss so that the gradients are calculated\nwe then update our image data by some constant times our gradients (just like we do in the normal ML training loop)\nwe zero out the gradients prior to the next iteration\n\n(Every 15 iterations we output the loss as well as the image at that particular moment.)\n\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\nimage = transform(Image.open(not_an_eight).convert(\"RGB\"))\n_ = image.requires_grad_()\n\n\nimport numpy as np\n\ndef iterate_image(image, iota: int, update_rate: float = 0.1):\n    image.requires_grad_()\n    preds = torch.softmax(loaded_learn.model(image[None]), dim=1)\n    targets = torch.Tensor([[1.0, 0]])\n    # loss = torch.nn.functional.mse_loss(preds, targets)\n    loss = torch.nn.functional.l1_loss(preds, targets)\n    loss.backward()\n    if i % 15 == 0:\n        print(f\"grad_sum: {image.grad.data.sum()}, loss: {loss}\")\n    \n    image.data -= (update_rate * image.grad.data)\n    image.grad.zero_()\n    \n    if i % 15 == 0:\n        # N.B. Use v1 to get a sense of what's changing, v2 for the current values\n        \n        # VERSION 1\n        # plt.imshow(np.log1p(image[0].detach().numpy()))\n        # plt.show(plt.gcf())\n        \n        # VERSION 2\n        plt.imshow(image[0].detach().numpy())\n        plt.show()\n\nWe iterate 50 times, and we our constant for the update_rate is fairly high (i.e. 1). Normally for a learning_rate (the equivalent for this value) we would choose something between 0.01 and 0.1 so as not to update too far in any one direction.\nAlso, just to confirm, these are our predictions for the not_an_eight image we’re using as the basis for this iterative process:\n\nlearn = load_learner(\"./eight_classifier.pkl\")\nlearn.predict(not_an_eight)\n\n\n\n\n\n\n\n\n('not_8', TensorBase(1), TensorBase([    0.000268,     0.999732]))\n\n\n\nfor i in range(50):\n    iterate_image(image, i, update_rate = 1)\n\ngrad_sum: -0.0006039840518496931, loss: 0.9997323155403137\n\n\n\n\n\n\n\n\n\ngrad_sum: -0.003538962686434388, loss: 0.0007090563885867596\n\n\n\n\n\n\n\n\n\ngrad_sum: -0.0011799032799899578, loss: 0.00022288746549747884\n\n\n\n\n\n\n\n\n\ngrad_sum: -0.0007250444614328444, loss: 0.00012537218572106212\n\n\n\n\n\n\n\n\n\nAs you can see, the loss reduces throughout (and would continue to do so were I to let it continue onwards). The image still looks like a two, however, and doesn’t seem to resemble an eight. So is it doing what we want? A quick check is to run that image we’ve updated / ‘generated’ into the model and get the predictions:\n\ntorch.sigmoid(loaded_learn.model(image[None]))\n\nTensorBase([[0.989906, 0.010533]], grad_fn=&lt;AliasBackward0&gt;)\n\n\n\n# double-checking using a freshly imported learner to make sure\nlearn = load_learner(\"./eight_classifier.pkl\")\ntorch.sigmoid(learn.model(image[None]))\n\nTensorBase([[0.989906, 0.010533]], grad_fn=&lt;AliasBackward0&gt;)\n\n\nSo even though the image still looks like a two, our model has been bamboozled into thinking it’s an eight. The iteration process has updated the weights just enough that we’ve minimised our loss, and our model now is almost completely certain that this is an image of an eight.\nI’ve tried the same process on the random noise and you don’t get quite as impressive updating as quickly as with our two, but maybe with more iterations it would gradually get there.\nNeedless to say, this was a surprising result. I expected to have a neat progression through which the image would gradually look more and more like a number eight. This updating where we adjust enough to completely switch the predictions of the model from certainty that it isn’t an eight, to certainty that it is an eight, that was unexpected to say the least.\nI’m really appreciative of the various people who helped me during the process of working on this little experiment: Yogya, Kevin, and all those with whom I have the pleasure to meet each week in the Delft FastAI Study Group."
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#appendix-some-pytorch-things",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#appendix-some-pytorch-things",
    "title": "Tricking my digits classifier with diffusion",
    "section": "Appendix: Some PyTorch Things",
    "text": "Appendix: Some PyTorch Things\nI learned or encountered a lot of small PyTorch tricks while working on this, so making a note of them as a record for future reference:\n\nautograd - this is PyTorch’s way of automatically calculating the gradients. I’ve never done this process manually myself, nor do I know how to do it, but I reckon this probably saves us a lot of time and complication.\nsqueeze() and unsqueeze() - these remove and add a dimension to a tensor. It can help in getting things into the right shape.\nview() - maybe even more helpful when it comes to getting tensors in the right shape. I ran into lots of places where I was passing something of slightly the wrong shape or dimensions in somewhere and that was causing problems.\nzero_() - a way of zeroing out the gradients in-place\ndetach() - this is a way of getting a copy of the current tensor, but one that isn’t attached to the graph of gradient calculations that are going on.\nrequires_grad_() - again, another way to ensure that a tensor has the gradient calculations enabled, and it happens in-place.\ntorch.equals(t1, t2) - a way of comparing two tensors to see if they’re identical or not.\n\nI discovered that FastAI’s learn.predict() function does some things to whatever you pass in, such that if you use it in the process above you’ll hit an error where it says that there are no longer any gradients to update / accumulate. This is why we’re passing our image values or our tensor directly into the model to get our predictions since this bypasses whatever FastAI is doing. I wasn’t able to fully grok / dig into exactly what’s happening there, but perhaps this is something to look into in the future.\n\nDebugging tips\nOn Saturday, I had basically a whole day of hitting lots of walls, somewhat-opaque PyTorch error messages galore. The thing that really seemed to help the most (and that my study group colleagues did with me on Sunday morning) was just to break out each individual step separately, inspecting the return values and the .shape and so on.\nDoing this systematically, interrogating what you expect to happen vs what you see being returned or happening definetely seems like the way to go. At this level of (non-)abstraction, however, there were quite a few PyTorch messages that were hard to wrap my head around. I suppose when you do this often enough you get an intuition for the kinds of ways these things fail, and how to massage your data into the formats required."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html",
    "href": "posts/2023-06-01-why-tokenisation.html",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "",
    "text": "For the types of machine learning that involve neural networks, the training process generally involves passing data and some weights into a function which we use to continually and iteratively optimise the weights. We hope that by showing lots of examples of the right way to do things (as per our data and annotations) we’ll emerge with a model (i.e. the updated weights) that performs the way we’d expect.\nThis whole process has various kinds of mathematics at its core, some basic calculations and some higher-order ideas to help figure out how to improve the weights. For all this, we need our data to be in a form that can pass through these calculations. We’re in the domain of natural / human languages at the moment, so we need somehow to turn our words into some kind of numerical form. Tokenisation is a big part of that process.\nMost of what goes on with tokenisation is — to some extent — around finding a way to optimise the amount of data we have to feed into our model either during training or inference. We want to do both of these in the most efficient manner possible. Smaller amounts of data needed to train (or faster ways of processing the data) means you can do more with less."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#simple-tokenization",
    "href": "posts/2023-06-01-why-tokenisation.html#simple-tokenization",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "🔡 Simple tokenization",
    "text": "🔡 Simple tokenization\nIf you think about a text string, a naive approach might be to just split it up by character.\n\nsentence = \"Some 10 million people speak the Balochi language, and most of them are located in Iran and Pakistan.\"\nprint(list(sentence))\n\n['S', 'o', 'm', 'e', ' ', '1', '0', ' ', 'm', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 's', 'p', 'e', 'a', 'k', ' ', 't', 'h', 'e', ' ', 'B', 'a', 'l', 'o', 'c', 'h', 'i', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'm', 'o', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', 'm', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'c', 'a', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'I', 'r', 'a', 'n', ' ', 'a', 'n', 'd', ' ', 'P', 'a', 'k', 'i', 's', 't', 'a', 'n', '.']\n\n\nWe can get the unique characters from our sentence to save a bit of space:\n\nprint(set(list(sentence)))\n\n{'f', 'a', '1', 'm', 'd', 'e', 'k', '.', 'g', 'B', 'c', 's', 'i', 'r', 'u', 't', 'p', 'l', ',', 'I', '0', 'o', 'S', 'h', 'n', ' ', 'P'}\n\n\nWe can save even more space by transforming our sentence into lowercase text:\n\nprint(set(list(sentence.lower())))\n\n{'f', 'a', '1', 'm', 'd', 'e', 'k', '.', 'g', 'c', 's', 'i', 'r', 'u', 't', 'p', 'l', ',', '0', 'o', 'h', 'n', 'b', ' '}\n\n\nFor Balochi this might look something like this:\n\nbalochi_sentence = \" اِدا کسے است کہ انگریزی ءَ گپ جت بہ کنت\"\n# translates to \"Is there someone here who speaks English?\"\nbalochi_chars = set(list(balochi_sentence.lower()))\nprint(balochi_chars)\n\n{'ء', 'ک', 'پ', 'ج', 'َ', 'ب', 'ے', 'ن', 'ر', 'ا', 'س', 'د', 'ِ', 'ی', 'ت', 'ہ', 'ز', ' ', 'گ'}\n\n\nAnd we can get a mapping of characters to integers quite easily from here:\n\nbalochi_char_mapping = {char: index for index, char in enumerate(sorted(balochi_chars))}\nprint(balochi_char_mapping)\n\n{' ': 0, 'ء': 1, 'ا': 2, 'ب': 3, 'ت': 4, 'ج': 5, 'د': 6, 'ر': 7, 'ز': 8, 'س': 9, 'ن': 10, 'َ': 11, 'ِ': 12, 'پ': 13, 'ک': 14, 'گ': 15, 'ہ': 16, 'ی': 17, 'ے': 18}\n\n\nYou can already see some wonkiness in how the sorted mapping is displayed. This derives from the fact that the Balochi script is written from right-to-left and this pattern is not well supported in a world dominated by English.\nThe mapping is what we want, and we can use this to map our original sentence into a sequence of numbers:\n\nbalochi_sentence_ids = [balochi_char_mapping[char] for char in balochi_chars]\nprint(balochi_sentence_ids)\n\n[1, 14, 13, 5, 11, 3, 18, 10, 7, 2, 9, 6, 12, 17, 4, 16, 8, 0, 15]\n\n\nWhen it comes to language, the things we care at the tail end of all our modelling all relate to sequences of words and not characters. While our vocabulary (i.e. our list of unique characters) would be pretty small with character-level tokenization, we’d have some other issues:\n\nloss of semantic meaning – our model would likely find it harder to ‘learn’ the higher level concepts without first finding a way past the idea of words and how they represent meaning in a way that pure characters don’t)\nincreased sequence length – if we think of a sentence as a sequence of words, a sequence of characters would be much longer in sheer numbers. This adds overhead in terms of the complexity of processing and training on the text.\n\nAt the other end of the spectrum we have word-based tokenisation:\n\nbalochi_words = set(balochi_sentence.split())\nprint(balochi_words)\nword_mapping = {word: index for index, word in enumerate(sorted(balochi_words))}\nprint(word_mapping)\nword_ids = [word_mapping[word] for word in balochi_words]\nprint(word_ids)\n\n{'گپ', 'جت', 'بہ', 'اِدا', 'است', 'کہ', 'انگریزی', 'کسے', 'ءَ', 'کنت'}\n{'ءَ': 0, 'است': 1, 'انگریزی': 2, 'اِدا': 3, 'بہ': 4, 'جت': 5, 'کسے': 6, 'کنت': 7, 'کہ': 8, 'گپ': 9}\n[9, 5, 4, 3, 1, 8, 2, 6, 0, 7]\n\n\nThis has the advantage of keeping our data at what feesl like an appropriate level of semantic abstraction, but you can probably imagine that our vocabulary size could well get out of control. If we have enough data, eventually our vocabulary size could grow to hundreds of thousands of items and then we’re going to have the same problem we had with long sequences in character-level tokenisation.\nThere are various ways of dealing with this. The blunt-force aproach would be to discard the words with a low frequency. We could pick some number (100,000 perhaps) and say that we’ll only include the 100,000 most common words from our corpus. Anything else will get replaced with something like “UNK” or “xxunk” that we’ll know isn’t a real word but just signifies that there was a low-frequency word there. This keeps our vocabulary (relatively) limited, but as you can imagine we might lose important information by discarding all those ‘uncommon’ words."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#linguistically-enhanced-tokenization",
    "href": "posts/2023-06-01-why-tokenisation.html#linguistically-enhanced-tokenization",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "📚 Linguistically-enhanced tokenization",
    "text": "📚 Linguistically-enhanced tokenization\nBefore we get to the current best-in-class solution to this problem, it’s worth mentioning that there are some approaches that use some hand-crafted features derived from a linguistic understanding of a particular language.\nFor example, in deciding which words to leave out of tokenization, we might want to ignore ones which tend not to give so much useful information. For English, these are works like “the”, “or” or “a”. (You can get a sense of these words here.\nWe also might want to use ‘stemming’ and/or ‘lemmatisation’ as a way of reducing the total number of words in our vocabulary:\n\nStemming reduces the word to a more basic form, i.e. ‘the stem’. So the words “connection”, “connected” and “connects” might all reduce down to “connect”. Note that this stemmed word might not actually exist in English.\nLemmatisation is similar, but it uses a bit of extra knowledge of the language to reduce the words. For example, “good”, “better” and “best” might all reduce down to “good” even though they are spelled in quite different ways.\n\nBoth stemming and lemmatisation (as far as I know) and some other related techniques require a pre-existing knowledge base to exist and to have been hand-coded or hard-coded into the software you use to process your text. For some languages that’s not a problem, but for Balochi these resources haven’t yet been created. A few years back it might even have been the next step for me in my Balochi project to go ahead and work on preparing these kinds of linguistic features and work using these techniques. They require a considerable amount of expertise in the specific language you’re working on, and I’m assuming they take a long time to put together as well.\nLuckily, there is a technique which allows us the best of many worlds: small(ish) vocabulary and no need for years constructing language-specific lists of words and their roots. Let the CPU handle all that!"
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#subword-tokenisation",
    "href": "posts/2023-06-01-why-tokenisation.html#subword-tokenisation",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "👶 Subword tokenisation",
    "text": "👶 Subword tokenisation\nSubword tokenisation is when you let the computer decide how to figure out the right balance between characters and words when it comes to splitting the text corpus up. The technique seems to have gained popularity for tokenisation in only the last decade, though the original algorithm on which some of it was based dates back to 1994.\nThe basic rule of thumb is this: split the words into the optimum number of pieces given a specific text corpus. So if we had two words, “car” and “cat”, in our corpus, the tokens we might generate from these would be: “ca##” “##r” and “##t”. The ‘##’ means that something can join to the letter from that side. Obviously in this small example, we didn’t really save any space, but for large volumes of data we’re going to generate down to just the right balance between characters and letters.\nThis technique was actually first proposed by Philip Gage as a compression algorithm in 1994, but then presumably rediscovered or reimplemented for tokenisation in a series of updates building on the original idea. There have thus been several implementations of this algorithmic family:\n\nWordPiece (Schuster & Nakajima in 2012) – used in BERT and DistinBERT\nByte Pair Encoding (BPE) (Sennrich in 2015)\nSentence Piece / Unigram (Kudo & Richardson in 2018) – used in XLM-RoBERTa\nBPE Dropout (Provilkov et al. in 2020)\nDynamic programming encoding (DPE) (He et al. in 2020)\n\n(Thanks to Masato Hagiwara for a useful summary of the history and key developments on his blog here.)\nThis is my summary of some of the key differences to bear in mind:\n\nThe key difference between the tokenisation process we’ve seen and subword tokenisation is that now we need a text dataset and a ‘training’ process to ‘learn’ how to split words down into smaller chunks. I’ll get into the specific details of how this works along with some examples for Balochi in the next blog."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#extra-meta-tokens",
    "href": "posts/2023-06-01-why-tokenisation.html#extra-meta-tokens",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "😜 Extra Meta-Tokens",
    "text": "😜 Extra Meta-Tokens\nThere are a few extra tokens that get generated during some of the above tokenisation methods that it’s probably worth talking about now. These tokens are added to the vocabulary of tokens and they represent various contextual information. For example:\n\n\n\n\n\n\n\nToken\nPurpose / Meaning\n\n\n\n\nCLS\n‘classification’. Token prepended to the start of each text chunk.\n\n\nSEP\n‘separate’. Token used to separate sentences inside a text chunk.\n\n\n##\n(mentioned above). Used to denote other tokens can be attached here.\n\n\nBOS\n‘beginning of stream’. Also used to denote the beginning of a sentence.\n\n\nPAD\n‘pad’. A way to make arrays of tokens the same length / size.\n\n\nMASK\n‘mask’. Used to mask a word in a sentence and used in training.\n\n\nxxmaj\nindicates that the next word begins with a capital letter.\n\n\nUNK\n‘unknown’. Used when you need to limit your vocabulary size.\n\n\n\nNote that these aren’t universally used. The xx prefix is something that FastAI uses in its tokenisation to avoid the chance that something like ‘PAD’ is being used as an actual word in the text."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#numericalising-the-tokens",
    "href": "posts/2023-06-01-why-tokenisation.html#numericalising-the-tokens",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "🔢 Numericalising the tokens",
    "text": "🔢 Numericalising the tokens\nOnce we have our list of tokens and their ids (see above), it isn’t enough for us just to pass that in for training our models. Neural networks will attach to anything that gives a bit of signal when they are learning from data. If we have a list of tokens and ‘dog’ is assigned the number 3 and ‘cat’ is assigned the number 10, our model might assign some kind of ranking or importance to those numbers. So we have to pass our values in a way that doesn’t lead to this kind of unanticipated signal. The way we do this for language is to ‘one-hot encode’ the values.\nSo instead of:\n\nbalochi_sentence_ids = [word_mapping[word] for word in balochi_sentence.split()]\nprint(balochi_sentence_ids)\n\n[3, 6, 1, 8, 2, 0, 9, 5, 4, 7]\n\n\n…we can generate an array of arrays. For each word in the sentence, we have a subarray that has a length of our vocabulary and then we turn the value in the word’s index to 1 if that’s the word at this point in our sentence. It’ll be easier to see in an example :)\n\nimport torch\nimport torch.nn.functional as F\n\n\ninput_ids = torch.tensor(balochi_sentence_ids)\none_hot_encodings = F.one_hot(input_ids, num_classes=len(balochi_words))\nprint(one_hot_encodings)\n\ntensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])\n\n\nSo you can see for the first word (i.e. the first subarray) we have a 1 at index 3 and this corresponds exactly to our sentence and the mapping of words. (I hope it’s clear now also why we might want to have some kind of limitation of just how large our vocabulary gets.)\nIn my next post I’ll walk through all of the details showing how you train your own subword tokenizer, compare how it works in two popular Python libraries (Spacy and 🤗 Tokenisers and in general show how all of this fits together in the bigger picture."
  },
  {
    "objectID": "posts/2024-06-02-isafpr-prompting-baseline.html",
    "href": "posts/2024-06-02-isafpr-prompting-baseline.html",
    "title": "Structured Data Extraction for ISAF Press Releases with Instructor",
    "section": "",
    "text": "I’m currently participating in the Maven LLM course / conference. Originally focused on finetuning LLMs, it’s since expanded to encompass a wide range of LLM-related topics. I thought I’d try to work through a small project alongside the course to get some practical experience with fine-tuning LLMs.\nI previously published a dataset from my time working in Afghanistan: the ISAF Press Releases dataset. (See here for a blogpost I wrote describing the dataset in more detail.) Even though it was not really intended as a dataset to be used for any kind of model training, I thought it might serve well to finetune an LLM on top of it. The dataset is made up of press releases from the International Security Assistance Force (ISAF) and I had previously annotated them, extracting out metadata of interest.\nHere’s an example:\nFrom this I extracted the following metadata:\nThere were a few other pieces of metadata captured but you probably get the idea. Check out the dataset’s card which gives full details.\nThere are 4822 such events in the dataset and as you might imagine it took quite a long time to manually annotate all this data. An example like the one above is fairly straightforward but it’s sometimes unclear exactly how many people were involved. Take this press release:\nThe number of people killed is specified as “multiple” and the number of those captured is specified as “dozens”. So that means a minimum of 3 killed and a minimum of 24 captured. But you have to be reading fairly closely to pick all of this up, and it gets even more complicated when they refer to multiple events in the same press release (and so on).\nIt occurred to me recently that it might make for an interesting test of an LLM’s ability to extract this data out of the raw text in a structured format. So ideally I’d input the press release and I’d get out a JSON object (or Pydantic or whatever) which populates all the various fields.\nI’m lucky in that I’ve already lovingly labeled such a large dataset so I can be really confident in the quality which will allow me to focus on the task of finetuning an LLM to do this task.\nSo my learning goals from this project are to:\nFor the project itself, I keep reading (and watching) that you can get GPT-4 level performance on specific focused tasks by finetuning LLMs and I wanted to see how much can be done with limited resources (or just how cherry-picked those public examples actually are.)\nI have a ‘complete’ dataset which includes press releases published after I finished working on my report, so ideally I’ll be able to use the model to label the remaining data (if it’s good enough in terms of accuracy). I’d also like to see how the speed of a finetuned model compares to using something like GPT-4.\nLet’s try this out with a simple prompt and a single example to see how it performs out of the box! We load the dataset first:\n# get data from datasets\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\nimport tqdm as notebook_tqdm\n\n# Load the dataset\ndataset = load_dataset(\"strickvl/isafpressreleases\", split=\"train\")\n\n# Convert the dataset to a pandas DataFrame\ndf = pd.DataFrame(dataset)\n\n# Print the first few rows of the DataFrame\nprint(df.head())\n\n/home/strickvl/.pyenv/versions/3.10.14/envs/isafpr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n                              name  eventrefnumber  \\\n0          Taliban Compound Struck  2009-11-CA-056   \n1   Militants Detained in Kandahar  2009-11-CA-056   \n2      Militants Detained in Khost  2009-12-CA–057   \n3     Militants Detained in Wardak  2009-12-CA–057   \n4  Insurgents Detained in Kandahar  2009-12-CA-058   \n\n                                                text  StartDate  eventtype  \\\n0  Dec. 2: Taliban Compound Struck\\n\\nNEWS RELEAS... 2009-12-01  airstrike   \n1  Militants Detained in Kandahar\\nNEWS RELEASE I... 2009-12-02  detention   \n2  Dec. 3: Militants Detained in Khwost\\nNEWS REL... 2009-12-02  detention   \n3  Dec. 3: Militants Detained in Wardak\\n\\n\\n\\nNE... 2009-12-03  detention   \n4  Dec. 4: Insurgents Detained in Kandahar\\nISAF ... 2009-12-04  detention   \n\n   province     citydistrict       village targetgroup commander  ...  \\\n0     Kunar     Dara-ye Noor                   Taliban            ...   \n1  Kandahar    Kandahar City                   Taliban            ...   \n2     Khost  Sabari district      Khatekah     Taliban            ...   \n3    Wardak       Sayyedabad    Jamad Khel     Taliban            ...   \n4  Kandahar        Arghandab  Nurayo Kariz     Taliban            ...   \n\n  airstrike noshotsfired dataprocessed flagged glossarymeta minleaderskilled  \\\n0      true        false          true   false        false                0   \n1     false         true          true   false        false                0   \n2     false         true          true   false        false                0   \n3     false         true          true   false        false                0   \n4     false         true          true   false        false                0   \n\n  minfacilitatorskilled minleaderscaptured minfacilitatorscaptured leaderq  \n0                     0                  0                       0   false  \n1                     0                  0                       0   false  \n2                     0                  0                       1   false  \n3                     0                  0                       0   false  \n4                     0                  0                       1   false  \n\n[5 rows x 28 columns]\nWe can take a peek at the columns in the dataset:\ndf.columns\n\nIndex(['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province',\n       'citydistrict', 'village', 'targetgroup', 'commander', 'position',\n       'minkilled', 'mincaptured', 'capturedcharacterisation',\n       'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid',\n       'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta',\n       'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured',\n       'minfacilitatorscaptured', 'leaderq'],\n      dtype='object')\nIf we look at the options for the ‘eventtype’ column, you’ll see that we have some single-word event types at the top, but also some semi-colon-separated event types. We’ll need to handle these a bit differently when we get to finetuning but perhaps let’s ignore that for now.\neventtype_options = df[\"eventtype\"].unique().tolist()\n\nprint(eventtype_options)\n\n[\n    'airstrike',\n    'detention',\n    'captureandkill',\n    'insurgentskilled',\n    'exchangeoffire',\n    '',\n    'civiliancasualty',\n    '2010-07-CA-124',\n    'insurgentskilled;civiliancasualty',\n    'airstrike;detention',\n    'detention;airstrike',\n    'civiliancasualty;airstrike',\n    'airstrike;civiliancasualty',\n    'insurgentskilled;detention',\n    'detention;insurgentskilled'\n]\nWe have everything we need to set up the task and the data structures that will be filled by our LLM. Let’s start with the event type where we just create an enum to store the options:\nfrom pydantic import BaseModel, Field\nfrom datetime import date\nfrom enum import Enum\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\nWe can create a similar enum to store the provinces in Afghanistan:\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jawzjan = \"jawzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktia = \"paktia\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pol = \"sar_e_pol\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\nFinally we can create an IsafEvent which is a Pydantic model where we’ll store all the various pieces of data we’re interested in. We include descriptions of the different fields which will help our LLM to understand the data it’s working with.\nclass IsafEvent(BaseModel):\n    name: str = Field(description=\"A title or name for the event\")\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    end_date: date = Field(description=\"The end date of the event in YYYY-MM-DD format\")\n    event_type: EventType = Field(description=\"The event type\")\n    province: Province = Field(description=\"The province in which the event occurred\")\n    target_group: str = Field(\n        description=\"The group that was targetted during the event.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\nNow let’s get a sample article to work with:\narticle_id = 15\narticle_text = df[\"text\"][article_id]\nprint(article_text)\n\nDec. 11: Haqqani Facilitator Detained in Khowst; Security Discussed in Farah\nNEWS RELEASE ISAF Joint Command - Afghanistan   2009-12-CA-065 For Immediate Release  KABUL, Afghanistan (Dec. 11) - An\nAfghan-international security force detained a couple of militants in Khowst province today, one of whom was a \nsought-after Haqqani facilitator.  The facilitator is responsible for the shipment and distribution of weapons to \nother militant elements in the area.\n The joint security force searched a compound near the village of Badal Kalay in the Nader Shakhot district where \nintelligence sources indicated the facilitator was located.  The facilitator identified himself and surrendered \nwithout incident.  No shots were fired and no one was injured.\nNow we can construct a simple prompt to help guide the LLM to extract the right data:\nquery = f\"\"\"\nThe following is a press release issued by ISAF (formerly operating in Afghanistan):\n{article_text}\n\nPlease extract the following information from the press release:\n- The name of the event\n- The start date of the event\n- The end date of the event\n- The event type\n- The province in which the event occurred\n- The target group of the event\n- The minimum number of people killed during the event\n- The minimum number of people captured during the event\n- Whether someone was killed or not during the event\n- Whether someone was captured or not during the event\n- Whether the event was a so-called 'kill-capture raid'\n- Whether an airstrike was used during the event\n- Whether no shots were fired during the event\n- The minimum number of leaders killed during the event\n- The minimum number of leaders captured during the event\n\"\"\""
  },
  {
    "objectID": "posts/2024-06-02-isafpr-prompting-baseline.html#structured-data-extraction-with-instructor",
    "href": "posts/2024-06-02-isafpr-prompting-baseline.html#structured-data-extraction-with-instructor",
    "title": "Structured Data Extraction for ISAF Press Releases with Instructor",
    "section": "Structured data extraction with Instructor",
    "text": "Structured data extraction with Instructor\nNow we can use Instructor to help ensure that our data is extracted out according to our Pydantic model and just use GPT-3.5 to see how it performs:\n\nimport instructor\nfrom openai import OpenAI\n\n# patch the client to add `response_model` to the `create` method\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nopenai_resp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    response_model=IsafEvent,\n)\n\nprint(openai_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=&lt;EventType.detention: 'detention'&gt;,\n    province=&lt;Province.khost: 'khost'&gt;,\n    target_group='Haqqani facilitator',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\nAs you can see, GPT-3.5 did pretty well! It was able to extract all the data more or less as I’d have hoped. It was able to determine that “Khowst” province in the article was “Khost” province in the schema, and it correctly determined that two individuals were detained. The only thing where I would have done things differently was to state that a minimum of one leader was captured. In this project I didn’t consider a ‘faciliator’ to be a leader so in the original dataset this would have been a 0:\n\ndf[\"minleaderscaptured\"][article_id]\n\n'0'\n\n\nIt’s hard for the LLM to have known that we’re taking that approach without having specified it in the prompt, but we can try again, updating the prompt with that rule:\n\nupdated_query = f\"\"\"\nThe following is a press release issued by ISAF (formerly operating in Afghanistan):\n{article_text}\n\nPlease extract the following information from the press release:\n- The name of the event\n- The start date of the event\n- The end date of the event\n- The event type\n- The province in which the event occurred\n- The target group of the event\n- The minimum number of people killed during the event\n- The minimum number of people captured during the event\n- Whether someone was killed or not during the event\n- Whether someone was captured or not during the event\n- Whether the event was a so-called 'kill-capture raid'\n- Whether an airstrike was used during the event\n- Whether no shots were fired during the event\n- The minimum number of leaders killed during the event\n- The minimum number of leaders captured during the event\n\nSo-called 'facilitators' aren't considered leaders in this task.\n\"\"\"\n\n\nimport instructor\nfrom openai import OpenAI\n\n# patch the client to add `response_model` to the `create` method\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nopenai_resp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    response_model=IsafEvent,\n)\n\nprint(openai_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=&lt;EventType.detention: 'detention'&gt;,\n    province=&lt;Province.khost: 'khost'&gt;,\n    target_group='Haqqani facilitator',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=True,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\nUnfortunately it’s still getting it wrong. Let’s try with GPT-4 this time and hope that it’s better at following instructions:\n\nopenai_resp = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    response_model=IsafEvent,\n)\n\nprint(openai_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=&lt;EventType.detention: 'detention'&gt;,\n    province=&lt;Province.khost: 'khost'&gt;,\n    target_group='Haqqani',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=1\n)\n\n\n\nUnfortunately we’re still getting the same response. I could fiddle around with the prompt a bit more to get the result I wanted, but you can see that this approach of encoding all these edge cases into the prompt isn’t going to scale very well. It’s also going to overfit a little to the training data and really what we want is a model that can do well at any new articles we pass into it.\nEven when we try Claude’s Opus model we get the same result, which tells us that for sure we’d have to amend this in the prompt to fix the problem.\n\nfrom anthropic import Anthropic\nimport instructor\n\nclient = instructor.from_anthropic(Anthropic())\n\n# note that client.chat.completions.create will also work\nclaude_opus_resp = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    max_tokens=4096,\n    response_model=IsafEvent,\n)\n\nprint(claude_opus_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=&lt;EventType.detention: 'detention'&gt;,\n    province=&lt;Province.khost: 'khost'&gt;,\n    target_group='Haqqani',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=1\n)\n\n\n\nAnd again, using Ollama locally with their mixtral model we get the same result:\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nmixtral_resp = client.chat.completions.create(\n    model=\"mixtral\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        }\n    ],\n    response_model=IsafEvent,\n)\nprint(mixtral_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detention in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=&lt;EventType.detention: 'detention'&gt;,\n    province=&lt;Province.khost: 'khost'&gt;,\n    target_group='Haqqani militants',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=1\n)"
  },
  {
    "objectID": "posts/2024-06-02-isafpr-prompting-baseline.html#next-steps",
    "href": "posts/2024-06-02-isafpr-prompting-baseline.html#next-steps",
    "title": "Structured Data Extraction for ISAF Press Releases with Instructor",
    "section": "Next steps",
    "text": "Next steps\nIf there’s one thing the Maven conference has done well it’s to emphasise the importance of getting a solid sense of an initial baseline from which you can (measureably) improve. So what I’d like to do next is to make a simple evaluation of the performance of the different models on this task.\nComing up with a score will be interesting as there are multiple pieces of information to compare. Numbers to numbers is an easy comparison, but what happens when it gets the wrong category? Do I subtract a mark from the score, or do I keep scores for all the different attributes? It isn’t clear to me how best to construct this score.\nIt also occurred to me while writing the above code that when I was doing the annotation I did so in ‘passes’. So I’d read the article and I’d be reading it looking only for the numbers of killed and captured individuals and nothing else. Then I’d reread the article and extract the data and location, and so on with multiple passes. This is to keep me focused on small details as if I tried to make note of everything at a single pass then I’d certainly miss things or get things wrong. So extrapolating out to LLMs (which, to be clear, don’t work like humans when they read text) I’m wondering whether it might make sense to finetune multiple specialised LLMs to be really best in class at extracting one or two data points instead of expecting it to do what I was unable (i.e. extracting everything at a single pass). Obviously it’d be preferable to have a single model, but I’m wondering whether we might push the limits of what an LLM can do at some of the longer or more complex articles. We’ll have to keep that in mind going forward.\nThe one thing I’ll have to make sure to do before I run the evaluation is to make sure that my data structures are set up to match the original dataset. You’ll remember that above we ignored the fact the eventtype field could have multiple types separated by a semicolon. So I’ll have to make sure that my data structures are set up to handle that. I’ll also improve the descriptions of the fields where possible and try to make the prompt a bit more performant.\nIn the next blog I’ll show the results of evaluating some of our baseline LLMs (proprietary and open-source) to see how they perform for this task. Once we have that baseline then we can continue to the task of actually finetuning an LLM."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html",
    "href": "posts/2024-06-15-isafpr-first-finetune.html",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "",
    "text": "We previously looked into how well the top LLMs could do when given press releases and asked to extract structured data from them. I was glad that this clearly wasn’t a task they struggled with, but it was by no means a simple task for them and some basic evaluations that I performed showed that there was room for improvement.\nSince writing that post I also heard from readers to say that perhaps I wasn’t using the OpenAI API in a way that would get the best results. In particular, function calling would give a better accuracy over the raw prompting that I was using. I’ll probably return to that in a separate post when I compare how well we’re doing with finetuning.\nAs a quick reminder, we’re hoping to create something that will allow us to go from an unstructured text (a press release, in our case) to a structured output that accurately extracts certain pieces of metadata from the text. Please give the first post in the series a read if you want more of the context of what we’re doing.\nThis blogpost will be about my first finetune(s) of some models and I’ll showcase how I got the data ready and then some observations about the finetuning process in general."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#loading-the-datasets",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#loading-the-datasets",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Loading the datasets",
    "text": "Loading the datasets\n\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\n\n# Loadthe dataset\ntrain_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"train\")\ntest_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"test\")\n\n# Convert the dataset to a pandas DataFrame\ntrain_df = pd.DataFrame(train_dataset)\ntest_df = pd.DataFrame(test_dataset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],\n    num_rows: 4098\n})\n\n\n\ntest_dataset\n\nDataset({\n    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],\n    num_rows: 724\n})\n\n\nWe have 4098 training examples and 724 test examples. This seems a good split to me. I experimented a bit with the exact split and found that 15% seemed like a good compromise. We want enough data to get a good evaluation, but we also want to give our model enough examples to learn. In the course people were frequently talking about somewhere in the order of mid hundreds to low thousands as being the sweet spot, so I hope I’m firmly in that range.\nIt’s also worth reflecting that I’m lucky that I have such a large clean dataset to work with. In a later project I’d like to try working with much less and slowly building up something more complex since that’s a skill in and of itself."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#setting-up-our-pydantic-models-with-validation",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#setting-up-our-pydantic-models-with-validation",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Setting up our Pydantic models with validation",
    "text": "Setting up our Pydantic models with validation\nThere’s a decent amount of code in the next cell, and definitely read the previous posts to understand what all the pieces are about, but in a nutshell we’re setting ourselves up to extract structured data from the text. This Pydantic model is what will hold the data we’re interested in.\n\nfrom enum import Enum\nfrom typing import Set, Annotated, Optional\nfrom pydantic import BaseModel, Field, validator, ValidationInfo\nfrom datetime import date\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\n\n\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jowzjan = \"jowzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktya = \"paktya\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pul = \"sar_e_pul\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\n\n\nclass TargetGroup(str, Enum):\n    taliban = \"taliban\"\n    haqqani = \"haqqani\"\n    criminals = \"criminals\"\n    aq = \"aq\"\n    hig = \"hig\"\n    let = \"let\"\n    imu = \"imu\"\n    judq = \"judq\"\n    iju = \"iju\"\n    hik = \"hik\"\n    ttp = \"ttp\"\n    other = \"other\"\n\n\ndef validate_event_type(value: str):\n    valid_values = [\n        \"airstrike\",\n        \"detention\",\n        \"captureandkill\",\n        \"insurgentskilled\",\n        \"exchangeoffire\",\n        \"civiliancasualty\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_province(value: str):\n    valid_values = [\n        \"badakhshan\",\n        \"badghis\",\n        \"baghlan\",\n        \"balkh\",\n        \"bamyan\",\n        \"day_kundi\",\n        \"farah\",\n        \"faryab\",\n        \"ghazni\",\n        \"ghor\",\n        \"helmand\",\n        \"herat\",\n        \"jowzjan\",\n        \"kabul\",\n        \"kandahar\",\n        \"kapisa\",\n        \"khost\",\n        \"kunar\",\n        \"kunduz\",\n        \"laghman\",\n        \"logar\",\n        \"nangarhar\",\n        \"nimroz\",\n        \"nuristan\",\n        \"paktya\",\n        \"paktika\",\n        \"panjshir\",\n        \"parwan\",\n        \"samangan\",\n        \"sar_e_pul\",\n        \"takhar\",\n        \"uruzgan\",\n        \"wardak\",\n        \"zabul\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_target_group(value: str):\n    valid_values = [\n        \"taliban\",\n        \"haqqani\",\n        \"criminals\",\n        \"aq\",\n        \"hig\",\n        \"let\",\n        \"imu\",\n        \"judq\",\n        \"iju\",\n        \"hik\",\n        \"ttp\",\n        \"other\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\nclass IsafEvent(BaseModel):\n    name: str = Field(\n        description=\"A title or name for the event which summarises the event as a headline\"\n    )\n    text: Optional[str] = Field(description=\"The full text of the press release\")\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    event_type: Set[Annotated[str, Field(validator=validate_event_type)]] = Field(\n        description=\"The event type. Can be multiple types.\"\n    )\n    province: Set[Annotated[str, Field(validator=validate_province)]] = Field(\n        description=\"The province in which the event occurred. Can be multiple provinces.\"\n    )\n    target_group: Set[Annotated[str, Field(validator=validate_target_group)]] = Field(\n        description=\"The group that was targetted during the event. Can be multiple groups.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\nHere’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:\n\nfrom typing import List\n\nevents: List[IsafEvent] = []\n\nfor i, row in list(train_df.iterrows()):\n    event_types = set(\n        eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n    )\n    provinces = set(province.strip().lower() for province in row[\"province\"].split(\",\"))\n    target_groups = set(\n        target_group.strip().lower() for target_group in row[\"targetgroup\"].split(\",\")\n    )\n\n    events.append(\n        IsafEvent(\n            name=row[\"name\"],\n            text=row[\"text\"],\n            start_date=row[\"StartDate\"].to_pydatetime().date(),\n            event_type=event_types,\n            province=provinces,\n            target_group=target_groups,\n            min_killed=int(row[\"minkilled\"]),\n            min_captured=int(row[\"mincaptured\"]),\n            killq=row[\"killq\"] == \"true\",\n            captureq=row[\"captureq\"] == \"true\",\n            killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n            airstrike=row[\"airstrike\"] == \"true\",\n            noshotsfired=row[\"noshotsfired\"] == \"true\",\n            min_leaders_killed=int(row[\"minleaderskilled\"]),\n            min_leaders_captured=int(row[\"minleaderscaptured\"]),\n        )\n    )\n\nprint(events[:2])\n\n[\n    IsafEvent(\n        name='Several insurgents killed in Helmand',\n        text='ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \nAfghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \nservice members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \ntheir position talking on radios today. After gaining positive identification of the insurgent positions, the \ncoalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \nin the area with weapons. After positive identification, coalition forces continued firing on the various insurgent\npositions, resulting in several more insurgents being killed.',\n        start_date=datetime.date(2011, 2, 18),\n        event_type={'insurgentskilled'},\n        province={'helmand'},\n        target_group={''},\n        min_killed=6,\n        min_captured=0,\n        killq=True,\n        captureq=False,\n        killcaptureraid=False,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=0,\n        min_leaders_captured=0\n    ),\n    IsafEvent(\n        name='Force Continues Targeting Haqqani Leadership',\n        text='Force Continues Targeting Haqqani Leadership\\nISAF Joint Command - Afghanistan\\u20282010-09-CA-211 \nFor Immediate Release\\u2028Download PDF \\u2028\\u2028\\u2028\\xa0KABUL, Afghanistan (Sept. 20) - An Afghan and \ncoalition security force detained two insurgents, including a Haqqani Network sub-commander operating in Khost \nprovince, Sunday. \\u2028\\u2028The commander coordinated and conducted attacks on coalition forces operating in the \nprovince and was formerly active in Kabul. \\u2028\\u2028Intelligence reports led the security force to a compound \nnorthwest of Khost City to search for the commander. Afghan forces called for all occupants to exit the buildings \npeacefully and then the combined force cleared and secured the compound. During the clearance, an armed individual \ncame out of an adjacent building toward the security force. The forced engaged the individual and killed him. \n\\u2028\\u2028After the area was secure, the security force questioned the residents at the scene and detained the \ncommander and one of his associates. The security force also found multiple automatic weapons, magazines and \ngrenades at the scene. \\u2028\\u2028The assault force protected the women and children throughout the search.',\n        start_date=datetime.date(2010, 9, 19),\n        event_type={'captureandkill'},\n        province={'khost'},\n        target_group={'haqqani'},\n        min_killed=1,\n        min_captured=2,\n        killq=True,\n        captureq=True,\n        killcaptureraid=True,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=0,\n        min_leaders_captured=0\n    )\n]\n\n\n\nSo this is data that we’ve already labelled. You can see the text that we’ll provide as input to our model, and then you can see the various fields that we’re hoping our model can learn to extract. As a JSON string, the prediction that we’re hoping our model will output would look like this:\n\njson_str = events[0].model_dump_json(exclude={\"text\"})\nprint(json_str)\n\n{\"name\":\"Several insurgents killed in \nHelmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi\nn_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"\n:false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}\n\n\n\nIf you wish to view more examples of the data, I created an interim dataset which I uploaded to the Hugging Face Hub, but it’s not completely in the required form for finetuning so I’ll just link to it here and you can explore it to see the pairings of input and output if you’re interested."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#writing-alpaca-sharegpt-format-jsonl",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#writing-alpaca-sharegpt-format-jsonl",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Writing Alpaca / ShareGPT format JSONL",
    "text": "Writing Alpaca / ShareGPT format JSONL\nIn the course, Hamel shared an example from work he’d done with Honeycomb to output in a particular format. He shared his notebooks and I found this useful as an initial example to move forward with. He uses the ShareGPT template to format his data, and I followed that format for my data. This looks something like this:\n{\n    \"conversations\": [\n        {\n            \"from\": \"system\",\n            \"value\": \"Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produce a Honeycomb query.\"\n        },\n        {\n            \"from\": \"human\",\n            \"value\": \"\\n\\nNLQ: \\\"group by HTTP method\\\"\\n\\nColumns: ['query_string_num_tokens', 'query_string_length', 'data_queries', 'http.target', 'task.id', 'trace_root.http.target', 'topic', 'http.host', 'total_hits', 'db.user', 'domain_types', 'db.name', 'graphql.document', 'history', 'http.scheme', 'http.method', 'frontend.version', 'disposition_for_dBVVysC8x4Ymwg9rtjMckgw9', 'db.system', 'event_name', 'organization', 'auth.logout', 'organizations', 'name', 'net.transport', 'db.operation', 'disposition_for_UvsPPBVUn9FDuzDjsjYCqopq', 'disposition_for_1RUGSd7GdnP5tuKdgqBRZUm2', 'process.pid', 'disposition_for_6uyAoBc3PuvEcTTPFgPM3Rtk', 'exception.stacktrace', 'data_ingestion_individuals_count', 'disposition_for_qrnUBUz8YBfNX7Liekq6nKi3', 'task_type.type', 'disposition_for_JQDNbuUdaQcEbEwQNxUbV5EF', 'disposition_for_rAcWoXfbHw4eWoJFH4ZcY8ue', 'disposition_for_eShqQoC9jUi9VQBidpp2oXHP', 'parent_name', 'template', 'graphql.operation.name', 'span.num_links', 'disposition_for_kNSPtvsCWkDoEyFP2QE6VPmQ', 'disposition_for_UUqf9L1qkFxDNEvcgsVMA2yy', 'disposition_for_vwbbN76HZ7uitLubvkUjPFQE', 'disposition_for_aAto1pGrdF5RunpSX8sY5hvn', 'disposition_for_UbKCMdnkPQ6TuHrfdBo5juZu', 'disposition_for_QfrvmoHxSgLPJXPKZCrZfGo8', 'disposition_for_NoKSSruBRCX6UG28PzmkybUd', 'disposition_for_UZAqvZ5XVBZjKKWuMeRkRayS', 'organization_token', 'duration_ms', 'trace.parent_id', 'db.statement', 'exception.message', 'error', 'service.name', 'http.status_code', 'http.route']\"\n        },\n        {\n            \"from\": \"gpt\",\n            \"value\": \"\\n{\\\"breakdowns\\\": [\\\"http.method\\\"], \\\"calculations\\\": [{\\\"op\\\": \\\"COUNT\\\"}], \\\"time_range\\\": 7200}\"\n        }\n    ]\n}\nYou can see that a single entry is part of a conversations key, and then you have a series of messages with system, human and gpt roles. I followed this closely for my data, but with a base model I’m pretty sure this isn’t strictly necessary. (The second option is what I’ll show a bit later).\nWriting the data is a case of wrangling the data so it fits the format above, and doing it separately for our train and test datasets. You’ll note that in the system call I stuff in the event types, provinces and target groups as a way to provide some context to the model. This was something that Hamel did in his example.\n\nimport os\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\nfrom typing import List\nimport json\n\n# Load the dataset\ndataset = load_dataset(\"strickvl/isafpressreleases\")\ntrain_target_file_path = \"../data/sharegpt_isaf_press_releases_ft_train.jsonl\"\ntest_target_file_path = \"../data/sharegpt_isaf_press_releases_ft_test.jsonl\"\n\n\ndef write_data_to_jsonl(df: pd.DataFrame, target_file_path: str) -&gt; None:\n    events: List[IsafEvent] = []\n\n    for i, row in list(df.iterrows()):\n        event_types = set(\n            eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n        )\n        provinces = set(\n            province.strip().lower() for province in row[\"province\"].split(\",\")\n        )\n        target_groups = set(\n            target_group.strip().lower()\n            for target_group in row[\"targetgroup\"].split(\",\")\n        )\n\n        events.append(\n            IsafEvent(\n                name=row[\"name\"],\n                text=row[\"text\"],\n                start_date=row[\"StartDate\"].to_pydatetime().date(),\n                event_type=event_types,\n                province=provinces,\n                target_group=target_groups,\n                min_killed=int(row[\"minkilled\"]),\n                min_captured=int(row[\"mincaptured\"]),\n                killq=row[\"killq\"] == \"true\",\n                captureq=row[\"captureq\"] == \"true\",\n                killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n                airstrike=row[\"airstrike\"] == \"true\",\n                noshotsfired=row[\"noshotsfired\"] == \"true\",\n                min_leaders_killed=int(row[\"minleaderskilled\"]),\n                min_leaders_captured=int(row[\"minleaderscaptured\"]),\n            )\n        )\n\n    processed_data = [\n        {\n            \"conversations\": [\n                {\n                    \"from\": \"system\",\n                    \"value\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\",\n                },\n                {\"from\": \"human\", \"value\": f\"PRESS RELEASE TEXT: {event.text}\"},\n                {\"from\": \"gpt\", \"value\": f\"{event.model_dump_json(exclude={'text'})}\"},\n            ]\n        }\n        for event in events\n    ]\n\n    # Write the processed data to a JSONL file\n    os.makedirs(os.path.dirname(target_file_path), exist_ok=True)\n    with open(target_file_path, \"w\") as f:\n        for item in processed_data:\n            f.write(json.dumps(item) + \"\\n\")\n\n\n# Write the data to disk\ntrain_df = pd.DataFrame(dataset[\"train\"])\ntest_df = pd.DataFrame(dataset[\"test\"])\n\nwrite_data_to_jsonl(train_df, train_target_file_path)\nwrite_data_to_jsonl(test_df, test_target_file_path)\n\nThe first line of the training file looks like this now:\n\nwith open(train_target_file_path, \"r\") as f:\n    print(f.readline())\n\n{\"conversations\": [{\"from\": \"system\", \"value\": \"You are an expert at identifying events in a press release. You are\nprecise and always make sure you are correct, drawing inference from the text of the press release. event_types = \n['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces =\n['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', \n'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', \n'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', \n'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', \n'judq', 'iju', 'hik', 'ttp', 'other']\"}, {\"from\": \"human\", \"value\": \"PRESS RELEASE TEXT: ISAF Joint Command Evening\nOperational Update Feb. 19, 2011\\nISAF Joint Command - Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \n\\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF service members at a compound in Sangin district, Helmand \nprovince observed numerous insurgents north and south of their position talking on radios today. After gaining \npositive identification of the insurgent positions, the coalition troops engaged, killing several insurgents. \nLater, the ISAF troops observed more insurgents positioning in the area with weapons. After positive \nidentification, coalition forces continued firing on the various insurgent positions, resulting in several more \ninsurgents being killed.\"}, {\"from\": \"gpt\", \"value\": \"{\\\"name\\\":\\\"Several insurgents killed in \nHelmand\\\",\\\"start_date\\\":\\\"2011-02-18\\\",\\\"event_type\\\":[\\\"insurgentskilled\\\"],\\\"province\\\":[\\\"helmand\\\"],\\\"target_g\nroup\\\":[\\\"\\\"],\\\"min_killed\\\":6,\\\"min_captured\\\":0,\\\"killq\\\":true,\\\"captureq\\\":false,\\\"killcaptureraid\\\":false,\\\"air\nstrike\\\":false,\\\"noshotsfired\\\":false,\\\"min_leaders_killed\\\":0,\\\"min_leaders_captured\\\":0}\"}]}"
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#writing-template-free-json",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#writing-template-free-json",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Writing Template-Free JSON",
    "text": "Writing Template-Free JSON\nAnother option available to us, especially if we are finetuning a base LLM (as opposed to one that has been instruction-tuned), is to write our data in a different format. Hamel’s written a guide for this on his blog and that has also been absorbed into the official axolotl documentation, so read the blog if you want more information.\nThe basic idea is that instead of following a format like the one above, we can essentially create our own that’s custom to our own needs. You want this kind of freedom because to follow one of the standard templates is sometimes to shoot yourself in the food with artifacts of those templates that you don’t need in your output.\nThe key is to specify train_on_inputs as false in our axolotl config which will allow us to mask certain segments of our input data. This means that our model won’t learn the inputs but only the outputs (which we’ll specify).\nAll that we have to do is set up the JSONL output in a way that makes sense for our use case:\n\ntemplate_free_train_target_file_path = (\n    \"../data/templatefree_isaf_press_releases_ft_train.jsonl\"\n)\ntemplate_free_test_target_file_path = (\n    \"../data/templatefree_isaf_press_releases_ft_test.jsonl\"\n)\n\n\ndef write_data_to_jsonl(df: pd.DataFrame, target_file_path: str) -&gt; None:\n    events: List[IsafEvent] = []\n\n    for i, row in list(df.iterrows()):\n        event_types = set(\n            eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n        )\n        provinces = set(\n            province.strip().lower() for province in row[\"province\"].split(\",\")\n        )\n        target_groups = set(\n            target_group.strip().lower()\n            for target_group in row[\"targetgroup\"].split(\",\")\n        )\n\n        events.append(\n            IsafEvent(\n                name=row[\"name\"],\n                text=row[\"text\"],\n                start_date=row[\"StartDate\"].to_pydatetime().date(),\n                event_type=event_types,\n                province=provinces,\n                target_group=target_groups,\n                min_killed=int(row[\"minkilled\"]),\n                min_captured=int(row[\"mincaptured\"]),\n                killq=row[\"killq\"] == \"true\",\n                captureq=row[\"captureq\"] == \"true\",\n                killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n                airstrike=row[\"airstrike\"] == \"true\",\n                noshotsfired=row[\"noshotsfired\"] == \"true\",\n                min_leaders_killed=int(row[\"minleaderskilled\"]),\n                min_leaders_captured=int(row[\"minleaderscaptured\"]),\n            )\n        )\n\n    processed_data = [\n        {\n            \"segments\": [\n                {\n                    \"label\": False,\n                    \"text\": \"&lt;s&gt;You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\",\n                },\n                {\"label\": False, \"text\": f\"PRESS RELEASE TEXT: {event.text}\"},\n                {\n                    \"label\": True,\n                    \"text\": f\"{event.model_dump_json(exclude={'text'})}&lt;/s&gt;\",\n                },\n            ]\n        }\n        for event in events\n    ]\n\n    # Write the processed data to a JSONL file\n    os.makedirs(os.path.dirname(target_file_path), exist_ok=True)\n    with open(target_file_path, \"w\") as f:\n        for item in processed_data:\n            f.write(json.dumps(item) + \"\\n\")\n\n\nwrite_data_to_jsonl(train_df, template_free_train_target_file_path)\nwrite_data_to_jsonl(test_df, template_free_test_target_file_path)\n\nAnd you can now see the difference in the format of the JSONL dataset we’ve constructured:\n\nwith open(template_free_train_target_file_path, \"r\") as f:\n    print(f.readline())\n\n{\"segments\": [{\"label\": false, \"text\": \"&lt;s&gt;You are an expert at identifying events in a press release. You are \nprecise and always make sure you are correct, drawing inference from the text of the press release. event_types = \n['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces =\n['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', \n'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', \n'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', \n'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', \n'judq', 'iju', 'hik', 'ttp', 'other']\"}, {\"label\": false, \"text\": \"PRESS RELEASE TEXT: ISAF Joint Command Evening \nOperational Update Feb. 19, 2011\\nISAF Joint Command - Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \n\\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF service members at a compound in Sangin district, Helmand \nprovince observed numerous insurgents north and south of their position talking on radios today. After gaining \npositive identification of the insurgent positions, the coalition troops engaged, killing several insurgents. \nLater, the ISAF troops observed more insurgents positioning in the area with weapons. After positive \nidentification, coalition forces continued firing on the various insurgent positions, resulting in several more \ninsurgents being killed.\"}, {\"label\": true, \"text\": \"{\\\"name\\\":\\\"Several insurgents killed in \nHelmand\\\",\\\"start_date\\\":\\\"2011-02-18\\\",\\\"event_type\\\":[\\\"insurgentskilled\\\"],\\\"province\\\":[\\\"helmand\\\"],\\\"target_g\nroup\\\":[\\\"\\\"],\\\"min_killed\\\":6,\\\"min_captured\\\":0,\\\"killq\\\":true,\\\"captureq\\\":false,\\\"killcaptureraid\\\":false,\\\"air\nstrike\\\":false,\\\"noshotsfired\\\":false,\\\"min_leaders_killed\\\":0,\\\"min_leaders_captured\\\":0}&lt;/s&gt;\"}]}"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "",
    "text": "My last post outlined the kinds of evaluation I need and want to understand how well my finetuned LLM is performing in the task of structured data extraction from press releases. Let’s start with the core metric I’m interested in, accuracy, and then later we can dive into some of the other evaluation metrics as well."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#tldr",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#tldr",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "TL;DR",
    "text": "TL;DR\nThe headline for this post could well have been: finetuned models beat OpenAI, but evals were a bit painful to implement. There’s a lot of hidden code here in this post and it was slow to run. This step was the first time during the work for the finetuning course where I felt the pain and tradeoffs around the choice to finetune. I can see that without a system of some kind to handle this, the complexity of maintaining it all will start to mount up. But more on that at the end!\nThis is a long post with lots of detail. I’ve tried to minimise the amount of code you see, but if you want to see how the charts or evals were done, expand the ‘code’ sections. If you’re interested in cutting straight to the aggregate results, click here to go to the end of this post. (To see the rest of the blog posts about this project, please click here. Some context: I’m doing some finetuning as part of the Hamel Husain / Dan Becker Finetuning course on Maven using some data I collected and labeled a few years back that makes for a cool little test of how good it works for structured data extraction.)"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#loading-the-datasets",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#loading-the-datasets",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Loading the datasets",
    "text": "Loading the datasets\nThe data is all available on the Hugging Face Hub in a public repository, and for the purposes of these evaluations I want to use the test split of the dataset since none of our models have seen that data yet so it’s good for determining how well our model performs with new data.\n\n\nCode\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\n\ntest_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"test\")\ntest_df = pd.DataFrame(test_dataset)\n\n\n\ntest_dataset\n\nDataset({\n    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],\n    num_rows: 724\n})\n\n\nWe’ll first add an extra column to our DataFrame and then make a prediction for each and every row in the dataset. We’ll store a copy of the prediction to the column so as to make sure we don’t have to do this compute-intensive step repeatedly.\nBut first we’ll assemple the data as Pydantic objects so as to handle validation and other quality of life features.\n\n\nCode\nfrom enum import Enum\nfrom typing import Dict, Set, Annotated, Optional\nfrom pydantic import BaseModel, Field, validator, ValidationInfo\nfrom datetime import date\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\n\n\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jowzjan = \"jowzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktya = \"paktya\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pul = \"sar_e_pul\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\n\n\nclass TargetGroup(str, Enum):\n    taliban = \"taliban\"\n    haqqani = \"haqqani\"\n    criminals = \"criminals\"\n    aq = \"aq\"\n    hig = \"hig\"\n    let = \"let\"\n    imu = \"imu\"\n    judq = \"judq\"\n    iju = \"iju\"\n    hik = \"hik\"\n    ttp = \"ttp\"\n    other = \"other\"\n\n\ndef validate_event_type(value: str):\n    valid_values = [\n        \"airstrike\",\n        \"detention\",\n        \"captureandkill\",\n        \"insurgentskilled\",\n        \"exchangeoffire\",\n        \"civiliancasualty\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_province(value: str):\n    valid_values = [\n        \"badakhshan\",\n        \"badghis\",\n        \"baghlan\",\n        \"balkh\",\n        \"bamyan\",\n        \"day_kundi\",\n        \"farah\",\n        \"faryab\",\n        \"ghazni\",\n        \"ghor\",\n        \"helmand\",\n        \"herat\",\n        \"jowzjan\",\n        \"kabul\",\n        \"kandahar\",\n        \"kapisa\",\n        \"khost\",\n        \"kunar\",\n        \"kunduz\",\n        \"laghman\",\n        \"logar\",\n        \"nangarhar\",\n        \"nimroz\",\n        \"nuristan\",\n        \"paktya\",\n        \"paktika\",\n        \"panjshir\",\n        \"parwan\",\n        \"samangan\",\n        \"sar_e_pul\",\n        \"takhar\",\n        \"uruzgan\",\n        \"wardak\",\n        \"zabul\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_target_group(value: str):\n    valid_values = [\n        \"taliban\",\n        \"haqqani\",\n        \"criminals\",\n        \"aq\",\n        \"hig\",\n        \"let\",\n        \"imu\",\n        \"judq\",\n        \"iju\",\n        \"hik\",\n        \"ttp\",\n        \"other\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\nclass IsafEvent(BaseModel):\n    name: str = Field(\n        description=\"A title or name for the event which summarises the event as a headline\"\n    )\n    text: Optional[str] = Field(description=\"The full text of the press release\")\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    event_type: Set[Annotated[str, Field(validator=validate_event_type)]] = Field(\n        description=\"The event type. Can be multiple types.\"\n    )\n    province: Set[Annotated[str, Field(validator=validate_province)]] = Field(\n        description=\"The province in which the event occurred. Can be multiple provinces.\"\n    )\n    target_group: Set[Annotated[str, Field(validator=validate_target_group)]] = Field(\n        description=\"The group that was targetted during the event. Can be multiple groups.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n    predictions: Dict[str, str] = Field(\n        default={},\n        description=\"The predictions from the model. Keys are the model name and the value is the prediction\",\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\nHere’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:\n\n\nCode\nfrom typing import List\n\nevents: List[IsafEvent] = []\n\nfor i, row in list(test_df.iterrows()):\n    event_types = set(\n        eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n    )\n    provinces = set(province.strip().lower() for province in row[\"province\"].split(\",\"))\n    target_groups = set(\n        target_group.strip().lower() for target_group in row[\"targetgroup\"].split(\",\")\n    )\n\n    events.append(\n        IsafEvent(\n            name=row[\"name\"],\n            text=row[\"text\"],\n            start_date=row[\"StartDate\"].to_pydatetime().date(),\n            event_type=event_types,\n            province=provinces,\n            target_group=target_groups,\n            min_killed=int(row[\"minkilled\"]),\n            min_captured=int(row[\"mincaptured\"]),\n            killq=row[\"killq\"] == \"true\",\n            captureq=row[\"captureq\"] == \"true\",\n            killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n            airstrike=row[\"airstrike\"] == \"true\",\n            noshotsfired=row[\"noshotsfired\"] == \"true\",\n            min_leaders_killed=int(row[\"minleaderskilled\"]),\n            min_leaders_captured=int(row[\"minleaderscaptured\"]),\n        )\n    )\n\nprint(events[:2])\n\n\n[\n    IsafEvent(\n        name='5',\n        text='2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n        start_date=datetime.date(2013, 1, 24),\n        event_type={'insurgentskilled'},\n        province={'ghazni'},\n        target_group={'taliban'},\n        min_killed=1,\n        min_captured=0,\n        killq=True,\n        captureq=False,\n        killcaptureraid=False,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=1,\n        min_leaders_captured=0,\n        predictions={}\n    ),\n    IsafEvent(\n        name='2',\n        text='2011-11-S-034\\nISAF Joint Command - Afghanistan\\nFor Immediate Release\\n\\nKABUL, Afghanistan (Nov. \n20, 2011)\\nA coalition security force detained numerous suspected insurgents during an operation in Marjeh \ndistrict, Helmand province, yesterday.  The force conducted the operation after receiving information that a group \nof insurgents were at a compound in the area.  After calling for the men inside to come out peacefully, the \ninsurgents emerged and were detained without incident.',\n        start_date=datetime.date(2011, 11, 19),\n        event_type={'detention'},\n        province={'helmand'},\n        target_group={''},\n        min_killed=0,\n        min_captured=4,\n        killq=False,\n        captureq=True,\n        killcaptureraid=True,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=0,\n        min_leaders_captured=0,\n        predictions={}\n    )\n]\n\n\n\nSo when we’re making the prediction we’re hoping to get a JSON string like this out from the model:\n\njson_str = events[0].model_dump_json(exclude={\"text\", \"predictions\"})\nprint(json_str)\n\n{\"name\":\"5\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tali\nban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}\n\n\n\nI’m starting with full evaluations using the GPT models and I’ll need a slightly more elaborate prompt in order to get decent results. I can’t pass in the exact same prompt as the one I used for the finetuned model since the GPT models haven’t been trained or finetuned to respond to those specific prompts. This is sort of an interesting problem to have: how much effort do we put into the GPT prompts to try to get the same level of accuracy as the finetuned model? Or in other words, is there even a way to really compare like to like between models that must accept different prompts?\nLet’s try this out for OpenAI GPT-4o and GPT-4 Turbo and see how we get on. You’ll note how long the prompt has to be to give the GPT models a fighting chance against the finetuned models. Ideally I’d stuff in even more examples into the context, but I also don’t want to explode the number of tokens I’m using.\n\nfrom openai import OpenAI\nfrom rich import print\nimport json\nimport os\n\n\ndef query_openai(article_text: str, model: str) -&gt; str:\n    query = (\n        f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):\\n{article_text}\\n\\n\"\n        \"## Extraction request\\n\"\n        \"Please extract the following information from the press release:\\n\"\n        \"- The name of the event (summarising the event / text as a headline)\\n\"\n        \"- The start date of the event\\n\"\n        \"- The event type(s)\\n\"\n        \"- The province(s) in which the event occurred\\n\"\n        \"- The target group(s) of the event\\n\"\n        \"- The minimum number of people killed during the event\\n\"\n        \"- The minimum number of people captured during the event\\n\"\n        \"- Whether someone was killed or not during the event\\n\"\n        \"- Whether someone was captured or not during the event\\n\"\n        \"- Whether the event was a so-called 'kill-capture raid'\\n\"\n        \"- Whether an airstrike was used during the event\\n\"\n        \"- Whether no shots were fired during the event\\n\"\n        \"- The minimum number of leaders killed during the event\\n\"\n        \"- The minimum number of leaders captured during the event\\n\\n\"\n        \"## Annotation notes:\\n\"\n        \"- A 'faciliator' is not a leader.\\n\"\n        \"- If a press release states that 'insurgents' were detained without further \"\n        \"details, assign a minimum number of two detained. Interpret 'a couple' as \"\n        \"two. Interpret 'several' as at least three, even though it may sometimes \"\n        \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \"\n        \"small group', and 'multiple' as denoting at least three, even if they \"\n        \"sometimes refer to larger numbers. Choose the smaller number if no other \"\n        \"information is available in the press release to come up with a minimally \"\n        \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \"\n        \"and 'a large number' as at least five.\\n\\n\"\n        \"## Example:\\n\"\n        \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \"\n        \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \"\n        \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \"\n        \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \"\n        \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \"\n        \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \"\n        \"positions, resulting in several more insurgents being killed.'\\n\\n\"\n        'Output: `{\"name\":\"Several insurgents killed in '\n        'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi'\n        'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"'\n        ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`'\n    )\n\n    # set up the prediction harness\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n    response = client.chat.completions.create(\n        model=model,\n        response_format={\"type\": \"json_object\"},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise \"\n                \"and always make sure you are correct, drawing inference from the text of the \"\n                \"press release.\\n\\n You always return a JSON string with the following schema: \"\n                \"## JSON Schema details\\n\"\n                \"Here is some of the schema for the JSON output string you \"\n                \"should make use of: event_types = ['airstrike', 'detention', \"\n                \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \"\n                \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \"\n                \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \"\n                \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \"\n                \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \"\n                \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \"\n                \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \"\n                \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        temperature=1,\n    )\n\n    return response.choices[0].message.content\n\nWe can make sure this function works with a quick example:\n\njson_str = query_openai(events[0].text, \"gpt-4o\")\nprint(json.loads(json_str))\n\n{\n    'name': 'Taliban leader Alaudin killed in Ghazni',\n    'start_date': '2013-01-24',\n    'event_type': ['insurgentskilled'],\n    'province': ['ghazni'],\n    'target_group': ['taliban'],\n    'min_killed': 1,\n    'min_captured': 0,\n    'killq': True,\n    'captureq': False,\n    'killcaptureraid': True,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 1,\n    'min_leaders_captured': 0\n}\n\n\n\nOur model is working (as expected) and we’re also getting a JSON string back. Let’s assemble something that will iterate through all of our test data, get predictions, and then store those predictions on our Pydantic object.\nFor the bulk predictions, we’ll make sure to do this async, since there are lots of events and we don’t want to waiting all day. You’ll see I also had to add some retries to the function to account for rate limiting on the GPT-3.5-turbo model.\n\n\nCode\n# make async work within a notebook\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nimport aiohttp\nimport asyncio\nfrom typing import List\nfrom openai import OpenAI\n\n\nasync def async_query_openai(\n    session,\n    article_text: str,\n    model: str,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n) -&gt; str:\n    query = (\n        f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):\\n{article_text}\\n\\n\"\n        \"## Extraction request\\n\"\n        \"Please extract the following information from the press release:\\n\"\n        \"- The name of the event (summarising the event / text as a headline)\\n\"\n        \"- The start date of the event\\n\"\n        \"- The event type(s)\\n\"\n        \"- The province(s) in which the event occurred\\n\"\n        \"- The target group(s) of the event\\n\"\n        \"- The minimum number of people killed during the event\\n\"\n        \"- The minimum number of people captured during the event\\n\"\n        \"- Whether someone was killed or not during the event\\n\"\n        \"- Whether someone was captured or not during the event\\n\"\n        \"- Whether the event was a so-called 'kill-capture raid'\\n\"\n        \"- Whether an airstrike was used during the event\\n\"\n        \"- Whether no shots were fired during the event\\n\"\n        \"- The minimum number of leaders killed during the event\\n\"\n        \"- The minimum number of leaders captured during the event\\n\\n\"\n        \"## Annotation notes:\\n\"\n        \"- A 'faciliator' is not a leader.\\n\"\n        \"- If a press release states that 'insurgents' were detained without further \"\n        \"details, assign a minimum number of two detained. Interpret 'a couple' as \"\n        \"two. Interpret 'several' as at least three, even though it may sometimes \"\n        \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \"\n        \"small group', and 'multiple' as denoting at least three, even if they \"\n        \"sometimes refer to larger numbers. Choose the smaller number if no other \"\n        \"information is available in the press release to come up with a minimally \"\n        \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \"\n        \"and 'a large number' as at least five.\\n\\n\"\n        \"## Example:\\n\"\n        \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \"\n        \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \"\n        \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \"\n        \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \"\n        \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \"\n        \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \"\n        \"positions, resulting in several more insurgents being killed.'\\n\\n\"\n        'Output: `{\"name\":\"Several insurgents killed in '\n        'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi'\n        'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"'\n        ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`'\n    )\n\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n    retries = 0\n    while retries &lt; max_retries:\n        async with session.post(\n            \"https://api.openai.com/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {client.api_key}\"},\n            json={\n                \"model\": model,\n                \"response_format\": {\"type\": \"json_object\"},\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are an expert at identifying events in a press release. You are precise \"\n                        \"and always make sure you are correct, drawing inference from the text of the \"\n                        \"press release.\\n\\n You always return a JSON string with the following schema: \"\n                        \"## JSON Schema details\\n\"\n                        \"Here is some of the schema for the JSON output string you \"\n                        \"should make use of: event_types = ['airstrike', 'detention', \"\n                        \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \"\n                        \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \"\n                        \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \"\n                        \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \"\n                        \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \"\n                        \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \"\n                        \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \"\n                        \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n\",\n                    },\n                    {\"role\": \"user\", \"content\": query},\n                ],\n                \"temperature\": 1,\n            },\n        ) as response:\n            result = await response.json()\n            if \"error\" in result:\n                error_message = result[\"error\"][\"message\"]\n                if \"Rate limit reached\" in error_message:\n                    # retry_delay_ms = float(\n                    #     error_message.split(\"Please try again in \")[1].split(\"ms\")[0]\n                    # )\n                    retry_delay_ms = 35000\n                    retry_delay_seconds = retry_delay_ms / 1000\n                    print(\n                        f\"Rate limit exceeded. Retrying in {retry_delay_seconds} seconds...\"\n                    )\n                    await asyncio.sleep(retry_delay_seconds)\n                    retries += 1\n                    continue\n                else:\n                    print(f\"Error during prediction.\\nFull result object: {result}\")\n                    return \"\"\n            try:\n                return result[\"choices\"][0][\"message\"][\"content\"]\n            except KeyError:\n                print(f\"Error during prediction.\\nFull result object: {result}\")\n                return \"\"\n\n    print(f\"Max retries exceeded for event.\\nFull result object: {result}\")\n    return \"\"\n\n\nasync def get_gpt_predictions_async(\n    model: str,\n    events: List[IsafEvent],\n    logging_n: int = 100,\n    max_concurrent_requests: int = 5,\n) -&gt; List[IsafEvent]:\n    async with aiohttp.ClientSession() as session:\n        semaphore = asyncio.Semaphore(max_concurrent_requests)\n        tasks = []\n        for i, event in enumerate(events, start=1):\n            if i % logging_n == 0:\n                print(f\"Predicting event {i} of {len(events)} using {model}\")\n\n            async def make_request(session, event):\n                async with semaphore:\n                    return await async_query_openai(\n                        session, event.text, model, max_retries=5\n                    )\n\n            task = asyncio.ensure_future(make_request(session, event))\n            tasks.append(task)\n\n        predictions = await asyncio.gather(*tasks)\n        for event, prediction in zip(events, predictions):\n            event.predictions[model] = prediction\n\n    return events\n\n\nasync def main():\n    events_4o = await get_gpt_predictions_async(\n        \"gpt-4o\", events, max_concurrent_requests=10\n    )\n    events_4turbo = await get_gpt_predictions_async(\n        \"gpt-4-turbo\", events_4o, max_concurrent_requests=10\n    )\n    full_events = await get_gpt_predictions_async(\n        \"gpt-3.5-turbo\", events_4turbo, max_concurrent_requests=10\n    )\n\n\nawait main()\n\n\nSo as you can now see, we have three predictions attached to each event.\n\nprint(events[0])\n\nIsafEvent(\n    name='5',\n    text='2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n    start_date=datetime.date(2013, 1, 24),\n    event_type={'insurgentskilled'},\n    province={'ghazni'},\n    target_group={'taliban'},\n    min_killed=1,\n    min_captured=0,\n    killq=True,\n    captureq=False,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=False,\n    min_leaders_killed=1,\n    min_leaders_captured=0,\n    predictions={\n        'gpt-4o': '{\\n  \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n  \"start_date\": \"2013-01-24\",\\n  \n\"event_type\": [\"insurgentskilled\", \"captureandkill\"],\\n  \"province\": [\"ghazni\"],\\n  \"target_group\": [\"taliban\"],\\n \n\"min_killed\": 1,\\n  \"min_captured\": 0,\\n  \"killq\": true,\\n  \"captureq\": false,\\n  \"killcaptureraid\": true,\\n  \n\"airstrike\": false,\\n  \"noshotsfired\": false,\\n  \"min_leaders_killed\": 1,\\n  \"min_leaders_captured\": 0\\n}',\n        'gpt-4-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": true,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-3.5-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni province\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": false,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}'\n    }\n)\n\n\n\nI have all these predictions living in memory right now so it’s probably a good time to commit these to a dataset and push them to the Hugging Face Hub in case the notebook crashes or my local machine shuts down or something else unexpected.\nI’ll create a function to handle this as we’ll be repeating this process for the other models as well. It’s a bit verbose but I thought it preferable so you can see what’s going on.\n\n\nCode\nfrom datasets import Dataset\n\n\ndef convert_to_dataset(data: List[IsafEvent]) -&gt; Dataset:\n    names = []\n    texts = []\n    start_dates = []\n    provinces = []\n    target_groups = []\n    event_types = []\n    predictions = []\n    min_killeds = []\n    min_captureds = []\n    killqs = []\n    captureqs = []\n    killcaptureraids = []\n    airstrikes = []\n    noshotsfireds = []\n    min_leaders_killeds = []\n    min_leaders_captureds = []\n\n    for item in data:\n        names.append(item.name)\n        texts.append(item.text)\n        start_dates.append(item.start_date)\n        provinces.append(item.province)\n        target_groups.append(item.target_group)\n        event_types.append(item.event_type)\n        predictions.append(item.predictions)\n        min_killeds.append(item.min_killed)\n        min_captureds.append(item.min_captured)\n        killqs.append(item.killq)\n        captureqs.append(item.captureq)\n        killcaptureraids.append(item.killcaptureraid)\n        airstrikes.append(item.airstrike)\n        noshotsfireds.append(item.noshotsfired)\n        min_leaders_killeds.append(item.min_leaders_killed)\n        min_leaders_captureds.append(item.min_leaders_captured)\n\n    dataset_dict = {\n        \"name\": names,\n        \"text\": texts,\n        \"predictions\": predictions,\n        \"start_date\": start_dates,\n        \"province\": provinces,\n        \"target_group\": target_groups,\n        \"event_type\": event_types,\n        \"min_killed\": min_killeds,\n        \"min_captured\": min_captureds,\n        \"killq\": killqs,\n        \"captureq\": captureqs,\n        \"killcaptureraid\": killcaptureraids,\n        \"airstrike\": airstrikes,\n        \"noshotsfired\": noshotsfireds,\n        \"min_leaders_killed\": min_leaders_killeds,\n        \"min_leaders_captured\": min_leaders_captureds,\n    }\n    dataset = Dataset.from_dict(dataset_dict)\n\n    return dataset\n\n\ndef convert_and_push_dataset(\n    events: List[IsafEvent], name: str, split_name: str = \"train\"\n):\n    \"\"\"Convert a list of Pydantic objects to a HF Dataset object, then push to\n    the hub.\"\"\"\n    hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n\n    dataset = convert_to_dataset(events)\n    dataset.push_to_hub(\n        f\"strickvl/{name}\",\n        token=hf_token,\n        private=True,\n        create_pr=True,\n        split=split_name,\n    )\n\n\nA more concise and abstract version of the convert_to_dataset function could be something like:\ndef convert_to_dataset(data: List[BaseModel]) -&gt; Dataset:\n    dataset_dict = {}\n\n    for field_name, field_value in data[0].__fields__.items():\n        field_type = field_value.outer_type_\n        if field_type in [str, int, float, bool, date]:\n            dataset_dict[field_name] = [getattr(item, field_name) for item in data]\n        elif field_type == set:\n            dataset_dict[field_name] = [list(getattr(item, field_name)) for item in data]\n        elif issubclass(field_type, BaseModel):\n            dataset_dict[field_name] = [getattr(item, field_name).dict() for item in data]\n        else:\n            dataset_dict[field_name] = [getattr(item, field_name) for item in data]\n\n    dataset = Dataset.from_dict(dataset_dict)\n    return dataset\nBut for now let’s just push our data to the Hub.\n\nconvert_and_push_dataset(events, \"isafpressreleases_with_preds\", split_name=\"test\")"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#reloading-the-predictions-dataset",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#reloading-the-predictions-dataset",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Reloading the predictions dataset",
    "text": "Reloading the predictions dataset\nLet’s start by loading our dataset and then we can get into adding some local model predictions:\n\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds\")[\n    \"test\"\n].to_list()\n\nWe trained some local models, so let’s add those predictions to the dataset."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-tinyllama-predictions",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-tinyllama-predictions",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned TinyLlama predictions",
    "text": "Finetuned TinyLlama predictions\n\n\nCode\nfrom typing import Union\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\ndef prompt(press_release: str) -&gt; str:\n    return f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\n\n### Instruction:\n\nPRESS RELEASE TEXT: \"{press_release}\"\n\n### Response:\n\"\"\"\n\n\ndef prompt_tok(\n    model: AutoPeftModelForCausalLM,\n    tokenizer: AutoTokenizer,\n    press_release: str,\n    return_ids: bool = False,\n) -&gt; Union[str, torch.Tensor]:\n    _p = prompt(press_release)\n    input_ids = tokenizer(_p, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n    out_ids = model.generate(input_ids=input_ids, max_new_tokens=5000, do_sample=False)\n    ids = out_ids.detach().cpu().numpy()\n    if return_ids:\n        return out_ids\n    return tokenizer.batch_decode(ids, skip_special_tokens=True)[0][len(_p) :]\n\n\ntinyllama_sharegpt_model_id = \"strickvl/isafpr-tiny-llama-lora-templatefree\"\nmodel = AutoPeftModelForCausalLM.from_pretrained(tinyllama_sharegpt_model_id).cuda()\ntokenizer = AutoTokenizer.from_pretrained(tinyllama_sharegpt_model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\nfor row in preds_test_data:\n    out = prompt_tok(model, tokenizer, row[\"text\"])\n    row[\"predictions\"][\"tinyllama-templatefree\"] = out\n\n\nNow if we inspect we’ll see that the new model predictions have been saved into the dataset:\n\nfrom rich import print\n\nprint(preds_test_data[0])\n\n{\n    'name': '5',\n    'text': '2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n    'predictions': {\n        'gpt-3.5-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni province\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": false,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": true,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4o': '{\\n  \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n  \"start_date\": \"2013-01-24\",\\n  \n\"event_type\": [\"insurgentskilled\", \"captureandkill\"],\\n  \"province\": [\"ghazni\"],\\n  \"target_group\": [\"taliban\"],\\n \n\"min_killed\": 1,\\n  \"min_captured\": 0,\\n  \"killq\": true,\\n  \"captureq\": false,\\n  \"killcaptureraid\": true,\\n  \n\"airstrike\": false,\\n  \"noshotsfired\": false,\\n  \"min_leaders_killed\": 1,\\n  \"min_leaders_captured\": 0\\n}',\n        'tinyllama-templatefree': '\\n{\"name\":\"Taliban leader killed in \nGhazni\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"\n],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsf\nired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'tinyllama-sharegpt': \n'{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"airstrike\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"],\n\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":true,\"noshotsfire\nd\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}'\n    },\n    'start_date': datetime.date(2013, 1, 24),\n    'province': ['ghazni'],\n    'target_group': ['taliban'],\n    'event_type': ['insurgentskilled'],\n    'min_killed': 1,\n    'min_captured': 0,\n    'killq': True,\n    'captureq': False,\n    'killcaptureraid': False,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 1,\n    'min_leaders_captured': 0\n}"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-predictions",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-predictions",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Mistral predictions",
    "text": "Finetuned Mistral predictions\nAs I noted previously, it was impossible to get the finetuned Mistral model working locally so I did the inference over on Modal where I could spin up a juicy A100 to make the predictions. You’ll see below that the model didn’t perform very well, failing almost all of the evaluations. This is the mistral-lora-templatefree model you’ll see in the charts."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-openai-predictions",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-openai-predictions",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned OpenAI predictions",
    "text": "Finetuned OpenAI predictions\nI used OpenAI’s one-click finetuning service to finetune the gpt-3.5-turbo-1106 model. I iterated over my dataset to generate predictions using that finetuned model using the OpenAI SDK.\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds_2\")[\n    \"train\"\n].to_list()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nfor row in preds_test_data:\n    response = client.chat.completions.create(\n        model=\"ft:gpt-3.5-turbo-1106:SOME_MODEL_ID_GOES_HERE\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n            },\n            {\"role\": \"user\", \"content\": row[\"text\"]},\n        ],\n        temperature=0,\n    )\n    row[\"predictions\"][\"finetuned-openai-gpt-3.5-turbo-1106\"] = response.choices[\n        0\n    ].message.content"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-models-via-openpipe",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-models-via-openpipe",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Mistral models (via OpenPipe)",
    "text": "Finetuned Mistral models (via OpenPipe)\nI finetuned Mistral 7B and Mistral 8x7B models using OpenPipe so as to have something reasonable to compare the other models to. As always, OpenPipe makes it pretty easy to spin up a finetuning job and get predictions.\n\n\nCode\nfrom openpipe import OpenAI\nimport os\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_test_predictions_old\")[\n    \"train\"\n].to_list()\n\nclient = OpenAI(openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")})\n\nfor i, row in enumerate(preds_test_data, 1):\n    completion_7b = client.chat.completions.create(\n        model=\"openpipe:twelve-pumas-invent\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n            },\n            {\"role\": \"user\", \"content\": row[\"text\"]},\n        ],\n        temperature=0,\n        openpipe={\"tags\": {\"prompt_id\": \"counting\", \"any_key\": \"any_value\"}},\n    )\n\n    row[\"predictions\"][\"finetuned-mistral-7b-optimised-openpipe\"] = completion_7b.choices[0].message.content\n    \n    if i % 100 == 0:\n        print(f\"{i}/724 rows complete\")"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-solar-llm-via-predibase",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-solar-llm-via-predibase",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Solar LLM (via Predibase)",
    "text": "Finetuned Solar LLM (via Predibase)\nPredibase announced a new best-in-class model for finetuning, the Solar LLM from Upstage, a week or so ago so I thought I’d try it out. The advantage of this model is that it’s trained to be good at the kinds of tasks people commonly finetune models for, like structured data extraction. As you’ll see below, it did pretty well! The base model is this one, I think, on the Hugging Face Hub so it’s available for you all to use as well.\n\n\nCode\nfrom predibase import Predibase\n\npb = Predibase(api_token=\"MY_API_TOKEN_GOES_HERE\")\n\nlorax_client = pb.deployments.client(\"solar-1-mini-chat-240612\")\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_test_predictions\")[\n    \"train\"\n].to_list()\n\nfor i, row in enumerate(preds_test_data, 1):\n    prompt = f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n### Instruction:\\n\\nPRESS RELEASE TEXT: \"{row['text']}\"\\n\\n### Response:\"\"\"\n    response = lorax_client.generate(prompt, adapter_id=\"isafpr/2\", max_new_tokens=300).generated_text\n\n    row[\"predictions\"][\"ft-solar-1-mini-chat-240612-predibase\"] = response\n\n    if i % 100 == 0:\n        print(f\"{i}/724 rows complete\")"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-llama3-predictions-via-openpipe",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-llama3-predictions-via-openpipe",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Llama3 predictions (via OpenPipe)",
    "text": "Finetuned Llama3 predictions (via OpenPipe)\nMy locally finetuned Llama3 model hadn’t really worked well, but on OpenPipe the outputs seemed to look ok, so I used these predictions for the final evaluation.\n\n\nCode\nfrom openpipe import OpenAI\nimport os\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds_3\")[\n    \"train\"\n].to_list()\n\nclient = OpenAI(openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")})\n\nfor row in preds_test_data:\n    completion = client.chat.completions.create(\n        model=\"openpipe:fine-steaks-taste\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n            },\n            {\"role\": \"user\", \"content\": row[\"text\"]},\n        ],\n        temperature=0,\n        openpipe={\"tags\": {\"prompt_id\": \"counting\", \"any_key\": \"any_value\"}},\n    )\n\n    row[\"predictions\"][\"finetuned-llama3-7b-32k-openpipe\"] = completion.choices[\n        0\n    ].message.content\n\n\nBy the end of this process you can see we have a bunch of predictions attached to each entry in our dataset. You can view all of these in the public dataset I published on the Hugging Face Hub.\n\nfrom rich import print\n\nprint(preds_test_data[0])\n\n{\n    'name': '5',\n    'text': '2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n    'predictions': {\n        'finetuned-llama3-7b-32k-openpipe': \n'{\"name\":\"1\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal\niban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'finetuned-mistral-7b-optimised-openpipe': \n'{\"name\":\"1\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal\niban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'finetuned-openai-gpt-3.5-turbo-1106': \n'{\"name\":\"4\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal\niban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'gpt-3.5-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni province\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": false,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": true,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4o': '{\\n  \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n  \"start_date\": \"2013-01-24\",\\n  \n\"event_type\": [\"insurgentskilled\", \"captureandkill\"],\\n  \"province\": [\"ghazni\"],\\n  \"target_group\": [\"taliban\"],\\n \n\"min_killed\": 1,\\n  \"min_captured\": 0,\\n  \"killq\": true,\\n  \"captureq\": false,\\n  \"killcaptureraid\": true,\\n  \n\"airstrike\": false,\\n  \"noshotsfired\": false,\\n  \"min_leaders_killed\": 1,\\n  \"min_leaders_captured\": 0\\n}',\n        'mistral-lora-templatefree': '1',\n        'tinyllama-sharegpt': \n'{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"airstrike\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"],\n\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":true,\"noshotsfire\nd\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'tinyllama-templatefree': '\\n{\"name\":\"Taliban leader killed in \nGhazni\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"\n],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsf\nired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'ft-solar-1-mini-chat-240612-predibase': \n'\\n\\n{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\n\"taliban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"\nnoshotsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}'\n    },\n    'start_date': datetime.date(2013, 1, 24),\n    'province': ['ghazni'],\n    'target_group': ['taliban'],\n    'event_type': ['insurgentskilled'],\n    'min_killed': 1,\n    'min_captured': 0,\n    'killq': True,\n    'captureq': False,\n    'killcaptureraid': False,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 1,\n    'min_leaders_captured': 0\n}\n\n\n\nUnfortunately the Qwen2 inference on Predibase is still not working so I’ll skip that finetuned model for the moment.\nNow that we have predictions from seven finetuned models and three OpenAI models (to compare against), we can run our evaluations. I’ll start with a simple check to see what proportion of the predictions are even valid JSON."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#evals-were-a-pain-this-time-round",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#evals-were-a-pain-this-time-round",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Evals were a pain (this time round)…",
    "text": "Evals were a pain (this time round)…\nMost of the evaluation work is represented here in this notebook, and that was perhaps the seeds of my own misfortune. I had some models that worked locally, and then a bunch of other models deployed in different environments and with different services.\nNot only that, but it was pretty slow to iterate through the 724 row so my test data (which the models hadn’t seen during finetuning, just to be clear) since I implemented it fairly naively.\nIf I were to now make some updates to the models, or get them working locally, I’d really want to make sure that I have a way to run these evals locally as well. Moreover, I’d want a way to run a subset of the evals (i.e. on a slice of the data) and then at some point switch that out so that they could run across all the data.\nAll of this is completely within the realm of possible, but for this round I was more focused on getting the results than I was about making the process repeatable and/or efficient. I know I can’t run all the models concurrently on the same machine, so maybe the way forward is simply to have a reliable cloud GPU provider like Modal where I can farm out these evaluations. I had a really good experience with them when I used them, so that’s probably the way forward there.\nIn general, it was also painful having the models living in different places. I had to remember so many things. In any ideal world, you want a standard interface for inference to all your models, especially if they’re for the same use case or project. It’s convenient that my finetuned GPT3.5 is automatically deployed and served by OpenAI, and the same goes for Llama3 and Solar or Mistral, but I want a single place where I can see them all. Until now I hadn’t really seen this project or problem as being so much about MLOps, but when you have multiple models in play and you’re finetuning and updating them and data is changing all the time, then you’ll need a way of managing all this.\nThis is funny to me since I work at an MLOps company – we build an open-source MLOps framework that helps you set up a platform – but I hadn’t anticipated it’d reach this point where I’d need something like a ZenML so soon. This is, of course, one of the major tradeoffs of finetuning LLMs, in that you have to manage all this stuff in order to make it work reliably and repeatably. Even at this early stage of my project, it’s clear that you need a way to keep everything straight without making mistakes."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#but-evals-give-me-a-way-to-know-if-im-making-progress",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#but-evals-give-me-a-way-to-know-if-im-making-progress",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "…but evals give me a way to know if I’m making progress",
    "text": "…but evals give me a way to know if I’m making progress\nEven though the evaluations were somewhat painful to implement (at least in the form of this Jupyter notebook), they have given me an amazing gift in that I now have a task-specific way to know whether any of the improvements or refinements to either the training data or to the model are helping move me forward. Without this I’d essentially be flying blind."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#next-steps",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#next-steps",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Next Steps",
    "text": "Next Steps\nI had originally thought and suggested that I’d want to train multiple models to be super-specialists in their field, so for example to have one model that was really good at estimating how many people were captured in a particular event. Seeing the performance of my models, I’m not sure that’s the obvious next step for this project, or if I’d really be able to boost the accuracy by a significant amount by taking that approach.\nThis project is all about accuracy, so it’s possible that I might want to try that out, but for now I’m still exploring all the different phases of the LLM finetuning process so I’ll put the submodels idea on the backburner.\nThe first obvious next step is to run some evaluations for the non-accuracy-related tests mentioned in my last blog. For example, I’d like to see how it performs with out of domain data (i.e. completely made up data about something completely different).\nThe other next step is to get into some of the details around model serving. I’d like to take my top three performers and dive into how LLM model serving is done. I’m familiar with non-LLM model serving and some of the ways people do that through my work, but LLM serving has it’s own tricks, tradeoffs and tools and I’m eager to learn more about those.\nIf this was a problem that I was deeply invested in solving beyond these already excellent results, I’d probably also want to dive into the areas where my LLMs struggled. So I’d take all the places where my LLMs failed to get the answer correct, load them up into some kind of web interface like Lilac or Argilla and really inspect my data further. Understanding the failure scenarios will probably do more for the accuracy than any tweaking of the finetuning parameters or the like.\nFor now, I’m just happy the finetuned models beat GPT-4!"
  }
]