<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Alex Strick van Linschoten</title>
<link>https://mlops.systems/index.html</link>
<atom:link href="https://mlops.systems/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Thu, 20 Feb 2025 23:00:00 GMT</lastBuildDate>
<item>
  <title>Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-02-21-beeminder-mcp.html</link>
  <description><![CDATA[ 




<p>I spent the morning <a href="https://github.com/strickvl/mcp-beeminder">building an MCP server</a> for <a href="https://www.beeminder.com">Beeminder</a>, bridging the gap between AI assistants and my personal goal tracking data. This project emerged from a practical need — ok, desire :) — to interact more effectively with my Beeminder data through AI interfaces like Claude Desktop and Cursor.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/mcp-bm.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">The MCP-Beeminder mashup in action!</figcaption><p></p>
</figure>
</div>
<section id="understanding-beeminder" class="level2">
<h2 class="anchored" data-anchor-id="understanding-beeminder">Understanding Beeminder</h2>
<p>For those unfamiliar with <a href="https://www.beeminder.com">Beeminder</a>, it’s a tool that combines self-tracking with commitment devices to help users achieve their goals. The platform draws what they call a “Bright Red Line” – a visual commitment path that shows exactly where you need to be to stay on track. What makes Beeminder unique is its approach to accountability: users pledge real money to stay on their path, and there’s a seven-day “akrasia horizon” that prevents immediate goal changes, helping to overcome moments of impulsivity.</p>
<p>I’ve <a href="https://www.google.com/search?q=site%3Aalexstrick.com+beeminder">written a <em>lot</em> about Beeminder</a> over on my personal blog in the past so do go check that out if you’re interested to learn more about how I use it. I can attest that if it clicks with you, you’ll find it incredibly valuable. I have used in the past to write books, learn languages, <a href="https://www.alexstrick.com/blog/2016/8/phd-tools-beeminder">finish my PhD</a> and many many other things.</p>
</section>
<section id="the-role-of-mcp" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-mcp">The Role of MCP</h2>
<p>The <a href="https://modelcontextprotocol.io/">Model Context Protocol (MCP)</a> serves as a standardised way for AI assistants to interact with various data sources and tools. Think of it as a universal adapter that allows AI systems to directly access and manipulate data in your applications. Instead of copying and pasting information between your AI assistant and Beeminder, MCP creates a secure, direct connection.</p>
<p>This standardisation is particularly valuable because it means you can build one interface that works across multiple AI platforms. Whether you’re using Claude Desktop, Cursor, or <a href="https://modelcontextprotocol.io/clients">other MCP-compatible tools</a>, the same server provides consistent access to your Beeminder data.</p>
</section>
<section id="building-the-server" class="level2">
<h2 class="anchored" data-anchor-id="building-the-server">Building the Server</h2>
<p>The development process was surprisingly straightforward, largely due to two factors: the well-documented MCP specification from Anthropic and an <a href="https://github.com/ianm199/beeminder_api_client">existing Python client</a> for Beeminder’s API by <a href="https://github.com/ianm199"><span class="citation" data-cites="ianm118">@ianm118</span></a>. Most of the implementation work involved mapping Beeminder’s API endpoints to MCP’s expected interfaces and ensuring proper error handling.</p>
<p>And obviously, much of the code was actually written by Claude itself. After providing the initial structure, writing a couple of tools the way I wanted them and providing documentation, I found that Claude could generate the remainder of the code, requiring only minor adjustments and debugging from me.</p>
</section>
<section id="using-the-beeminder-mcp-server" class="level2">
<h2 class="anchored" data-anchor-id="using-the-beeminder-mcp-server">Using the Beeminder MCP Server</h2>
<p>Having an MCP server for Beeminder opens up several practical possibilities. You can have natural conversations with AI assistants about your goals, analyse patterns in your data, and even update your tracking information – all while the AI has direct access to your actual Beeminder account. This direct connection means the AI can provide more contextual and accurate assistance, whether you’re adjusting goal parameters or analysing your progress trends.</p>
<p>I’ve found that sometimes Claude needs a bit of coaxing to display the information it’s getting back from the Beeminder API in appropriate formats, which is to say, in table format. I will probably update my Claude settings so that it knows it should use tables (either Markdown or React components) to display Beeminder results that would benefit from such a presentation.</p>
</section>
<section id="looking-forward" class="level2">
<h2 class="anchored" data-anchor-id="looking-forward">Looking Forward</h2>
<p>Now that I have my Beeminder MCP server, I also want one for <a href="https://www.omnigroup.com/omnifocus">Omnifocus</a>, my task management app of choice. That’ll probably have to wait since it doesn’t appear that they offer a REST API, but it’ll be great when I can mash up the results of those two tool queries as that’s what I currently do manually as part of my process.</p>
<p>The ease of building this MCP server suggests an interesting future where more of our tools and services become directly accessible to AI assistants. The real value isn’t in any single connection, but in the potential for creating a network of interconnected tools that AI can help us manage more effectively.</p>
<p>If you’re interested in trying this out yourself, you can find the code and setup instructions in the <a href="https://github.com/strickvl/mcp-beeminder">GitHub repository</a>. While this implementation focuses on Beeminder, the same principles could be applied to create MCP servers for other services and tools.</p>


</section>

 ]]></description>
  <category>tools</category>
  <category>anthropic</category>
  <category>claude</category>
  <category>miniproject</category>
  <guid>https://mlops.systems/posts/2025-02-21-beeminder-mcp.html</guid>
  <pubDate>Thu, 20 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/mcp-bm.png" medium="image" type="image/png" height="172" width="144"/>
</item>
<item>
  <title>Tinbox: an LLM-based document translation tool</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html</link>
  <description><![CDATA[ 




<p>Large Language Models have transformed how we interact with text, offering capabilities that seemed like science fiction just a few years ago. They can write poetry, generate code, and engage in sophisticated reasoning. Yet surprisingly, one seemingly straightforward task – document translation – remains a significant challenge. This is a challenge I understand intimately, both as a developer and as a historian who has spent years working with multilingual primary sources.</p>
<p>Before the era of LLMs, I spent years conducting historical research in Afghanistan, working extensively with documents in Dari, Pashto, and Arabic. This wasn’t just casual reading – it was deep archival work that resulted in publications like <a href="https://www.amazon.com/Poetry-Taliban-Columbia-Strick-Linschoten/dp/0231704046?dib_tag=AUTHOR&amp;ref_=ast_author_dp_rw&amp;dib=eyJ2IjoiMSJ9.1mVaySVqbTMQoyHNw9jr729HdyTrJqF63q_dK--vp8ZIMJilxU2L9GFro0SDlAHwkLCNvm6uzLyUFyfhIMgnqHsy6OcH29oAJydmPZBO_nk.1ryRpL7upcac5fthUBd3TPVG15nuXehSARppPP0GLz8&amp;tag=soumet-20">“Poetry of the Taliban”</a> and <a href="https://www.amazon.com/Taliban-Reader-Islam-Politics-their-ebook/dp/B07F37SB8S?sr=1-1&amp;qid=1739727492&amp;keywords=taliban%2Breader&amp;sprefix=taliban%2Breader%252Cdigital-text%252C182&amp;tag=soumet-20&amp;dib_tag=se&amp;crid=1YY9Y6SJN9YLO&amp;dib=eyJ2IjoiMSJ9.rJDPRcIzPe3NY83zHYXFevNXoERWWiJ6BTyj9SXHYfv_jixdQKUV27Qwh1NJYX0qMhAV02Z4r75o_tYUCysq_Obf_9wo3qk9AzLlZWyTWO-dSa88xUIQP3MCe9dgIUf2Okhj6DAyqjgHQDdgrivgTmN0eJNQ_IOp2MKhnWLbOpEcdZtxrI7VisVAITML4b4dwYyjKbfbKnsk1IHWtk_P0NU-XcV2ChEHBcbqlx3jh4OvKXYZ1h39-RJ7W5Tm1eo1R0T673keyXEstEi4j6msDfDu99000EMSNsvWmLIQAxPQKBsbMjEeDneokEA0-dM3.3Yy2jETrAfRcOZLuWM8Cc5f_sfS_RuVMsh0bpSCp6TA&amp;s=digital-text">“The Taliban Reader”</a>, projects that required painstaking translation work with teams of skilled translators. The process was time-consuming and resource-intensive, but it was the only way to make these primary sources accessible to a broader audience.</p>
<p>As someone who has dedicated significant time to making historical sources more accessible, I’ve watched the rise of LLMs with great interest. These models promise to democratise access to multilingual content, potentially transforming how historians and researchers work with primary sources. However, the reality has proven more complex. Current models, while powerful, often struggle with or outright refuse to translate certain content. This is particularly problematic when working with historical documents about Afghanistan – for instance, a 1984 document discussing the Soviet-Afghan conflict might be flagged or refused translation simply because it contains the word “jihad”, even in a purely historical context. The models’ aggressive content filtering, while well-intentioned, can make them unreliable for serious academic work.</p>
<p>After repeatedly bumping into these limitations in my own work, I built <a href="https://github.com/strickvl/tinbox"><code>tinbox</code></a> (shortened from ‘translation in a box’), a tool that approaches document translation through a different lens. What if we had a tool that could handle these sensitive historical texts without balking at their content? What if researchers could quickly get working translations of primary sources, even if they’re not perfect, to accelerate their research process? As a historian, having access to even rough translations of primary source materials would have dramatically accelerated my research process. As a developer, I knew we could build something better than the current solutions.</p>
<p>The name “tinbox” is a nod to the simple yet effective nature of the tool – it’s about taking the powerful capabilities of LLMs and packaging them in a way that actually works for real-world document translation needs. Whether you’re a researcher working with historical documents, an academic handling multilingual sources, or anyone needing to translate documents at scale, <a href="https://github.com/strickvl/tinbox"><code>tinbox</code></a> aims to provide a more reliable and practical solution.</p>
<section id="the-hidden-complexity-of-document-translation" class="level2">
<h2 class="anchored" data-anchor-id="the-hidden-complexity-of-document-translation">The Hidden Complexity of Document Translation</h2>
<p>The problem of document translation sits at an interesting intersection of challenges. On the surface, it might seem straightforward – after all, if an LLM can engage in complex dialogue, surely it can translate a document? It can, but there are some edge cases and limitations.</p>
<p>When working with real-world documents, particularly PDFs, we encounter a cascade of complications. First, there’s the issue of model refusal. LLMs frequently decline to translate documents, citing copyright concerns or content sensitivity. This isn’t just an occasional hiccup – it’s a systematic limitation occurring regularly that makes these models unreliable for production use out of the box.</p>
<p>Then there’s the scale problem. Most documents aren’t just a few paragraphs; they’re often dozens or hundreds of pages long. This runs headlong into the context window limitations of current models. Breaking documents into smaller chunks might seem like an obvious solution, but this introduces its own set of challenges. How do you maintain coherence across chunks? What happens when a sentence spans two pages? How do you handle formatting and structure?</p>
<p>The PDF format adds another layer of complexity. Most existing tools rely on Optical Character Recognition (OCR), which introduces its own set of problems. OCR can mangle formatting, struggle with complex layouts, and introduce errors that propagate through to the translation. Even when OCR works perfectly, you’re still left with the challenge of maintaining the document’s original structure and presentation.</p>
</section>
<section id="a-word-about-translations-fidelity-and-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="a-word-about-translations-fidelity-and-accuracy">A Word About Translations, Fidelity and Accuracy</h2>
<p>Having worked professionally as a translator and worked as an editor for teams of translators, I’m acutely aware of the challenges and limitations of LLM-provided translations. While these models have made remarkable strides, they face several significant hurdles that are worth examining in detail.</p>
<p>One of the most prominent issues is consistency. LLMs often struggle to maintain consistent terminology across multiple API calls, which becomes particularly evident in longer documents. Technical terms, product names, and industry-specific jargon might be translated differently each time they appear, creating confusion and reducing the professional quality of the output. This problem extends beyond mere terminology – the writing style and tone can drift significantly between chunks of text, especially when using the chunking approach necessary for longer documents. You might find yourself with a document that switches unexpectedly between formal and informal registers, or that handles technical depth inconsistently across sections.</p>
<p>Even formatting poses challenges. The way LLMs handle structural elements like bullet points, numbered lists, or text emphasis can vary dramatically across sections. What starts as a consistently formatted document can end up with a patchwork of different styling approaches, requiring additional cleanup work.</p>
<p>Perhaps more fundamentally, LLMs struggle to find the right balance between literal and fluent translation. Sometimes they produce awkwardly literal translations that technically convey the meaning but lose the natural flow of the target language. Other times, they swing too far in the opposite direction, producing fluid but unfaithful translations that lose important nuances from the source text. This challenge becomes particularly acute when dealing with idioms and cultural references, where literal translation would be meaningless but too free a translation risks losing the author’s intent.</p>
<p>Cultural nuances present another significant challenge. LLMs often miss or mishandle culture-specific references, humour, and wordplay. They struggle with regional variations in language and historical context, potentially stripping away layers of meaning that a human translator would carefully preserve. This limitation becomes even more apparent in specialised fields – medical texts, legal documents, technical manuals, and academic writing all require domain expertise that LLMs don’t consistently demonstrate.</p>
<p>The technical limitations of these models add another layer of complexity. The necessity of breaking longer texts into chunks means that broader document context can be lost, making it difficult to maintain coherence across section boundaries. While tools like <code>tinbox</code> attempt to address this through seam repair and sliding window approaches, it remains a significant challenge. Cross-references between different parts of the document might be missed, and maintaining a consistent voice across a long text can prove difficult.</p>
<p>Format-specific problems abound as well. Tables and figures might be misinterpreted, special characters can be mangled, and the connections between footnotes or endnotes and their references might be lost. Page layout elements can be corrupted in the translation process, requiring additional post-processing work.</p>
<p>Reliability and trust present another set of concerns. LLMs are prone to hallucination, sometimes adding content that wasn’t present in the original text or filling in perceived gaps with invented information. They might create plausible but incorrect translations or embellish technical details. Moreover, they provide no indication of their confidence in different parts of the translation, no flags for potentially problematic passages, and no highlighting of ambiguous terms or phrases that might benefit from human review.</p>
<p>When it comes to handling source texts, LLMs show particular weakness with poor quality inputs. They struggle with grammatically incorrect text, informal or colloquial language, and dialectal variations. Their handling of abbreviations and acronyms can be inconsistent, potentially introducing errors into technical or specialised documents.</p>
<p>The ethical and professional implications of these limitations are significant. There’s often a lack of transparency about the translation process, no clear audit trail for translation decisions, and limited ability to explain why particular choices were made. This raises concerns about professional displacement – not just in terms of jobs, but in terms of the valuable human judgment that professional translators bring to sensitive translations, the opportunity for cultural consultation, and the role of specialist translators in maintaining high standards in their fields.</p>
<p>These various limitations underscore an important point: while LLMs are powerful tools for translation, they should be seen as aids to human translators rather than replacements, especially in contexts requiring high accuracy, cultural sensitivity, technical precision, legal compliance, or creative fidelity. The future of translation likely lies in finding ways to combine the efficiency and broad capabilities of LLMs with the nuanced understanding and expertise of human translators.</p>
<p>So why build a tool like this given all these problems? I think there’s still a use for something like this in fields where there are few translators and a huge backlog of materials where there’s a benefit to reading them in your own mother tongue, even in a ‘bad’ translation. (That said, having done a decent amount of comparison of outputs for languages like Arabic, Dari and Pashto, I actually don’t find the translations to be terrible, especially for domains like the news or political commentary.) For myself, I am working on a separate tool or system which takes in primary sources and incrementally populates a knowledge database. Having ways to ingest materials written in foreign languages is incredibly important for this, and having a way to do it that doesn’t break the bang (i.e.&nbsp;by using local models) is similarly important.</p>
</section>
<section id="engineering-a-solution" class="level2">
<h2 class="anchored" data-anchor-id="engineering-a-solution">Engineering a Solution</h2>
<p><code>tinbox</code> takes a simple approach to solving these issues through two core algorithmic features. The first is what I call “page-by-page with seam repair.” Instead of treating a document as one continuous piece of text, we acknowledge its natural segmentation into pages. Each page is translated independently, but – and this is crucial – we then apply a repair process to the seams between pages.</p>
<p>This seam repair is where things get interesting. When a sentence spans a page boundary, we identify the overlap and re-translate that specific section with full context from both pages. This ensures that the translation flows naturally, even across page boundaries. It’s a bit like being a careful tailor, making sure the stitches between pieces of fabric are invisible in the final garment.</p>
<p>For continuous text documents (read: a <code>.txt</code> file containing multiple tens of thousands of words), we take a different approach using a sliding window algorithm. Think of it like moving a magnifying glass across the text, where the edges of the glass overlap with the previous and next positions. This overlap is crucial – it provides the context necessary for coherent translation across chunk boundaries.</p>
<p>The implementation details matter here. We need to carefully manage memory, handle errors gracefully, and provide progress tracking for long-running translations. The codebase is structured around clear separation of concerns, making it easy to add support for new document types or translation models.</p>
<p>Moreover, we need to ensure that in the case of failure we’re able to resume without wasting what we spent translating earlier parts of the document.</p>
</section>
<section id="the-engineering-details" class="level2">
<h2 class="anchored" data-anchor-id="the-engineering-details">The Engineering Details</h2>
<p>The architecture reflects these needs. At its core, <code>tinbox</code> uses a modular design that separates document processing from translation logic. This allows us to handle different document types (PDFs, Word documents, plain text) with specialised processors while maintaining a consistent interface for translation.</p>
<p>Error handling is particularly crucial. Translation is inherently error-prone, and when you’re dealing with large documents, you need robust recovery mechanisms. We implement comprehensive retry logic with exponential backoff, ensuring that temporary failures (like rate limits) don’t derail entire translation jobs.</p>
<p>For large documents, we provide checkpointing and progress tracking. This means you can resume interrupted translations and get detailed insights into the translation process. The progress tracking isn’t just about displaying a percentage – it provides granular information about token usage, costs, and potential issues.</p>
<section id="page-by-page-with-seam-repair" class="level3">
<h3 class="anchored" data-anchor-id="page-by-page-with-seam-repair">Page-by-Page with Seam Repair</h3>
<p>The page-by-page algorithm handles PDFs by treating each page as a separate unit while ensuring smooth transitions between pages. Pseudocode that can help you understand how this works goes something like this:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;">def</span> translate_with_seam_repair(document, overlap_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">200</span>):</span>
<span id="cb1-2">    translated_pages <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb1-3">    </span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;">for</span> page_num, page <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(document.pages):</span>
<span id="cb1-5">        <span class="co" style="color: #5E5E5E;"># Translate current page</span></span>
<span id="cb1-6">        current_translation <span class="op" style="color: #5E5E5E;">=</span> translate_page(page)</span>
<span id="cb1-7">        </span>
<span id="cb1-8">        <span class="cf" style="color: #003B4F;">if</span> page_num <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb1-9">            <span class="co" style="color: #5E5E5E;"># Extract and repair the seam between pages</span></span>
<span id="cb1-10">            previous_end <span class="op" style="color: #5E5E5E;">=</span> translated_pages[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>][<span class="op" style="color: #5E5E5E;">-</span>overlap_size:]</span>
<span id="cb1-11">            current_start <span class="op" style="color: #5E5E5E;">=</span> current_translation[:overlap_size]</span>
<span id="cb1-12">            </span>
<span id="cb1-13">            <span class="co" style="color: #5E5E5E;"># Re-translate the overlapping section with full context</span></span>
<span id="cb1-14">            repaired_seam <span class="op" style="color: #5E5E5E;">=</span> translate_with_context(</span>
<span id="cb1-15">                text<span class="op" style="color: #5E5E5E;">=</span>current_start,</span>
<span id="cb1-16">                previous_context<span class="op" style="color: #5E5E5E;">=</span>previous_end</span>
<span id="cb1-17">            )</span>
<span id="cb1-18">            </span>
<span id="cb1-19">            <span class="co" style="color: #5E5E5E;"># Update translations with repaired seam</span></span>
<span id="cb1-20">            translated_pages[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">=</span> translated_pages[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>][:<span class="op" style="color: #5E5E5E;">-</span>overlap_size] <span class="op" style="color: #5E5E5E;">+</span> repaired_seam</span>
<span id="cb1-21">            current_translation <span class="op" style="color: #5E5E5E;">=</span> repaired_seam <span class="op" style="color: #5E5E5E;">+</span> current_translation[overlap_size:]</span>
<span id="cb1-22">        </span>
<span id="cb1-23">        translated_pages.append(current_translation)</span>
<span id="cb1-24">    </span>
<span id="cb1-25">    <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span>.join(translated_pages)</span></code></pre></div>
</section>
<section id="sliding-window-for-text-documents" class="level3">
<h3 class="anchored" data-anchor-id="sliding-window-for-text-documents">Sliding Window for Text Documents</h3>
<p>For continuous text documents, we use a sliding window approach. Again, pseudocode to help understand how this works goes something like this, though the actual implementation is different:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">def</span> translate_with_sliding_window(text, window_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2000</span>, overlap<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">200</span>):</span>
<span id="cb2-2">    chunks <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb2-3">    position <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-4">    </span>
<span id="cb2-5">    <span class="cf" style="color: #003B4F;">while</span> position <span class="op" style="color: #5E5E5E;">&lt;</span> <span class="bu" style="color: null;">len</span>(text):</span>
<span id="cb2-6">        <span class="co" style="color: #5E5E5E;"># Create window with overlap</span></span>
<span id="cb2-7">        end <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">min</span>(<span class="bu" style="color: null;">len</span>(text), position <span class="op" style="color: #5E5E5E;">+</span> window_size)</span>
<span id="cb2-8">        window <span class="op" style="color: #5E5E5E;">=</span> text[position:end]</span>
<span id="cb2-9">        </span>
<span id="cb2-10">        <span class="co" style="color: #5E5E5E;"># Translate window</span></span>
<span id="cb2-11">        translation <span class="op" style="color: #5E5E5E;">=</span> translate_window(window)</span>
<span id="cb2-12">        chunks.append(translation)</span>
<span id="cb2-13">        </span>
<span id="cb2-14">        <span class="co" style="color: #5E5E5E;"># Slide window forward, accounting for overlap</span></span>
<span id="cb2-15">        position <span class="op" style="color: #5E5E5E;">=</span> end <span class="op" style="color: #5E5E5E;">-</span> overlap</span>
<span id="cb2-16">    </span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;">return</span> merge_chunks(chunks, overlap)</span></code></pre></div>
</section>
<section id="cli-usage-examples" class="level3">
<h3 class="anchored" data-anchor-id="cli-usage-examples">CLI Usage Examples</h3>
<p>The tool provides a simple command-line interface:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="co" style="color: #5E5E5E;"># Basic translation of a PDF to Spanish</span></span>
<span id="cb3-2"><span class="ex" style="color: null;">tinbox</span> <span class="at" style="color: #657422;">--to</span> es document.pdf</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;"># Specify source language and model</span></span>
<span id="cb3-5"><span class="ex" style="color: null;">tinbox</span> <span class="at" style="color: #657422;">--from</span> zh <span class="at" style="color: #657422;">--to</span> en <span class="at" style="color: #657422;">--model</span> anthropic:claude-3-5-sonnet-latest chinese_doc.pdf</span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;"># Use local model via Ollama for sensitive content</span></span>
<span id="cb3-8"><span class="ex" style="color: null;">tinbox</span> <span class="at" style="color: #657422;">--model</span> ollama:mistral-small <span class="at" style="color: #657422;">--to</span> en sensitive_doc.pdf</span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;"># Advanced options for large documents</span></span>
<span id="cb3-11"><span class="ex" style="color: null;">tinbox</span> <span class="at" style="color: #657422;">--to</span> fr <span class="at" style="color: #657422;">--algorithm</span> sliding-window <span class="dt" style="color: #AD0000;">\</span></span>
<span id="cb3-12">       <span class="at" style="color: #657422;">--window-size</span> 3000 <span class="at" style="color: #657422;">--overlap</span> 300 <span class="dt" style="color: #AD0000;">\</span></span>
<span id="cb3-13">       large_document.txt</span></code></pre></div>
</section>
</section>
<section id="other-notable-features" class="level2">
<h2 class="anchored" data-anchor-id="other-notable-features">Other notable features</h2>
<p>The CLI interface for <code>tinbox</code> currently is built on top of <code>litellm</code> so it technically supports most models you might want to use with it, though I’ve only enabled OpenAI, Anthropic, Google/Gemini and Ollama as base providers for now.</p>
<p>The Ollama support was one I was keen to offer since translation is such a token-heavy task. I also really worry about the level of sensitivity / monitoring on the cloud APIs and have run into that in the past (particularly with regard to my previous work as a historian working on issues relating to Afghanistan). Ollama-provided local models should solve that issue, perhaps at the expense of access to the very latest and greatest models.</p>
</section>
<section id="things-still-to-be-done" class="level2">
<h2 class="anchored" data-anchor-id="things-still-to-be-done">Things still to be done</h2>
<p>There’s lots of improvements still to be made. I’m particularly interested in exploring semantic section detection, which could make the chunking process more intelligent. There’s also work to be done on preserving more complex document formatting and supporting additional output formats.</p>
<p>Currently the tool is driven by whatever you tell it to do. Most decisions are in your hands. You have to choose the model to use for translation, notably. I am most interested in using this tool for some other side-projects and for low-resource languages so one of the important things I’ll be doing is to pick sensible defaults depending on the language and input document type you choose.</p>
<p>For example, some vision language models like GPT-4o are able to handle translating directly from an image in Urdu to English, the open-source versions (like <code>llama3.2-vision</code>) struggle much more with these kinds of tasks so it’s possible I might even need to insert an intermediary step of transcribe, then translate the transcribed text into English etc. In fact, for highest-fidelity of translation I almost certainly might want to enable that option.</p>
<p>The code is available at <a href="https://github.com/strickvl/tinbox">GitHub</a>, and I welcome contributions and feedback.</p>


</section>

 ]]></description>
  <category>translation</category>
  <category>llm</category>
  <category>llms</category>
  <category>languages</category>
  <category>research</category>
  <category>miniproject</category>
  <category>python</category>
  <category>tools</category>
  <guid>https://mlops.systems/posts/2025-02-16-tinbox-an-llm-based-document-translation-tool.html</guid>
  <pubDate>Sat, 15 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/tinbox-gh-small.png" medium="image" type="image/png" height="99" width="144"/>
</item>
<item>
  <title>Starting the Hugging Face Agents course</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-02-11-starting-the-hugging-face-agents-course.html</link>
  <description><![CDATA[ 




<p>I finished the first unit of the <a href="https://huggingface.co/learn/agents-course/">Hugging Face Agents course</a>, at least the reading part. I still want to play around with the code a bit more, since I imagine we’ll be doing that more going forward. In the meanwhile I wanted to write up some reflections on the course materials from unit one, in no particular order…</p>
<section id="code-agents-prominence" class="level2">
<h2 class="anchored" data-anchor-id="code-agents-prominence">Code agents’ prominence</h2>
<p>The course materials and <code>smolagents</code> in general places special emphasis on code agents, citing <a href="https://huggingface.co/papers/2402.01030">multiple</a> <a href="https://huggingface.co/papers/2411.01747">research</a> <a href="https://huggingface.co/papers/2401.00812">papers</a> and they <em>seem</em> to make some solid arguments for it but it also seems pretty risk at the same time. Having code agents instead of pre-defined tool use is good because:</p>
<blockquote class="blockquote">
<p><strong>Composability</strong>: could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?</p>
<p><strong>Object management</strong>: how do you store the output of an action like generate_image in JSON?</p>
<p><strong>Generality</strong>: code is built to express simply anything you can have a computer do.</p>
<p><strong>Representation in LLM training data</strong>: plenty of quality code actions is already included in LLMs’ training data which means they’re already trained for this!</p>
</blockquote>
<p>The thing that gives me pause is that it seems like we moved through the spectrum from highly structured and known workflows (a chain, perhaps, or even something like a DAG) to tool use in a loop (which had some arbitrary or dynamic parts but ultimately was at least a little defined), and all the way out then to code agents where basically anything is possible.</p>
<p>If I think about this as an engineer tasked with building a robust, dependable and reliable system, then the <em>last</em> thing I think I want to add into the system is an agent that can basically do any thing under the sun (i.e.&nbsp;code agents). Perhaps I’m misrepresenting the position here of code agents, so I’m looking forward to reading the papers cited above as well as understanding it more from the course authors’ perspective.</p>
</section>
<section id="evals-testing" class="level2">
<h2 class="anchored" data-anchor-id="evals-testing">Evals &amp; testing</h2>
<p>Following on to my confusion around code agents, I’m very curious how the course will recommend one tests and evaluates these arbitrary code agents. Things I could imagine:</p>
<ul>
<li>testing out the specific scenarios that your application or use case requires (i.e.&nbsp;end to end)</li>
<li>testing out each component of the system, such as you can break it down into smaller sub-components</li>
<li>including things like linting / unit tests maybe once code is generated by the agent (?) i.e.&nbsp;real-time evaluation of the robustness of the system?</li>
<li>probably LLM as a judge somewhere in the mix, though that opens up its own can of worms…</li>
</ul>
<p>I do hope they talk about that in the later units of the course.</p>
</section>
<section id="general-patterns" class="level2">
<h2 class="anchored" data-anchor-id="general-patterns">General patterns</h2>
<p>The core loop that came up in unit 1 was:</p>
<blockquote class="blockquote">
<p>plan -&gt; act -&gt; feedback/reflection</p>
</blockquote>
<p>And all of that gets packaged up in a loop and repeated in various forms depending on exactly how you’re using it. And this pattern is related to the ReACT loop which lots of people cite but seems to be a specific version of the general idea mentioned above.</p>
<p>And the fact that all of this works is somehow all powered by the very useful enablement of tool use, which is itself powered by the fact that the model providers finetuned this ability into the models. Crazy, brittle, impressive and many other words for the fact that this ‘hack’ has such power.</p>
</section>
<section id="chat-templates" class="level2">
<h2 class="anchored" data-anchor-id="chat-templates">Chat templates</h2>
<p>I liked how the unit really impresses on you the impact and importance of chat templates as the <em>real</em> way that LLMs are implemented. You may pass in your requests through a handy Python SDK, passing your tools as a list of function definitions, but in the end this is all being parsed down and out into very precise syntax with many tokens not intended for human consumption.</p>
</section>
<section id="points-of-leverage" class="level2">
<h2 class="anchored" data-anchor-id="points-of-leverage">Points of leverage</h2>
<p>At the end of the unit, I was thinking about all the places where an engineer has leverage over agents. What I could initially think of was:</p>
<ul>
<li>the variety and usefulness of tools that you provide to your agent (or perhaps the extent to which you allow your code agent to ‘write’ things out into the world)</li>
<li>the discrimination in the volume or choice of a combination of tools or APIs</li>
<li>how you chain everything together</li>
<li>(how robustly you handle failure)</li>
</ul>
<p>Beyond that there are quite a few things that are somewhat out of your hands unless you decide to custom finetune your own models for a specific use case.</p>
<p>Overall it was a good start to the course: made me think and also got my hands dirty working on a very simple agent with tools using <code>smolagent</code> and a Gradio demo app in the Hugging Face Hub. I’ll write more after unit two next week.</p>


</section>

 ]]></description>
  <category>agents</category>
  <category>huggingface</category>
  <category>skillbuilding</category>
  <category>llmops</category>
  <category>llms</category>
  <guid>https://mlops.systems/posts/2025-02-11-starting-the-hugging-face-agents-course.html</guid>
  <pubDate>Mon, 10 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/agents-certificate.png" medium="image" type="image/png" height="101" width="144"/>
</item>
<item>
  <title>AI Engineering Architecture and User Feedback</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-02-09-ai-eg-chapter-10.html</link>
  <description><![CDATA[ 




<p>Chapter 10 of Chip Huyen’s “AI Engineering,” focuses on two fundamental aspects: architectural patterns in AI engineering and methods for gathering and using user feedback. The chapter presents a progressive architectural framework that evolves from simple API calls to complex agent-based systems, while also diving deep into the crucial aspect of user feedback collection and analysis.</p>
<section id="progressive-architecture-patterns" class="level2">
<h2 class="anchored" data-anchor-id="progressive-architecture-patterns">1. Progressive Architecture Patterns</h2>
<p>The evolution of AI engineering architecture typically follows a pattern of increasing complexity and capability. Each stage builds upon the previous one, adding new functionality while managing increased complexity.</p>
<section id="base-layer-direct-model-integration" class="level3">
<h3 class="anchored" data-anchor-id="base-layer-direct-model-integration">Base Layer: Direct Model Integration</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/simple.png" class="img-fluid"></p>
<p>The simplest architectural pattern begins with direct queries to model APIs. While straightforward, this approach lacks the sophistication needed for most production applications.</p>
</section>
<section id="enhancement-layer-context-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="enhancement-layer-context-augmentation">Enhancement Layer: Context Augmentation</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/context.png" class="img-fluid"></p>
<p>The first major enhancement comes through <strong>Retrieval-Augmented Generation (RAG)</strong>. This layer enriches model responses by incorporating custom data and sources into LLM queries, significantly improving response quality and relevance.</p>
</section>
<section id="protection-layer-guardrails-implementation" class="level3">
<h3 class="anchored" data-anchor-id="protection-layer-guardrails-implementation">Protection Layer: Guardrails Implementation</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/guardrails.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p><strong>Guardrails</strong>: Protective mechanisms that filter both inputs and outputs to ensure system safety and reliability.</p>
</blockquote>
<p>The protection layer implements two types of guardrails:</p>
<ol type="1">
<li><p><strong>Input Guardrails</strong>: Filter sensitive information before it reaches the LLM, such as:</p>
<ul>
<li>Personal customer information</li>
<li>API keys</li>
<li>Other confidential data</li>
</ul></li>
<li><p><strong>Output Guardrails</strong>: Monitor and manage model outputs for:</p>
<ul>
<li>Format compliance (e.g., valid JSON)</li>
<li>Factual consistency</li>
<li>Hallucination detection</li>
<li>Toxic content filtering</li>
<li>Privacy protection</li>
</ul></li>
</ol>
</section>
<section id="routing-layer-gateway-and-model-selection" class="level3">
<h3 class="anchored" data-anchor-id="routing-layer-gateway-and-model-selection">Routing Layer: Gateway and Model Selection</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/extra_modules.png" class="img-fluid"></p>
<p>This layer introduces two key components:</p>
<blockquote class="blockquote">
<p><strong>AI Gateway</strong>: A centralized access point for LLM interactions that manages costs, usage tracking, and API key abstraction.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Model Router</strong>: An intent classifier that directs queries to appropriate models based on complexity and requirements.</p>
</blockquote>
<p>The routing layer enables cost optimization by directing simpler queries (like FAQ responses) to less expensive models while routing complex tasks to more sophisticated systems.</p>
</section>
<section id="performance-layer-caching-strategies" class="level3">
<h3 class="anchored" data-anchor-id="performance-layer-caching-strategies">Performance Layer: Caching Strategies</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/cache.png" class="img-fluid"></p>
<p>The architecture implements two distinct caching approaches:</p>
<ol type="1">
<li><p><strong>Exact Caching</strong>:</p>
<ul>
<li>Stores identical queries and their responses</li>
<li>Particularly valuable for multi-step operations</li>
<li>Requires careful consideration of cache eviction policies:
<ul>
<li>Least Recently Used (LRU)</li>
<li>Least Frequently Used (LFU)</li>
<li>First In, First Out (FIFO)</li>
</ul></li>
</ul></li>
<li><p><strong>Semantic Caching</strong>:</p>
<ul>
<li>Uses embedding-based search to identify similar queries</li>
<li>Depends on high-quality embeddings and reliable similarity metrics</li>
<li>More prone to failure due to component complexity</li>
</ul></li>
</ol>
<blockquote class="blockquote">
<p><strong>Security Note</strong>: Cache implementations must carefully consider potential data leaks between users accessing similar queries.</p>
</blockquote>
</section>
<section id="agent-layer-advanced-functionality" class="level3">
<h3 class="anchored" data-anchor-id="agent-layer-advanced-functionality">Agent Layer: Advanced Functionality</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/write.png" class="img-fluid"></p>
<p>The final architectural layer introduces agent patterns, enabling:</p>
<ul>
<li>Retry loops for reliability</li>
<li>Tool usage capabilities</li>
<li>Action execution (email sending, file operations)</li>
<li>Complex workflow orchestration</li>
</ul>
</section>
</section>
<section id="monitoring-and-observability" class="level2">
<h2 class="anchored" data-anchor-id="monitoring-and-observability">Monitoring and Observability</h2>
<p>The complete architecture requires robust monitoring systems tracking key metrics:</p>
<ul>
<li><strong>Mean Time to Detection (MTTD)</strong>: Time to identify issues</li>
<li><strong>Mean Time to Response (MTTR)</strong>: Time to resolve detected issues</li>
<li><strong>Change Failure Rate (CFR)</strong>: Percentage of deployments requiring fixes</li>
</ul>
<p>The monitoring system should track:</p>
<ul>
<li>Factual consistency</li>
<li>Generation relevancy</li>
<li>Safety metrics (toxicity, PII detection)</li>
<li>Model quality through conversational signals</li>
<li>Component-specific metrics (RAG, generation, vector database performance)</li>
</ul>
<section id="ai-pipeline-orchestration" class="level3">
<h3 class="anchored" data-anchor-id="ai-pipeline-orchestration">AI Pipeline Orchestration</h3>
<p>a discussion of AI pipeline orchestration, addressing the trade-offs between using existing frameworks (Langchain, Haystack, Llama Index) versus custom implementations. This decision should be based on specific project requirements, team expertise, and maintenance considerations.</p>
</section>
</section>
<section id="user-feedback-systems" class="level2">
<h2 class="anchored" data-anchor-id="user-feedback-systems">2. User Feedback Systems</h2>
<p>The second major focus of the chapter explores comprehensive user feedback collection and utilization strategies.</p>
<section id="feedback-collection-methods" class="level3">
<h3 class="anchored" data-anchor-id="feedback-collection-methods">Feedback Collection Methods</h3>
<ol type="1">
<li><p><strong>Direct Feedback</strong>:</p>
<ul>
<li>Explicit mechanisms (thumbs up/down)</li>
<li>Rating systems</li>
<li>Free-form comments</li>
</ul></li>
<li><p><strong>Implicit Feedback</strong>:</p>
<ul>
<li>Early termination patterns</li>
<li>Error corrections</li>
<li>Sentiment analysis</li>
<li>Response regeneration requests</li>
<li>Dialogue diversity metrics</li>
</ul></li>
</ol>
</section>
<section id="feedback-collection-timing" class="level3">
<h3 class="anchored" data-anchor-id="feedback-collection-timing">Feedback Collection Timing</h3>
<p>Feedback can be gathered at various stages:</p>
<ul>
<li>Initial user preference specification</li>
<li>During negative experiences</li>
<li>When model confidence is low</li>
<li>Through comparative choice interfaces (e.g., ChatGPT’s response preference selection)</li>
</ul>
</section>
<section id="feedback-limitations" class="level3">
<h3 class="anchored" data-anchor-id="feedback-limitations">Feedback Limitations</h3>
<blockquote class="blockquote">
<p><strong>Feedback Bias</strong>: User feedback systems inherently contain various biases that must be considered when making system improvements.</p>
</blockquote>
<p>Key limitations include:</p>
<ul>
<li>Negative experience bias (users more likely to report negative experiences)</li>
<li>Self-selection bias in respondent demographics</li>
<li>Preference and position biases</li>
<li>Potential feedback loops affecting system evolution</li>
</ul>
</section>
<section id="implementation-considerations" class="level3">
<h3 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h3>
<p>The implementation of feedback systems requires careful attention to:</p>
<ul>
<li>UI/UX design for feedback collection</li>
<li>Balance between different user needs</li>
<li>Monitoring feedback impact on system performance</li>
<li>Regular inspection of production data</li>
<li>Detection of system drift (prompts, user behavior, model changes)</li>
</ul>


</section>
</section>

 ]]></description>
  <category>books-i-read</category>
  <category>llm</category>
  <category>llms</category>
  <category>llmops</category>
  <category>evaluation</category>
  <guid>https://mlops.systems/posts/2025-02-09-ai-eg-chapter-10.html</guid>
  <pubDate>Sat, 08 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-02-09-ai-eg-chapter-10/write.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Notes on ‘AI Engineering’ chapter 9: Inference Optimisation</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-02-07-ai-engineering-chapter-9.html</link>
  <description><![CDATA[ 




<p>What follows are my notes on chapter 9 of Chip Huyen’s ‘AI Engineering’ book. This chapter was on optimising your inference and I learned a lot while reading it! There are interesting techniques like prompt caching and architectural considerations that I was vaguely aware of but hadn’t fully appreciated how they might work in real inference systems.</p>
<section id="chapter-9-overview" class="level2">
<h2 class="anchored" data-anchor-id="chapter-9-overview">Chapter 9: Overview</h2>
<p>Machine learning inference optimization operates across three fundamental domains: model optimization, hardware optimization, and service optimization. While hardware optimization often requires significant investment and may offer limited individual leverage, model and service optimizations provide substantial opportunities for AI engineers to improve performance.</p>
<blockquote class="blockquote">
<p><strong>Critical Cost Insight</strong>: A 2023 survey revealed that inference can account for up to 90% of machine learning costs in deployed AI systems, often exceeding training costs. This emphasizes why inference optimization isn’t just an engineering challenge - it’s a critical business necessity.</p>
</blockquote>
</section>
<section id="core-concepts-and-bottlenecks" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts-and-bottlenecks">Core Concepts and Bottlenecks</h2>
<p>Understanding inference bottlenecks is essential for effective optimization. Two primary types of computational bottlenecks impact inference performance:</p>
<blockquote class="blockquote">
<p><strong>Compute-Bound Bottlenecks</strong>: Tasks that are limited by raw computational capacity, typically involving complex mathematical operations that take significant time to complete. These bottlenecks are particularly evident in computationally intensive operations within neural networks.</p>
<p><strong>Memory Bandwidth-Bound Bottlenecks</strong>: Limitations arising from data transfer requirements between system components, particularly between memory and processors. This becomes especially relevant in Large Language Models where significant amounts of data need to be moved between different memory hierarchies.</p>
</blockquote>
<p>In Large Language Models (LLMs), different operations exhibit varying profiles of these bottlenecks. This understanding has led to architectural decisions such as decoupling the prefilling step from the decode step in production environments - a practice that has become increasingly common as organizations optimize their inference pipelines.</p>
</section>
<section id="inference-apis-and-service-patterns" class="level2">
<h2 class="anchored" data-anchor-id="inference-apis-and-service-patterns">Inference APIs and Service Patterns</h2>
<p>Two fundamental approaches to inference deployment exist:</p>
<ol type="1">
<li><strong>Online Inference APIs</strong>
<ul>
<li>Optimized for minimal latency</li>
<li>Designed for real-time responses</li>
<li>Typically more expensive per inference</li>
<li>Critical for interactive applications</li>
</ul></li>
<li><strong>Batch Inference APIs</strong>
<ul>
<li>Optimized for cost efficiency</li>
<li>Can tolerate longer processing times (potentially hours)</li>
<li>Allows providers to optimize resource utilization</li>
<li>Ideal for bulk processing tasks</li>
</ul></li>
</ol>
</section>
<section id="inference-performance-metrics" class="level2">
<h2 class="anchored" data-anchor-id="inference-performance-metrics">Inference Performance Metrics</h2>
<p>Several key metrics help quantify inference performance:</p>
<section id="latency-components" class="level3">
<h3 class="anchored" data-anchor-id="latency-components">Latency Components</h3>
<ol type="1">
<li><strong>Time to First Token</strong>
<ul>
<li>Measures duration between query submission and initial response</li>
<li>Critical for user experience in interactive applications</li>
<li>Often a key optimization target for real-time systems</li>
</ul></li>
<li><strong>Time per Output Token</strong>
<ul>
<li>Generation speed after the first token</li>
<li>Impacts overall completion time</li>
<li>Can vary based on model architecture and optimization</li>
</ul></li>
<li><strong>Inter-token Latency</strong>
<ul>
<li>Time intervals between consecutive tokens</li>
<li>Affects perceived smoothness of generation</li>
<li>Important for streaming applications</li>
</ul></li>
</ol>
<p>Total latency can be expressed as: <code>time_to_first_token + (time_per_token × number_of_tokens)</code></p>
</section>
<section id="throughput-and-goodput-metrics" class="level3">
<h3 class="anchored" data-anchor-id="throughput-and-goodput-metrics">Throughput and Goodput Metrics</h3>
<blockquote class="blockquote">
<p><strong>Throughput</strong>: The number of output tokens per second an inference service can generate across all users and requests. This raw metric provides insight into system capacity.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Goodput</strong>: The number of requests per second that successfully meet the Service Level Objective (SLO). This metric offers a more realistic view of useful system capacity.</p>
</blockquote>
</section>
<section id="resource-utilization-metrics" class="level3">
<h3 class="anchored" data-anchor-id="resource-utilization-metrics">Resource Utilization Metrics</h3>
<ol type="1">
<li><strong>Model FLOPS Utilization (MFU)</strong>
<ul>
<li>Ratio of actual to theoretical FLOPS</li>
<li>Indicates computational efficiency</li>
<li>Key metric for hardware optimization</li>
</ul></li>
<li><strong>Model Bandwidth Utilization (MBU)</strong>
<ul>
<li>Percentage of achievable memory bandwidth utilized</li>
<li>Critical for memory-intensive operations</li>
<li>Helps identify memory bottlenecks</li>
</ul></li>
</ol>
</section>
</section>
<section id="hardware-considerations-and-ai-accelerators" class="level2">
<h2 class="anchored" data-anchor-id="hardware-considerations-and-ai-accelerators">Hardware Considerations and AI Accelerators</h2>
<p>While NVIDIA GPUs dominate the market, various specialized chips exist for inference:</p>
<section id="popular-ai-accelerators" class="level3">
<h3 class="anchored" data-anchor-id="popular-ai-accelerators">Popular AI Accelerators</h3>
<ul>
<li>NVIDIA GPUs (market leader)</li>
<li>AMD accelerators</li>
<li>Google TPUs</li>
<li>Various emerging specialized chips</li>
</ul>
<blockquote class="blockquote">
<p><strong>Inference vs Training Hardware</strong>: Inference-optimized chips prioritize lower precision and faster memory access over large memory capacity, contrasting with training-focused hardware that requires substantial memory capacity.</p>
</blockquote>
<p>Key hardware optimization considerations include:</p>
<ul>
<li>Memory size and bandwidth requirements</li>
<li>Chip architecture specifics</li>
<li>Power consumption profiles</li>
<li>Physical chip architecture variations</li>
<li>Cost-performance ratios</li>
</ul>
</section>
</section>
<section id="model-optimization-techniques" class="level2">
<h2 class="anchored" data-anchor-id="model-optimization-techniques">Model Optimization Techniques</h2>
<p><img src="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/inference-optimization-differences.png" class="img-fluid"></p>
<section id="core-approaches" class="level3">
<h3 class="anchored" data-anchor-id="core-approaches">Core Approaches</h3>
<ol type="1">
<li><strong>Quantization</strong>
<ul>
<li>Reduces numerical precision (e.g., 32-bit to 16-bit)</li>
<li>Decreases memory footprint</li>
<li>Weight-only quantization is particularly common</li>
<li>Can halve model size with minimal performance impact</li>
</ul></li>
<li><strong>Pruning</strong>
<ul>
<li>Removes non-essential parameters</li>
<li>Preserves core model behavior</li>
<li>Multiple techniques available</li>
<li>Requires careful validation</li>
</ul></li>
<li><strong>Distillation</strong>
<ul>
<li>Creates smaller, more efficient models</li>
<li>Maintains key capabilities</li>
<li>Covered extensively in Chapter 8</li>
</ul></li>
</ol>
</section>
<section id="advanced-decoding-strategies" class="level3">
<h3 class="anchored" data-anchor-id="advanced-decoding-strategies">Advanced Decoding Strategies</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/pytorch-llama3-optimization.png" class="img-fluid"></p>
<section id="speculative-decoding" class="level4">
<h4 class="anchored" data-anchor-id="speculative-decoding">Speculative Decoding</h4>
<p>This approach combines a large model with a smaller, faster model:</p>
<ul>
<li>Small model generates rapid initial outputs</li>
<li>Large model verifies and corrects as needed</li>
<li>Provides faster token generation</li>
<li>Easy to implement</li>
<li>Integrated into frameworks like VLLM and LamaCPU</li>
</ul>
</section>
<section id="inference-with-reference" class="level4">
<h4 class="anchored" data-anchor-id="inference-with-reference">Inference with Reference</h4>
<p><img src="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/inference_with_reference.png" class="img-fluid"></p>
<ul>
<li>Performs mini-RAG operations during decoding</li>
<li>Retrieves relevant context from input query</li>
<li>Requires additional memory overhead</li>
<li>Useful for maintaining context accuracy</li>
</ul>
</section>
<section id="parallel-decoding" class="level4">
<h4 class="anchored" data-anchor-id="parallel-decoding">Parallel Decoding</h4>
<p>Rather than strictly sequential token generation, this method:</p>
<ul>
<li>Generates multiple tokens simultaneously</li>
<li>Uses resolution mechanisms to maintain coherence</li>
<li>Implements look-ahead techniques</li>
<li>Algorithmically complex but offers significant speed benefits</li>
<li>Demonstrated success with look-ahead decoding method</li>
</ul>
</section>
<section id="attention-optimization" class="level4">
<h4 class="anchored" data-anchor-id="attention-optimization">Attention Optimization</h4>
<p>Several strategies exist for optimizing attention mechanisms:</p>
<ol type="1">
<li><strong>Key-Value Cache Optimization</strong>
<ul>
<li>Critical for large context windows</li>
<li>Requires substantial memory</li>
<li>Various techniques for size reduction</li>
</ul></li>
<li><strong>Specialized Attention Kernels</strong>
<ul>
<li>Flash Attention as leading example</li>
<li>Hardware-specific implementations</li>
<li>Flash Attention 3 for H100 GPUs</li>
</ul></li>
</ol>
<p><img src="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/flash-attention.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="service-level-optimization" class="level2">
<h2 class="anchored" data-anchor-id="service-level-optimization">Service-Level Optimization</h2>
<section id="batching-strategies" class="level3">
<h3 class="anchored" data-anchor-id="batching-strategies">Batching Strategies</h3>
<ol type="1">
<li><strong>Static Batching</strong>
<ul>
<li>Processes fixed-size batches</li>
<li>Waits for complete batch (e.g., 100 requests)</li>
<li>Simple but potentially inefficient</li>
</ul></li>
<li><strong>Dynamic Batching</strong>
<ul>
<li>Uses time windows for batch formation</li>
<li>Processes incomplete batches after timeout</li>
<li>Balances latency and throughput</li>
</ul></li>
<li><strong>Continuous Batching</strong>
<ul>
<li>Returns completed responses immediately</li>
<li>Dynamically manages resource utilization</li>
<li>Similar to a bus route that continuously picks up new passengers</li>
<li>Optimizes occupation rate</li>
<li>Based on Orca paper’s findings</li>
</ul></li>
</ol>
</section>
<section id="prefill-decode-decoupling" class="level3">
<h3 class="anchored" data-anchor-id="prefill-decode-decoupling">Prefill-Decode Decoupling</h3>
<ul>
<li>Separates prefill and decode operations</li>
<li>Essential for large-scale inference providers</li>
<li>Allows optimal resource allocation</li>
<li>Improves overall system efficiency</li>
</ul>
</section>
<section id="prompt-caching" class="level3">
<h3 class="anchored" data-anchor-id="prompt-caching">Prompt Caching</h3>
<p><img src="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/prompt_caching.png" class="img-fluid"></p>
<ul>
<li>Stores computations for overlapping text segments</li>
<li>Offered by providers like Gemini and Anthropic</li>
<li>May incur storage costs</li>
<li>Requires careful cost-benefit analysis</li>
<li>Must be explicitly enabled</li>
</ul>
</section>
<section id="parallelism-strategies" class="level3">
<h3 class="anchored" data-anchor-id="parallelism-strategies">Parallelism Strategies</h3>
<ol type="1">
<li><strong>Replica Parallelism</strong>
<ul>
<li>Creates multiple copies of the model</li>
<li>Distributes requests across replicas</li>
<li>Simplest form of parallelism</li>
</ul></li>
<li><strong>Tensor Parallelism</strong>
<ul>
<li>Splits individual tensors across devices</li>
<li>Enables processing of larger models</li>
<li>Requires careful coordination</li>
</ul></li>
<li><strong>Pipeline Parallelism</strong>
<ul>
<li>Divides model computation into stages</li>
<li>Assigns stages to different devices</li>
<li>Optimizes resource utilization</li>
<li>Reduces memory requirements</li>
</ul></li>
<li><strong>Context Parallelism</strong>
<ul>
<li>Processes different parts of input context in parallel</li>
<li>Particularly useful for long sequences</li>
<li>Can significantly reduce latency</li>
</ul></li>
<li><strong>Sequence Parallelism</strong>
<ul>
<li>Processes multiple sequences simultaneously</li>
<li>Leverages hardware-specific features</li>
<li>Requires careful implementation</li>
</ul></li>
</ol>
</section>
</section>
<section id="implementation-considerations" class="level2">
<h2 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h2>
<p>When implementing inference optimizations:</p>
<ul>
<li>Multiple optimization techniques are typically combined in production</li>
<li>Hardware-specific optimizations require careful testing</li>
<li>Service-level optimizations often provide significant gains with minimal model modifications</li>
<li>Optimization choices depend heavily on specific use cases and requirements</li>
</ul>


</section>

 ]]></description>
  <category>books-i-read</category>
  <category>inference</category>
  <category>llm</category>
  <category>llms</category>
  <category>hardware</category>
  <guid>https://mlops.systems/posts/2025-02-07-ai-engineering-chapter-9.html</guid>
  <pubDate>Thu, 06 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/flash-attention.png" medium="image" type="image/png" height="83" width="144"/>
</item>
<item>
  <title>Dataset Engineering: The Art and Science of Data Preparation</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html</link>
  <description><![CDATA[ 




<p>Finally back on track and reading the next chapter of Chip Huyen’s book, ‘AI Engineering’. Here are my notes on the chapter.</p>
<section id="overview-and-core-philosophy" class="level2">
<h2 class="anchored" data-anchor-id="overview-and-core-philosophy">Overview and Core Philosophy</h2>
<blockquote class="blockquote">
<p>“Data will be mostly just toil, tears and sweat.”</p>
</blockquote>
<p>This is how we start the chapter :) This candid assessment frames dataset engineering as a discipline that requires both technical sophistication and pragmatic persistence. While the chapter’s placement might have been suitable earlier in the book, its position allows it to build effectively on previously established concepts.</p>
</section>
<section id="data-curation-the-foundation" class="level2">
<h2 class="anchored" data-anchor-id="data-curation-the-foundation">Data Curation: The Foundation</h2>
<p>Data curation addresses various use cases including fine-tuning, pre-training, and training from scratch, with specific considerations for chain of thought reasoning and tool use. The process addresses three fundamental aspects:</p>
<blockquote class="blockquote">
<p><strong>Data Quality</strong>: The equivalent of ingredient quality in cooking</p>
<p><strong>Data Coverage</strong>: Analogous to having the right mix of ingredients</p>
<p><strong>Data Quantity</strong>: Determining the optimal volume of ingredients</p>
</blockquote>
<section id="quality-criteria" class="level3">
<h3 class="anchored" data-anchor-id="quality-criteria">Quality Criteria</h3>
<p>Data quality encompasses multiple dimensions:</p>
<ul>
<li>Relevance to task requirements</li>
<li>Consistency in format and structure</li>
<li>Sufficient uniqueness</li>
<li>Regulatory compliance (especially critical in regulated industries)</li>
</ul>
</section>
<section id="coverage-considerations" class="level3">
<h3 class="anchored" data-anchor-id="coverage-considerations">Coverage Considerations</h3>
<p>Coverage involves strategic decisions about data proportions:</p>
<ul>
<li>Large language models often utilize significant code data (up to 50%) in training, which appears to enhance logical reasoning capabilities beyond just coding</li>
<li>Language distribution can be surprisingly efficient (even 1% representation of a language can enable meaningful capabilities)</li>
<li>Training proportions may vary across different stages of the training process</li>
</ul>
</section>
<section id="quantity-and-optimization" class="level3">
<h3 class="anchored" data-anchor-id="quantity-and-optimization">Quantity and Optimization</h3>
<p>A key phenomenon discussed is <strong>ossification</strong>, where extensive pre-training can effectively freeze model weights, potentially hampering fine-tuning adaptability. This effect is particularly pronounced in smaller models.</p>
<p>Key quantity considerations include:</p>
<ul>
<li>Task complexity correlation with data requirements</li>
<li>Base model performance implications</li>
<li>Model size considerations (OpenAI notes that with ~100 examples, more advanced models show superior fine-tuning performance)</li>
<li>Potential for using lower quality or less relevant data for initial fine-tuning to reduce high-quality data requirements</li>
<li>Recognition of performance plateaus where additional data yields diminishing returns</li>
</ul>
</section>
<section id="data-acquisition-process" class="level3">
<h3 class="anchored" data-anchor-id="data-acquisition-process">Data Acquisition Process</h3>
<p>The chapter provides a detailed example workflow for creating an instruction-response dataset:</p>
<ol type="1">
<li>Initial dataset identification (~10,000 examples)</li>
<li>Low-quality instruction removal (reducing to ~9,000)</li>
<li>Low-quality response filtering (removing 3,000)</li>
<li>Manual response writing for remaining high-quality instructions</li>
<li>Topic gap identification and template creation (100 templates)</li>
<li>AI synthesis of 2,000 new instructions</li>
<li>Manual annotation of synthetic instructions</li>
</ol>
<p>Final result: 11,000 high-quality examples</p>
</section>
</section>
<section id="data-augmentation-and-synthesis" class="level2">
<h2 class="anchored" data-anchor-id="data-augmentation-and-synthesis">Data Augmentation and Synthesis</h2>
<section id="synthesis-objectives" class="level3">
<h3 class="anchored" data-anchor-id="synthesis-objectives">Synthesis Objectives</h3>
<ol type="1">
<li>Increasing data quantity</li>
<li>Expanding coverage</li>
<li>Enhancing quality</li>
<li>Addressing privacy concerns</li>
<li>Enabling model distillation</li>
</ol>
<blockquote class="blockquote">
<p><strong>Notable Research</strong>: An Anthropic paper (2022) found that language model-generated datasets can match or exceed human-written ones in quality for certain tasks.</p>
</blockquote>
<p>Note that some teams actually prefer AI-generated preference data due to human fatigue and inconsistency factors.</p>
</section>
<section id="synthesis-applications" class="level3">
<h3 class="anchored" data-anchor-id="synthesis-applications">Synthesis Applications</h3>
<p>The chapter distinguishes between pre-training and post-training synthesis:</p>
<ul>
<li>Synthetic data appears more frequently in post-training</li>
<li>Pre-training limitation: AI can reshape existing knowledge but struggles to synthesize new knowledge</li>
</ul>
</section>
<section id="llama-3-synthesis-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="llama-3-synthesis-pipeline">LLaMA 3 Synthesis Pipeline</h3>
<p>A comprehensive workflow example:</p>
<ol type="1">
<li>AI generation of problem descriptions</li>
<li>Solution generation in multiple programming languages</li>
<li>Unit test generation</li>
<li>Error correction</li>
<li>Cross-language translation with test verification</li>
<li>Conversation and documentation generation with back-translation verification</li>
</ol>
<p>This pipeline generated 2.7 million synthetic coding examples for LLaMA 3.1’s supervised fine-tuning.</p>
</section>
<section id="model-collapse-considerations" class="level3">
<h3 class="anchored" data-anchor-id="model-collapse-considerations">Model Collapse Considerations</h3>
<p>The chapter addresses the risk of <strong>model collapse</strong> in synthetic data usage:</p>
<ul>
<li>Potential loss of training signal through repeated synthetic data use</li>
<li>Current research suggests proper implementation can avoid collapse</li>
<li>Importance of quality control in synthetic data generation</li>
</ul>
</section>
<section id="model-distillation" class="level3">
<h3 class="anchored" data-anchor-id="model-distillation">Model Distillation</h3>
<p>Notable example: BuzzFeed’s fine-tuning of Flan T5 using LoRa and OpenAI’s <code>text-davinci-003</code> generated examples, achieving 80% inference cost reduction.</p>
</section>
</section>
<section id="data-processing-best-practices" class="level2">
<h2 class="anchored" data-anchor-id="data-processing-best-practices">Data Processing Best Practices</h2>
<blockquote class="blockquote">
<p><strong>Expert Tip</strong>: “Manual inspection of data has probably the highest value to prestige ratio of any activity in machine learning.” - Greg Brockman, OpenAI co-founder</p>
</blockquote>
<section id="processing-guidelines" class="level3">
<h3 class="anchored" data-anchor-id="processing-guidelines">Processing Guidelines</h3>
<p>The chapter emphasizes efficiency optimization:</p>
<ol type="1">
<li><p>Order optimization (e.g., deduplication before cleaning if computationally advantageous)</p></li>
<li><p>Trial run validation before full dataset processing</p></li>
<li><p>Data preservation (avoid in-place modifications)</p></li>
<li><p>Original data retention for:</p>
<ul>
<li>Alternative processing needs</li>
<li>Team requirements</li>
<li>Error recovery</li>
</ul></li>
</ol>
</section>
<section id="technical-processing-approaches" class="level3">
<h3 class="anchored" data-anchor-id="technical-processing-approaches">Technical Processing Approaches</h3>
<p>Deduplication strategies include:</p>
<ul>
<li>Pairwise comparison</li>
<li>Hashing methods</li>
<li>Dimensionality reduction techniques</li>
</ul>
<p>Multiple libraries are referenced (page 400) for implementation.</p>
</section>
<section id="data-cleaning-and-formatting" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning-and-formatting">Data Cleaning and Formatting</h3>
<ul>
<li>HTML tag removal for signal enhancement</li>
<li>Careful prompt template formatting, crucial for:
<ul>
<li>Fine-tuning operations</li>
<li>Instruction tuning</li>
<li>Model performance optimization</li>
</ul></li>
</ul>
</section>
<section id="data-inspection" class="level3">
<h3 class="anchored" data-anchor-id="data-inspection">Data Inspection</h3>
<p>The chapter emphasizes the importance of manual data inspection:</p>
<ul>
<li>Utilize various data exploration tools</li>
<li>Dedicate time to direct data examination (recommended: 15 minutes of direct observation)</li>
<li>Consider this step non-optional in the process</li>
</ul>


</section>
</section>

 ]]></description>
  <category>books-i-read</category>
  <category>datasets</category>
  <category>datalabelling</category>
  <category>llm</category>
  <category>llms</category>
  <category>finetuning</category>
  <guid>https://mlops.systems/posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html</guid>
  <pubDate>Tue, 04 Feb 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/ch8-proportions.png" medium="image" type="image/png" height="50" width="144"/>
</item>
<item>
  <title>Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html</link>
  <description><![CDATA[ 




<p>I enjoyed chapter 7 on finetuning. It jams a lot of detail into the 50 pages she takes to explain things. Some areas had more detail than you’d expect, and others less, but overall this was a solid summary / review.</p>
<blockquote class="blockquote">
<p><strong>Core Narrative</strong>: Fine-tuning represents a significant technical and organisational investment that should be approached as a last resort, not a first solution.</p>
</blockquote>
<p>The chapter’s essential message can be distilled into three key points:</p>
<ol type="1">
<li>The decision to fine-tune should follow exhausting simpler approaches like prompt engineering and RAG. At the end she sums it up: <em>fine-tuning is for form, while RAG is for facts</em>.</li>
<li>Memory considerations dominate the technical landscape of fine-tuning, leading to the emergence of techniques like PEFT (particularly LoRA) that make fine-tuning more accessible. The chapter emphasises that while the actual process of fine-tuning isn’t necessarily complex, the surrounding infrastructure and maintenance requirements are substantial.</li>
<li>A clear progression pathway emerges: start with prompt engineering, move to examples (up to ~50), implement RAG if needed, and only then consider fine-tuning. Even then, breaking down complex tasks into simpler components might be preferable to full fine-tuning.</li>
</ol>
<p>So fine-tuning can be incredibly powerful when applied judiciously, but it requires careful consideration of both technical capabilities and organisational readiness.</p>
<section id="chapter-overview-and-context" class="level2">
<h2 class="anchored" data-anchor-id="chapter-overview-and-context">Chapter Overview and Context</h2>
<p>This long chapter (approximately 50 pages, much like the others) was notably one of the most challenging for Chip to write. It presents fine-tuning as an advanced approach that moves beyond basic prompt engineering, covering everything from fundamental concepts to practical implementation strategies.</p>
<p>The depth and breadth of the chapter reflect the complexity of fine-tuning as both a technical and organisational challenge, though the things she writes about doesn’t really cover the reality of what it’s like to work on these kinds of initiatives within a team.</p>
</section>
<section id="core-decision-when-to-fine-tune" class="level2">
<h2 class="anchored" data-anchor-id="core-decision-when-to-fine-tune">Core Decision: When to Fine-tune</h2>
<p>The decision to fine-tune should never be taken lightly. While the potential benefits are significant, including improved model quality and task-specific capabilities, the chapter emphasises that fine-tuning should be considered a last resort rather than a default approach.</p>
<blockquote class="blockquote">
<p><strong>Notable Case Study</strong>: Grammarly achieved remarkable results with their fine-tuned T5 models, which outperformed GPT-3 variants despite being 60 times smaller. This example illustrates how targeted fine-tuning can sometimes achieve better results than using larger, more general models.</p>
</blockquote>
<section id="reasons-to-avoid-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="reasons-to-avoid-fine-tuning">Reasons to Avoid Fine-tuning</h3>
<p>The chapter presents several compelling reasons why organisations might want to exhaust other options before pursuing fine-tuning:</p>
<ol type="1">
<li>Performance Degradation: Fine-tuning can actually degrade model performance on tasks outside the specific target domain</li>
<li>Engineering Complexity: The process introduces significant technical overhead</li>
<li>Specialised Knowledge Requirements: Teams need expertise in model training</li>
<li>Infrastructure Demands: Self-serving infrastructure becomes necessary</li>
<li>Ongoing Maintenance: Requires dedicated policies and budgets for monitoring and updates</li>
</ol>
</section>
<section id="fine-tuning-vs.-rag-a-critical-distinction" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-vs.-rag-a-critical-distinction">Fine-tuning vs.&nbsp;RAG: A Critical Distinction</h3>
<p>One of the most important conceptual frameworks presented is the distinction between fine-tuning and RAG:</p>
<ul>
<li>Fine-tuning focuses on form - how the model expresses information</li>
<li>RAG specialises in facts - what information the model can access and use</li>
</ul>
<p>This separation provides a clear decision framework, though the chapter acknowledges there are exceptions to this general rule.</p>
</section>
</section>
<section id="progressive-implementation-workflow" class="level2">
<h2 class="anchored" data-anchor-id="progressive-implementation-workflow">Progressive Implementation Workflow</h2>
<p><img src="https://mlops.systems/posts/images/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning/prompting_to_rag_to_finetuning.png" class="img-fluid"></p>
<p>The chapter outlines a thoughtful progression of implementation strategies, suggesting organisations should:</p>
<ol type="1">
<li>Begin with prompt engineering optimisation</li>
<li>Expand to include more examples (up to approximately 50)</li>
<li>Implement dynamic data source connections through RAG</li>
<li>Consider advanced RAG methodologies</li>
<li>Explore fine-tuning only after exhausting other options</li>
<li>Consider task decomposition if still unsuccessful</li>
</ol>
</section>
<section id="memory-bottlenecks-and-technical-considerations" class="level2">
<h2 class="anchored" data-anchor-id="memory-bottlenecks-and-technical-considerations">Memory Bottlenecks and Technical Considerations</h2>
<section id="critical-memory-factors" class="level3">
<h3 class="anchored" data-anchor-id="critical-memory-factors">Critical Memory Factors</h3>
<p>The chapter emphasises three key contributors to a model’s memory footprint during fine-tuning:</p>
<ul>
<li>Parameter count</li>
<li>Trainable parameter count</li>
<li>Numeric representations</li>
</ul>
<blockquote class="blockquote">
<p><strong>Technical Note</strong>: The relationship between trainable parameters and memory requirements becomes a key motivator for PEFT (Parameter Efficient Fine Tuning) approaches.</p>
</blockquote>
</section>
<section id="quantisation-strategies" class="level3">
<h3 class="anchored" data-anchor-id="quantisation-strategies">Quantisation Strategies</h3>
<p>The chapter provides a detailed examination of quantisation approaches, particularly noting the distinction between:</p>
<ol type="1">
<li>Post-Training Quantisation (PTQ)
<ul>
<li>Most common approach</li>
<li>Particularly relevant for AI application developers</li>
<li>Supported by major frameworks with minimal code requirements</li>
</ul></li>
<li>Training Quantisation
<ul>
<li>Emerging approach gaining traction</li>
<li>Aims to optimise both inference performance and training costs</li>
</ul></li>
</ol>
</section>
</section>
<section id="advanced-fine-tuning-techniques" class="level2">
<h2 class="anchored" data-anchor-id="advanced-fine-tuning-techniques">Advanced Fine-tuning Techniques</h2>
<section id="peft-methodologies" class="level3">
<h3 class="anchored" data-anchor-id="peft-methodologies">PEFT Methodologies</h3>
<p>The chapter identifies two primary PEFT approaches:</p>
<ol type="1">
<li>Adapter-based methods (Additive):
<ul>
<li>LoRA emerges as the most popular implementation</li>
<li>Includes variants like Dora and qDora from Anthropic</li>
<li>Involves adding new modules to existing model weights</li>
</ul></li>
<li>Soft prompt-based methods:
<ul>
<li>Less common but growing in popularity</li>
<li>Introduces trainable tokens for input processing modification</li>
<li>Offers a middle ground between full fine-tuning and basic prompting, so maybe interesting for teams who don’t <em>really</em> want to go too deep into finetuning (?)</li>
</ul></li>
</ol>
</section>
<section id="model-merging-and-multitask-considerations" class="level3">
<h3 class="anchored" data-anchor-id="model-merging-and-multitask-considerations">Model Merging and Multitask Considerations</h3>
<p>The chapter presents model merging as an evolving science, requiring significant expertise. Three primary approaches are discussed:</p>
<ul>
<li>Summing</li>
<li>Layer stacking</li>
<li>Concatenation (generally not recommended due to memory implications)</li>
</ul>
<p><img src="https://mlops.systems/posts/images/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning/model_merging.png" class="img-fluid"></p>
<p>There’s a lot of detail in this section (much more than I’d expected) but it was interesting to read about something that I haven’t much practical expertise with.</p>
</section>
</section>
<section id="core-approaches-to-model-merging" class="level2">
<h2 class="anchored" data-anchor-id="core-approaches-to-model-merging">Core Approaches to Model Merging</h2>
<p>The chapter outlines three fundamental approaches to model merging, each with its own technical considerations and trade-offs:</p>
<blockquote class="blockquote">
<p><strong>Technical Architecture</strong>: The three primary merging strategies</p>
<ol type="1">
<li><strong>Summing</strong>: Direct weight combination</li>
<li><strong>Layer stacking</strong>: Vertical integration of model components</li>
<li><strong>Concatenation</strong>: Horizontal expansion (though notably discouraged due to memory implications)</li>
</ol>
</blockquote>
<p>The relative simplicity of these approaches belies their potential impact on model architecture and performance. Particularly interesting is how these techniques interface with the broader challenge of multitask learning.</p>
</section>
<section id="multitask-learning-a-new-paradigm" class="level2">
<h2 class="anchored" data-anchor-id="multitask-learning-a-new-paradigm">Multitask Learning: A New Paradigm</h2>
<p>Traditional approaches to multitask learning have typically forced practitioners into one of two suboptimal paths:</p>
<ol type="1">
<li><strong>Simultaneous Training</strong>
<ul>
<li>Requires creation of a comprehensive dataset containing examples for all tasks</li>
<li>Necessitates careful balancing of task representation</li>
<li>Often leads to compromise in per-task performance</li>
</ul></li>
<li><strong>Sequential Training</strong>
<ul>
<li>Fine-tunes the model on each task in sequence</li>
<li>Risks catastrophic forgetting as new tasks overwrite previous learning</li>
<li>Requires careful orchestration of task order and learning rates</li>
</ul></li>
</ol>
<blockquote class="blockquote">
<p><strong>Key Innovation</strong>: Model merging introduces a third path - parallel fine-tuning followed by strategic combination. This approach fundamentally alters the landscape of multitask learning optimisation.</p>
</blockquote>
</section>
<section id="the-parallel-processing-advantage" class="level2">
<h2 class="anchored" data-anchor-id="the-parallel-processing-advantage">The Parallel Processing Advantage</h2>
<p>Model merging enables a particularly elegant solution to the multitask learning challenge through parallel processing:</p>
<ol type="1">
<li>Individual models can be fine-tuned for specific tasks independently</li>
<li>Training can occur in parallel, optimising computational resource usage</li>
<li>Models can be merged post-training, preserving task-specific optimisations</li>
</ol>
<p>This approach brings several compelling advantages:</p>
<blockquote class="blockquote">
<p><strong>Strategic Benefits</strong>: - Parallel training efficiency - Independent task optimisation - Flexible deployment options - Reduced risk of inter-task interference</p>
</blockquote>
</section>
<section id="practical-implications" class="level2">
<h2 class="anchored" data-anchor-id="practical-implications">Practical Implications</h2>
<p>While the implementation details remain somewhat experimental, the potential applications are significant. Organisations can:</p>
<ul>
<li>Develop specialised models in parallel</li>
<li>Optimise individual task performance without compromise</li>
<li>Maintain flexibility in deployment architecture</li>
<li>Scale their multitask capabilities more efficiently</li>
</ul>
</section>
<section id="implementation-pathways" class="level2">
<h2 class="anchored" data-anchor-id="implementation-pathways">Implementation Pathways</h2>
<p>The chapter concludes with two distinct development approaches:</p>
<section id="progression-path" class="level3">
<h3 class="anchored" data-anchor-id="progression-path">Progression Path</h3>
<ol type="1">
<li>Begin with the most economical and fastest model</li>
<li>Validate with a mid-tier model</li>
<li>Push boundaries with the optimal model</li>
<li>Map the price-performance frontier</li>
<li>Select the most appropriate model based on requirements</li>
</ol>
</section>
<section id="distillation-path" class="level3">
<h3 class="anchored" data-anchor-id="distillation-path">Distillation Path</h3>
<ol type="1">
<li>Start with a small dataset and the strongest affordable model</li>
<li>Generate additional training data using the fine-tuned model</li>
<li>Train a more cost-effective model using the expanded dataset</li>
</ol>
</section>
</section>
<section id="final-observations" class="level2">
<h2 class="anchored" data-anchor-id="final-observations">Final Observations</h2>
<p>The chapter emphasises that while the technical process of fine-tuning isn’t necessarily complex, the surrounding context and implications are highly nuanced. Success requires careful consideration of business priorities, resource availability, and long-term maintenance capabilities. This holistic perspective is crucial for organisations considering fine-tuning as part of their AI strategy.</p>


</section>

 ]]></description>
  <category>books-i-read</category>
  <category>finetuning</category>
  <category>llm</category>
  <category>llms</category>
  <guid>https://mlops.systems/posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning.html</guid>
  <pubDate>Sat, 25 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7-finetuning/finetuning.png" medium="image" type="image/png" height="99" width="144"/>
</item>
<item>
  <title>Notes on ‘AI Engineering’ (Chip Huyen) chapter 6</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html</link>
  <description><![CDATA[ 




<p>This chapter was all about RAG and agents. It’s only 50 pages, so clearly there’s only so much of the details she can get into, but it was pretty good nonetheless and there were a few things in here I’d never really read. Also Chip does a good job bringing the RAG story into the story about agents, particularly in terms of how she defines agents. (Note that <a href="https://huyenchip.com/2025/01/07/agents.html">the second half of this chapter</a>, on agents, is available <a href="https://huyenchip.com/2025/01/07/agents.html">on Chip’s blog</a> as a free excerpt!)</p>
<p>As always, what follows is just my notes on the things that seemed interesting to me (and a high-level overview of the main points of the chapter just for future reference). YMMV!</p>
<section id="chapter-structure-and-framing" class="level2">
<h2 class="anchored" data-anchor-id="chapter-structure-and-framing">Chapter Structure and Framing</h2>
<p>This chapter undertakes the ambitious task of unifying two major paradigms in AI engineering: Retrieval-Augmented Generation (RAG) and Agents. At first glance, combining these topics might seem surprising given their scope and complexity. However, Chip creates a compelling framework that positions both as sophisticated approaches to <em>context construction</em>.</p>
<p>The unifying thesis presents RAG as a specialised case of the agent pattern, where the retriever functions as a tool at the model’s disposal. Both patterns serve to transcend context limitations and maintain current information, though agents ultimately offer broader capabilities. This framing provides an elegant theoretical bridge between these technologies while acknowledging their distinct characteristics.</p>
</section>
<section id="retrieval-augmented-generation-rag" class="level2">
<h2 class="anchored" data-anchor-id="retrieval-augmented-generation-rag">Retrieval-Augmented Generation (RAG)</h2>
<section id="core-concepts-and-context-windows" class="level3">
<h3 class="anchored" data-anchor-id="core-concepts-and-context-windows">Core Concepts and Context Windows</h3>
<p>The discussion begins with a fundamental examination of RAG’s purpose: enhancing model outputs with query-specific context to produce more grounded and useful results. Chip introduces a fascinating variation on Parkinson’s Law:</p>
<blockquote class="blockquote">
<p><strong>Context Expansion Law</strong>: Application context tends to expand to fill the context limits supported by the model.</p>
</blockquote>
<p>This observation challenges the common assumption that RAG might become obsolete with infinite context models. Chip argues that larger context windows don’t necessarily solve the fundamental challenges RAG addresses, particularly noting that models often struggle with information buried in the middle of large context windows.</p>
</section>
<section id="retrieval-architecture-and-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-architecture-and-algorithms">Retrieval Architecture and Algorithms</h3>
<p>The retrieval architecture discussion introduces two primary paradigms:</p>
<blockquote class="blockquote">
<p><strong>Sparse Retrieval</strong>: Term-based approaches that rely on explicit matching of terms between queries and documents. The primary example is the <strong>TFIDF</strong> (Term Frequency-Inverse Document Frequency) algorithm, which evaluates term importance based on frequency patterns.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Dense Retrieval</strong>: Embedding-based approaches that transform text into vector representations, requiring specialised vector databases for storage and sophisticated nearest-neighbour search algorithms for retrieval.</p>
</blockquote>
</section>
<section id="cost-considerations-and-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="cost-considerations-and-trade-offs">Cost Considerations and Trade-offs</h3>
<p>A striking revelation emerges regarding the cost structure of RAG systems: vector database expenses often consume between one-fifth to half of a company’s total model API spending. This cost burden becomes particularly acute for systems requiring frequent embedding updates due to changing data. Chip notes that both vector storage and vector search queries can be surprisingly expensive operations.</p>
</section>
<section id="retrieval-optimisation-techniques" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-optimisation-techniques">Retrieval Optimisation Techniques</h3>
<p><img src="https://mlops.systems/posts/images/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6/chunks-for-chunks.png" class="img-fluid"></p>
<p>The chapter presents several sophisticated approaches to optimisation:</p>
<p><strong>Chunking Strategies</strong>: While the section is brief, it addresses the critical trade-offs in how documents are segmented for retrieval.</p>
<p><strong>Query Rewriting</strong>: A powerful but potentially complex technique that enhances initial queries with contextual information. For example, transforming a query like “how about her?” into “how about Aunt Mabel from the previous question?” Chip notes this can introduce latency issues and suggests careful consideration before implementation.</p>
<p><strong>Contextual Retrieval</strong>: Introduces the innovative “chunks-for-chunks” approach, where each retrieved chunk triggers additional retrievals for supplementary context. This might include retrieving related tags or associated metadata to enrich the initial results.</p>
<p><strong>Hybrid Search</strong>: Combines term-based and embedding-based retrieval, typically implementing a re-ranking process. A common pattern involves using term-based retrieval (like Elasticsearch) to obtain an initial set of ~50 (or however many!) documents, followed by embedding-based re-ranking to identify the most relevant subset.</p>
</section>
<section id="evaluation-framework" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-framework">Evaluation Framework</h3>
<p>The evaluation framework centres on two primary metrics:</p>
<blockquote class="blockquote">
<p><strong>Context Precision</strong>: The percentage of retrieved documents that are relevant to the query. Generally easier to measure and optimise.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Context Recall</strong>: The percentage of all relevant documents that are successfully retrieved. More challenging to measure as it requires comprehensive dataset annotation.</p>
</blockquote>
</section>
</section>
<section id="agents" class="level2">
<h2 class="anchored" data-anchor-id="agents">Agents</h2>
<section id="foundational-definition" class="level3">
<h3 class="anchored" data-anchor-id="foundational-definition">Foundational Definition</h3>
<p>Chip provides a clear definition of an agent:</p>
<blockquote class="blockquote">
<p><strong>Agent Definition</strong>: An entity capable of perceiving its environment and acting upon it, characterised by: - The environment it operates in (defined by use case) - The set of actions it can perform (augmented by tools)</p>
</blockquote>
</section>
<section id="tool-types-and-capabilities" class="level3">
<h3 class="anchored" data-anchor-id="tool-types-and-capabilities">Tool Types and Capabilities</h3>
<p>The chapter delineates three primary categories of tools:</p>
<p><strong>Knowledge Augmentation Tools</strong>: - RAG systems - Web search capabilities - API calls for information retrieval</p>
<p><strong>Capability Extension Tools</strong>: - Code interpreters - Terminal access - Function execution capabilities These have been shown to significantly boost model performance compared to prompting or fine-tuning alone.</p>
<p><strong>Write Actions</strong>: - Data manipulation capabilities - Storage and deletion operations</p>
</section>
<section id="planning-architecture" class="level3">
<h3 class="anchored" data-anchor-id="planning-architecture">Planning Architecture</h3>
<p>The planning process emerges as a four-stage cycle:</p>
<ol type="1">
<li><strong>Plan Generation</strong>: Task decomposition and strategy development</li>
<li><strong>Initial Reflection</strong>: Plan evaluation and potential revision</li>
<li><strong>Execution</strong>: Implementation of planned actions, often involving specific function calls</li>
<li><strong>Final Reflection</strong>: Outcome evaluation and error correction</li>
</ol>
<p>Chip includes an interesting debate about foundation models as planners, noting Yan LeCun’s assertion that autoregressive models cannot truly plan, though this remains a point of discussion in the field.</p>
</section>
<section id="plan-execution-patterns" class="level3">
<h3 class="anchored" data-anchor-id="plan-execution-patterns">Plan Execution Patterns</h3>
<p><img src="https://mlops.systems/posts/images/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6/agent-execution-order.png" class="img-fluid"></p>
<p>The execution of agent plans reveals a fascinating interplay between computational patterns and practical implementation. Chip identifies several fundamental execution patterns that form the backbone of agent behaviour, each offering distinct advantages and trade-offs in different scenarios.</p>
<blockquote class="blockquote">
<p><strong>Execution Paradigms</strong>: The core patterns through which agents transform plans into actions, ranging from simple sequential execution to complex conditional logic.</p>
</blockquote>
<p>The primary execution patterns include:</p>
<p><strong>Sequential Execution</strong>: The most straightforward pattern, where actions are performed one after another in a predetermined order. This approach offers predictability and simplicity but may not maximise efficiency when actions could be performed concurrently.</p>
<p><strong>Parallel Execution</strong>: Enables multiple actions to be performed simultaneously when dependencies permit. While this pattern can significantly improve performance, it introduces complexity in managing concurrent operations and handling potential conflicts.</p>
<p><strong>Conditional Execution</strong>: Implements decision points through <code>if</code> statements, allowing agents to adapt their execution path based on intermediate results or environmental conditions. This pattern introduces crucial flexibility but requires careful handling of branch logic and state management.</p>
<p><strong>Iterative Execution</strong>: Utilises <code>for</code> loops to handle repetitive tasks or process collections of items. This pattern is particularly powerful when dealing with datasets or when similar actions need to be performed multiple times with variations.</p>
<blockquote class="blockquote">
<p><strong>Pattern Selection</strong>: The choice of execution pattern often emerges from the intersection of task requirements, system constraints, and performance goals.</p>
</blockquote>
<p>The effectiveness of these patterns depends heavily on the underlying system architecture and the specific requirements of the task at hand. For instance, parallel execution might offer theoretical performance benefits but could introduce unnecessary complexity for simple, linear tasks. Similarly, conditional execution provides valuable flexibility but requires robust error handling and state management to maintain system reliability.</p>
<p>Chip emphasises that these patterns aren’t mutually exclusive - sophisticated agent systems often combine multiple patterns to create more complex and capable execution strategies. This hybrid approach allows for the development of highly adaptable agents that can handle a wide range of tasks while maintaining system stability and performance.</p>
</section>
<section id="planning-optimisation" class="level3">
<h3 class="anchored" data-anchor-id="planning-optimisation">Planning Optimisation</h3>
<p>The chapter provides several practical tips for improving agent planning:</p>
<ol type="1">
<li>Enhance system prompts with more examples</li>
<li>Provide better tool descriptions and parameter documentation</li>
<li>Simplify complex functions through refactoring</li>
<li>Consider using stronger models or fine-tuning for plan generation</li>
</ol>
</section>
<section id="function-calling-implementation" class="level3">
<h3 class="anchored" data-anchor-id="function-calling-implementation">Function Calling Implementation</h3>
<p>The function calling architecture requires:</p>
<ol type="1">
<li>Tool inventory creation, including:
<ul>
<li>Function names and entry points</li>
<li>Parameter specifications</li>
<li>Comprehensive documentation</li>
</ul></li>
<li>Tool usage specification (required vs.&nbsp;optional)</li>
<li>Version control for function names, parameters, and documentation</li>
</ol>
</section>
<section id="planning-granularity" class="level3">
<h3 class="anchored" data-anchor-id="planning-granularity">Planning Granularity</h3>
<p>Chip introduces an important discussion of planning levels, analogous to temporal planning horizons (yearly plans vs.&nbsp;daily tasks). This presents a fundamental trade-off:</p>
<blockquote class="blockquote">
<p><strong>Planning Trade-off</strong>: Higher-level plans are easier to generate but harder to execute, while detailed plans are harder to generate but easier to execute.</p>
</blockquote>
</section>
<section id="tool-selection-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="tool-selection-and-evaluation">Tool Selection and Evaluation</h3>
<p>The chapter provides a systematic approach to tool selection:</p>
<ol type="1">
<li>Conduct ablation studies to measure performance impact</li>
<li>Monitor tool usage patterns and error rates</li>
<li>Analyze tool call distribution</li>
<li>Consider model-specific tool preferences (noting that GPT-4 tends to use a wider tool set than ChatGPT)</li>
</ol>
</section>
<section id="memory-systems" class="level3">
<h3 class="anchored" data-anchor-id="memory-systems">Memory Systems</h3>
<p>The memory architecture comprises two core functions:</p>
<blockquote class="blockquote">
<p><strong>Memory Functions</strong>: - Memory management - Memory retrieval</p>
</blockquote>
<p>The system supports three types of memory:</p>
<ul>
<li>Internal knowledge</li>
<li>Short-term memory</li>
<li>Long-term memory</li>
</ul>
<p>These systems prove crucial for:</p>
<ul>
<li>Managing information overflow</li>
<li>Maintaining session persistence</li>
<li>Ensuring model consistency</li>
<li>Preserving data structural integrity</li>
</ul>
</section>
<section id="evaluation-and-failure-modes" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-and-failure-modes">Evaluation and Failure Modes</h3>
<p>The comprehensive evaluation framework considers:</p>
<ul>
<li>Planning effectiveness</li>
<li>Tool execution accuracy</li>
<li>System latency</li>
<li>Overall efficiency</li>
<li>Memory system performance</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The unifying thread of context construction provides a compelling framework for understanding these technologies not as separate entities, but as complementary approaches to extending model capabilities.</p>


</section>

 ]]></description>
  <category>books-i-read</category>
  <category>llm</category>
  <category>llms</category>
  <category>agents</category>
  <category>rag</category>
  <category>evaluation</category>
  <guid>https://mlops.systems/posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html</guid>
  <pubDate>Thu, 23 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6/agent-execution-order.png" medium="image" type="image/png" height="91" width="144"/>
</item>
<item>
  <title>Notes on ‘AI Engineering’ (Chip Huyen) chapter 4</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html</link>
  <description><![CDATA[ 




<p>This chapter represents a crucial bridge between academic research and production engineering practice in AI system evaluation. What sets it apart is the Chip’s very balanced perspective - neither succumbing to the prevalent hype in the field nor becoming overly academic. Instead, she melds together practical insights with theoretical foundations, creating a useful framework for evaluation that acknowledges both technical and ethical considerations.</p>
<section id="introduction-and-context" class="level2">
<h2 class="anchored" data-anchor-id="introduction-and-context">Introduction and Context</h2>
<blockquote class="blockquote">
<p><strong>Key Insight</strong>: The author’s approach demonstrates that effective AI system evaluation requires a synthesis of academic rigour and practical engineering concerns, much like how traditional software engineering evolved to balance theoretical computer science with practical development methodologies.</p>
</blockquote>
<p>The chapter is structured in three main parts, each building upon the previous to create a complete picture of AI system evaluation:</p>
<ol type="1">
<li>Evaluation criteria fundamentals</li>
<li>Model selection and benchmark navigation</li>
<li>Practical pipeline implementation</li>
</ol>
</section>
<section id="part-1-evaluation-criteria---a-deep-dive" class="level2">
<h2 class="anchored" data-anchor-id="part-1-evaluation-criteria---a-deep-dive">Part 1: Evaluation Criteria - A Deep Dive</h2>
<section id="the-evolution-of-evaluation-driven-development" class="level3">
<h3 class="anchored" data-anchor-id="the-evolution-of-evaluation-driven-development">The Evolution of Evaluation-Driven Development</h3>
<p>The author introduces <strong>evaluation-driven development</strong> (EDD), a methodological evolution that adapts the principles of test-driven development to the unique challenges of AI systems.</p>
<blockquote class="blockquote">
<p><strong>Evaluation-Driven Development</strong>: A methodology where AI application development begins with explicit evaluation criteria, similar to how test-driven development starts with test cases. However, EDD encompasses a broader range of metrics and considerations specific to AI systems.</p>
</blockquote>
<p>The fundamental principle here is that AI applications require a more nuanced and multifaceted approach to evaluation than traditional software. Where traditional software might have binary pass/fail criteria, AI systems often operate in a spectrum of performance across multiple dimensions.</p>
</section>
<section id="the-four-pillars-of-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="the-four-pillars-of-evaluation">The Four Pillars of Evaluation</h3>
<section id="domain-specific-capability" class="level4">
<h4 class="anchored" data-anchor-id="domain-specific-capability">1. Domain-Specific Capability</h4>
<p>The author presents domain-specific capability evaluation as the foundational layer of AI system assessment. This approach is particularly innovative in its use of <strong>multiple choice evaluation techniques</strong> - a method that bridges the gap between human-interpretable results and machine performance metrics.</p>
<p><em>For example</em>, when evaluating code generation capabilities, presenting a model with multiple implementations where only one is functionally correct serves as both a test and a teaching tool. This mimics how human experts often evaluate junior developers’ understanding of coding patterns and best practices.</p>
</section>
<section id="generation-capability" class="level4">
<h4 class="anchored" data-anchor-id="generation-capability">2. Generation Capability</h4>
<p>The section on generation capability draws parallels with the historical development of Natural Language Generation (NLG) in computational linguistics. This historical context provides valuable insights into how we can approach modern language model evaluation.</p>
<p>The author breaks down factual consistency into two crucial dimensions:</p>
<blockquote class="blockquote">
<p><strong>Local Factual Consistency</strong>: The internal coherence of generated content and its alignment with the immediate context of the prompt. This is analogous to maintaining logical consistency within a single conversation or document.</p>
<p><strong>Global Factual Consistency</strong>: The accuracy of generated content when compared against established knowledge and facts. This represents the model’s ability to maintain truthfulness in a broader context.</p>
</blockquote>
<p>The discussion of hallucination detection is particularly noteworthy, presenting three complementary approaches:</p>
<ol type="1">
<li><strong>Basic Prompting</strong>: Direct detection through carefully crafted prompts</li>
<li><strong>Self-Verification</strong>: A novel approach using internal consistency checks across multiple generations</li>
<li><strong>Knowledge-Augmented Verification</strong>: Advanced techniques like Google DeepMind’s SAFE paper (search augmented factuality evaluator)</li>
</ol>
<p>The knowledge-augmented verification system represents a fascinating approach to fact-checking that mirrors how human experts verify information:</p>
<ul>
<li>It breaks down complex statements into atomic claims</li>
<li>Each claim is independently verified through search</li>
<li>The results are synthesised into a final accuracy assessment</li>
</ul>
<p>Seems pricey, though :)</p>
</section>
<section id="instruction-following-capability" class="level4">
<h4 class="anchored" data-anchor-id="instruction-following-capability">3. Instruction Following Capability</h4>
<p>The author makes a crucial observation about the bidirectional nature of instruction following evaluation. Poor performance might indicate either model limitations or instruction ambiguity - a distinction that’s often overlooked in practice.</p>
<blockquote class="blockquote">
<p><strong>Instruction-Performance Paradox</strong>: The quality of instruction following cannot be evaluated in isolation from the quality of the instructions themselves, creating a circular dependency that must be carefully managed in evaluation design.</p>
</blockquote>
<p>The solution proposed is the development of custom benchmarks that specifically target your application’s requirements. This approach ensures that your evaluation criteria align perfectly with your practical needs rather than relying solely on generic benchmarks.</p>
</section>
<section id="cost-and-latency-considerations" class="level4">
<h4 class="anchored" data-anchor-id="cost-and-latency-considerations">4. Cost and Latency Considerations</h4>
<p>The author introduces the concept of <strong>Pareto optimization</strong> in the context of AI system evaluation, demonstrating how different performance metrics often involve trade-offs that must be carefully balanced.</p>
<blockquote class="blockquote">
<p><strong>Pareto Optimization</strong>: A multi-objective optimization approach where improvements in one metric cannot be achieved without degrading another, leading to a set of optimal trade-off solutions rather than a single optimal point.</p>
</blockquote>
</section>
</section>
</section>
<section id="part-2-model-selection---a-strategic-approach" class="level2">
<h2 class="anchored" data-anchor-id="part-2-model-selection---a-strategic-approach">Part 2: Model Selection - A Strategic Approach</h2>
<section id="the-four-step-evaluation-workflow" class="level3">
<h3 class="anchored" data-anchor-id="the-four-step-evaluation-workflow">The Four-Step Evaluation Workflow</h3>
<p>The author presents a sophisticated workflow that combines both quantitative and qualitative factors in model selection. This approach is particularly valuable because it acknowledges the complexity of real-world deployment while providing a structured path forward.</p>
<ol type="1">
<li><p><strong>Initial Filtering</strong> The first step involves filtering based on hard constraints, which might include:</p>
<ul>
<li>Deployment requirements (on-premise vs.&nbsp;cloud)</li>
<li>Security and privacy considerations</li>
<li>Licensing restrictions</li>
<li>Resource constraints</li>
</ul></li>
<li><p><strong>Public Information Assessment</strong> This stage involves a systematic review of:</p>
<ul>
<li>Benchmark performances across relevant tasks</li>
<li>Leaderboard rankings with context</li>
<li>Published latency and cost metrics</li>
</ul>
<p>The author emphasises the importance of looking beyond raw numbers to understand the context and limitations of public benchmarks.</p></li>
<li><p><strong>Experimental Evaluation</strong> This phase involves hands-on testing with your specific use case, considering:</p>
<ul>
<li>Custom evaluation metrics</li>
<li>Integration requirements</li>
<li>Real-world performance characteristics</li>
</ul></li>
<li><p><strong>Continuous Monitoring</strong> The final step acknowledges that evaluation is an ongoing process, not a one-time event. This involves:</p>
<ul>
<li>Regular performance monitoring</li>
<li>Failure detection and analysis</li>
<li>Feedback collection and incorporation</li>
<li>Continuous improvement cycles</li>
</ul></li>
</ol>
</section>
<section id="the-build-vs.-buy-decision-matrix" class="level3">
<h3 class="anchored" data-anchor-id="the-build-vs.-buy-decision-matrix">The Build vs.&nbsp;Buy Decision Matrix</h3>
<p>The author provides an analysis of the build vs.&nbsp;buy decision, going beyond simple cost comparisons to consider factors like:</p>
<blockquote class="blockquote">
<p><strong>Total Cost of Ownership (TCO)</strong>: The complete cost picture including: - Direct costs (API fees, computing resources) - Indirect costs (engineering time, maintenance) - Opportunity costs (time to market, feature development) - Risk costs (security, reliability, vendor lock-in)</p>
</blockquote>
<p>This section particularly shines in its discussion of the often-overlooked aspects of model deployment, such as the hidden costs of maintaining self-hosted models and the true value of vendor-provided updates and improvements.</p>
</section>
</section>
<section id="part-3-building-evaluation-pipelines---practical-implementation" class="level2">
<h2 class="anchored" data-anchor-id="part-3-building-evaluation-pipelines---practical-implementation">Part 3: Building Evaluation Pipelines - Practical Implementation</h2>
<section id="system-component-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="system-component-evaluation">System Component Evaluation</h3>
<p>The author advocates for a <strong>dual-track evaluation approach</strong>:</p>
<ol type="1">
<li>End-to-end system evaluation</li>
<li>Component-level assessment</li>
</ol>
<p>This approach allows organisations to:</p>
<ul>
<li>Identify bottlenecks and failure points</li>
<li>Understand component interactions</li>
<li>Make targeted improvements</li>
<li>Maintain system reliability during updates</li>
</ul>
</section>
<section id="creating-effective-evaluation-guidelines" class="level3">
<h3 class="anchored" data-anchor-id="creating-effective-evaluation-guidelines">Creating Effective Evaluation Guidelines</h3>
<p>The author emphasises the importance of creating clear, actionable evaluation guidelines that bridge technical and business metrics. This section introduces the concept of <strong>metric alignment</strong> - ensuring that technical evaluation metrics directly correspond to business value.</p>
<blockquote class="blockquote">
<p><strong>Metric Alignment</strong>: The process of mapping technical performance metrics to business outcomes, creating a clear connection between model improvements and business value.</p>
</blockquote>
</section>
<section id="data-management-and-sampling" class="level3">
<h3 class="anchored" data-anchor-id="data-management-and-sampling">Data Management and Sampling</h3>
<p>Chip provides valuable insights into data management for evaluation, including:</p>
<blockquote class="blockquote">
<p><strong>Data Slicing</strong>: The strategic separation of evaluation data into meaningful subsets to: - Identify performance variations across different use cases - Detect potential biases - Enable targeted improvement efforts - Avoid Simpson’s paradox in performance analysis</p>
</blockquote>
<p>The discussion of sample size is particularly practical, providing concrete guidelines based on statistical confidence levels and desired detection thresholds. The author cites OpenAI’s research suggesting that sample sizes between 100 and 1,000 are typically sufficient for most evaluation needs, depending on the required confidence level.</p>
<p><img src="https://mlops.systems/posts/images/chip4-sample-number.png" class="img-fluid"></p>
</section>
<section id="meta-evaluation-evaluating-your-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="meta-evaluation-evaluating-your-evaluation">Meta-Evaluation: Evaluating Your Evaluation</h3>
<p>The chapter concludes with a crucial discussion of meta-evaluation - the process of assessing and improving your evaluation pipeline itself. This includes considerations of:</p>
<ul>
<li>Signal quality and reliability</li>
<li>Metric correlation and redundancy</li>
<li>Resource utilisation and efficiency</li>
<li>Integration with development workflows</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The author concludes around the inherent limitations of AI system evaluation: no single metric or method can fully capture the complexity of these systems. However, this acknowledgment leads to a constructive approach: combining multiple evaluation methods, maintaining awareness of their limitations, and continuously iterating based on real-world feedback.</p>
<p>This chapter ultimately provides a solid framework for AI system evaluation that is both theoretically sound and practically applicable. It serves as a valuable resource for organisations working to implement effective evaluation strategies for their AI systems, while maintaining a clear-eyed view of both the possibilities and limitations of current evaluation methods.</p>


</section>

 ]]></description>
  <category>books-i-read</category>
  <category>llm</category>
  <category>llms</category>
  <category>evaluation</category>
  <guid>https://mlops.systems/posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html</guid>
  <pubDate>Tue, 21 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/chip-ch4.png" medium="image" type="image/png" height="85" width="144"/>
</item>
<item>
  <title>Notes on ‘AI Engineering’ (Chip Huyen) chapter 3</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html</link>
  <description><![CDATA[ 




<p>Really enjoyed this chapter. My tidied notes from my readings follow below. 150 pages in and we’re starting to get to the good stuff :)</p>
<section id="overview-and-context" class="level2">
<h2 class="anchored" data-anchor-id="overview-and-context">Overview and Context</h2>
<p>This chapter serves as the first of two chapters (Chapters 3 and 4) dealing with evaluation in AI Engineering. While Chapter 4 will delve into evaluation within systems, Chapter 3 addresses the fundamental question of how to evaluate open-ended responses from foundation models and LLMs at a high level. The importance of evaluation cannot be overstated, though the author perhaps takes this somewhat for granted. The chapter provides a comprehensive framework for understanding various evaluation methodologies and their applications.</p>
</section>
<section id="challenges-in-evaluating-foundation-models" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-evaluating-foundation-models">Challenges in Evaluating Foundation Models</h2>
<p>The evaluation of foundation models presents several unique and complex challenges that make systematic assessment difficult:</p>
<ul>
<li>Existing benchmarks become increasingly inadequate as models improve in their capabilities</li>
<li>As models become better at writing and mimicking human-like responses, evaluation becomes more complex and nuanced</li>
<li>Many foundation models are API-driven black boxes, limiting access to internal workings</li>
<li>Models continuously develop new capabilities, requiring constant adaptation of evaluation methods</li>
<li>There has been notably limited investment in evaluation studies and technologies compared to the extensive resources devoted to enhancing model capabilities</li>
<li>The improvement in model performance necessitates the continuous development of new benchmarks</li>
<li>Without a systematic approach to evaluation, progress can be hindered by various headwinds</li>
</ul>
</section>
<section id="language-model-metrics" class="level2">
<h2 class="anchored" data-anchor-id="language-model-metrics">Language Model Metrics</h2>
<p>The chapter includes a technically detailed section on understanding language model metrics, which while math-heavy, provides fundamental insights into model capabilities:</p>
<ul>
<li>Entropy</li>
<li>Cross-entropy</li>
<li>Perplexity</li>
</ul>
<p>These metrics serve as underlying measures to understand what’s happening within the models and assess their power and conversational abilities. While this section spans 4-5 pages of technical content, it provides some useful foundational understanding of how we can measure a language model’s intrinsic capabilities.</p>
</section>
<section id="downstream-task-performance-measurement" class="level2">
<h2 class="anchored" data-anchor-id="downstream-task-performance-measurement">Downstream Task Performance Measurement</h2>
<p>The chapter transitions from intrinsic metrics to evaluating actual capabilities, dividing evaluation into exact and subjective approaches.</p>
<section id="exact-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="exact-evaluation">Exact Evaluation</h3>
<p>There are two principal approaches to exact evaluation:</p>
<ol type="1">
<li><p><strong>Functional Correctness Assessment</strong></p>
<ul>
<li>Evaluates whether the LLM can successfully complete its assigned tasks</li>
<li>Focuses on practical capability rather than theoretical metrics</li>
<li>Example: In coding tasks, checking if generated code passes all unit tests</li>
<li>Provides clear, objective measures of success</li>
</ul></li>
<li><p><strong>Similarity Measurements Against Reference Data</strong> Four distinct methods are identified:</p>
<ol type="a">
<li><strong>Human Evaluator Judgment</strong>
<ul>
<li>Requires manual comparison of texts by human evaluators</li>
<li>Highly accurate but time and resource-intensive</li>
<li>Limited scalability due to human involvement</li>
<li>Often considered the gold standard despite limitations</li>
</ul></li>
<li><strong>Exact Match Checking</strong>
<ul>
<li>Compares generated response against reference responses for exact matches</li>
<li>Most effective with shorter, specific outputs</li>
<li>Less useful for verbose or creative outputs</li>
<li>Provides binary yes/no results</li>
</ul></li>
<li><strong>Lexical Similarity</strong>
<ul>
<li>Employs established metrics like BLEU, ROUGE, and METEOR</li>
<li>Focuses on word overlap and structural similarities</li>
<li>Known to be somewhat crude in their assessment</li>
<li>Widely used despite limitations due to ease of implementation</li>
</ul></li>
<li><strong>Semantic Similarity</strong>
<ul>
<li>Utilizes embeddings for comparing textual meaning</li>
<li>Less sensitive to specific word choices than lexical approaches</li>
<li>Quality depends entirely on the underlying embeddings algorithm</li>
<li>May require significant computational resources</li>
<li>Generally provides more nuanced comparison than lexical methods</li>
</ul></li>
</ol></li>
</ol>
<p>The chapter includes a brief but relevant sidebar on embeddings and their significance in evaluation, though this digression seemed a bit out of place in the overall flow.</p>
</section>
</section>
<section id="ai-as-judge" class="level2">
<h2 class="anchored" data-anchor-id="ai-as-judge">AI as Judge</h2>
<p>This section explores the increasingly popular approach of using AI systems to evaluate other AI systems.</p>
<section id="benefits" class="level3">
<h3 class="anchored" data-anchor-id="benefits">Benefits</h3>
<ul>
<li>Significantly faster than human evaluation processes</li>
<li>Generally more cost-effective than human evaluation at scale</li>
<li>Studies have shown strong correlation with human evaluations in many cases</li>
<li>AI judges can provide detailed explanations for their decisions</li>
<li>Offers greater flexibility in evaluation approaches</li>
<li>Enables systematic and consistent evaluation at scale</li>
</ul>
</section>
<section id="three-main-approaches" class="level3">
<h3 class="anchored" data-anchor-id="three-main-approaches">Three Main Approaches</h3>
<ol type="1">
<li><strong>Individual Response Evaluation</strong>
<ul>
<li>Assesses response quality based solely on the original question</li>
<li>Often implements numerical scoring systems (e.g., 1-5 scale)</li>
<li>Evaluates responses in isolation without comparison</li>
</ul></li>
<li><strong>Reference Response Comparison</strong>
<ul>
<li>Evaluates generated response against established reference responses</li>
<li>Usually produces binary (true/false) outcomes</li>
<li>Helps ensure responses meet specific criteria</li>
</ul></li>
<li><strong>Generated Response Comparison</strong>
<ul>
<li>Compares two generated responses to determine relative quality</li>
<li>Predicts likely user preferences between options</li>
<li>Particularly useful for:
<ul>
<li>Post-training alignment</li>
<li>Test-time compute optimization</li>
<li>Model ranking through comparative evaluation</li>
<li>Generating preference data</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="implementation-considerations" class="level3">
<h3 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h3>
<p><img src="https://mlops.systems/posts/images/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3/ch3.png" class="img-fluid"></p>
<ul>
<li>Table 3-3 (page 139) provides an overview of different AI judge criteria used by various AI tools</li>
<li>Notable lack of standardization across different platforms and approaches (see above)</li>
<li>Various scoring systems available, each with their own trade-offs</li>
<li>Adding examples to prompts can improve accuracy but increases token count and costs</li>
<li>Careful balance needed between evaluation quality and resource consumption</li>
</ul>
</section>
<section id="limitations-and-challenges" class="level3">
<h3 class="anchored" data-anchor-id="limitations-and-challenges">Limitations and Challenges</h3>
<ul>
<li>AI judges can show inconsistency in their judgments</li>
<li>Costs can escalate quickly, especially when using stronger models or including more context</li>
<li>Evaluation criteria often remain ambiguous and difficult to standardize</li>
<li>Several inherent biases identified:
<ul>
<li>Self-bias: Models tend to favor responses generated by themselves</li>
<li>Verbosity bias: Tendency to favor longer, more detailed answers</li>
<li>Other biases common to AI applications in general</li>
</ul></li>
</ul>
</section>
</section>
<section id="specialized-judges" class="level2">
<h2 class="anchored" data-anchor-id="specialized-judges">Specialized Judges</h2>
<p>This section presents an innovative challenge to the conventional wisdom of using the strongest available model as a judge. The author introduces a compelling alternative approach:</p>
<ul>
<li>Small, specialized judges can be as effective as larger models for specific evaluation tasks</li>
<li>More cost-effective and efficient than using large language models</li>
<li>Can be trained for highly specific evaluation criteria</li>
<li>Demonstrates comparable performance to larger models like GPT-4 in specific domains</li>
</ul>
<p>Three types of specialized judges are identified: 1. Reward models (evaluating prompt-response pairs) 2. Reference-based judges 3. Preference models</p>
<p>This represents a novel approach that could significantly impact evaluation methodology in the field.</p>
</section>
<section id="comparative-evaluation-for-model-ranking" class="level2">
<h2 class="anchored" data-anchor-id="comparative-evaluation-for-model-ranking">Comparative Evaluation for Model Ranking</h2>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology</h3>
<ul>
<li>Focuses on binary choices between two samples</li>
<li>Simpler for both humans and AI to make comparative judgments</li>
<li>Used in major leaderboards like LMSIS</li>
<li>Requires evaluation of multiple combinations to establish rankings</li>
<li>Various algorithms available for efficient comparison</li>
</ul>
</section>
<section id="advantages" class="level3">
<h3 class="anchored" data-anchor-id="advantages">Advantages</h3>
<ul>
<li>More intuitive evaluation process</li>
<li>Often more reliable than absolute scoring</li>
<li>Reduces cognitive load on evaluators</li>
<li>Provides clear preference data</li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li>Highly data-intensive nature affects scalability</li>
<li>Lacks standardization across implementations</li>
<li>Difficulty in converting comparative measures to absolute metrics</li>
<li>Quality control remains a significant concern</li>
<li>Number of required comparisons can grow rapidly with model count</li>
</ul>
</section>
</section>
<section id="key-takeaways-and-future-implications" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways-and-future-implications">Key Takeaways and Future Implications</h2>
<ol type="1">
<li>The emergence of smaller, specialized judge models represents a significant shift from the traditional approach of using the largest available models</li>
<li>Comparative evaluation offers promising approaches but requires careful consideration of scalability and implementation</li>
<li>The field continues to evolve rapidly, requiring flexible and adaptable evaluation strategies</li>
<li>Sets up crucial discussion for system-level evaluation in Chapter 4</li>
<li>Highlights the ongoing tension between evaluation quality and resource efficiency</li>
</ol>
<p>The chapter effectively establishes the foundational understanding necessary for the more practical, system-focused evaluation discussions to follow in Chapter 4.</p>


</section>

 ]]></description>
  <category>books-i-read</category>
  <category>llm</category>
  <category>llms</category>
  <category>evaluation</category>
  <guid>https://mlops.systems/posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html</guid>
  <pubDate>Mon, 20 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3/ch3.png" medium="image" type="image/png" height="66" width="144"/>
</item>
<item>
  <title>Notes on ‘AI Engineering’ (Chip Huyen) chapter 1</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-19-notes-on-ai-engineering-chapter-1.html</link>
  <description><![CDATA[ 




<p>Had the first of <a href="https://www.meetup.com/delft-fast-ai-study-group/">a series of meet-ups</a> I’m organising in which we discuss Chip Huyen’s new book. My notes from reading the chapter follow this, and then I’ll try to summarise what we discussed in the group.</p>
<p>At a high-level, I <em>really</em> enjoyed the final part of the chapter where she got into how she was thinking about the practice of ‘AI Engineering’ and how it differs from ML engineering. Also the use of the term ‘model adaptation’ was an interesting way of encompassing all the different things that engineers are doing to get the LLM to better follow their instructions.</p>
<section id="chapter-1-notes" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1-notes">Chapter 1 Notes</h2>
<p>The chapter begins by establishing AI Engineering as the preferred term over alternatives like GenAI Ops or LLM Ops. This preference stems from a fundamental shift in the field, where application development has become increasingly central to working with AI models. The “ops” suffix inadequately captures the breadth and nature of work involved in modern AI applications.</p>
<section id="foundation-models-and-language-models" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models-and-language-models">Foundation Models and Language Models</h3>
<p>The text provides important technical context about different types of language models. A notable comparison shows that while Mistral 7B has a vocabulary of 32,000 tokens, GPT-4 possesses a much larger vocabulary of 100,256 tokens, highlighting the significant variation in model capabilities and design choices.</p>
<p>Two primary categories of language models are discussed:</p>
<ol type="1">
<li>Masked Language Models (like BERT and modern BERT variants)</li>
<li>Autoregressive Language Models (like those used in ChatGPT)</li>
</ol>
<p>The term “foundation model” carries dual significance, referring both to these models’ fundamental importance and their adaptability for various applications. This terminology also marks an important transition from task-specific models to general-purpose ones, especially relevant in the era of multimodal capabilities.</p>
</section>
</section>
<section id="ai-engineering-vs-traditional-approaches" class="level2">
<h2 class="anchored" data-anchor-id="ai-engineering-vs-traditional-approaches">AI Engineering vs Traditional Approaches</h2>
<p>AI Engineering differs substantially from ML Engineering, warranting its distinct terminology. The key distinction lies in its focus on adapting and evaluating models rather than building them from scratch. Model adaptation techniques fall into two main categories:</p>
<ol type="1">
<li>Prompt-based techniques (prompt engineering) - These methods adapt models without updating weights</li>
<li>Fine-tuning techniques - These approaches require weight updates</li>
</ol>
<p>The shift from ML Engineering to AI Engineering brings new challenges, particularly in handling open-ended outputs. While this flexibility enables a broader range of applications, it also introduces significant complexity in evaluation and implementation of guardrails.</p>
</section>
<section id="the-ai-engineering-stack" class="level2">
<h2 class="anchored" data-anchor-id="the-ai-engineering-stack">The AI Engineering Stack</h2>
<p>The framework consists of three distinct layers:</p>
<section id="application-development-layer" class="level3">
<h3 class="anchored" data-anchor-id="application-development-layer">1. Application Development Layer</h3>
<ul>
<li>Focuses on prompt crafting and context provision</li>
<li>Requires rigorous evaluation methods</li>
<li>Emphasizes interface design and user experience</li>
<li>Primary responsibilities include evaluation, prompt engineering, and AI interface development</li>
</ul>
</section>
<section id="model-development-layer" class="level3">
<h3 class="anchored" data-anchor-id="model-development-layer">2. Model Development Layer</h3>
<ul>
<li>Provides tooling for model development</li>
<li>Includes frameworks for training, functioning, and inference optimisation</li>
<li>Requires systematic evaluation approaches</li>
</ul>
</section>
<section id="infrastructure-layer" class="level3">
<h3 class="anchored" data-anchor-id="infrastructure-layer">3. Infrastructure Layer</h3>
<ul>
<li>Handles model serving</li>
<li>Manages underlying technical requirements</li>
</ul>
</section>
</section>
<section id="planning-ai-applications" class="level2">
<h2 class="anchored" data-anchor-id="planning-ai-applications">Planning AI Applications</h2>
<p>The chapter outlines a modern approach to AI application development that differs significantly from traditional ML projects. Rather than beginning with data collection and model training, AI engineering often starts with product development, leveraging existing models. This approach allows teams to validate product concepts before investing heavily in data and model development.</p>
<p>Key planning considerations include:</p>
<ul>
<li>Setting appropriate expectations</li>
<li>Determining user exposure levels</li>
<li>Deciding between internal and external deployment</li>
<li>Understanding maintenance requirements</li>
</ul>
<p>A notable insight is the “80/20” development pattern: while reaching 80% functionality can be relatively quick, achieving the final 20% often requires equal or greater effort than the initial development phase.</p>
</section>
<section id="evaluation-and-implementation-challenges" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-and-implementation-challenges">Evaluation and Implementation Challenges</h2>
<p>The chapter emphasises that working with AI models presents unique evaluation challenges compared to traditional ML systems. This complexity stems from:</p>
<ul>
<li>The open-ended nature of outputs</li>
<li>Difficulty in implementing strict guardrails</li>
<li>Challenges in type enforcement</li>
<li>The need for comprehensive evaluation strategies</li>
</ul>
</section>
<section id="data-and-model-adaptation" class="level2">
<h2 class="anchored" data-anchor-id="data-and-model-adaptation">Data and Model Adaptation</h2>
<p>The text discusses how data set engineering and inference optimisation, while still relevant, take on different forms in AI engineering compared to traditional ML engineering. The focus shifts from raw data collection and processing to effective model adaptation and deployment strategies.</p>
</section>
<section id="modern-development-paradigm" class="level2">
<h2 class="anchored" data-anchor-id="modern-development-paradigm">Modern Development Paradigm</h2>
<p>A significant paradigm shift is highlighted in the development approach: unlike traditional ML engineering, which typically begins with data collection and model training, AI engineering enables a product-first approach. This allows teams to validate concepts using existing models before committing to extensive data collection or model development efforts.</p>
</section>
<section id="discussion-summary" class="level2">
<h2 class="anchored" data-anchor-id="discussion-summary">Discussion summary</h2>
<p>The conversation started with a bit on how AI Engineering represents an interesting shift in the software engineering landscape, potentially opening new career paths for traditional software engineers. While developers may not need deep mathematical knowledge of derivatives and linear algebra upfront, there’s a growing recognition that understanding how AI systems behave - their constraints and opportunities - is becoming increasingly valuable.</p>
<p>A key tension emerged in the discussion around enterprise adoption. While there’s significant enthusiasm around AI applications, particularly on social media where developers showcase apps with substantial user bases, enterprise companies often maintain their traditional team structures. This creates an interesting dynamic where companies might maintain their existing ML engineering teams while simultaneously forming new “tiger teams” focused on generative AI initiatives, leading to organisational friction.</p>
<p>The group discussed how while it’s now possible for software engineers to quickly build AI applications by calling APIs, they often hit limitations that require deeper understanding. This raises questions about whether the “shallow” approach of purely application-level development is sustainable, or whether engineers will inevitably need to develop deeper technical knowledge around model behaviour, evaluation, and fine-tuning.</p>
<p>A particularly notable challenge discussed was handling the non-deterministic nature of AI systems. Traditional software engineering practices, like unit testing, don’t translate cleanly to systems where outputs can vary even with temperature set to zero. This highlights how AI Engineering requires new patterns and practices beyond traditional software engineering approaches.</p>
<p>The discussion also touched on evaluation techniques, including the use of log probabilities to understand model confidence and improve prompts. This represents an emerging area where traditional ML evaluation meets new challenges in assessing large language model outputs.</p>


</section>

 ]]></description>
  <category>books-i-read</category>
  <category>llm</category>
  <category>llms</category>
  <category>finetuning</category>
  <category>prompt-engineering</category>
  <guid>https://mlops.systems/posts/2025-01-19-notes-on-ai-engineering-chapter-1.html</guid>
  <pubDate>Sat, 18 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/aieng1-small.png" medium="image" type="image/png" height="101" width="144"/>
</item>
<item>
  <title>Final notes on ‘Prompt Engineering for LLMs’</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html</link>
  <description><![CDATA[ 




<p>Here are the final notes from ‘<a href="https://app.thestorygraph.com/books/8535f61d-1dcd-4610-9cd9-6bcaf774f392">Prompt Engineering for LLMs</a>’, a book I’ve been reading over the past few days (and enjoying!).</p>
<section id="chapter-10-evaluating-llm-applications" class="level2">
<h2 class="anchored" data-anchor-id="chapter-10-evaluating-llm-applications">Chapter 10: Evaluating LLM Applications</h2>
<p>The chapter begins with an interesting anecdote about GitHub Copilot - the first code written in their repository was the evaluation harness, highlighting the importance of testing in LLM applications. The authors, who worked on the project from its inception, emphasise this as a best practice.</p>
<section id="evaluation-framework" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-framework">Evaluation Framework</h3>
<p>When evaluating LLM applications, three main aspects can be assessed:</p>
<ul>
<li>The model itself - its capabilities and limitations</li>
<li>Individual interactions with the model (prompts and responses)</li>
<li>The integration of multiple interactions within the broader application</li>
</ul>
<p>As a general rule of thumb, you should always track and record:</p>
<ul>
<li>Latency</li>
<li>Token consumption statistics</li>
<li>Overall system approach metrics</li>
</ul>
</section>
<section id="offline-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="offline-evaluation">Offline Evaluation</h3>
<section id="example-suites" class="level4">
<h4 class="anchored" data-anchor-id="example-suites">Example Suites</h4>
<p>The foundation of offline evaluation is creating example suites - collections of 10-20 (minimum) input-output pairs that serve as test cases. These should be accompanied by scripts that apply your application’s logic to each example and compare the results.</p>
<p>Example sources come from three main areas:</p>
<ul>
<li>Existing examples from your project</li>
<li>Real-time user data collection</li>
<li>Synthetic creation</li>
</ul>
<p>When using synthetic data, it’s crucial to use different LLMs for creation versus application/judging to avoid potential biases.</p>
</section>
<section id="evaluation-approaches" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-approaches">Evaluation Approaches</h4>
<ol type="1">
<li><strong>Gold Standard Matching</strong></li>
</ol>
<ul>
<li>Can be exact or partial matching</li>
<li>Particularly effective for binary decisions or multi-label classification</li>
<li>Can leverage “logical frogs” tricks from Chapter 7 to assess model confidence</li>
<li>Free-form text requires more creative evaluation approaches</li>
<li>Tool-use scenarios may be easier to evaluate, especially in agent-driven applications</li>
</ul>
<ol start="2" type="1">
<li><strong>Functional Testing</strong></li>
</ol>
<ul>
<li>A step up from unit tests but not full end-to-end testing</li>
<li>Focuses on testing specific system components</li>
</ul>
<ol start="3" type="1">
<li><strong>LLM as Judge</strong></li>
</ol>
<ul>
<li>Currently trendy but requires careful implementation</li>
<li>Should include human verification loop, preferably multiple humans</li>
<li>Key insight: Always frame the evaluation as if the LLM is grading someone else’s work, never its own</li>
<li>Recommendations for quantitative measures:
<ul>
<li>Use gradient and multi-aspect coverage (MA)</li>
<li>Implement 1-5 scales with specific criteria</li>
<li>Place all instructions and criteria before the content to be evaluated</li>
<li>Break down “Goldilocks” questions (was it just right?) into separate questions about whether it was enough and whether it was too much</li>
</ul></li>
</ul>
</section>
</section>
<section id="online-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="online-evaluation">Online Evaluation</h3>
<p>The chapter transitions to discussing why we need online testing despite having offline evaluation capabilities. While offline testing is safer and more scalable, real human interactions are unpredictable and require live testing.</p>
<p>Key points about online evaluation:</p>
<ul>
<li>AB testing is the standard approach</li>
<li>Existing solutions include Optimizely, VWO Consulting, and AB Tasty</li>
<li>Applications need to support running in two modes (A and B)</li>
<li>Consider rollout timing and users on older versions</li>
</ul>
<p>Five main metrics for online evaluation (from most to least straightforward):</p>
<ol type="1">
<li>Direct feedback (user responses to suggestions)</li>
<li>Functional correctness</li>
<li>User acceptance (following suggestions)</li>
<li>Achieved impact (user benefit)</li>
<li>Incidental metrics (surrounding measurements)</li>
</ol>
<p>Direct feedback data is particularly valuable as it can later be used for model fine-tuning. It’s recommended to track more incidental metrics rather than fewer, both for quality indicators and investigating unexpected changes.</p>
</section>
</section>
<section id="chapter-11-looking-ahead" class="level2">
<h2 class="anchored" data-anchor-id="chapter-11-looking-ahead">Chapter 11: Looking Ahead</h2>
<p>The final chapter covers several forward-looking topics:</p>
<ul>
<li>Multimodality in LLMs</li>
<li>User experience and interface considerations</li>
<li>Published artifacts from Anthropic</li>
<li>Risks and rewards of custom interfaces</li>
<li>Trends in model intelligence, cost, and speed</li>
</ul>
<section id="book-level-conclusions" class="level3">
<h3 class="anchored" data-anchor-id="book-level-conclusions">Book-Level Conclusions</h3>
<p>Two main lessons emerge from the book:</p>
<ol type="1">
<li><strong>LLMs as Text Completion Engines</strong>
<ul>
<li>They fundamentally mimic training data</li>
<li>Success comes from aligning prompts with training data patterns</li>
<li>Particularly relevant for completion models</li>
</ul></li>
<li><strong>Empathy with LLMs</strong></li>
</ol>
<ul>
<li>Think of them as mechanical friends with internet knowledge</li>
<li>Five key insights:
<ul>
<li>LLMs are easily distracted; keep prompts focused</li>
<li>If humans can’t understand the prompt, LLMs will struggle</li>
<li>Provide clear instructions and examples</li>
<li>Include all necessary information (LLMs aren’t psychic)</li>
<li>Give space for “thinking out loud” (chain of thought)</li>
</ul></li>
</ul>
</section>
</section>
<section id="personal-reflections" class="level2">
<h2 class="anchored" data-anchor-id="personal-reflections">Personal Reflections</h2>
<p>The book, while not revolutionary, provides valuable insights and is a recommended read at 250 pages. It can be completed in about 10-11 days. The heavy focus on completion models versus chat models is interesting, likely due to the authors’ experience with GitHub Copilot. While some points were novel, none were completely mind-blowing. The book’s emphasis on completion models versus chat models is both intriguing and occasionally confusing, though this perspective is understandable given the authors’ background with GitHub Copilot.</p>


</section>

 ]]></description>
  <category>llm</category>
  <category>prompt-engineering</category>
  <category>books-i-read</category>
  <category>evaluation</category>
  <guid>https://mlops.systems/posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html</guid>
  <pubDate>Thu, 16 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/chapter-10-prompt-eng.png" medium="image" type="image/png" height="98" width="144"/>
</item>
<item>
  <title>Assembling the Prompt: Notes on ‘Prompt Engineering for LLMs’ ch 6</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-13-assembling-the-prompt-notes-on-prompt-engineering-for-llms-ch-6.html</link>
  <description><![CDATA[ 




<p>Chapter 6 of “Prompt Engineering for LLMs” is devoted to how to structure the prompt and compose its various elements. We first learn about the different kinds of ‘documents’ that we can mimic with our prompts, then think about how to pick which pieces of context to include, and then think through how we might compose all of this together.</p>
<p><img src="https://mlops.systems/posts/images/2025-01-13-assembling-the-prompt-ch-6/well-constructed-prompt.png" class="img-fluid"></p>
<p>There’s a great figure to give you an idea of ‘the anatomy of a well-constructed prompt’ early on. The introduction is where you introduce the task, then you have the ‘valley of meh’ (which the LLM can struggle to recall or obey) and finally you have the refocusing and restatement of the task.</p>
<p>There are two key tips at this point:</p>
<ul>
<li>the closer a piece of information is to the end of the prompt, the more impact it has on the model</li>
<li>the model often struggles with the information stuffed in the middle of the prompt</li>
</ul>
<p>So craft your prompts accordingly!</p>
<p>A prompt plus the resulting completion is defined as a ‘document’ in this book, and there are various templates that you can follow: an ‘advice conversation’, an ‘analytic report’ (often formatted with Markdown headers), and a ‘structured document’.</p>
<p>We learn that analytic report-type documents seem to offer a lighter ‘cognitive load’ for an LLM since it doesn’t have to handle the intricacies of social interaction that it would in the case of an advice conversation. 🤔</p>
<p>Two other tips or possible things to include in the analytic report-style document:</p>
<ul>
<li>a table of contents at the beginning to set the scene</li>
<li>a scratchpad or notebook section for the model to ‘think’ in</li>
</ul>
<p>I haven’t had much use of either of these myself but I can see why they’d be powerful.</p>
<p>Structured documents can be really powerful, especially when the model has been trained to expect certain kinds of structure (be it JSON or XML or YAML etc). Also TIL that apparently OpenAI’s models are very strong when dealing with JSON as inputs.</p>
<p>The context to be inserted into the prompt (usually dynamically depending on use case or needs) can be large or small depending on what is available in terms of context window or latency requirements. There are different strategies to how to select what goes in.</p>
<p>I was curious about the idea of what they call ‘elastic snippets’, i.e.&nbsp;dynamic decisions that get taken as to what makes it way into the prompt depending on how much space is available etc.</p>
<p>And even then you have to decide about the:</p>
<ul>
<li>position (which order do all the elements appear in the prompt)</li>
<li>importance (how much will dropping this element from the prompt effect the response)</li>
<li>dependency (if you include one element, can you drop another and vice versa…)</li>
</ul>
<p>In the end, you have a kind of optimisation problem: given a theoretical unlimited potential prompt length, how to combine all the elements together to get the most value given the space limitations that the LLM dictates.</p>
<p><img src="https://mlops.systems/posts/images/2025-01-13-assembling-the-prompt-ch-6/additive-greedy.png" class="img-fluid"></p>
<p>And then what strategy do you use to get rid of elements that your prompt budget cannot afford; we learn about the ‘additive greedy approach’ and the ‘subtractive greedy approach’, all the while bearing in mind that these are all just basic prototypes to play around with.</p>
<p><img src="https://mlops.systems/posts/images/2025-01-13-assembling-the-prompt-ch-6/subtr-greedy.png" class="img-fluid"></p>
<p>The next chapter is all about the completion and how to make sure we receive meaningful and accurate responses from our LLM!</p>



 ]]></description>
  <category>llm</category>
  <category>prompt-engineering</category>
  <category>books-i-read</category>
  <guid>https://mlops.systems/posts/2025-01-13-assembling-the-prompt-notes-on-prompt-engineering-for-llms-ch-6.html</guid>
  <pubDate>Sun, 12 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-01-13-assembling-the-prompt-ch-6/well-constructed-prompt.png" medium="image" type="image/png" height="119" width="144"/>
</item>
<item>
  <title>Prompt Content: Notes on ‘Prompt Engineering for LLMs’ ch 5</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-12-prompt-content-notes-on-chapter-6-prompt-engineering-for-llms.html</link>
  <description><![CDATA[ 




<p>Chapter 5 of ‘<a href="https://www.amazon.com/Prompt-Engineering-LLMs-Model-Based-Applications/dp/1098156153?tag=soumet-20">Prompt Engineering for LLMs</a>’ tackles the kinds of things you might want to include in your prompt. (Chapter 6 thinks through the order, structuring and weighting of these different pieces of content, so this is purely about the ‘what’ and not the ‘how’).</p>
<p>We split the kinds of content up into static and dynamic content. For static you can think of fixed instructions (‘always respond politely, and in the first person’) whereas dynamic content is assembled on the fly and is (potentially) custom to each user or query. The classic example of dynamic content insertion is your bog standard RAG app.</p>
<p>This was a very tactical chapter and I think the rest of the book will keep this up. Most of what gets discussed usually has a little bitesize example to illustrate which I found helpful. This illustration was used to illustrate the differences between static and dynamic context inclusion.</p>
<p><img src="https://mlops.systems/posts/images/2025-01-12-chapter-5/ch6-eg.png" class="img-fluid"></p>
<p>For static content, the book explores two types: lists of instructions and few-shot prompting. For instructions, we get some useful rules of thumb:</p>
<ul>
<li>ask for positives vs negatives and does instead of don’ts.</li>
<li>give reasons for the things you’re asking</li>
<li>avoid absolutes</li>
</ul>
<p>(and use the system message for these kinds of instruction as most LLMs have been trained to follow them)</p>
<p>For few-shot prompting, we dive into all the tradeoffs around how many does ‘few’ mean, what order they should be included, how to get a representative sample and so on. We also consider the tradeoffs and biases that can be subtly introduced with few-shot prompts: issues around scaling the # of examples and accidental picking up of spurious patterns.</p>
<p>One thing I’ve often done is to have a ‘best examples first, then the edge cases’ pattern for how I include these examples but we learn how this can bias the LLM to be unduly pessimistic and cautious. (All this stuff is really super tactical / in the weeds, but it’s what I was hoping for…)</p>
<p>TL;DR: use static prompting where it’s appropriate. use few-shot prompts as well, but be SUPER careful about how these get used and make sure to run evals to see if there are not better orders and amounts of those few-shot examples.</p>
<p>For dynamic context, the chapter first thinks through how we might think through which dynamic examples or context to include. This will be influenced by latency requirements, cost considerations + how much can be prepared ahead of time as well as thinking about how to decide which parts of a theoretically infinite amount of extra context you could provide.</p>
<p>The bulk of the dynamic context discussion focuses around RAG, and we even get a tiny POC RAG implementation using FAISS as vector storage. We read about the tradeoffs of choosing lexical retrieval (e.g.&nbsp;ElasticSearch + some naive algorithm like Jaccard similarity) vs neural search (e.g.&nbsp;embeddings-driven retrieval with cosine similarity).</p>
<p>Finally, the chapter closes with a discussion of summarization. What if, e.g., you have so much context that you want to include but you hit the limit? Then you might want to compress it somehow. We read about hierarchical summarisation (which sometimes has to be recursive if there’s too much context).</p>
<p>We also get a nice warning about the ‘rumour problem’ which I’ve personally experienced when you summarise a summary (and maybe you summarise a summary of a summary) and things get lost or misrepresented along the way. But for just one level of summarisation that shouldn’t be too big an issue with modern LLMs.</p>
<p>We also get into general vs specific summaries. In other words, when you ask the LLM to summarise some text, do you do it with a certain task in mind or do you get a general summary? A general summary is more flexible and can be used in many places, but while specific summaries might give better results for a specific task, you might end up having to rerun your summarization for different tasks.</p>
<p>Next up: chapter six which tackles exactly how you put all these pieces of context and content together in your prompt.</p>



 ]]></description>
  <category>llm</category>
  <category>prompt-engineering</category>
  <category>books-i-read</category>
  <category>RAG</category>
  <guid>https://mlops.systems/posts/2025-01-12-prompt-content-notes-on-chapter-6-prompt-engineering-for-llms.html</guid>
  <pubDate>Sat, 11 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-01-12-chapter-5/cover.png" medium="image" type="image/png" height="114" width="144"/>
</item>
<item>
  <title>Starting to read Prompt Engineering for LLMs</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html</link>
  <description><![CDATA[ 




<p>I’m posting some of my summary notes while reading through John Berryman and Albert Ziegler’s “Prompt Engineering for LLMs”. What follows are my notes from the first two chapters. It was a bit too long for a post to LinkedIn so I’m posting my notes in full here.</p>
<section id="chapter-1-introduction-to-prompt-engineering" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1-introduction-to-prompt-engineering">Chapter 1: Introduction to Prompt Engineering</h2>
<p>The opening chapter frames prompt engineering as a comprehensive discipline that extends far beyond just crafting individual prompts. It positions prompt engineering as an integral part of the entire lifecycle of LLM-based applications.</p>
<section id="key-points" class="level3">
<h3 class="anchored" data-anchor-id="key-points">Key Points</h3>
<ul>
<li>The field of language modeling has seen exponential growth, as evidenced by the GPT series progression from 2018 to 2022:
<ul>
<li>GPT-1 (2018): 117M parameters</li>
<li>GPT-2 (2019): 1.5B parameters</li>
<li>GPT-3 (2020): 175B parameters</li>
<li>Subsequent models showing continued scaling</li>
</ul></li>
<li>Prompt engineering encompasses:
<ul>
<li>The structural design of prompts themselves</li>
<li>Strategic thinking about prompt implementation throughout the application lifecycle</li>
<li>Integration of prompts into larger systems and workflows</li>
</ul></li>
<li>Historical Context:
<ul>
<li>The chapter provides background on language modeling evolution</li>
<li>Places modern LLMs in the broader context of NLP development</li>
</ul></li>
</ul>
<p>This introductory framework suggests that effective prompt engineering requires both technical skill in prompt construction and strategic understanding of how prompts function within larger systems and applications.</p>
</section>
</section>
<section id="chapter-2-understanding-llms" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2-understanding-llms">Chapter 2: Understanding LLMs</h2>
<p>Chapter two tries to peel back the layers of how LLMs produce their output. If you can understand how they work (at least a bit more than ‘it’s magic’), you can better guide them to produce outputs that are valuable for you.</p>
<p>A very hard chapter to write, I imagine. It is almost certainly a bit too technical for someone ‘non-technical’, but a more experienced user might find some of the analogies too simplistic. I thought the balance was well handled but I probably wouldn’t recommend this to just anyone..</p>
<p>Some key insights: expect LLMs to respond in a similar way to the training data that went into creating them. (Unfortunately, many model providers are pretty tight-lipped as to the specific composition of that training data, though you can make some guesses…)</p>
<blockquote class="blockquote">
<p>“The better you know the training data, the better the intuition you can form about the likely output of an LLM trained on that training data.”</p>
</blockquote>
<p>We then get into a section on tokenization and what that means for how LLMs ‘see’ the world and why this results in certain weaknesses. Most importantly, just remember that LLMs don’t process and interact with text in the same way that humans do. Easy to forget when you’re interacting through a chat interface, but important nonetheless.</p>
<p><img src="https://mlops.systems/posts/images/2025-01-09-understanding-llms-with-prompt-engineering-for-llms/first.png" class="img-fluid"></p>
<p>I liked this example about capitalization and how tokenization made it hard for the earlier generations of models to do something as ‘simple’ as turning words into upper-case versions.</p>
<p>Even though this isn’t a problem with more recent models, it reminds you to be cognisant of how much extra work you’re having your model do. The more you can remove extra work, the better responses you’ll get. If you try to have your model do too many things at the same time, you’ll have poorer results.</p>
<p>The section on LLMs as auto-regressive models was excellent, though, again, probably not the easiest read for a non-technical reader. Key: LLMs move forward through their text as they ‘read’ the contents. They cannot backtrack, they cannot take things back that they write. They just have to keep moving forward.</p>
<p>This can lead to repetitions, getting lost in certain patterns and behaviours. One solution to this: filtering out after the response is given. Another option: playing with temperature and randomness.</p>
<p>I loved this section on temperatures and how to think about which to choose. Very practical, even amidst a chapter targeted at helping you understand why LLMs behave the way they do.</p>
<p><img src="https://mlops.systems/posts/images/2025-01-09-understanding-llms-with-prompt-engineering-for-llms/second.png" class="img-fluid"></p>
<p>Also a useful insight that errors often compound when it comes to temperatures greater than 1. I hadn’t realised that before.</p>
<p>After tokens and temperature we move on to transformers! I found the explanation worked, though the really technical again are bound to be disappointed and the non-technical might find it a bit too hand-wavy. YMMV. Overall enough information was given to understand the key insight around attention:</p>
<blockquote class="blockquote">
<p>“Information flows from left to right. Information flows from bottom to top.”</p>
</blockquote>
<p>After we understand this, we can also understand how processing (‘reading’) text happens much faster than the output: it’s ~ an order of magnitude slower to to output than it is to read the input, even with caching and parallelism in the computation.</p>
<p>So the order of the contents of the prompt matters <em>a lot</em>, as does the formulation and the extent to which you make the LLM work hard on the problem at hand or other extraneous tasks.</p>
<p>A nice illustrative summation:</p>
<blockquote class="blockquote">
<p>“Could a human expert who knows all the relevant general knowledge by heart complete the prompt in a single go without backtracking, editing or note-taking?” (if not, then you might find the LLM will struggle with the task or completion)</p>
</blockquote>
<p>So to sum up:</p>
<ul>
<li>LLMs are completion engines</li>
<li>LLMs mimic their training data</li>
<li>LLMs produce one token at a time and can’t backtrack</li>
<li>LLMs read through the text a single time, from beginning to end</li>
</ul>
<p>Simple-seeming insights, but ones with large consequences. Tomorrow we move beyond the static models and on to RLHF, the chat models and the differences that come with using the API.</p>


</section>

 ]]></description>
  <category>llm</category>
  <category>prompt-engineering</category>
  <category>books-i-read</category>
  <category>tokenisation</category>
  <guid>https://mlops.systems/posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html</guid>
  <pubDate>Wed, 08 Jan 2025 23:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/2025-01-09-understanding-llms-with-prompt-engineering-for-llms/first.png" medium="image" type="image/png" height="52" width="144"/>
</item>
<item>
  <title>All the things I learned while trending on Hacker News</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html</link>
  <description><![CDATA[ 




<p>My previous two blog posts — <a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">here</a> and <a href="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html">here</a> — were trending / on the front page of Hacker News, driving over 20,000 new visitors to this blog. Welcome! I learned a few new tricks (and some mistakes I’d made) during the ensuing discussion so I thought I’d share some of these here. Some of them might trigger some mini side-investigations into certain hypotheses, too, which is even more exciting. Let’s dive in.</p>
<section id="temperature-0" class="level2">
<h2 class="anchored" data-anchor-id="temperature-0">Temperature = 0</h2>
<p>Some commenters rightly pointed out that setting the temperature to 1 for some of the OpenAI inference meant that I was more likely to have less stable and less factually consistent responses. I also heard back from the OpenPipe team that there was maybe no hard and fast rule on this but that I should experiment around for my specific use case.</p>
<p>There was enough strongly-voiced opinions on this that I might see if I can rerun the evals using <code>0</code> as the temperature to see how much of a difference it makes.</p>
</section>
<section id="function-calling-vs-json-mode-vs-prompt-vs-some-schema-forcing-library" class="level2">
<h2 class="anchored" data-anchor-id="function-calling-vs-json-mode-vs-prompt-vs-some-schema-forcing-library">Function calling vs JSON mode vs prompt vs some schema-forcing library</h2>
<p>In <a href="https://mlops.systems/posts/2024-06-03-isafpr-evaluating-baseline.html">a previous baseline eval for OpenAI’s models</a> I used <code>instructor</code> to coerce the output into Pydantic objects. <a href="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html">This time round</a> I just used a strongly-worded request in the prompt to request a JSON response and turned on JSON mode (with the <code>response_format={"type": "json_object"}</code> passed into the <code>create</code> method). That was enough to ensure that <a href="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html#json-validity-test">every response I got back was valid JSON</a>.</p>
<p>I’ve since been reading about the performance differences between these different responses, and how certain models (like the OpenAI GPT class) do much better with function-calling than with just a prompt and/or JSON mode.</p>
<p>I’ll be blogging about the differences between these options (and how exactly they work) but I think there’s also enough potential here for me to try this as well in a separate round of reruns of the evals.</p>
<p>Specifically: what’s the difference in performance between the prompt that I used (which effectively stuffed the schema into the prompt) and using a more formalised function-calling approach? Given what I’ve read, I suspect function-calling will prove superior, but by how much?</p>
</section>
<section id="llama3-eos-and-pad-tokens" class="level2">
<h2 class="anchored" data-anchor-id="llama3-eos-and-pad-tokens">Llama3 EOS and PAD tokens</h2>
<p>I had some helpful comments suggesting there was maybe something untoward going on with these tokens during my Llama3 local finetune. I did set them in my <code>axolotl</code> config, but it’s well possible that something went wrong there. I’m planning to return to some local finetunes (since my credits on the one-click providers is not infinite and I want to make the local setup work) so I will dive into this soon. Llama3 performed really well so it seems there’s just some small bug here.</p>
</section>
<section id="the-one-click-finetuned-models-can-be-run-locally" class="level2">
<h2 class="anchored" data-anchor-id="the-one-click-finetuned-models-can-be-run-locally">The one-click finetuned models can be run locally</h2>
<p>I found out that it <strong>is</strong> possible to download the adapters from places like Predibase and OpenPipe and just set things up to run them locally, but it’s just buried a bit in the docs (or not documented at all.)</p>
<p>Part of the difficulty with documenting how users can do this is that (thanks to CUDA setup intricacies) there isn’t really an easy one-approach-fits-all option. Docker is maybe the closest to this, but at the moment you have to do some of the legwork yourself.</p>
</section>
<section id="others-have-had-similar-success-with-finetuned-models" class="level2">
<h2 class="anchored" data-anchor-id="others-have-had-similar-success-with-finetuned-models">Others have had similar success with finetuned models</h2>
<p>There were a few other links to successes that others had with finetuning models for structured data extraction posted in the HN thread. See <a href="https://jacobsgill.es/phdobtained">this doctoral dissertation</a>. Also <a href="https://www.nature.com/articles/s41467-024-45563-x">this article in Nature</a>. And of course the OG <a href="https://arxiv.org/abs/2405.00732">LoRA Land paper</a>.</p>
</section>
<section id="controversial-content-means-openai-tries-less-hard" class="level2">
<h2 class="anchored" data-anchor-id="controversial-content-means-openai-tries-less-hard">Controversial content means OpenAI tries less hard?</h2>
<p>One commenter suggested that the nature of the content might be the reason why OpenAI’s GPT performance wasn’t as good as the finetuned models. My experience of this is that it’s binary — either you get a real response or you get a canned ‘this is too sensitive a topic’ reply — but (s)he suggested that instead of getting rejected I might just get a degraded-in-quality response.</p>
<p>This would need some further testing to confirm or deny. A nice little experiment for someone.</p>
</section>
<section id="what-about-anthropics-claude-models" class="level2">
<h2 class="anchored" data-anchor-id="what-about-anthropics-claude-models">What about Anthropic’s Claude models?</h2>
<p>I should probably have done this as well, but I just hit up against the timebox I allocated for the evaluation work. I’ll try to do some experiments with Haiku and the new Sonnet 3.5 to see if a mixture of tool use (aka function calling) and stuffing the prompt with more examples might be able to get us to feature parity with the finetuned models. Watch this space.</p>
</section>
<section id="data-labelling-issues" class="level2">
<h2 class="anchored" data-anchor-id="data-labelling-issues">Data labelling issues</h2>
<p>One commenter found some inconsistencies in the data labelling around dates. I’ll admit to not really having a good answer around this, but also I didn’t dive into the issues raised too deeply. They showed some examples of where the ‘ground truth’ date assigned to a press release was wrong. There’s of course the possibility I may have made mistakes while doing the labelling, and there might be some cases where press releases were emailed out earlier than when they were published on the website, but that’s much harder to show. I’ll dig into this a bit at some point, though this is lower priority on the ‘next steps’ list.</p>
</section>
<section id="you-cant-predict-what-people-want-to-read" class="level2">
<h2 class="anchored" data-anchor-id="you-cant-predict-what-people-want-to-read">You can’t predict what people want to read!</h2>
<p>The title I chose was clearly designed to be a bit provocative and/or draw readers in, but it wasn’t hyperbolic. My evals did actually show my finetuned models ‘beating’ GPT-4o. That said, <a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">the blog before it</a>, setting up the evals I did, was written in a fairly general way and I was really surprised that people enjoyed reading that one so much. It just goes to show that you just need to keep showing up, writing and publishing and you don’t know what people will like. Most of the time I’m writing just for me anyway, so anything on top is just a bonus.</p>
<p>Alongside this is the understanding that the things that a Hacker News audience enjoys are not necessarily the same things that the wider world and readership enjoys. That’s worth bearing in mind.</p>
</section>
<section id="github-pages-hosting-holds-up" class="level2">
<h2 class="anchored" data-anchor-id="github-pages-hosting-holds-up">Github Pages hosting holds up!</h2>
<p>My blog is <a href="https://quarto.org/">a Quarto blog</a> hosted on <a href="https://pages.github.com/">Github Pages</a>. As such I don’t pay anything for this hosting. I was pleasantly surprised that Github Pages did well in scaling up in response to the traffic. There was no slowness or downtime on the site. Good to know that just because you’re using open-source software and free tools that you’re not penalised.</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>My next effort will be to dive into the deployment side of these finetuned LLMs along with some of the low-hanging fruit mentioned above.</p>


</section>

 ]]></description>
  <category>llms</category>
  <category>miniproject</category>
  <category>finetuning</category>
  <category>isafpr</category>
  <category>evaluation</category>
  <category>nlp</category>
  <guid>https://mlops.systems/posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html</guid>
  <pubDate>Sat, 06 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/isafpr-hackernews.png" medium="image" type="image/png" height="115" width="144"/>
</item>
<item>
  <title>My finetuned models beat OpenAI’s GPT-4</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html</link>
  <description><![CDATA[ 




<p><a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">My last post</a> outlined the kinds of evaluation I need and want to understand how well my finetuned LLM is performing in the task of structured data extraction from press releases. Let’s start with the core metric I’m interested in, accuracy, and then later we can dive into some of the other evaluation metrics as well.</p>
<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>The headline for this post could well have been: finetuned models beat OpenAI, but evals were a bit painful to implement. There’s a lot of hidden code here in this post and it was slow to run. This step was the first time during the work for the finetuning course where I felt the pain and tradeoffs around the choice to finetune. I can see that without a system of some kind to handle this, the complexity of maintaining it all will start to mount up. But more on that at the end!</p>
<p>This is a long post with lots of detail. I’ve tried to minimise the amount of code you see, but if you want to see how the charts or evals were done, expand the ‘code’ sections. If you’re interested in cutting straight to the aggregate results, click here to go to the end of this post. (To see the rest of the blog posts about this project, please click <a href="https://mlops.systems/index.html#category=isafpr">here</a>. Some context: I’m doing some finetuning as part of <a href="https://maven.com/parlance-labs/fine-tuning">the Hamel Husain / Dan Becker Finetuning course on Maven</a> using <a href="https://mlops.systems/posts/2024-03-24-publishing-afghanistan-dataset-huggingface.html">some data I collected and labeled</a> a few years back that makes for a cool little test of how good it works for structured data extraction.)</p>
</section>
<section id="loading-the-datasets" class="level2">
<h2 class="anchored" data-anchor-id="loading-the-datasets">Loading the datasets</h2>
<p>The data is all available on the Hugging Face Hub in a public repository, and for the purposes of these evaluations I want to use the <code>test</code> split of the dataset since none of our models have seen that data yet so it’s good for determining how well our model performs with new data.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb1-4"></span>
<span id="cb1-5">test_dataset <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases"</span>, split<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"test"</span>)</span>
<span id="cb1-6">test_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(test_dataset)</span></code></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">test_dataset</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>Dataset({
    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],
    num_rows: 724
})</code></pre>
</div>
</div>
<p>We’ll first add an extra column to our <code>DataFrame</code> and then make a prediction for each and every row in the dataset. We’ll store a copy of the prediction to the column so as to make sure we don’t have to do this compute-intensive step repeatedly.</p>
<p>But first we’ll assemple the data as Pydantic objects so as to handle validation and other quality of life features.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">from</span> enum <span class="im" style="color: #00769E;">import</span> Enum</span>
<span id="cb4-2"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Dict, Set, Annotated, Optional</span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> pydantic <span class="im" style="color: #00769E;">import</span> BaseModel, Field, validator, ValidationInfo</span>
<span id="cb4-4"><span class="im" style="color: #00769E;">from</span> datetime <span class="im" style="color: #00769E;">import</span> date</span>
<span id="cb4-5"></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="kw" style="color: #003B4F;">class</span> EventType(<span class="bu" style="color: null;">str</span>, Enum):</span>
<span id="cb4-8">    airstrike <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"airstrike"</span></span>
<span id="cb4-9">    detention <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"detention"</span></span>
<span id="cb4-10">    captureandkill <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"captureandkill"</span></span>
<span id="cb4-11">    insurgentskilled <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"insurgentskilled"</span></span>
<span id="cb4-12">    exchangeoffire <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"exchangeoffire"</span></span>
<span id="cb4-13">    civiliancasualty <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"civiliancasualty"</span></span>
<span id="cb4-14"></span>
<span id="cb4-15"></span>
<span id="cb4-16"><span class="kw" style="color: #003B4F;">class</span> Province(<span class="bu" style="color: null;">str</span>, Enum):</span>
<span id="cb4-17">    badakhshan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"badakhshan"</span></span>
<span id="cb4-18">    badghis <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"badghis"</span></span>
<span id="cb4-19">    baghlan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"baghlan"</span></span>
<span id="cb4-20">    balkh <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"balkh"</span></span>
<span id="cb4-21">    bamyan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"bamyan"</span></span>
<span id="cb4-22">    day_kundi <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"day_kundi"</span></span>
<span id="cb4-23">    farah <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"farah"</span></span>
<span id="cb4-24">    faryab <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"faryab"</span></span>
<span id="cb4-25">    ghazni <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"ghazni"</span></span>
<span id="cb4-26">    ghor <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"ghor"</span></span>
<span id="cb4-27">    helmand <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"helmand"</span></span>
<span id="cb4-28">    herat <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"herat"</span></span>
<span id="cb4-29">    jowzjan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"jowzjan"</span></span>
<span id="cb4-30">    kabul <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kabul"</span></span>
<span id="cb4-31">    kandahar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kandahar"</span></span>
<span id="cb4-32">    kapisa <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kapisa"</span></span>
<span id="cb4-33">    khost <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"khost"</span></span>
<span id="cb4-34">    kunar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kunar"</span></span>
<span id="cb4-35">    kunduz <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kunduz"</span></span>
<span id="cb4-36">    laghman <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"laghman"</span></span>
<span id="cb4-37">    logar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"logar"</span></span>
<span id="cb4-38">    nangarhar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"nangarhar"</span></span>
<span id="cb4-39">    nimroz <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"nimroz"</span></span>
<span id="cb4-40">    nuristan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"nuristan"</span></span>
<span id="cb4-41">    paktya <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"paktya"</span></span>
<span id="cb4-42">    paktika <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"paktika"</span></span>
<span id="cb4-43">    panjshir <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"panjshir"</span></span>
<span id="cb4-44">    parwan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"parwan"</span></span>
<span id="cb4-45">    samangan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"samangan"</span></span>
<span id="cb4-46">    sar_e_pul <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"sar_e_pul"</span></span>
<span id="cb4-47">    takhar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"takhar"</span></span>
<span id="cb4-48">    uruzgan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"uruzgan"</span></span>
<span id="cb4-49">    wardak <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"wardak"</span></span>
<span id="cb4-50">    zabul <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"zabul"</span></span>
<span id="cb4-51"></span>
<span id="cb4-52"></span>
<span id="cb4-53"><span class="kw" style="color: #003B4F;">class</span> TargetGroup(<span class="bu" style="color: null;">str</span>, Enum):</span>
<span id="cb4-54">    taliban <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"taliban"</span></span>
<span id="cb4-55">    haqqani <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"haqqani"</span></span>
<span id="cb4-56">    criminals <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"criminals"</span></span>
<span id="cb4-57">    aq <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"aq"</span></span>
<span id="cb4-58">    hig <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"hig"</span></span>
<span id="cb4-59">    let <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"let"</span></span>
<span id="cb4-60">    imu <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"imu"</span></span>
<span id="cb4-61">    judq <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"judq"</span></span>
<span id="cb4-62">    iju <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"iju"</span></span>
<span id="cb4-63">    hik <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"hik"</span></span>
<span id="cb4-64">    ttp <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"ttp"</span></span>
<span id="cb4-65">    other <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb4-66"></span>
<span id="cb4-67"></span>
<span id="cb4-68"><span class="kw" style="color: #003B4F;">def</span> validate_event_type(value: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb4-69">    valid_values <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb4-70">        <span class="st" style="color: #20794D;">"airstrike"</span>,</span>
<span id="cb4-71">        <span class="st" style="color: #20794D;">"detention"</span>,</span>
<span id="cb4-72">        <span class="st" style="color: #20794D;">"captureandkill"</span>,</span>
<span id="cb4-73">        <span class="st" style="color: #20794D;">"insurgentskilled"</span>,</span>
<span id="cb4-74">        <span class="st" style="color: #20794D;">"exchangeoffire"</span>,</span>
<span id="cb4-75">        <span class="st" style="color: #20794D;">"civiliancasualty"</span>,</span>
<span id="cb4-76">    ]</span>
<span id="cb4-77">    <span class="cf" style="color: #003B4F;">if</span> value.lower() <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> valid_values:</span>
<span id="cb4-78">        <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb4-79">    <span class="cf" style="color: #003B4F;">return</span> value.lower()</span>
<span id="cb4-80"></span>
<span id="cb4-81"></span>
<span id="cb4-82"><span class="kw" style="color: #003B4F;">def</span> validate_province(value: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb4-83">    valid_values <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb4-84">        <span class="st" style="color: #20794D;">"badakhshan"</span>,</span>
<span id="cb4-85">        <span class="st" style="color: #20794D;">"badghis"</span>,</span>
<span id="cb4-86">        <span class="st" style="color: #20794D;">"baghlan"</span>,</span>
<span id="cb4-87">        <span class="st" style="color: #20794D;">"balkh"</span>,</span>
<span id="cb4-88">        <span class="st" style="color: #20794D;">"bamyan"</span>,</span>
<span id="cb4-89">        <span class="st" style="color: #20794D;">"day_kundi"</span>,</span>
<span id="cb4-90">        <span class="st" style="color: #20794D;">"farah"</span>,</span>
<span id="cb4-91">        <span class="st" style="color: #20794D;">"faryab"</span>,</span>
<span id="cb4-92">        <span class="st" style="color: #20794D;">"ghazni"</span>,</span>
<span id="cb4-93">        <span class="st" style="color: #20794D;">"ghor"</span>,</span>
<span id="cb4-94">        <span class="st" style="color: #20794D;">"helmand"</span>,</span>
<span id="cb4-95">        <span class="st" style="color: #20794D;">"herat"</span>,</span>
<span id="cb4-96">        <span class="st" style="color: #20794D;">"jowzjan"</span>,</span>
<span id="cb4-97">        <span class="st" style="color: #20794D;">"kabul"</span>,</span>
<span id="cb4-98">        <span class="st" style="color: #20794D;">"kandahar"</span>,</span>
<span id="cb4-99">        <span class="st" style="color: #20794D;">"kapisa"</span>,</span>
<span id="cb4-100">        <span class="st" style="color: #20794D;">"khost"</span>,</span>
<span id="cb4-101">        <span class="st" style="color: #20794D;">"kunar"</span>,</span>
<span id="cb4-102">        <span class="st" style="color: #20794D;">"kunduz"</span>,</span>
<span id="cb4-103">        <span class="st" style="color: #20794D;">"laghman"</span>,</span>
<span id="cb4-104">        <span class="st" style="color: #20794D;">"logar"</span>,</span>
<span id="cb4-105">        <span class="st" style="color: #20794D;">"nangarhar"</span>,</span>
<span id="cb4-106">        <span class="st" style="color: #20794D;">"nimroz"</span>,</span>
<span id="cb4-107">        <span class="st" style="color: #20794D;">"nuristan"</span>,</span>
<span id="cb4-108">        <span class="st" style="color: #20794D;">"paktya"</span>,</span>
<span id="cb4-109">        <span class="st" style="color: #20794D;">"paktika"</span>,</span>
<span id="cb4-110">        <span class="st" style="color: #20794D;">"panjshir"</span>,</span>
<span id="cb4-111">        <span class="st" style="color: #20794D;">"parwan"</span>,</span>
<span id="cb4-112">        <span class="st" style="color: #20794D;">"samangan"</span>,</span>
<span id="cb4-113">        <span class="st" style="color: #20794D;">"sar_e_pul"</span>,</span>
<span id="cb4-114">        <span class="st" style="color: #20794D;">"takhar"</span>,</span>
<span id="cb4-115">        <span class="st" style="color: #20794D;">"uruzgan"</span>,</span>
<span id="cb4-116">        <span class="st" style="color: #20794D;">"wardak"</span>,</span>
<span id="cb4-117">        <span class="st" style="color: #20794D;">"zabul"</span>,</span>
<span id="cb4-118">    ]</span>
<span id="cb4-119">    <span class="cf" style="color: #003B4F;">if</span> value.lower() <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> valid_values:</span>
<span id="cb4-120">        <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb4-121">    <span class="cf" style="color: #003B4F;">return</span> value.lower()</span>
<span id="cb4-122"></span>
<span id="cb4-123"></span>
<span id="cb4-124"><span class="kw" style="color: #003B4F;">def</span> validate_target_group(value: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb4-125">    valid_values <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb4-126">        <span class="st" style="color: #20794D;">"taliban"</span>,</span>
<span id="cb4-127">        <span class="st" style="color: #20794D;">"haqqani"</span>,</span>
<span id="cb4-128">        <span class="st" style="color: #20794D;">"criminals"</span>,</span>
<span id="cb4-129">        <span class="st" style="color: #20794D;">"aq"</span>,</span>
<span id="cb4-130">        <span class="st" style="color: #20794D;">"hig"</span>,</span>
<span id="cb4-131">        <span class="st" style="color: #20794D;">"let"</span>,</span>
<span id="cb4-132">        <span class="st" style="color: #20794D;">"imu"</span>,</span>
<span id="cb4-133">        <span class="st" style="color: #20794D;">"judq"</span>,</span>
<span id="cb4-134">        <span class="st" style="color: #20794D;">"iju"</span>,</span>
<span id="cb4-135">        <span class="st" style="color: #20794D;">"hik"</span>,</span>
<span id="cb4-136">        <span class="st" style="color: #20794D;">"ttp"</span>,</span>
<span id="cb4-137">        <span class="st" style="color: #20794D;">"other"</span>,</span>
<span id="cb4-138">    ]</span>
<span id="cb4-139">    <span class="cf" style="color: #003B4F;">if</span> value.lower() <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> valid_values:</span>
<span id="cb4-140">        <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb4-141">    <span class="cf" style="color: #003B4F;">return</span> value.lower()</span>
<span id="cb4-142"></span>
<span id="cb4-143"></span>
<span id="cb4-144"><span class="kw" style="color: #003B4F;">class</span> IsafEvent(BaseModel):</span>
<span id="cb4-145">    name: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-146">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"A title or name for the event which summarises the event as a headline"</span></span>
<span id="cb4-147">    )</span>
<span id="cb4-148">    text: Optional[<span class="bu" style="color: null;">str</span>] <span class="op" style="color: #5E5E5E;">=</span> Field(description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The full text of the press release"</span>)</span>
<span id="cb4-149">    start_date: date <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-150">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The start date of the event in YYYY-MM-DD format"</span></span>
<span id="cb4-151">    )</span>
<span id="cb4-152">    event_type: Set[Annotated[<span class="bu" style="color: null;">str</span>, Field(validator<span class="op" style="color: #5E5E5E;">=</span>validate_event_type)]] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-153">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The event type. Can be multiple types."</span></span>
<span id="cb4-154">    )</span>
<span id="cb4-155">    province: Set[Annotated[<span class="bu" style="color: null;">str</span>, Field(validator<span class="op" style="color: #5E5E5E;">=</span>validate_province)]] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-156">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The province in which the event occurred. Can be multiple provinces."</span></span>
<span id="cb4-157">    )</span>
<span id="cb4-158">    target_group: Set[Annotated[<span class="bu" style="color: null;">str</span>, Field(validator<span class="op" style="color: #5E5E5E;">=</span>validate_target_group)]] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-159">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The group that was targetted during the event. Can be multiple groups."</span></span>
<span id="cb4-160">    )</span>
<span id="cb4-161">    min_killed: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-162">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of people killed during the event"</span></span>
<span id="cb4-163">    )</span>
<span id="cb4-164">    min_captured: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-165">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of people captured during the event"</span></span>
<span id="cb4-166">    )</span>
<span id="cb4-167">    killq: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-168">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether someone was killed or not during the event"</span></span>
<span id="cb4-169">    )</span>
<span id="cb4-170">    captureq: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-171">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether someone was captured or not during the event"</span></span>
<span id="cb4-172">    )</span>
<span id="cb4-173">    killcaptureraid: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-174">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether the event was a so-called 'kill-capture raid'."</span></span>
<span id="cb4-175">    )</span>
<span id="cb4-176">    airstrike: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-177">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether an airstrike was used during the event"</span></span>
<span id="cb4-178">    )</span>
<span id="cb4-179">    noshotsfired: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-180">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether no shots were fired during the event"</span></span>
<span id="cb4-181">    )</span>
<span id="cb4-182">    min_leaders_killed: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-183">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of leaders killed during the event"</span></span>
<span id="cb4-184">    )</span>
<span id="cb4-185">    min_leaders_captured: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-186">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of leaders captured during the event"</span></span>
<span id="cb4-187">    )</span>
<span id="cb4-188">    predictions: Dict[<span class="bu" style="color: null;">str</span>, <span class="bu" style="color: null;">str</span>] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb4-189">        default<span class="op" style="color: #5E5E5E;">=</span>{},</span>
<span id="cb4-190">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The predictions from the model. Keys are the model name and the value is the prediction"</span>,</span>
<span id="cb4-191">    )</span>
<span id="cb4-192"></span>
<span id="cb4-193">    <span class="kw" style="color: #003B4F;">class</span> Config:</span>
<span id="cb4-194">        arbitrary_types_allowed <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span></span></code></pre></div>
</details>
</div>
<p>Here’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> List</span>
<span id="cb5-2"></span>
<span id="cb5-3">events: List[IsafEvent] <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">list</span>(test_df.iterrows()):</span>
<span id="cb5-6">    event_types <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb5-7">        eventtype.strip().lower() <span class="cf" style="color: #003B4F;">for</span> eventtype <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"eventtype"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb5-8">    )</span>
<span id="cb5-9">    provinces <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(province.strip().lower() <span class="cf" style="color: #003B4F;">for</span> province <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"province"</span>].split(<span class="st" style="color: #20794D;">","</span>))</span>
<span id="cb5-10">    target_groups <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb5-11">        target_group.strip().lower() <span class="cf" style="color: #003B4F;">for</span> target_group <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"targetgroup"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb5-12">    )</span>
<span id="cb5-13"></span>
<span id="cb5-14">    events.append(</span>
<span id="cb5-15">        IsafEvent(</span>
<span id="cb5-16">            name<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"name"</span>],</span>
<span id="cb5-17">            text<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"text"</span>],</span>
<span id="cb5-18">            start_date<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"StartDate"</span>].to_pydatetime().date(),</span>
<span id="cb5-19">            event_type<span class="op" style="color: #5E5E5E;">=</span>event_types,</span>
<span id="cb5-20">            province<span class="op" style="color: #5E5E5E;">=</span>provinces,</span>
<span id="cb5-21">            target_group<span class="op" style="color: #5E5E5E;">=</span>target_groups,</span>
<span id="cb5-22">            min_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minkilled"</span>]),</span>
<span id="cb5-23">            min_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"mincaptured"</span>]),</span>
<span id="cb5-24">            killq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb5-25">            captureq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"captureq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb5-26">            killcaptureraid<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killcaptureraid"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb5-27">            airstrike<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"airstrike"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb5-28">            noshotsfired<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"noshotsfired"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb5-29">            min_leaders_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderskilled"</span>]),</span>
<span id="cb5-30">            min_leaders_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderscaptured"</span>]),</span>
<span id="cb5-31">        )</span>
<span id="cb5-32">    )</span>
<span id="cb5-33"></span>
<span id="cb5-34"><span class="bu" style="color: null;">print</span>(events[:<span class="dv" style="color: #AD0000;">2</span>])</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
    <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">IsafEvent</span><span style="font-weight: bold">(</span>
        <span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'5'</span>,
        <span style="color: #808000; text-decoration-color: #808000">text</span>=<span style="color: #008000; text-decoration-color: #008000">'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span style="color: #008000; text-decoration-color: #008000">Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span style="color: #008000; text-decoration-color: #008000">group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span style="color: #008000; text-decoration-color: #008000">attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span style="color: #008000; text-decoration-color: #008000">National Police in Ghazni province.'</span>,
        <span style="color: #808000; text-decoration-color: #808000">start_date</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime</span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2013</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">24</span><span style="font-weight: bold">)</span>,
        <span style="color: #808000; text-decoration-color: #808000">event_type</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'insurgentskilled'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">province</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'ghazni'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">target_group</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'taliban'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">killq</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">captureq</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">killcaptureraid</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">airstrike</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">noshotsfired</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">predictions</span>=<span style="font-weight: bold">{}</span>
    <span style="font-weight: bold">)</span>,
    <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">IsafEvent</span><span style="font-weight: bold">(</span>
        <span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'2'</span>,
        <span style="color: #808000; text-decoration-color: #808000">text</span>=<span style="color: #008000; text-decoration-color: #008000">'2011-11-S-034\nISAF Joint Command - Afghanistan\nFor Immediate Release\n\nKABUL, Afghanistan (Nov. </span>
<span style="color: #008000; text-decoration-color: #008000">20, 2011)\nA coalition security force detained numerous suspected insurgents during an operation in Marjeh </span>
<span style="color: #008000; text-decoration-color: #008000">district, Helmand province, yesterday.  The force conducted the operation after receiving information that a group </span>
<span style="color: #008000; text-decoration-color: #008000">of insurgents were at a compound in the area.  After calling for the men inside to come out peacefully, the </span>
<span style="color: #008000; text-decoration-color: #008000">insurgents emerged and were detained without incident.'</span>,
        <span style="color: #808000; text-decoration-color: #808000">start_date</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime</span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2011</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19</span><span style="font-weight: bold">)</span>,
        <span style="color: #808000; text-decoration-color: #808000">event_type</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'detention'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">province</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'helmand'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">target_group</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">''</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
        <span style="color: #808000; text-decoration-color: #808000">killq</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">captureq</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">killcaptureraid</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">airstrike</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">noshotsfired</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">predictions</span>=<span style="font-weight: bold">{}</span>
    <span style="font-weight: bold">)</span>
<span style="font-weight: bold">]</span>
</pre>
</div>
</div>
<p>So when we’re making the prediction we’re hoping to get a JSON string like this out from the model:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">json_str <span class="op" style="color: #5E5E5E;">=</span> events[<span class="dv" style="color: #AD0000;">0</span>].model_dump_json(exclude<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"text"</span>, <span class="st" style="color: #20794D;">"predictions"</span>})</span>
<span id="cb6-2"><span class="bu" style="color: null;">print</span>(json_str)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"name"</span>:<span style="color: #008000; text-decoration-color: #008000">"5"</span>,<span style="color: #008000; text-decoration-color: #008000">"start_date"</span>:<span style="color: #008000; text-decoration-color: #008000">"2013-01-24"</span>,<span style="color: #008000; text-decoration-color: #008000">"event_type"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"insurgentskilled"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"province"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"ghazni"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"target_group"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"tali</span>
<span style="color: #008000; text-decoration-color: #008000">ban"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"min_killed"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,<span style="color: #008000; text-decoration-color: #008000">"min_captured"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,<span style="color: #008000; text-decoration-color: #008000">"killq"</span>:true,<span style="color: #008000; text-decoration-color: #008000">"captureq"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"airstrike"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"nosh</span>
<span style="color: #008000; text-decoration-color: #008000">otsfired"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"min_leaders_killed"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>I’m starting with full evaluations using the GPT models and I’ll need a slightly more elaborate prompt in order to get decent results. I can’t pass in the exact same prompt as the one I used for the finetuned model since the GPT models haven’t been trained or finetuned to respond to those specific prompts. This is sort of an interesting problem to have: how much effort do we put into the GPT prompts to try to get the same level of accuracy as the finetuned model? Or in other words, is there even a way to really compare like to like between models that must accept different prompts?</p>
<p>Let’s try this out for OpenAI GPT-4o and GPT-4 Turbo and see how we get on. You’ll note how long the prompt has to be to give the GPT models a fighting chance against the finetuned models. Ideally I’d stuff in even more examples into the context, but I also don’t want to explode the number of tokens I’m using.</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> openai <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb7-2"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb7-3"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb7-4"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb7-5"></span>
<span id="cb7-6"></span>
<span id="cb7-7"><span class="kw" style="color: #003B4F;">def</span> query_openai(article_text: <span class="bu" style="color: null;">str</span>, model: <span class="bu" style="color: null;">str</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="bu" style="color: null;">str</span>:</span>
<span id="cb7-8">    query <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb7-9">        <span class="ss" style="color: #20794D;">f"The following is a press release issued by ISAF (formerly operating in Afghanistan):</span><span class="ch" style="color: #20794D;">\n</span><span class="sc" style="color: #5E5E5E;">{</span>article_text<span class="sc" style="color: #5E5E5E;">}</span><span class="ch" style="color: #20794D;">\n\n</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb7-10">        <span class="st" style="color: #20794D;">"## Extraction request</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-11">        <span class="st" style="color: #20794D;">"Please extract the following information from the press release:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-12">        <span class="st" style="color: #20794D;">"- The name of the event (summarising the event / text as a headline)</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-13">        <span class="st" style="color: #20794D;">"- The start date of the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-14">        <span class="st" style="color: #20794D;">"- The event type(s)</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-15">        <span class="st" style="color: #20794D;">"- The province(s) in which the event occurred</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-16">        <span class="st" style="color: #20794D;">"- The target group(s) of the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-17">        <span class="st" style="color: #20794D;">"- The minimum number of people killed during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-18">        <span class="st" style="color: #20794D;">"- The minimum number of people captured during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-19">        <span class="st" style="color: #20794D;">"- Whether someone was killed or not during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-20">        <span class="st" style="color: #20794D;">"- Whether someone was captured or not during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-21">        <span class="st" style="color: #20794D;">"- Whether the event was a so-called 'kill-capture raid'</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-22">        <span class="st" style="color: #20794D;">"- Whether an airstrike was used during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-23">        <span class="st" style="color: #20794D;">"- Whether no shots were fired during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-24">        <span class="st" style="color: #20794D;">"- The minimum number of leaders killed during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-25">        <span class="st" style="color: #20794D;">"- The minimum number of leaders captured during the event</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-26">        <span class="st" style="color: #20794D;">"## Annotation notes:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-27">        <span class="st" style="color: #20794D;">"- A 'faciliator' is not a leader.</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-28">        <span class="st" style="color: #20794D;">"- If a press release states that 'insurgents' were detained without further "</span></span>
<span id="cb7-29">        <span class="st" style="color: #20794D;">"details, assign a minimum number of two detained. Interpret 'a couple' as "</span></span>
<span id="cb7-30">        <span class="st" style="color: #20794D;">"two. Interpret 'several' as at least three, even though it may sometimes "</span></span>
<span id="cb7-31">        <span class="st" style="color: #20794D;">"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a "</span></span>
<span id="cb7-32">        <span class="st" style="color: #20794D;">"small group', and 'multiple' as denoting at least three, even if they "</span></span>
<span id="cb7-33">        <span class="st" style="color: #20794D;">"sometimes refer to larger numbers. Choose the smaller number if no other "</span></span>
<span id="cb7-34">        <span class="st" style="color: #20794D;">"information is available in the press release to come up with a minimally "</span></span>
<span id="cb7-35">        <span class="st" style="color: #20794D;">"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, "</span></span>
<span id="cb7-36">        <span class="st" style="color: #20794D;">"and 'a large number' as at least five.</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-37">        <span class="st" style="color: #20794D;">"## Example:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-38">        <span class="st" style="color: #20794D;">"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">ISAF Joint Command - "</span></span>
<span id="cb7-39">        <span class="st" style="color: #20794D;">"Afghanistan</span><span class="ch" style="color: #20794D;">\u2028</span><span class="st" style="color: #20794D;">2011-02-S-143</span><span class="ch" style="color: #20794D;">\u2028</span><span class="st" style="color: #20794D;">For Immediate Release </span><span class="ch" style="color: #20794D;">\u2028\u2028</span><span class="st" style="color: #20794D;">KABUL, Afghanistan (Feb. 19)</span><span class="ch" style="color: #20794D;">\u2028\u2028</span><span class="st" style="color: #20794D;">ISAF "</span></span>
<span id="cb7-40">        <span class="st" style="color: #20794D;">"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of "</span></span>
<span id="cb7-41">        <span class="st" style="color: #20794D;">"their position talking on radios today. After gaining positive identification of the insurgent positions, the "</span></span>
<span id="cb7-42">        <span class="st" style="color: #20794D;">"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning "</span></span>
<span id="cb7-43">        <span class="st" style="color: #20794D;">"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent "</span></span>
<span id="cb7-44">        <span class="st" style="color: #20794D;">"positions, resulting in several more insurgents being killed.'</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-45">        <span class="st" style="color: #20794D;">'Output: `{"name":"Several insurgents killed in '</span></span>
<span id="cb7-46">        <span class="st" style="color: #20794D;">'Helmand","start_date":"2011-02-18","event_type":["insurgentskilled"],"province":["helmand"],"target_group":[""],"mi'</span></span>
<span id="cb7-47">        <span class="st" style="color: #20794D;">'n_killed":6,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsfired"'</span></span>
<span id="cb7-48">        <span class="st" style="color: #20794D;">':false,"min_leaders_killed":0,"min_leaders_captured":0}`'</span></span>
<span id="cb7-49">    )</span>
<span id="cb7-50"></span>
<span id="cb7-51">    <span class="co" style="color: #5E5E5E;"># set up the prediction harness</span></span>
<span id="cb7-52">    client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(api_key<span class="op" style="color: #5E5E5E;">=</span>os.getenv(<span class="st" style="color: #20794D;">"OPENAI_API_KEY"</span>))</span>
<span id="cb7-53"></span>
<span id="cb7-54">    response <span class="op" style="color: #5E5E5E;">=</span> client.chat.completions.create(</span>
<span id="cb7-55">        model<span class="op" style="color: #5E5E5E;">=</span>model,</span>
<span id="cb7-56">        response_format<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"type"</span>: <span class="st" style="color: #20794D;">"json_object"</span>},</span>
<span id="cb7-57">        messages<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb7-58">            {</span>
<span id="cb7-59">                <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb7-60">                <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise "</span></span>
<span id="cb7-61">                <span class="st" style="color: #20794D;">"and always make sure you are correct, drawing inference from the text of the "</span></span>
<span id="cb7-62">                <span class="st" style="color: #20794D;">"press release.</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;"> You always return a JSON string with the following schema: "</span></span>
<span id="cb7-63">                <span class="st" style="color: #20794D;">"## JSON Schema details</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb7-64">                <span class="st" style="color: #20794D;">"Here is some of the schema for the JSON output string you "</span></span>
<span id="cb7-65">                <span class="st" style="color: #20794D;">"should make use of: event_types = ['airstrike', 'detention', "</span></span>
<span id="cb7-66">                <span class="st" style="color: #20794D;">"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], "</span></span>
<span id="cb7-67">                <span class="st" style="color: #20794D;">"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', "</span></span>
<span id="cb7-68">                <span class="st" style="color: #20794D;">"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', "</span></span>
<span id="cb7-69">                <span class="st" style="color: #20794D;">"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', "</span></span>
<span id="cb7-70">                <span class="st" style="color: #20794D;">"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', "</span></span>
<span id="cb7-71">                <span class="st" style="color: #20794D;">"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', "</span></span>
<span id="cb7-72">                <span class="st" style="color: #20794D;">"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', "</span></span>
<span id="cb7-73">                <span class="st" style="color: #20794D;">"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span>,</span>
<span id="cb7-74">            },</span>
<span id="cb7-75">            {<span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>, <span class="st" style="color: #20794D;">"content"</span>: query},</span>
<span id="cb7-76">        ],</span>
<span id="cb7-77">        temperature<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb7-78">    )</span>
<span id="cb7-79"></span>
<span id="cb7-80">    <span class="cf" style="color: #003B4F;">return</span> response.choices[<span class="dv" style="color: #AD0000;">0</span>].message.content</span></code></pre></div>
</div>
<p>We can make sure this function works with a quick example:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">json_str <span class="op" style="color: #5E5E5E;">=</span> query_openai(events[<span class="dv" style="color: #AD0000;">0</span>].text, <span class="st" style="color: #20794D;">"gpt-4o"</span>)</span>
<span id="cb8-2"><span class="bu" style="color: null;">print</span>(json.loads(json_str))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'name'</span>: <span style="color: #008000; text-decoration-color: #008000">'Taliban leader Alaudin killed in Ghazni'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'start_date'</span>: <span style="color: #008000; text-decoration-color: #008000">'2013-01-24'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'event_type'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'insurgentskilled'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'province'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'ghazni'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'target_group'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'taliban'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'captureq'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killcaptureraid'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'airstrike'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'noshotsfired'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>Our model is working (as expected) and we’re also getting a JSON string back. Let’s assemble something that will iterate through all of our test data, get predictions, and then store those predictions on our Pydantic object.</p>
<p>For the bulk predictions, we’ll make sure to do this async, since there are lots of events and we don’t want to waiting all day. You’ll see I also had to add some retries to the function to account for rate limiting on the GPT-3.5-turbo model.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;"># make async work within a notebook</span></span>
<span id="cb9-2"><span class="im" style="color: #00769E;">import</span> nest_asyncio</span>
<span id="cb9-3"></span>
<span id="cb9-4">nest_asyncio.<span class="bu" style="color: null;">apply</span>()</span>
<span id="cb9-5"></span>
<span id="cb9-6"><span class="im" style="color: #00769E;">import</span> aiohttp</span>
<span id="cb9-7"><span class="im" style="color: #00769E;">import</span> asyncio</span>
<span id="cb9-8"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> List</span>
<span id="cb9-9"><span class="im" style="color: #00769E;">from</span> openai <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb9-10"></span>
<span id="cb9-11"></span>
<span id="cb9-12"><span class="cf" style="color: #003B4F;">async</span> <span class="kw" style="color: #003B4F;">def</span> async_query_openai(</span>
<span id="cb9-13">    session,</span>
<span id="cb9-14">    article_text: <span class="bu" style="color: null;">str</span>,</span>
<span id="cb9-15">    model: <span class="bu" style="color: null;">str</span>,</span>
<span id="cb9-16">    max_retries: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">3</span>,</span>
<span id="cb9-17">    retry_delay: <span class="bu" style="color: null;">float</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.0</span>,</span>
<span id="cb9-18">) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="bu" style="color: null;">str</span>:</span>
<span id="cb9-19">    query <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb9-20">        <span class="ss" style="color: #20794D;">f"The following is a press release issued by ISAF (formerly operating in Afghanistan):</span><span class="ch" style="color: #20794D;">\n</span><span class="sc" style="color: #5E5E5E;">{</span>article_text<span class="sc" style="color: #5E5E5E;">}</span><span class="ch" style="color: #20794D;">\n\n</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb9-21">        <span class="st" style="color: #20794D;">"## Extraction request</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-22">        <span class="st" style="color: #20794D;">"Please extract the following information from the press release:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-23">        <span class="st" style="color: #20794D;">"- The name of the event (summarising the event / text as a headline)</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-24">        <span class="st" style="color: #20794D;">"- The start date of the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-25">        <span class="st" style="color: #20794D;">"- The event type(s)</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-26">        <span class="st" style="color: #20794D;">"- The province(s) in which the event occurred</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-27">        <span class="st" style="color: #20794D;">"- The target group(s) of the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-28">        <span class="st" style="color: #20794D;">"- The minimum number of people killed during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-29">        <span class="st" style="color: #20794D;">"- The minimum number of people captured during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-30">        <span class="st" style="color: #20794D;">"- Whether someone was killed or not during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-31">        <span class="st" style="color: #20794D;">"- Whether someone was captured or not during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-32">        <span class="st" style="color: #20794D;">"- Whether the event was a so-called 'kill-capture raid'</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-33">        <span class="st" style="color: #20794D;">"- Whether an airstrike was used during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-34">        <span class="st" style="color: #20794D;">"- Whether no shots were fired during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-35">        <span class="st" style="color: #20794D;">"- The minimum number of leaders killed during the event</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-36">        <span class="st" style="color: #20794D;">"- The minimum number of leaders captured during the event</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-37">        <span class="st" style="color: #20794D;">"## Annotation notes:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-38">        <span class="st" style="color: #20794D;">"- A 'faciliator' is not a leader.</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-39">        <span class="st" style="color: #20794D;">"- If a press release states that 'insurgents' were detained without further "</span></span>
<span id="cb9-40">        <span class="st" style="color: #20794D;">"details, assign a minimum number of two detained. Interpret 'a couple' as "</span></span>
<span id="cb9-41">        <span class="st" style="color: #20794D;">"two. Interpret 'several' as at least three, even though it may sometimes "</span></span>
<span id="cb9-42">        <span class="st" style="color: #20794D;">"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a "</span></span>
<span id="cb9-43">        <span class="st" style="color: #20794D;">"small group', and 'multiple' as denoting at least three, even if they "</span></span>
<span id="cb9-44">        <span class="st" style="color: #20794D;">"sometimes refer to larger numbers. Choose the smaller number if no other "</span></span>
<span id="cb9-45">        <span class="st" style="color: #20794D;">"information is available in the press release to come up with a minimally "</span></span>
<span id="cb9-46">        <span class="st" style="color: #20794D;">"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, "</span></span>
<span id="cb9-47">        <span class="st" style="color: #20794D;">"and 'a large number' as at least five.</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-48">        <span class="st" style="color: #20794D;">"## Example:</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-49">        <span class="st" style="color: #20794D;">"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">ISAF Joint Command - "</span></span>
<span id="cb9-50">        <span class="st" style="color: #20794D;">"Afghanistan</span><span class="ch" style="color: #20794D;">\u2028</span><span class="st" style="color: #20794D;">2011-02-S-143</span><span class="ch" style="color: #20794D;">\u2028</span><span class="st" style="color: #20794D;">For Immediate Release </span><span class="ch" style="color: #20794D;">\u2028\u2028</span><span class="st" style="color: #20794D;">KABUL, Afghanistan (Feb. 19)</span><span class="ch" style="color: #20794D;">\u2028\u2028</span><span class="st" style="color: #20794D;">ISAF "</span></span>
<span id="cb9-51">        <span class="st" style="color: #20794D;">"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of "</span></span>
<span id="cb9-52">        <span class="st" style="color: #20794D;">"their position talking on radios today. After gaining positive identification of the insurgent positions, the "</span></span>
<span id="cb9-53">        <span class="st" style="color: #20794D;">"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning "</span></span>
<span id="cb9-54">        <span class="st" style="color: #20794D;">"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent "</span></span>
<span id="cb9-55">        <span class="st" style="color: #20794D;">"positions, resulting in several more insurgents being killed.'</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-56">        <span class="st" style="color: #20794D;">'Output: `{"name":"Several insurgents killed in '</span></span>
<span id="cb9-57">        <span class="st" style="color: #20794D;">'Helmand","start_date":"2011-02-18","event_type":["insurgentskilled"],"province":["helmand"],"target_group":[""],"mi'</span></span>
<span id="cb9-58">        <span class="st" style="color: #20794D;">'n_killed":6,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsfired"'</span></span>
<span id="cb9-59">        <span class="st" style="color: #20794D;">':false,"min_leaders_killed":0,"min_leaders_captured":0}`'</span></span>
<span id="cb9-60">    )</span>
<span id="cb9-61"></span>
<span id="cb9-62">    client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(api_key<span class="op" style="color: #5E5E5E;">=</span>os.getenv(<span class="st" style="color: #20794D;">"OPENAI_API_KEY"</span>))</span>
<span id="cb9-63"></span>
<span id="cb9-64">    retries <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb9-65">    <span class="cf" style="color: #003B4F;">while</span> retries <span class="op" style="color: #5E5E5E;">&lt;</span> max_retries:</span>
<span id="cb9-66">        <span class="cf" style="color: #003B4F;">async</span> <span class="cf" style="color: #003B4F;">with</span> session.post(</span>
<span id="cb9-67">            <span class="st" style="color: #20794D;">"https://api.openai.com/v1/chat/completions"</span>,</span>
<span id="cb9-68">            headers<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"Authorization"</span>: <span class="ss" style="color: #20794D;">f"Bearer </span><span class="sc" style="color: #5E5E5E;">{</span>client<span class="sc" style="color: #5E5E5E;">.</span>api_key<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>},</span>
<span id="cb9-69">            json<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb9-70">                <span class="st" style="color: #20794D;">"model"</span>: model,</span>
<span id="cb9-71">                <span class="st" style="color: #20794D;">"response_format"</span>: {<span class="st" style="color: #20794D;">"type"</span>: <span class="st" style="color: #20794D;">"json_object"</span>},</span>
<span id="cb9-72">                <span class="st" style="color: #20794D;">"messages"</span>: [</span>
<span id="cb9-73">                    {</span>
<span id="cb9-74">                        <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb9-75">                        <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise "</span></span>
<span id="cb9-76">                        <span class="st" style="color: #20794D;">"and always make sure you are correct, drawing inference from the text of the "</span></span>
<span id="cb9-77">                        <span class="st" style="color: #20794D;">"press release.</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;"> You always return a JSON string with the following schema: "</span></span>
<span id="cb9-78">                        <span class="st" style="color: #20794D;">"## JSON Schema details</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span></span>
<span id="cb9-79">                        <span class="st" style="color: #20794D;">"Here is some of the schema for the JSON output string you "</span></span>
<span id="cb9-80">                        <span class="st" style="color: #20794D;">"should make use of: event_types = ['airstrike', 'detention', "</span></span>
<span id="cb9-81">                        <span class="st" style="color: #20794D;">"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], "</span></span>
<span id="cb9-82">                        <span class="st" style="color: #20794D;">"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', "</span></span>
<span id="cb9-83">                        <span class="st" style="color: #20794D;">"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', "</span></span>
<span id="cb9-84">                        <span class="st" style="color: #20794D;">"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', "</span></span>
<span id="cb9-85">                        <span class="st" style="color: #20794D;">"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', "</span></span>
<span id="cb9-86">                        <span class="st" style="color: #20794D;">"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', "</span></span>
<span id="cb9-87">                        <span class="st" style="color: #20794D;">"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', "</span></span>
<span id="cb9-88">                        <span class="st" style="color: #20794D;">"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">"</span>,</span>
<span id="cb9-89">                    },</span>
<span id="cb9-90">                    {<span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>, <span class="st" style="color: #20794D;">"content"</span>: query},</span>
<span id="cb9-91">                ],</span>
<span id="cb9-92">                <span class="st" style="color: #20794D;">"temperature"</span>: <span class="dv" style="color: #AD0000;">1</span>,</span>
<span id="cb9-93">            },</span>
<span id="cb9-94">        ) <span class="im" style="color: #00769E;">as</span> response:</span>
<span id="cb9-95">            result <span class="op" style="color: #5E5E5E;">=</span> <span class="cf" style="color: #003B4F;">await</span> response.json()</span>
<span id="cb9-96">            <span class="cf" style="color: #003B4F;">if</span> <span class="st" style="color: #20794D;">"error"</span> <span class="kw" style="color: #003B4F;">in</span> result:</span>
<span id="cb9-97">                error_message <span class="op" style="color: #5E5E5E;">=</span> result[<span class="st" style="color: #20794D;">"error"</span>][<span class="st" style="color: #20794D;">"message"</span>]</span>
<span id="cb9-98">                <span class="cf" style="color: #003B4F;">if</span> <span class="st" style="color: #20794D;">"Rate limit reached"</span> <span class="kw" style="color: #003B4F;">in</span> error_message:</span>
<span id="cb9-99">                    <span class="co" style="color: #5E5E5E;"># retry_delay_ms = float(</span></span>
<span id="cb9-100">                    <span class="co" style="color: #5E5E5E;">#     error_message.split("Please try again in ")[1].split("ms")[0]</span></span>
<span id="cb9-101">                    <span class="co" style="color: #5E5E5E;"># )</span></span>
<span id="cb9-102">                    retry_delay_ms <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">35000</span></span>
<span id="cb9-103">                    retry_delay_seconds <span class="op" style="color: #5E5E5E;">=</span> retry_delay_ms <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">1000</span></span>
<span id="cb9-104">                    <span class="bu" style="color: null;">print</span>(</span>
<span id="cb9-105">                        <span class="ss" style="color: #20794D;">f"Rate limit exceeded. Retrying in </span><span class="sc" style="color: #5E5E5E;">{</span>retry_delay_seconds<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> seconds..."</span></span>
<span id="cb9-106">                    )</span>
<span id="cb9-107">                    <span class="cf" style="color: #003B4F;">await</span> asyncio.sleep(retry_delay_seconds)</span>
<span id="cb9-108">                    retries <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb9-109">                    <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb9-110">                <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb9-111">                    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Error during prediction.</span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Full result object: </span><span class="sc" style="color: #5E5E5E;">{</span>result<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb9-112">                    <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb9-113">            <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb9-114">                <span class="cf" style="color: #003B4F;">return</span> result[<span class="st" style="color: #20794D;">"choices"</span>][<span class="dv" style="color: #AD0000;">0</span>][<span class="st" style="color: #20794D;">"message"</span>][<span class="st" style="color: #20794D;">"content"</span>]</span>
<span id="cb9-115">            <span class="cf" style="color: #003B4F;">except</span> <span class="pp" style="color: #AD0000;">KeyError</span>:</span>
<span id="cb9-116">                <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Error during prediction.</span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Full result object: </span><span class="sc" style="color: #5E5E5E;">{</span>result<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb9-117">                <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb9-118"></span>
<span id="cb9-119">    <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Max retries exceeded for event.</span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">Full result object: </span><span class="sc" style="color: #5E5E5E;">{</span>result<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb9-120">    <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">""</span></span>
<span id="cb9-121"></span>
<span id="cb9-122"></span>
<span id="cb9-123"><span class="cf" style="color: #003B4F;">async</span> <span class="kw" style="color: #003B4F;">def</span> get_gpt_predictions_async(</span>
<span id="cb9-124">    model: <span class="bu" style="color: null;">str</span>,</span>
<span id="cb9-125">    events: List[IsafEvent],</span>
<span id="cb9-126">    logging_n: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100</span>,</span>
<span id="cb9-127">    max_concurrent_requests: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">5</span>,</span>
<span id="cb9-128">) <span class="op" style="color: #5E5E5E;">-&gt;</span> List[IsafEvent]:</span>
<span id="cb9-129">    <span class="cf" style="color: #003B4F;">async</span> <span class="cf" style="color: #003B4F;">with</span> aiohttp.ClientSession() <span class="im" style="color: #00769E;">as</span> session:</span>
<span id="cb9-130">        semaphore <span class="op" style="color: #5E5E5E;">=</span> asyncio.Semaphore(max_concurrent_requests)</span>
<span id="cb9-131">        tasks <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb9-132">        <span class="cf" style="color: #003B4F;">for</span> i, event <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(events, start<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb9-133">            <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">%</span> logging_n <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb9-134">                <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Predicting event </span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> of </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">len</span>(events)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> using </span><span class="sc" style="color: #5E5E5E;">{</span>model<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb9-135"></span>
<span id="cb9-136">            <span class="cf" style="color: #003B4F;">async</span> <span class="kw" style="color: #003B4F;">def</span> make_request(session, event):</span>
<span id="cb9-137">                <span class="cf" style="color: #003B4F;">async</span> <span class="cf" style="color: #003B4F;">with</span> semaphore:</span>
<span id="cb9-138">                    <span class="cf" style="color: #003B4F;">return</span> <span class="cf" style="color: #003B4F;">await</span> async_query_openai(</span>
<span id="cb9-139">                        session, event.text, model, max_retries<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb9-140">                    )</span>
<span id="cb9-141"></span>
<span id="cb9-142">            task <span class="op" style="color: #5E5E5E;">=</span> asyncio.ensure_future(make_request(session, event))</span>
<span id="cb9-143">            tasks.append(task)</span>
<span id="cb9-144"></span>
<span id="cb9-145">        predictions <span class="op" style="color: #5E5E5E;">=</span> <span class="cf" style="color: #003B4F;">await</span> asyncio.gather(<span class="op" style="color: #5E5E5E;">*</span>tasks)</span>
<span id="cb9-146">        <span class="cf" style="color: #003B4F;">for</span> event, prediction <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(events, predictions):</span>
<span id="cb9-147">            event.predictions[model] <span class="op" style="color: #5E5E5E;">=</span> prediction</span>
<span id="cb9-148"></span>
<span id="cb9-149">    <span class="cf" style="color: #003B4F;">return</span> events</span>
<span id="cb9-150"></span>
<span id="cb9-151"></span>
<span id="cb9-152"><span class="cf" style="color: #003B4F;">async</span> <span class="kw" style="color: #003B4F;">def</span> main():</span>
<span id="cb9-153">    events_4o <span class="op" style="color: #5E5E5E;">=</span> <span class="cf" style="color: #003B4F;">await</span> get_gpt_predictions_async(</span>
<span id="cb9-154">        <span class="st" style="color: #20794D;">"gpt-4o"</span>, events, max_concurrent_requests<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb9-155">    )</span>
<span id="cb9-156">    events_4turbo <span class="op" style="color: #5E5E5E;">=</span> <span class="cf" style="color: #003B4F;">await</span> get_gpt_predictions_async(</span>
<span id="cb9-157">        <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, events_4o, max_concurrent_requests<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb9-158">    )</span>
<span id="cb9-159">    full_events <span class="op" style="color: #5E5E5E;">=</span> <span class="cf" style="color: #003B4F;">await</span> get_gpt_predictions_async(</span>
<span id="cb9-160">        <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>, events_4turbo, max_concurrent_requests<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb9-161">    )</span>
<span id="cb9-162"></span>
<span id="cb9-163"></span>
<span id="cb9-164"><span class="cf" style="color: #003B4F;">await</span> main()</span></code></pre></div>
</details>
</div>
<p>So as you can now see, we have three predictions attached to each event.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="bu" style="color: null;">print</span>(events[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">IsafEvent</span><span style="font-weight: bold">(</span>
    <span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'5'</span>,
    <span style="color: #808000; text-decoration-color: #808000">text</span>=<span style="color: #008000; text-decoration-color: #008000">'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span style="color: #008000; text-decoration-color: #008000">Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span style="color: #008000; text-decoration-color: #008000">group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span style="color: #008000; text-decoration-color: #008000">attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span style="color: #008000; text-decoration-color: #008000">National Police in Ghazni province.'</span>,
    <span style="color: #808000; text-decoration-color: #808000">start_date</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime</span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2013</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">24</span><span style="font-weight: bold">)</span>,
    <span style="color: #808000; text-decoration-color: #808000">event_type</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'insurgentskilled'</span><span style="font-weight: bold">}</span>,
    <span style="color: #808000; text-decoration-color: #808000">province</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'ghazni'</span><span style="font-weight: bold">}</span>,
    <span style="color: #808000; text-decoration-color: #808000">target_group</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'taliban'</span><span style="font-weight: bold">}</span>,
    <span style="color: #808000; text-decoration-color: #808000">min_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #808000; text-decoration-color: #808000">min_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #808000; text-decoration-color: #808000">killq</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #808000; text-decoration-color: #808000">captureq</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #808000; text-decoration-color: #808000">killcaptureraid</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #808000; text-decoration-color: #808000">airstrike</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #808000; text-decoration-color: #808000">noshotsfired</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #808000; text-decoration-color: #808000">min_leaders_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #808000; text-decoration-color: #808000">min_leaders_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #808000; text-decoration-color: #808000">predictions</span>=<span style="font-weight: bold">{</span>
        <span style="color: #008000; text-decoration-color: #008000">'gpt-4o'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n  "name": "Taliban leader Alaudin killed in Ghazni",\n  "start_date": "2013-01-24",\n  </span>
<span style="color: #008000; text-decoration-color: #008000">"event_type": ["insurgentskilled", "captureandkill"],\n  "province": ["ghazni"],\n  "target_group": ["taliban"],\n </span>
<span style="color: #008000; text-decoration-color: #008000">"min_killed": 1,\n  "min_captured": 0,\n  "killq": true,\n  "captureq": false,\n  "killcaptureraid": true,\n  </span>
<span style="color: #008000; text-decoration-color: #008000">"airstrike": false,\n  "noshotsfired": false,\n  "min_leaders_killed": 1,\n  "min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-4-turbo'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n    "name": "Taliban leader Alaudin killed in Ghazni",\n    "start_date": </span>
<span style="color: #008000; text-decoration-color: #008000">"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span style="color: #008000; text-decoration-color: #008000">["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid": true,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-3.5-turbo'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n    "name": "Taliban leader Alaudin killed in Ghazni province",\n    "start_date": </span>
<span style="color: #008000; text-decoration-color: #008000">"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span style="color: #008000; text-decoration-color: #008000">["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid": false,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured": 0\n}'</span>
    <span style="font-weight: bold">}</span>
<span style="font-weight: bold">)</span>
</pre>
</div>
</div>
<p>I have all these predictions living in memory right now so it’s probably a good time to commit these to a dataset and push them to the Hugging Face Hub in case the notebook crashes or my local machine shuts down or something else unexpected.</p>
<p>I’ll create a function to handle this as we’ll be repeating this process for the other models as well. It’s a bit verbose but I thought it preferable so you can see what’s going on.</p>
<div class="cell" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> Dataset</span>
<span id="cb11-2"></span>
<span id="cb11-3"></span>
<span id="cb11-4"><span class="kw" style="color: #003B4F;">def</span> convert_to_dataset(data: List[IsafEvent]) <span class="op" style="color: #5E5E5E;">-&gt;</span> Dataset:</span>
<span id="cb11-5">    names <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-6">    texts <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-7">    start_dates <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-8">    provinces <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-9">    target_groups <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-10">    event_types <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-11">    predictions <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-12">    min_killeds <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-13">    min_captureds <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-14">    killqs <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-15">    captureqs <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-16">    killcaptureraids <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-17">    airstrikes <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-18">    noshotsfireds <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-19">    min_leaders_killeds <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-20">    min_leaders_captureds <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb11-21"></span>
<span id="cb11-22">    <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> data:</span>
<span id="cb11-23">        names.append(item.name)</span>
<span id="cb11-24">        texts.append(item.text)</span>
<span id="cb11-25">        start_dates.append(item.start_date)</span>
<span id="cb11-26">        provinces.append(item.province)</span>
<span id="cb11-27">        target_groups.append(item.target_group)</span>
<span id="cb11-28">        event_types.append(item.event_type)</span>
<span id="cb11-29">        predictions.append(item.predictions)</span>
<span id="cb11-30">        min_killeds.append(item.min_killed)</span>
<span id="cb11-31">        min_captureds.append(item.min_captured)</span>
<span id="cb11-32">        killqs.append(item.killq)</span>
<span id="cb11-33">        captureqs.append(item.captureq)</span>
<span id="cb11-34">        killcaptureraids.append(item.killcaptureraid)</span>
<span id="cb11-35">        airstrikes.append(item.airstrike)</span>
<span id="cb11-36">        noshotsfireds.append(item.noshotsfired)</span>
<span id="cb11-37">        min_leaders_killeds.append(item.min_leaders_killed)</span>
<span id="cb11-38">        min_leaders_captureds.append(item.min_leaders_captured)</span>
<span id="cb11-39"></span>
<span id="cb11-40">    dataset_dict <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb11-41">        <span class="st" style="color: #20794D;">"name"</span>: names,</span>
<span id="cb11-42">        <span class="st" style="color: #20794D;">"text"</span>: texts,</span>
<span id="cb11-43">        <span class="st" style="color: #20794D;">"predictions"</span>: predictions,</span>
<span id="cb11-44">        <span class="st" style="color: #20794D;">"start_date"</span>: start_dates,</span>
<span id="cb11-45">        <span class="st" style="color: #20794D;">"province"</span>: provinces,</span>
<span id="cb11-46">        <span class="st" style="color: #20794D;">"target_group"</span>: target_groups,</span>
<span id="cb11-47">        <span class="st" style="color: #20794D;">"event_type"</span>: event_types,</span>
<span id="cb11-48">        <span class="st" style="color: #20794D;">"min_killed"</span>: min_killeds,</span>
<span id="cb11-49">        <span class="st" style="color: #20794D;">"min_captured"</span>: min_captureds,</span>
<span id="cb11-50">        <span class="st" style="color: #20794D;">"killq"</span>: killqs,</span>
<span id="cb11-51">        <span class="st" style="color: #20794D;">"captureq"</span>: captureqs,</span>
<span id="cb11-52">        <span class="st" style="color: #20794D;">"killcaptureraid"</span>: killcaptureraids,</span>
<span id="cb11-53">        <span class="st" style="color: #20794D;">"airstrike"</span>: airstrikes,</span>
<span id="cb11-54">        <span class="st" style="color: #20794D;">"noshotsfired"</span>: noshotsfireds,</span>
<span id="cb11-55">        <span class="st" style="color: #20794D;">"min_leaders_killed"</span>: min_leaders_killeds,</span>
<span id="cb11-56">        <span class="st" style="color: #20794D;">"min_leaders_captured"</span>: min_leaders_captureds,</span>
<span id="cb11-57">    }</span>
<span id="cb11-58">    dataset <span class="op" style="color: #5E5E5E;">=</span> Dataset.from_dict(dataset_dict)</span>
<span id="cb11-59"></span>
<span id="cb11-60">    <span class="cf" style="color: #003B4F;">return</span> dataset</span>
<span id="cb11-61"></span>
<span id="cb11-62"></span>
<span id="cb11-63"><span class="kw" style="color: #003B4F;">def</span> convert_and_push_dataset(</span>
<span id="cb11-64">    events: List[IsafEvent], name: <span class="bu" style="color: null;">str</span>, split_name: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"train"</span></span>
<span id="cb11-65">):</span>
<span id="cb11-66">    <span class="co" style="color: #5E5E5E;">"""Convert a list of Pydantic objects to a HF Dataset object, then push to</span></span>
<span id="cb11-67"><span class="co" style="color: #5E5E5E;">    the hub."""</span></span>
<span id="cb11-68">    hf_token <span class="op" style="color: #5E5E5E;">=</span> os.getenv(<span class="st" style="color: #20794D;">"HUGGINGFACE_API_KEY"</span>)</span>
<span id="cb11-69"></span>
<span id="cb11-70">    dataset <span class="op" style="color: #5E5E5E;">=</span> convert_to_dataset(events)</span>
<span id="cb11-71">    dataset.push_to_hub(</span>
<span id="cb11-72">        <span class="ss" style="color: #20794D;">f"strickvl/</span><span class="sc" style="color: #5E5E5E;">{</span>name<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>,</span>
<span id="cb11-73">        token<span class="op" style="color: #5E5E5E;">=</span>hf_token,</span>
<span id="cb11-74">        private<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,</span>
<span id="cb11-75">        create_pr<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,</span>
<span id="cb11-76">        split<span class="op" style="color: #5E5E5E;">=</span>split_name,</span>
<span id="cb11-77">    )</span></code></pre></div>
</details>
</div>
<p>A more concise and abstract version of the <code>convert_to_dataset</code> function could be something like:</p>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;">def</span> convert_to_dataset(data: List[BaseModel]) <span class="op" style="color: #5E5E5E;">-&gt;</span> Dataset:</span>
<span id="cb12-2">    dataset_dict <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb12-3"></span>
<span id="cb12-4">    <span class="cf" style="color: #003B4F;">for</span> field_name, field_value <span class="kw" style="color: #003B4F;">in</span> data[<span class="dv" style="color: #AD0000;">0</span>].__fields__.items():</span>
<span id="cb12-5">        field_type <span class="op" style="color: #5E5E5E;">=</span> field_value.outer_type_</span>
<span id="cb12-6">        <span class="cf" style="color: #003B4F;">if</span> field_type <span class="kw" style="color: #003B4F;">in</span> [<span class="bu" style="color: null;">str</span>, <span class="bu" style="color: null;">int</span>, <span class="bu" style="color: null;">float</span>, <span class="bu" style="color: null;">bool</span>, date]:</span>
<span id="cb12-7">            dataset_dict[field_name] <span class="op" style="color: #5E5E5E;">=</span> [<span class="bu" style="color: null;">getattr</span>(item, field_name) <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> data]</span>
<span id="cb12-8">        <span class="cf" style="color: #003B4F;">elif</span> field_type <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">set</span>:</span>
<span id="cb12-9">            dataset_dict[field_name] <span class="op" style="color: #5E5E5E;">=</span> [<span class="bu" style="color: null;">list</span>(<span class="bu" style="color: null;">getattr</span>(item, field_name)) <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> data]</span>
<span id="cb12-10">        <span class="cf" style="color: #003B4F;">elif</span> <span class="bu" style="color: null;">issubclass</span>(field_type, BaseModel):</span>
<span id="cb12-11">            dataset_dict[field_name] <span class="op" style="color: #5E5E5E;">=</span> [<span class="bu" style="color: null;">getattr</span>(item, field_name).<span class="bu" style="color: null;">dict</span>() <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> data]</span>
<span id="cb12-12">        <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb12-13">            dataset_dict[field_name] <span class="op" style="color: #5E5E5E;">=</span> [<span class="bu" style="color: null;">getattr</span>(item, field_name) <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> data]</span>
<span id="cb12-14"></span>
<span id="cb12-15">    dataset <span class="op" style="color: #5E5E5E;">=</span> Dataset.from_dict(dataset_dict)</span>
<span id="cb12-16">    <span class="cf" style="color: #003B4F;">return</span> dataset</span></code></pre></div>
<p>But for now let’s just push our data to the Hub.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">convert_and_push_dataset(events, <span class="st" style="color: #20794D;">"isafpressreleases_with_preds"</span>, split_name<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"test"</span>)</span></code></pre></div>
</div>
</section>
<section id="adding-predictions-from-our-finetuned-models" class="level1">
<h1>Adding predictions from our finetuned models</h1>
<p>We’ve added some baseline OpenAI models, so let’s now add <a href="https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html">the models</a> <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html">we previously finetuned</a>. This includes a mix of local models as well as models hosted by some <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html">one-click finetuning providers</a>.</p>
<p>I’ll hide a bunch of the code with folding arrows so you can see it if you’re interested but there isn’t actually much of interest or new there.</p>
<section id="reloading-the-predictions-dataset" class="level2">
<h2 class="anchored" data-anchor-id="reloading-the-predictions-dataset">Reloading the predictions dataset</h2>
<p>Let’s start by loading our dataset and then we can get into adding some local model predictions:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb14-2"></span>
<span id="cb14-3">preds_test_data <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases_with_preds"</span>)[</span>
<span id="cb14-4">    <span class="st" style="color: #20794D;">"test"</span></span>
<span id="cb14-5">].to_list()</span></code></pre></div>
</div>
<p>We trained some local models, so let’s add those predictions to the dataset.</p>
</section>
<section id="finetuned-tinyllama-predictions" class="level2">
<h2 class="anchored" data-anchor-id="finetuned-tinyllama-predictions">Finetuned TinyLlama predictions</h2>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Union</span>
<span id="cb15-2"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb15-3"><span class="im" style="color: #00769E;">from</span> peft <span class="im" style="color: #00769E;">import</span> AutoPeftModelForCausalLM</span>
<span id="cb15-4"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> AutoTokenizer</span>
<span id="cb15-5"></span>
<span id="cb15-6"></span>
<span id="cb15-7"><span class="kw" style="color: #003B4F;">def</span> prompt(press_release: <span class="bu" style="color: null;">str</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="bu" style="color: null;">str</span>:</span>
<span id="cb15-8">    <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f"""You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span></span>
<span id="cb15-9"></span>
<span id="cb15-10"><span class="ss" style="color: #20794D;">### Instruction:</span></span>
<span id="cb15-11"></span>
<span id="cb15-12"><span class="ss" style="color: #20794D;">PRESS RELEASE TEXT: "</span><span class="sc" style="color: #5E5E5E;">{</span>press_release<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb15-13"></span>
<span id="cb15-14"><span class="ss" style="color: #20794D;">### Response:</span></span>
<span id="cb15-15"><span class="ss" style="color: #20794D;">"""</span></span>
<span id="cb15-16"></span>
<span id="cb15-17"></span>
<span id="cb15-18"><span class="kw" style="color: #003B4F;">def</span> prompt_tok(</span>
<span id="cb15-19">    model: AutoPeftModelForCausalLM,</span>
<span id="cb15-20">    tokenizer: AutoTokenizer,</span>
<span id="cb15-21">    press_release: <span class="bu" style="color: null;">str</span>,</span>
<span id="cb15-22">    return_ids: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>,</span>
<span id="cb15-23">) <span class="op" style="color: #5E5E5E;">-&gt;</span> Union[<span class="bu" style="color: null;">str</span>, torch.Tensor]:</span>
<span id="cb15-24">    _p <span class="op" style="color: #5E5E5E;">=</span> prompt(press_release)</span>
<span id="cb15-25">    input_ids <span class="op" style="color: #5E5E5E;">=</span> tokenizer(_p, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>).input_ids.cuda()</span>
<span id="cb15-26">    out_ids <span class="op" style="color: #5E5E5E;">=</span> model.generate(input_ids<span class="op" style="color: #5E5E5E;">=</span>input_ids, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5000</span>, do_sample<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb15-27">    ids <span class="op" style="color: #5E5E5E;">=</span> out_ids.detach().cpu().numpy()</span>
<span id="cb15-28">    <span class="cf" style="color: #003B4F;">if</span> return_ids:</span>
<span id="cb15-29">        <span class="cf" style="color: #003B4F;">return</span> out_ids</span>
<span id="cb15-30">    <span class="cf" style="color: #003B4F;">return</span> tokenizer.batch_decode(ids, skip_special_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)[<span class="dv" style="color: #AD0000;">0</span>][<span class="bu" style="color: null;">len</span>(_p) :]</span>
<span id="cb15-31"></span>
<span id="cb15-32"></span>
<span id="cb15-33">tinyllama_sharegpt_model_id <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"strickvl/isafpr-tiny-llama-lora-templatefree"</span></span>
<span id="cb15-34">model <span class="op" style="color: #5E5E5E;">=</span> AutoPeftModelForCausalLM.from_pretrained(tinyllama_sharegpt_model_id).cuda()</span>
<span id="cb15-35">tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(tinyllama_sharegpt_model_id)</span>
<span id="cb15-36">tokenizer.pad_token <span class="op" style="color: #5E5E5E;">=</span> tokenizer.eos_token</span>
<span id="cb15-37"></span>
<span id="cb15-38"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> preds_test_data:</span>
<span id="cb15-39">    out <span class="op" style="color: #5E5E5E;">=</span> prompt_tok(model, tokenizer, row[<span class="st" style="color: #20794D;">"text"</span>])</span>
<span id="cb15-40">    row[<span class="st" style="color: #20794D;">"predictions"</span>][<span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>] <span class="op" style="color: #5E5E5E;">=</span> out</span></code></pre></div>
</details>
</div>
<p>Now if we inspect we’ll see that the new model predictions have been saved into the dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb16-2"></span>
<span id="cb16-3"><span class="bu" style="color: null;">print</span>(preds_test_data[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'name'</span>: <span style="color: #008000; text-decoration-color: #008000">'5'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'text'</span>: <span style="color: #008000; text-decoration-color: #008000">'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span style="color: #008000; text-decoration-color: #008000">Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span style="color: #008000; text-decoration-color: #008000">group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span style="color: #008000; text-decoration-color: #008000">attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span style="color: #008000; text-decoration-color: #008000">National Police in Ghazni province.'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'predictions'</span>: <span style="font-weight: bold">{</span>
        <span style="color: #008000; text-decoration-color: #008000">'gpt-3.5-turbo'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n    "name": "Taliban leader Alaudin killed in Ghazni province",\n    "start_date": </span>
<span style="color: #008000; text-decoration-color: #008000">"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span style="color: #008000; text-decoration-color: #008000">["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid": false,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-4-turbo'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n    "name": "Taliban leader Alaudin killed in Ghazni",\n    "start_date": </span>
<span style="color: #008000; text-decoration-color: #008000">"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span style="color: #008000; text-decoration-color: #008000">["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid": true,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-4o'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n  "name": "Taliban leader Alaudin killed in Ghazni",\n  "start_date": "2013-01-24",\n  </span>
<span style="color: #008000; text-decoration-color: #008000">"event_type": ["insurgentskilled", "captureandkill"],\n  "province": ["ghazni"],\n  "target_group": ["taliban"],\n </span>
<span style="color: #008000; text-decoration-color: #008000">"min_killed": 1,\n  "min_captured": 0,\n  "killq": true,\n  "captureq": false,\n  "killcaptureraid": true,\n  </span>
<span style="color: #008000; text-decoration-color: #008000">"airstrike": false,\n  "noshotsfired": false,\n  "min_leaders_killed": 1,\n  "min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'tinyllama-templatefree'</span>: <span style="color: #008000; text-decoration-color: #008000">'\n{"name":"Taliban leader killed in </span>
<span style="color: #008000; text-decoration-color: #008000">Ghazni","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["taliban"</span>
<span style="color: #008000; text-decoration-color: #008000">],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsf</span>
<span style="color: #008000; text-decoration-color: #008000">ired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'tinyllama-sharegpt'</span>: 
<span style="color: #008000; text-decoration-color: #008000">'{"name":"2","start_date":"2013-01-24","event_type":["airstrike"],"province":["ghazni"],"target_group":["taliban"],</span>
<span style="color: #008000; text-decoration-color: #008000">"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":true,"noshotsfire</span>
<span style="color: #008000; text-decoration-color: #008000">d":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>
    <span style="font-weight: bold">}</span>,
    <span style="color: #008000; text-decoration-color: #008000">'start_date'</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2013</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">24</span><span style="font-weight: bold">)</span>,
    <span style="color: #008000; text-decoration-color: #008000">'province'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'ghazni'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'target_group'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'taliban'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'event_type'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'insurgentskilled'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'captureq'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killcaptureraid'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'airstrike'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'noshotsfired'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
</section>
<section id="finetuned-mistral-predictions" class="level2">
<h2 class="anchored" data-anchor-id="finetuned-mistral-predictions">Finetuned Mistral predictions</h2>
<p>As <a href="https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html#finetuning-our-model">I noted previously</a>, it was impossible to get the finetuned Mistral model working locally so I did the inference over on Modal where I could spin up a juicy A100 to make the predictions. You’ll see below that the model didn’t perform very well, failing almost all of the evaluations. This is the <code>mistral-lora-templatefree</code> model you’ll see in the charts.</p>
</section>
<section id="finetuned-openai-predictions" class="level2">
<h2 class="anchored" data-anchor-id="finetuned-openai-predictions">Finetuned OpenAI predictions</h2>
<p>I used OpenAI’s one-click finetuning service <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html#openai">to finetune the <code>gpt-3.5-turbo-1106</code> model</a>. I iterated over my dataset to generate predictions using that finetuned model using the OpenAI SDK.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;">from</span> openai <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb17-2"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb17-3"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb17-4"></span>
<span id="cb17-5">preds_test_data <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases_with_preds_2"</span>)[</span>
<span id="cb17-6">    <span class="st" style="color: #20794D;">"train"</span></span>
<span id="cb17-7">].to_list()</span>
<span id="cb17-8"></span>
<span id="cb17-9">client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(api_key<span class="op" style="color: #5E5E5E;">=</span>os.getenv(<span class="st" style="color: #20794D;">"OPENAI_API_KEY"</span>))</span>
<span id="cb17-10"></span>
<span id="cb17-11"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> preds_test_data:</span>
<span id="cb17-12">    response <span class="op" style="color: #5E5E5E;">=</span> client.chat.completions.create(</span>
<span id="cb17-13">        model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"ft:gpt-3.5-turbo-1106:SOME_MODEL_ID_GOES_HERE"</span>,</span>
<span id="cb17-14">        messages<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb17-15">            {</span>
<span id="cb17-16">                <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb17-17">                <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span>,</span>
<span id="cb17-18">            },</span>
<span id="cb17-19">            {<span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>, <span class="st" style="color: #20794D;">"content"</span>: row[<span class="st" style="color: #20794D;">"text"</span>]},</span>
<span id="cb17-20">        ],</span>
<span id="cb17-21">        temperature<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb17-22">    )</span>
<span id="cb17-23">    row[<span class="st" style="color: #20794D;">"predictions"</span>][<span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>] <span class="op" style="color: #5E5E5E;">=</span> response.choices[</span>
<span id="cb17-24">        <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb17-25">    ].message.content</span></code></pre></div>
</details>
</div>
</section>
<section id="finetuned-mistral-models-via-openpipe" class="level2">
<h2 class="anchored" data-anchor-id="finetuned-mistral-models-via-openpipe">Finetuned Mistral models (via OpenPipe)</h2>
<p>I finetuned Mistral 7B and Mistral 8x7B models using OpenPipe so as to have something reasonable to compare the other models to. As always, OpenPipe makes it pretty easy to spin up a finetuning job and get predictions.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;">from</span> openpipe <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb18-2"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb18-3"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb18-4"></span>
<span id="cb18-5">preds_test_data <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases_test_predictions_old"</span>)[</span>
<span id="cb18-6">    <span class="st" style="color: #20794D;">"train"</span></span>
<span id="cb18-7">].to_list()</span>
<span id="cb18-8"></span>
<span id="cb18-9">client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(openpipe<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"api_key"</span>: os.getenv(<span class="st" style="color: #20794D;">"OPENPIPE_API_KEY"</span>)})</span>
<span id="cb18-10"></span>
<span id="cb18-11"><span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(preds_test_data, <span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb18-12">    completion_7b <span class="op" style="color: #5E5E5E;">=</span> client.chat.completions.create(</span>
<span id="cb18-13">        model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"openpipe:twelve-pumas-invent"</span>,</span>
<span id="cb18-14">        messages<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb18-15">            {</span>
<span id="cb18-16">                <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb18-17">                <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span>,</span>
<span id="cb18-18">            },</span>
<span id="cb18-19">            {<span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>, <span class="st" style="color: #20794D;">"content"</span>: row[<span class="st" style="color: #20794D;">"text"</span>]},</span>
<span id="cb18-20">        ],</span>
<span id="cb18-21">        temperature<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb18-22">        openpipe<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"tags"</span>: {<span class="st" style="color: #20794D;">"prompt_id"</span>: <span class="st" style="color: #20794D;">"counting"</span>, <span class="st" style="color: #20794D;">"any_key"</span>: <span class="st" style="color: #20794D;">"any_value"</span>}},</span>
<span id="cb18-23">    )</span>
<span id="cb18-24"></span>
<span id="cb18-25">    row[<span class="st" style="color: #20794D;">"predictions"</span>][<span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>] <span class="op" style="color: #5E5E5E;">=</span> completion_7b.choices[<span class="dv" style="color: #AD0000;">0</span>].message.content</span>
<span id="cb18-26">    </span>
<span id="cb18-27">    <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb18-28">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">/724 rows complete"</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="finetuned-solar-llm-via-predibase" class="level2">
<h2 class="anchored" data-anchor-id="finetuned-solar-llm-via-predibase">Finetuned Solar LLM (via Predibase)</h2>
<p>Predibase announced <a href="https://predibase.com/blog/solar-llm-on-predibase-the-best-llm-for-fine-tuning">a new best-in-class model for finetuning</a>, the Solar LLM from Upstage, a week or so ago so I thought I’d try it out. The advantage of this model is that it’s trained to be good at the kinds of tasks people commonly finetune models for, like structured data extraction. As you’ll see below, it did pretty well! <a href="https://huggingface.co/upstage/SOLAR-10.7B-v1.0">The base model is this one</a>, I think, on the Hugging Face Hub so it’s available for you all to use as well.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;">from</span> predibase <span class="im" style="color: #00769E;">import</span> Predibase</span>
<span id="cb19-2"></span>
<span id="cb19-3">pb <span class="op" style="color: #5E5E5E;">=</span> Predibase(api_token<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"MY_API_TOKEN_GOES_HERE"</span>)</span>
<span id="cb19-4"></span>
<span id="cb19-5">lorax_client <span class="op" style="color: #5E5E5E;">=</span> pb.deployments.client(<span class="st" style="color: #20794D;">"solar-1-mini-chat-240612"</span>)</span>
<span id="cb19-6"></span>
<span id="cb19-7">preds_test_data <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases_test_predictions"</span>)[</span>
<span id="cb19-8">    <span class="st" style="color: #20794D;">"train"</span></span>
<span id="cb19-9">].to_list()</span>
<span id="cb19-10"></span>
<span id="cb19-11"><span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">enumerate</span>(preds_test_data, <span class="dv" style="color: #AD0000;">1</span>):</span>
<span id="cb19-12">    prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f"""You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span><span class="ch" style="color: #20794D;">\n\n</span><span class="ss" style="color: #20794D;">### Instruction:</span><span class="ch" style="color: #20794D;">\n\n</span><span class="ss" style="color: #20794D;">PRESS RELEASE TEXT: "</span><span class="sc" style="color: #5E5E5E;">{</span>row[<span class="st" style="color: #20794D;">'text'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n\n</span><span class="ss" style="color: #20794D;">### Response:"""</span></span>
<span id="cb19-13">    response <span class="op" style="color: #5E5E5E;">=</span> lorax_client.generate(prompt, adapter_id<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"isafpr/2"</span>, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">300</span>).generated_text</span>
<span id="cb19-14"></span>
<span id="cb19-15">    row[<span class="st" style="color: #20794D;">"predictions"</span>][<span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>] <span class="op" style="color: #5E5E5E;">=</span> response</span>
<span id="cb19-16"></span>
<span id="cb19-17">    <span class="cf" style="color: #003B4F;">if</span> i <span class="op" style="color: #5E5E5E;">%</span> <span class="dv" style="color: #AD0000;">100</span> <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb19-18">        <span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">/724 rows complete"</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="finetuned-llama3-predictions-via-openpipe" class="level2">
<h2 class="anchored" data-anchor-id="finetuned-llama3-predictions-via-openpipe">Finetuned Llama3 predictions (via OpenPipe)</h2>
<p>My locally finetuned Llama3 model hadn’t really worked well, but on OpenPipe the outputs seemed to look ok, so I used these predictions for the final evaluation.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="im" style="color: #00769E;">from</span> openpipe <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb20-2"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb20-3"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb20-4"></span>
<span id="cb20-5">preds_test_data <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases_with_preds_3"</span>)[</span>
<span id="cb20-6">    <span class="st" style="color: #20794D;">"train"</span></span>
<span id="cb20-7">].to_list()</span>
<span id="cb20-8"></span>
<span id="cb20-9">client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(openpipe<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"api_key"</span>: os.getenv(<span class="st" style="color: #20794D;">"OPENPIPE_API_KEY"</span>)})</span>
<span id="cb20-10"></span>
<span id="cb20-11"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> preds_test_data:</span>
<span id="cb20-12">    completion <span class="op" style="color: #5E5E5E;">=</span> client.chat.completions.create(</span>
<span id="cb20-13">        model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"openpipe:fine-steaks-taste"</span>,</span>
<span id="cb20-14">        messages<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb20-15">            {</span>
<span id="cb20-16">                <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb20-17">                <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span>,</span>
<span id="cb20-18">            },</span>
<span id="cb20-19">            {<span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>, <span class="st" style="color: #20794D;">"content"</span>: row[<span class="st" style="color: #20794D;">"text"</span>]},</span>
<span id="cb20-20">        ],</span>
<span id="cb20-21">        temperature<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb20-22">        openpipe<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"tags"</span>: {<span class="st" style="color: #20794D;">"prompt_id"</span>: <span class="st" style="color: #20794D;">"counting"</span>, <span class="st" style="color: #20794D;">"any_key"</span>: <span class="st" style="color: #20794D;">"any_value"</span>}},</span>
<span id="cb20-23">    )</span>
<span id="cb20-24"></span>
<span id="cb20-25">    row[<span class="st" style="color: #20794D;">"predictions"</span>][<span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>] <span class="op" style="color: #5E5E5E;">=</span> completion.choices[</span>
<span id="cb20-26">        <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb20-27">    ].message.content</span></code></pre></div>
</details>
</div>
<p>By the end of this process you can see we have a bunch of predictions attached to each entry in our dataset. You can view all of these <a href="https://huggingface.co/datasets/strickvl/isafpressreleases_test_predictions">in the public dataset</a> I published <a href="https://huggingface.co/datasets/strickvl/isafpressreleases_test_predictions">on the Hugging Face Hub</a>.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb21-2"></span>
<span id="cb21-3"><span class="bu" style="color: null;">print</span>(preds_test_data[<span class="dv" style="color: #AD0000;">0</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'name'</span>: <span style="color: #008000; text-decoration-color: #008000">'5'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'text'</span>: <span style="color: #008000; text-decoration-color: #008000">'2013-01-S-025\n\nKABUL, Afghanistan (Jan. 25, 2013)\nDuring a security operation in Andar district, </span>
<span style="color: #008000; text-decoration-color: #008000">Ghazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a </span>
<span style="color: #008000; text-decoration-color: #008000">group of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire </span>
<span style="color: #008000; text-decoration-color: #008000">attacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan </span>
<span style="color: #008000; text-decoration-color: #008000">National Police in Ghazni province.'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'predictions'</span>: <span style="font-weight: bold">{</span>
        <span style="color: #008000; text-decoration-color: #008000">'finetuned-llama3-7b-32k-openpipe'</span>: 
<span style="color: #008000; text-decoration-color: #008000">'{"name":"1","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["tal</span>
<span style="color: #008000; text-decoration-color: #008000">iban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"nosh</span>
<span style="color: #008000; text-decoration-color: #008000">otsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'finetuned-mistral-7b-optimised-openpipe'</span>: 
<span style="color: #008000; text-decoration-color: #008000">'{"name":"1","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["tal</span>
<span style="color: #008000; text-decoration-color: #008000">iban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"nosh</span>
<span style="color: #008000; text-decoration-color: #008000">otsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'finetuned-openai-gpt-3.5-turbo-1106'</span>: 
<span style="color: #008000; text-decoration-color: #008000">'{"name":"4","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["tal</span>
<span style="color: #008000; text-decoration-color: #008000">iban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"nosh</span>
<span style="color: #008000; text-decoration-color: #008000">otsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-3.5-turbo'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n    "name": "Taliban leader Alaudin killed in Ghazni province",\n    "start_date": </span>
<span style="color: #008000; text-decoration-color: #008000">"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span style="color: #008000; text-decoration-color: #008000">["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid": false,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-4-turbo'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n    "name": "Taliban leader Alaudin killed in Ghazni",\n    "start_date": </span>
<span style="color: #008000; text-decoration-color: #008000">"2013-01-24",\n    "event_type": ["captureandkill"],\n    "province": ["ghazni"],\n    "target_group": </span>
<span style="color: #008000; text-decoration-color: #008000">["taliban"],\n    "min_killed": 1,\n    "min_captured": 0,\n    "killq": true,\n    "captureq": false,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid": true,\n    "airstrike": false,\n    "noshotsfired": false,\n    "min_leaders_killed": 1,\n    </span>
<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'gpt-4o'</span>: <span style="color: #008000; text-decoration-color: #008000">'{\n  "name": "Taliban leader Alaudin killed in Ghazni",\n  "start_date": "2013-01-24",\n  </span>
<span style="color: #008000; text-decoration-color: #008000">"event_type": ["insurgentskilled", "captureandkill"],\n  "province": ["ghazni"],\n  "target_group": ["taliban"],\n </span>
<span style="color: #008000; text-decoration-color: #008000">"min_killed": 1,\n  "min_captured": 0,\n  "killq": true,\n  "captureq": false,\n  "killcaptureraid": true,\n  </span>
<span style="color: #008000; text-decoration-color: #008000">"airstrike": false,\n  "noshotsfired": false,\n  "min_leaders_killed": 1,\n  "min_leaders_captured": 0\n}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'mistral-lora-templatefree'</span>: <span style="color: #008000; text-decoration-color: #008000">'1'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'tinyllama-sharegpt'</span>: 
<span style="color: #008000; text-decoration-color: #008000">'{"name":"2","start_date":"2013-01-24","event_type":["airstrike"],"province":["ghazni"],"target_group":["taliban"],</span>
<span style="color: #008000; text-decoration-color: #008000">"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":true,"noshotsfire</span>
<span style="color: #008000; text-decoration-color: #008000">d":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'tinyllama-templatefree'</span>: <span style="color: #008000; text-decoration-color: #008000">'\n{"name":"Taliban leader killed in </span>
<span style="color: #008000; text-decoration-color: #008000">Ghazni","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":["taliban"</span>
<span style="color: #008000; text-decoration-color: #008000">],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":false,"airstrike":false,"noshotsf</span>
<span style="color: #008000; text-decoration-color: #008000">ired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>,
        <span style="color: #008000; text-decoration-color: #008000">'ft-solar-1-mini-chat-240612-predibase'</span>: 
<span style="color: #008000; text-decoration-color: #008000">'\n\n{"name":"2","start_date":"2013-01-24","event_type":["insurgentskilled"],"province":["ghazni"],"target_group":[</span>
<span style="color: #008000; text-decoration-color: #008000">"taliban"],"min_killed":1,"min_captured":0,"killq":true,"captureq":false,"killcaptureraid":true,"airstrike":false,"</span>
<span style="color: #008000; text-decoration-color: #008000">noshotsfired":false,"min_leaders_killed":1,"min_leaders_captured":0}'</span>
    <span style="font-weight: bold">}</span>,
    <span style="color: #008000; text-decoration-color: #008000">'start_date'</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2013</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">24</span><span style="font-weight: bold">)</span>,
    <span style="color: #008000; text-decoration-color: #008000">'province'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'ghazni'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'target_group'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'taliban'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'event_type'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'insurgentskilled'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'captureq'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killcaptureraid'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'airstrike'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'noshotsfired'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>Unfortunately the Qwen2 inference on Predibase is still not working so I’ll skip that finetuned model for the moment.</p>
<p>Now that we have predictions from seven finetuned models and three OpenAI models (to compare against), we can run our evaluations. I’ll start with a simple check to see what proportion of the predictions are even valid JSON.</p>
</section>
</section>
<section id="json-validity-test" class="level1">
<h1>JSON Validity Test</h1>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb22-2"></span>
<span id="cb22-3">dataset_with_preds <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases_test_predictions"</span>)[</span>
<span id="cb22-4">    <span class="st" style="color: #20794D;">"train"</span></span>
<span id="cb22-5">].to_list()</span></code></pre></div>
</div>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb23-2"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb23-3"></span>
<span id="cb23-4">json_aggregate_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb23-5">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-6">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-7">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-8">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-9">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-10">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-11">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-12">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-13">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-14">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb23-15">}</span>
<span id="cb23-16"></span>
<span id="cb23-17"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb23-18">    <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>]:</span>
<span id="cb23-19">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb23-20">            json.loads(row[<span class="st" style="color: #20794D;">"predictions"</span>][model])</span>
<span id="cb23-21">            json_aggregate_scores[model] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb23-22">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb23-23">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb23-24"></span>
<span id="cb23-25"><span class="co" style="color: #5E5E5E;"># print(json_aggregate_scores)</span></span>
<span id="cb23-26"></span>
<span id="cb23-27"><span class="co" style="color: #5E5E5E;"># Separate GPT models and finetuned models</span></span>
<span id="cb23-28">gpt_models <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"gpt-4o"</span>, <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>]</span>
<span id="cb23-29">finetuned_models <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb23-30">    model <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> json_aggregate_scores.keys() <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> gpt_models</span>
<span id="cb23-31">]</span>
<span id="cb23-32"></span>
<span id="cb23-33"><span class="co" style="color: #5E5E5E;"># Create lists for plotting</span></span>
<span id="cb23-34">models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(json_aggregate_scores.keys())</span>
<span id="cb23-35">scores <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(json_aggregate_scores.values())</span>
<span id="cb23-36">colors <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"#1f77b4"</span> <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">in</span> gpt_models <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"#ff7f0e"</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models]</span>
<span id="cb23-37"></span>
<span id="cb23-38"><span class="co" style="color: #5E5E5E;"># Create the plot</span></span>
<span id="cb23-39">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb23-40"></span>
<span id="cb23-41"><span class="co" style="color: #5E5E5E;"># Plot horizontal bars</span></span>
<span id="cb23-42">bars <span class="op" style="color: #5E5E5E;">=</span> ax.barh(models, scores, color<span class="op" style="color: #5E5E5E;">=</span>colors)</span>
<span id="cb23-43"></span>
<span id="cb23-44"><span class="co" style="color: #5E5E5E;"># Customize the plot</span></span>
<span id="cb23-45">ax.set_xlabel(<span class="st" style="color: #20794D;">"Number of Valid JSON Outputs"</span>)</span>
<span id="cb23-46">ax.set_title(<span class="st" style="color: #20794D;">"Valid JSON Outputs by Model"</span>)</span>
<span id="cb23-47">ax.set_xlim(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">750</span>)  <span class="co" style="color: #5E5E5E;"># Set x-axis limit to slightly above max score</span></span>
<span id="cb23-48"></span>
<span id="cb23-49"><span class="co" style="color: #5E5E5E;"># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb23-50">ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"y"</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb23-51"></span>
<span id="cb23-52"><span class="co" style="color: #5E5E5E;"># Add value labels at the end of each bar</span></span>
<span id="cb23-53"><span class="cf" style="color: #003B4F;">for</span> bar <span class="kw" style="color: #003B4F;">in</span> bars:</span>
<span id="cb23-54">    width <span class="op" style="color: #5E5E5E;">=</span> bar.get_width()</span>
<span id="cb23-55">    ax.text(</span>
<span id="cb23-56">        width, bar.get_y() <span class="op" style="color: #5E5E5E;">+</span> bar.get_height() <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>, ha<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"left"</span>, va<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center"</span></span>
<span id="cb23-57">    )</span>
<span id="cb23-58"></span>
<span id="cb23-59"><span class="co" style="color: #5E5E5E;"># Create custom legend handles</span></span>
<span id="cb23-60"><span class="im" style="color: #00769E;">from</span> matplotlib.patches <span class="im" style="color: #00769E;">import</span> Patch</span>
<span id="cb23-61"></span>
<span id="cb23-62">legend_elements <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb23-63">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#ff7f0e"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Finetuned Models"</span>),</span>
<span id="cb23-64">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#1f77b4"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"GPT Models"</span>),</span>
<span id="cb23-65">]</span>
<span id="cb23-66"></span>
<span id="cb23-67"><span class="co" style="color: #5E5E5E;"># Add a legend outside the plot</span></span>
<span id="cb23-68">ax.legend(handles<span class="op" style="color: #5E5E5E;">=</span>legend_elements, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center left"</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb23-69"></span>
<span id="cb23-70"><span class="co" style="color: #5E5E5E;"># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb23-71">plt.tight_layout()</span>
<span id="cb23-72">plt.subplots_adjust(right<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb23-73"></span>
<span id="cb23-74"><span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb23-75">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It’s already instructive to see the difference between the templatefree and the sharegpt template’s ability to generate valid JSON for the TinyLlama finetune. The OpenAI models generate valid JSON every single time, as does the finetuned Mistral and Llama3 models.</p>
<p>While writing the code to evaluate the models, I noticed that some entries were blank or had no predictions at all, so I looked into that next.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="co" style="color: #5E5E5E;"># find out how many of the predictions are None values or empty strings</span></span>
<span id="cb24-2">missing_values <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb24-3">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-4">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-5">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-6">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-7">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-8">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-9">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-10">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-11">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-12">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb24-13">}</span>
<span id="cb24-14"></span>
<span id="cb24-15"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb24-16">    <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>]:</span>
<span id="cb24-17">        <span class="cf" style="color: #003B4F;">if</span> row[<span class="st" style="color: #20794D;">"predictions"</span>][model] <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span> <span class="kw" style="color: #003B4F;">or</span> row[<span class="st" style="color: #20794D;">"predictions"</span>][model] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">""</span>:</span>
<span id="cb24-18">            missing_values[model] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb24-19"></span>
<span id="cb24-20"><span class="bu" style="color: null;">print</span>(missing_values)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'gpt-4o'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'gpt-4-turbo'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'gpt-3.5-turbo'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'tinyllama-templatefree'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'tinyllama-sharegpt'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">38</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-openai-gpt-3.5-turbo-1106'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-llama3-7b-32k-openpipe'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'mistral-lora-templatefree'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-mistral-7b-optimised-openpipe'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'ft-solar-1-mini-chat-240612-predibase'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>So were it not for the missing values, the <code>tinyllama-sharegpt</code> model would have had all 724 predictions, and valid JSON as well.</p>
<p>Now we can get into what we’re really interested in: accuracy. I’ll calculate scores for all the properties where it makes sense for us to have a score and then show the results comparing the models.</p>
<p>These are:</p>
<ul>
<li><code>start_date</code></li>
<li><code>province</code></li>
<li><code>target_group</code></li>
<li><code>event_type</code></li>
<li><code>min_killed</code></li>
<li><code>min_captured</code></li>
<li><code>killq</code></li>
<li><code>captureq</code></li>
<li><code>killcaptureraid</code></li>
<li><code>airstrike</code></li>
<li><code>noshotsfired</code></li>
<li><code>min_leaders_killed</code></li>
<li><code>min_leaders_captured</code></li>
</ul>
<p>Important note, for all these charts that follow, the total number of tasks was 724, so the numbers are out of a total of 724.</p>
</section>
<section id="start-date-accuracy" class="level1">
<h1>Start Date Accuracy</h1>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">start_date_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb25-2">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-3">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-4">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-5">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-6">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-7">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-8">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-9">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-10">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-11">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb25-12">}</span>
<span id="cb25-13"></span>
<span id="cb25-14"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb25-15">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb25-16">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb25-17">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb25-18">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb25-19">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb25-20">            <span class="cf" style="color: #003B4F;">if</span> (</span>
<span id="cb25-21">                <span class="bu" style="color: null;">type</span>(pred_dict) <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> (<span class="bu" style="color: null;">int</span>, <span class="bu" style="color: null;">float</span>)</span>
<span id="cb25-22">                <span class="kw" style="color: #003B4F;">and</span> pred_dict.get(<span class="st" style="color: #20794D;">"start_date"</span>)</span>
<span id="cb25-23">                <span class="kw" style="color: #003B4F;">and</span> (pred_dict[<span class="st" style="color: #20794D;">"start_date"</span>] <span class="op" style="color: #5E5E5E;">==</span> row[<span class="st" style="color: #20794D;">"start_date"</span>].strftime(<span class="st" style="color: #20794D;">"%Y-%m-</span><span class="sc" style="color: #5E5E5E;">%d</span><span class="st" style="color: #20794D;">"</span>))</span>
<span id="cb25-24">            ):</span>
<span id="cb25-25">                start_date_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb25-26">        <span class="cf" style="color: #003B4F;">except</span> json.JSONDecodeError:</span>
<span id="cb25-27">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb25-28"></span>
<span id="cb25-29"><span class="bu" style="color: null;">print</span>(start_date_scores)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'gpt-4o'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">527</span>,
    <span style="color: #008000; text-decoration-color: #008000">'gpt-4-turbo'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">522</span>,
    <span style="color: #008000; text-decoration-color: #008000">'gpt-3.5-turbo'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">492</span>,
    <span style="color: #008000; text-decoration-color: #008000">'tinyllama-templatefree'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">231</span>,
    <span style="color: #008000; text-decoration-color: #008000">'tinyllama-sharegpt'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">479</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-openai-gpt-3.5-turbo-1106'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">646</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-llama3-7b-32k-openpipe'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">585</span>,
    <span style="color: #008000; text-decoration-color: #008000">'mistral-lora-templatefree'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-mistral-7b-optimised-openpipe'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">636</span>,
    <span style="color: #008000; text-decoration-color: #008000">'ft-solar-1-mini-chat-240612-predibase'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">649</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb26-2"></span>
<span id="cb26-3"><span class="co" style="color: #5E5E5E;"># Separate GPT models and finetuned models</span></span>
<span id="cb26-4">gpt_models <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"gpt-4o"</span>, <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>]</span>
<span id="cb26-5">finetuned_models <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb26-6">    model <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> start_date_scores.keys() <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> gpt_models</span>
<span id="cb26-7">]</span>
<span id="cb26-8"></span>
<span id="cb26-9"><span class="co" style="color: #5E5E5E;"># Create lists for plotting</span></span>
<span id="cb26-10">models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(start_date_scores.keys())</span>
<span id="cb26-11">scores <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(start_date_scores.values())</span>
<span id="cb26-12">colors <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"#1f77b4"</span> <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">in</span> gpt_models <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"#ff7f0e"</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models]</span>
<span id="cb26-13"></span>
<span id="cb26-14"><span class="co" style="color: #5E5E5E;"># Create the plot</span></span>
<span id="cb26-15">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb26-16"></span>
<span id="cb26-17"><span class="co" style="color: #5E5E5E;"># Plot horizontal bars</span></span>
<span id="cb26-18">bars <span class="op" style="color: #5E5E5E;">=</span> ax.barh(models, scores, color<span class="op" style="color: #5E5E5E;">=</span>colors)</span>
<span id="cb26-19"></span>
<span id="cb26-20"><span class="co" style="color: #5E5E5E;"># Customize the plot</span></span>
<span id="cb26-21">ax.set_xlabel(<span class="st" style="color: #20794D;">"Number of Correct Start Dates"</span>)</span>
<span id="cb26-22">ax.set_title(<span class="st" style="color: #20794D;">"Correct Start Dates by Model"</span>)</span>
<span id="cb26-23">ax.set_xlim(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">max</span>(scores) <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">50</span>)  <span class="co" style="color: #5E5E5E;"># Set x-axis limit to slightly above max score</span></span>
<span id="cb26-24"></span>
<span id="cb26-25"><span class="co" style="color: #5E5E5E;"># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb26-26">ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"y"</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb26-27"></span>
<span id="cb26-28"><span class="co" style="color: #5E5E5E;"># Add value labels at the end of each bar</span></span>
<span id="cb26-29"><span class="cf" style="color: #003B4F;">for</span> bar <span class="kw" style="color: #003B4F;">in</span> bars:</span>
<span id="cb26-30">    width <span class="op" style="color: #5E5E5E;">=</span> bar.get_width()</span>
<span id="cb26-31">    ax.text(</span>
<span id="cb26-32">        width, bar.get_y() <span class="op" style="color: #5E5E5E;">+</span> bar.get_height() <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>, ha<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"left"</span>, va<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center"</span></span>
<span id="cb26-33">    )</span>
<span id="cb26-34"></span>
<span id="cb26-35"><span class="co" style="color: #5E5E5E;"># Create custom legend handles</span></span>
<span id="cb26-36"><span class="im" style="color: #00769E;">from</span> matplotlib.patches <span class="im" style="color: #00769E;">import</span> Patch</span>
<span id="cb26-37"></span>
<span id="cb26-38">legend_elements <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb26-39">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#ff7f0e"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Finetuned Models"</span>),</span>
<span id="cb26-40">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#1f77b4"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"GPT Models"</span>),</span>
<span id="cb26-41">]</span>
<span id="cb26-42"></span>
<span id="cb26-43"><span class="co" style="color: #5E5E5E;"># Add a legend outside the plot</span></span>
<span id="cb26-44">ax.legend(handles<span class="op" style="color: #5E5E5E;">=</span>legend_elements, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center left"</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb26-45"></span>
<span id="cb26-46"><span class="co" style="color: #5E5E5E;"># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb26-47">plt.tight_layout()</span>
<span id="cb26-48">plt.subplots_adjust(right<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb26-49"></span>
<span id="cb26-50"><span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb26-51">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Both Solar and our finetuned GPT3.5 model performed best on predicting which date the event took place. I’m surprised how poorly the OpenAI models did here, actually. And even our best model still got 75 of the dates wrong. This feels like something that I’d want to improve on. Possibly synthetic data could help, or maybe just an improvement in the finetuning prompt as well.</p>
</section>
<section id="province-accuracy" class="level1">
<h1>Province Accuracy</h1>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">province_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb27-2">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-3">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-4">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-5">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-6">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-7">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-8">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-9">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-10">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-11">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb27-12">}</span>
<span id="cb27-13"></span>
<span id="cb27-14"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb27-15">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb27-16">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb27-17">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb27-18">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb27-19">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb27-20">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"province"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb27-21">                pred_provinces <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"province"</span>]</span>
<span id="cb27-22">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_provinces, <span class="bu" style="color: null;">str</span>):</span>
<span id="cb27-23">                    pred_provinces <span class="op" style="color: #5E5E5E;">=</span> [pred_provinces]</span>
<span id="cb27-24">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">set</span>(pred_provinces) <span class="op" style="color: #5E5E5E;">==</span> <span class="bu" style="color: null;">set</span>(row[<span class="st" style="color: #20794D;">"province"</span>]):</span>
<span id="cb27-25">                    province_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb27-26">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb27-27">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb27-28"></span>
<span id="cb27-29"><span class="bu" style="color: null;">print</span>(province_scores)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'gpt-4o'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">649</span>,
    <span style="color: #008000; text-decoration-color: #008000">'gpt-4-turbo'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">645</span>,
    <span style="color: #008000; text-decoration-color: #008000">'gpt-3.5-turbo'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">595</span>,
    <span style="color: #008000; text-decoration-color: #008000">'tinyllama-templatefree'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">335</span>,
    <span style="color: #008000; text-decoration-color: #008000">'tinyllama-sharegpt'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">660</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-openai-gpt-3.5-turbo-1106'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">704</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-llama3-7b-32k-openpipe'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">707</span>,
    <span style="color: #008000; text-decoration-color: #008000">'mistral-lora-templatefree'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'finetuned-mistral-7b-optimised-openpipe'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">711</span>,
    <span style="color: #008000; text-decoration-color: #008000">'ft-solar-1-mini-chat-240612-predibase'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">704</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb28-2"></span>
<span id="cb28-3"><span class="co" style="color: #5E5E5E;"># Separate GPT models and finetuned models</span></span>
<span id="cb28-4">gpt_models <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"gpt-4o"</span>, <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>]</span>
<span id="cb28-5">finetuned_models <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb28-6">    model <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> province_scores.keys() <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> gpt_models</span>
<span id="cb28-7">]</span>
<span id="cb28-8"></span>
<span id="cb28-9"><span class="co" style="color: #5E5E5E;"># Create lists for plotting</span></span>
<span id="cb28-10">models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(province_scores.keys())</span>
<span id="cb28-11">scores <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(province_scores.values())</span>
<span id="cb28-12">colors <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"#1f77b4"</span> <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">in</span> gpt_models <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"#ff7f0e"</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models]</span>
<span id="cb28-13"></span>
<span id="cb28-14"><span class="co" style="color: #5E5E5E;"># Create the plot</span></span>
<span id="cb28-15">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb28-16"></span>
<span id="cb28-17"><span class="co" style="color: #5E5E5E;"># Plot horizontal bars</span></span>
<span id="cb28-18">bars <span class="op" style="color: #5E5E5E;">=</span> ax.barh(models, scores, color<span class="op" style="color: #5E5E5E;">=</span>colors)</span>
<span id="cb28-19"></span>
<span id="cb28-20"><span class="co" style="color: #5E5E5E;"># Customize the plot</span></span>
<span id="cb28-21">ax.set_xlabel(<span class="st" style="color: #20794D;">"Number of Correct Provinces"</span>)</span>
<span id="cb28-22">ax.set_title(<span class="st" style="color: #20794D;">"Correct Provinces by Model"</span>)</span>
<span id="cb28-23">ax.set_xlim(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">max</span>(scores) <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">50</span>)  <span class="co" style="color: #5E5E5E;"># Set x-axis limit to slightly above max score</span></span>
<span id="cb28-24"></span>
<span id="cb28-25"><span class="co" style="color: #5E5E5E;"># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb28-26">ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"y"</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb28-27"></span>
<span id="cb28-28"><span class="co" style="color: #5E5E5E;"># Add value labels at the end of each bar</span></span>
<span id="cb28-29"><span class="cf" style="color: #003B4F;">for</span> bar <span class="kw" style="color: #003B4F;">in</span> bars:</span>
<span id="cb28-30">    width <span class="op" style="color: #5E5E5E;">=</span> bar.get_width()</span>
<span id="cb28-31">    ax.text(</span>
<span id="cb28-32">        width, bar.get_y() <span class="op" style="color: #5E5E5E;">+</span> bar.get_height() <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>, ha<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"left"</span>, va<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center"</span></span>
<span id="cb28-33">    )</span>
<span id="cb28-34"></span>
<span id="cb28-35"><span class="co" style="color: #5E5E5E;"># Create custom legend handles</span></span>
<span id="cb28-36"><span class="im" style="color: #00769E;">from</span> matplotlib.patches <span class="im" style="color: #00769E;">import</span> Patch</span>
<span id="cb28-37"></span>
<span id="cb28-38">legend_elements <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb28-39">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#ff7f0e"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Finetuned Models"</span>),</span>
<span id="cb28-40">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#1f77b4"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"GPT Models"</span>),</span>
<span id="cb28-41">]</span>
<span id="cb28-42"></span>
<span id="cb28-43"><span class="co" style="color: #5E5E5E;"># Add a legend outside the plot</span></span>
<span id="cb28-44">ax.legend(handles<span class="op" style="color: #5E5E5E;">=</span>legend_elements, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center left"</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb28-45"></span>
<span id="cb28-46"><span class="co" style="color: #5E5E5E;"># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb28-47">plt.tight_layout()</span>
<span id="cb28-48">plt.subplots_adjust(right<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb28-49"></span>
<span id="cb28-50"><span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb28-51">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In what will become a theme, the finetuned models actually outperform the OpenAI models, only making a few mistakes. Once again I’m surprised how poorly GPT3.5 did on this task.</p>
</section>
<section id="target-group-accuracy" class="level1">
<h1>Target Group Accuracy</h1>
<p>Here there are potentially multiple groups mentioned as target group so I’ll give a score out of 1 of how many of the groups the model predicted were correct.</p>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb29-2"></span>
<span id="cb29-3">target_group_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb29-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb29-14">}</span>
<span id="cb29-15"></span>
<span id="cb29-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb29-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb29-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb29-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb29-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb29-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb29-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"target_group"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb29-23">                pred_groups <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"target_group"</span>]</span>
<span id="cb29-24">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_groups, <span class="bu" style="color: null;">str</span>):</span>
<span id="cb29-25">                    pred_groups <span class="op" style="color: #5E5E5E;">=</span> [pred_groups]</span>
<span id="cb29-26">                correct_groups <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"target_group"</span>]</span>
<span id="cb29-27">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(correct_groups, <span class="bu" style="color: null;">str</span>):</span>
<span id="cb29-28">                    correct_groups <span class="op" style="color: #5E5E5E;">=</span> [correct_groups]</span>
<span id="cb29-29">                <span class="cf" style="color: #003B4F;">if</span> correct_groups:</span>
<span id="cb29-30">                    score <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(<span class="bu" style="color: null;">set</span>(pred_groups) <span class="op" style="color: #5E5E5E;">&amp;</span> <span class="bu" style="color: null;">set</span>(correct_groups)) <span class="op" style="color: #5E5E5E;">/</span> <span class="bu" style="color: null;">len</span>(</span>
<span id="cb29-31">                        <span class="bu" style="color: null;">set</span>(correct_groups)</span>
<span id="cb29-32">                    )</span>
<span id="cb29-33">                    target_group_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> score</span>
<span id="cb29-34">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb29-35">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb29-36"></span>
<span id="cb29-37"><span class="co" style="color: #5E5E5E;"># Separate GPT models and finetuned models</span></span>
<span id="cb29-38">gpt_models <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"gpt-4o"</span>, <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>]</span>
<span id="cb29-39">finetuned_models <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb29-40">    model <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> target_group_scores.keys() <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> gpt_models</span>
<span id="cb29-41">]</span>
<span id="cb29-42"></span>
<span id="cb29-43"><span class="co" style="color: #5E5E5E;"># Create lists for plotting</span></span>
<span id="cb29-44">models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(target_group_scores.keys())</span>
<span id="cb29-45">scores <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(target_group_scores.values())</span>
<span id="cb29-46">colors <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"#1f77b4"</span> <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">in</span> gpt_models <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"#ff7f0e"</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models]</span>
<span id="cb29-47"></span>
<span id="cb29-48"><span class="co" style="color: #5E5E5E;"># Create the plot</span></span>
<span id="cb29-49">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb29-50"></span>
<span id="cb29-51"><span class="co" style="color: #5E5E5E;"># Plot horizontal bars</span></span>
<span id="cb29-52">bars <span class="op" style="color: #5E5E5E;">=</span> ax.barh(models, scores, color<span class="op" style="color: #5E5E5E;">=</span>colors)</span>
<span id="cb29-53"></span>
<span id="cb29-54"><span class="co" style="color: #5E5E5E;"># Customize the plot</span></span>
<span id="cb29-55">ax.set_xlabel(<span class="st" style="color: #20794D;">"Target Group Accuracy Score"</span>)</span>
<span id="cb29-56">ax.set_title(<span class="st" style="color: #20794D;">"Target Group Accuracy by Model"</span>)</span>
<span id="cb29-57">ax.set_xlim(<span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">max</span>(scores) <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">50</span>)  <span class="co" style="color: #5E5E5E;"># Set x-axis limit to slightly above max score</span></span>
<span id="cb29-58"></span>
<span id="cb29-59"><span class="co" style="color: #5E5E5E;"># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb29-60">ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"y"</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb29-61"></span>
<span id="cb29-62"><span class="co" style="color: #5E5E5E;"># Add value labels at the end of each bar</span></span>
<span id="cb29-63"><span class="cf" style="color: #003B4F;">for</span> bar <span class="kw" style="color: #003B4F;">in</span> bars:</span>
<span id="cb29-64">    width <span class="op" style="color: #5E5E5E;">=</span> bar.get_width()</span>
<span id="cb29-65">    ax.text(</span>
<span id="cb29-66">        width,</span>
<span id="cb29-67">        bar.get_y() <span class="op" style="color: #5E5E5E;">+</span> bar.get_height() <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb29-68">        <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">"</span>,</span>
<span id="cb29-69">        ha<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"left"</span>,</span>
<span id="cb29-70">        va<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center"</span>,</span>
<span id="cb29-71">    )</span>
<span id="cb29-72"></span>
<span id="cb29-73"><span class="co" style="color: #5E5E5E;"># Create custom legend handles</span></span>
<span id="cb29-74"><span class="im" style="color: #00769E;">from</span> matplotlib.patches <span class="im" style="color: #00769E;">import</span> Patch</span>
<span id="cb29-75"></span>
<span id="cb29-76">legend_elements <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb29-77">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#ff7f0e"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Finetuned Models"</span>),</span>
<span id="cb29-78">    Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#1f77b4"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"GPT Models"</span>),</span>
<span id="cb29-79">]</span>
<span id="cb29-80"></span>
<span id="cb29-81"><span class="co" style="color: #5E5E5E;"># Add a legend outside the plot</span></span>
<span id="cb29-82">ax.legend(handles<span class="op" style="color: #5E5E5E;">=</span>legend_elements, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center left"</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb29-83"></span>
<span id="cb29-84"><span class="co" style="color: #5E5E5E;"># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb29-85">plt.tight_layout()</span>
<span id="cb29-86">plt.subplots_adjust(right<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb29-87"></span>
<span id="cb29-88"><span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb29-89">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-28-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Finetuned models doing significantly better than OpenAI for target group identification. I suspect that this would degrade if we added some new groups who weren’t in the training data (as I’d written about <a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">in my last post</a>.)</p>
</section>
<section id="event-type-accuracy" class="level1">
<h1>Event Type Accuracy</h1>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb30-2"><span class="im" style="color: #00769E;">from</span> matplotlib.patches <span class="im" style="color: #00769E;">import</span> Patch</span>
<span id="cb30-3"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Dict, Union</span>
<span id="cb30-4"></span>
<span id="cb30-5"></span>
<span id="cb30-6"><span class="kw" style="color: #003B4F;">def</span> create_accuracy_chart(</span>
<span id="cb30-7">    scores: Dict[<span class="bu" style="color: null;">str</span>, Union[<span class="bu" style="color: null;">int</span>, <span class="bu" style="color: null;">float</span>]], title: <span class="bu" style="color: null;">str</span>, xlabel: <span class="bu" style="color: null;">str</span></span>
<span id="cb30-8">) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb30-9">    <span class="co" style="color: #5E5E5E;"># Separate GPT models and finetuned models</span></span>
<span id="cb30-10">    gpt_models <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"gpt-4o"</span>, <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>]</span>
<span id="cb30-11">    finetuned_models <span class="op" style="color: #5E5E5E;">=</span> [model <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> scores.keys() <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> gpt_models]</span>
<span id="cb30-12"></span>
<span id="cb30-13">    <span class="co" style="color: #5E5E5E;"># Create lists for plotting</span></span>
<span id="cb30-14">    models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(scores.keys())</span>
<span id="cb30-15">    scores_list <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(scores.values())</span>
<span id="cb30-16">    colors <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"#1f77b4"</span> <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">in</span> gpt_models <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"#ff7f0e"</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models]</span>
<span id="cb30-17"></span>
<span id="cb30-18">    <span class="co" style="color: #5E5E5E;"># Create the plot</span></span>
<span id="cb30-19">    fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb30-20"></span>
<span id="cb30-21">    <span class="co" style="color: #5E5E5E;"># Plot horizontal bars</span></span>
<span id="cb30-22">    bars <span class="op" style="color: #5E5E5E;">=</span> ax.barh(models, scores_list, color<span class="op" style="color: #5E5E5E;">=</span>colors)</span>
<span id="cb30-23"></span>
<span id="cb30-24">    <span class="co" style="color: #5E5E5E;"># Customize the plot</span></span>
<span id="cb30-25">    ax.set_xlabel(xlabel)</span>
<span id="cb30-26">    ax.set_title(title)</span>
<span id="cb30-27">    ax.set_xlim(</span>
<span id="cb30-28">        <span class="dv" style="color: #AD0000;">0</span>, <span class="bu" style="color: null;">max</span>(scores_list) <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">50</span></span>
<span id="cb30-29">    )  <span class="co" style="color: #5E5E5E;"># Set x-axis limit to slightly above max score</span></span>
<span id="cb30-30"></span>
<span id="cb30-31">    <span class="co" style="color: #5E5E5E;"># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb30-32">    ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"y"</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb30-33"></span>
<span id="cb30-34">    <span class="co" style="color: #5E5E5E;"># Add value labels at the end of each bar</span></span>
<span id="cb30-35">    <span class="cf" style="color: #003B4F;">for</span> bar <span class="kw" style="color: #003B4F;">in</span> bars:</span>
<span id="cb30-36">        width <span class="op" style="color: #5E5E5E;">=</span> bar.get_width()</span>
<span id="cb30-37">        ax.text(</span>
<span id="cb30-38">            width,</span>
<span id="cb30-39">            bar.get_y() <span class="op" style="color: #5E5E5E;">+</span> bar.get_height() <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb30-40">            <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">"</span> <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(width, <span class="bu" style="color: null;">float</span>) <span class="cf" style="color: #003B4F;">else</span> <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>,</span>
<span id="cb30-41">            ha<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"left"</span>,</span>
<span id="cb30-42">            va<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center"</span>,</span>
<span id="cb30-43">        )</span>
<span id="cb30-44"></span>
<span id="cb30-45">    <span class="co" style="color: #5E5E5E;"># Create custom legend handles</span></span>
<span id="cb30-46">    legend_elements <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb30-47">        Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#ff7f0e"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Finetuned Models"</span>),</span>
<span id="cb30-48">        Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#1f77b4"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"GPT Models"</span>),</span>
<span id="cb30-49">    ]</span>
<span id="cb30-50"></span>
<span id="cb30-51">    <span class="co" style="color: #5E5E5E;"># Add a legend outside the plot</span></span>
<span id="cb30-52">    ax.legend(handles<span class="op" style="color: #5E5E5E;">=</span>legend_elements, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center left"</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb30-53"></span>
<span id="cb30-54">    <span class="co" style="color: #5E5E5E;"># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb30-55">    plt.tight_layout()</span>
<span id="cb30-56">    plt.subplots_adjust(right<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb30-57"></span>
<span id="cb30-58">    <span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb30-59">    plt.show()</span>
<span id="cb30-60"></span>
<span id="cb30-61"></span>
<span id="cb30-62">event_type_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb30-63">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-64">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-65">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-66">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-67">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-68">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-69">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-70">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-71">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-72">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb30-73">}</span>
<span id="cb30-74"></span>
<span id="cb30-75"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb30-76">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb30-77">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb30-78">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb30-79">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb30-80">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb30-81">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"event_type"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb30-82">                pred_types <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"event_type"</span>]</span>
<span id="cb30-83">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_types, <span class="bu" style="color: null;">str</span>):</span>
<span id="cb30-84">                    pred_types <span class="op" style="color: #5E5E5E;">=</span> [pred_types]</span>
<span id="cb30-85">                correct_types <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"event_type"</span>]</span>
<span id="cb30-86">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(correct_types, <span class="bu" style="color: null;">str</span>):</span>
<span id="cb30-87">                    correct_types <span class="op" style="color: #5E5E5E;">=</span> [correct_types]</span>
<span id="cb30-88">                <span class="cf" style="color: #003B4F;">if</span> correct_types:</span>
<span id="cb30-89">                    score <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(<span class="bu" style="color: null;">set</span>(pred_types) <span class="op" style="color: #5E5E5E;">&amp;</span> <span class="bu" style="color: null;">set</span>(correct_types)) <span class="op" style="color: #5E5E5E;">/</span> <span class="bu" style="color: null;">len</span>(</span>
<span id="cb30-90">                        <span class="bu" style="color: null;">set</span>(correct_types)</span>
<span id="cb30-91">                    )</span>
<span id="cb30-92">                    event_type_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> score</span>
<span id="cb30-93">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb30-94">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb30-95"></span>
<span id="cb30-96">create_accuracy_chart(</span>
<span id="cb30-97">    scores<span class="op" style="color: #5E5E5E;">=</span>event_type_scores,</span>
<span id="cb30-98">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Event Type Accuracy by Model"</span>,</span>
<span id="cb30-99">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Event Type Accuracy Score"</span>,</span>
<span id="cb30-100">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Event type is actually one of the hardest categories since there are actually some semantically overlapping categories and it’s sometimes even hard for a human annotator, but once again the finetuned models do pretty well.</p>
</section>
<section id="accuracy-for-min_killed" class="level1">
<h1>Accuracy for <code>min_killed</code></h1>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb31-2"></span>
<span id="cb31-3">min_killed_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb31-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb31-14">}</span>
<span id="cb31-15"></span>
<span id="cb31-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb31-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb31-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb31-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb31-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb31-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb31-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"min_killed"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb31-23">                pred_min_killed <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"min_killed"</span>]</span>
<span id="cb31-24">                correct_min_killed <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"min_killed"</span>]</span>
<span id="cb31-25">                <span class="cf" style="color: #003B4F;">if</span> pred_min_killed <span class="op" style="color: #5E5E5E;">==</span> correct_min_killed:</span>
<span id="cb31-26">                    min_killed_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb31-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb31-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb31-29"></span>
<span id="cb31-30">create_accuracy_chart(</span>
<span id="cb31-31">    scores<span class="op" style="color: #5E5E5E;">=</span>min_killed_scores,</span>
<span id="cb31-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'min_killed' Accuracy by Model"</span>,</span>
<span id="cb31-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'min_killed' Accuracy Score"</span></span>
<span id="cb31-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>For these number estimations, suddenly the playing fields gets leveled between the finetuned and the OpenAI models. Mistral still comes out on top, but not by much! And it’s impressive how the OpenAI models do really well at this. I suspect this is because of the whole section in the prompt which explained the rubric that was used for annotating the examples:</p>
<blockquote class="blockquote">
<p>Annotation notes: A ‘faciliator’ is not a leader. If a press release states that ‘insurgents’ were detained without further details, assign a minimum number of two detained. Interpret ‘a couple’ as two. Interpret ‘several’ as at least three, even though it may sometimes refer to seven or eight. Classify the terms ‘a few’, ‘some’, ‘a group’, ‘a small group’, and ‘multiple’ as denoting at least three, even if they sometimes refer to larger numbers. Choose the smaller number if no other information is available in the press release to come up with a minimally acceptable figure. Interpret ‘numerous’ and ‘a handful’ as at least four, and ‘a large number’ as at least five.</p>
</blockquote>
</section>
<section id="accuracy-for-min_captured" class="level1">
<h1>Accuracy for <code>min_captured</code></h1>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb32-2"></span>
<span id="cb32-3">min_captured_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb32-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb32-14">}</span>
<span id="cb32-15"></span>
<span id="cb32-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb32-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb32-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb32-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb32-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb32-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb32-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"min_captured"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb32-23">                pred_min_captured <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"min_captured"</span>]</span>
<span id="cb32-24">                correct_min_captured <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"min_captured"</span>]</span>
<span id="cb32-25">                <span class="cf" style="color: #003B4F;">if</span> pred_min_captured <span class="op" style="color: #5E5E5E;">==</span> correct_min_captured:</span>
<span id="cb32-26">                    min_captured_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb32-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb32-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb32-29"></span>
<span id="cb32-30">create_accuracy_chart(</span>
<span id="cb32-31">    scores<span class="op" style="color: #5E5E5E;">=</span>min_captured_scores,</span>
<span id="cb32-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'min_captured' Accuracy by Model"</span>,</span>
<span id="cb32-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'min_captured' Accuracy Score"</span></span>
<span id="cb32-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="accuracy-for-killq" class="level1">
<h1>Accuracy for <code>killq</code></h1>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb33-2"></span>
<span id="cb33-3">killq_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb33-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb33-14">}</span>
<span id="cb33-15"></span>
<span id="cb33-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb33-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb33-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb33-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb33-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb33-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb33-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"killq"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb33-23">                pred_killq <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"killq"</span>]</span>
<span id="cb33-24">                correct_killq <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"killq"</span>]</span>
<span id="cb33-25">                <span class="cf" style="color: #003B4F;">if</span> pred_killq <span class="op" style="color: #5E5E5E;">==</span> correct_killq:</span>
<span id="cb33-26">                    killq_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb33-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb33-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb33-29"></span>
<span id="cb33-30">create_accuracy_chart(</span>
<span id="cb33-31">    scores<span class="op" style="color: #5E5E5E;">=</span>killq_scores,</span>
<span id="cb33-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'killq' Accuracy by Model"</span>,</span>
<span id="cb33-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'killq' Accuracy Score"</span></span>
<span id="cb33-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-32-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I’d expect really high accuracy for these boolean attributes, and basically almost all the models were able to give this. Still, our finetuned Mistral still beats out GPT-4o best score.</p>
</section>
<section id="accuracy-for-captureq" class="level1">
<h1>Accuracy for <code>captureq</code></h1>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb34-2"></span>
<span id="cb34-3">captureq_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb34-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb34-14">}</span>
<span id="cb34-15"></span>
<span id="cb34-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb34-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb34-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb34-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb34-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb34-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb34-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"captureq"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb34-23">                pred_captureq <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"captureq"</span>]</span>
<span id="cb34-24">                correct_captureq <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"captureq"</span>]</span>
<span id="cb34-25">                <span class="cf" style="color: #003B4F;">if</span> pred_captureq <span class="op" style="color: #5E5E5E;">==</span> correct_captureq:</span>
<span id="cb34-26">                    captureq_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb34-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb34-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb34-29"></span>
<span id="cb34-30">create_accuracy_chart(</span>
<span id="cb34-31">    scores<span class="op" style="color: #5E5E5E;">=</span>captureq_scores,</span>
<span id="cb34-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'captureq' Accuracy by Model"</span>,</span>
<span id="cb34-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'captureq' Accuracy Score"</span></span>
<span id="cb34-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="accuracy-for-killcaptureraid" class="level1">
<h1>Accuracy for <code>killcaptureraid</code></h1>
<div class="cell" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb35-2"></span>
<span id="cb35-3">killcaptureraid_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb35-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb35-14">}</span>
<span id="cb35-15"></span>
<span id="cb35-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb35-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb35-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb35-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb35-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb35-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb35-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"killcaptureraid"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb35-23">                pred_killcaptureraid <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"killcaptureraid"</span>]</span>
<span id="cb35-24">                correct_killcaptureraid <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"killcaptureraid"</span>]</span>
<span id="cb35-25">                <span class="cf" style="color: #003B4F;">if</span> pred_killcaptureraid <span class="op" style="color: #5E5E5E;">==</span> correct_killcaptureraid:</span>
<span id="cb35-26">                    killcaptureraid_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb35-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb35-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb35-29"></span>
<span id="cb35-30">create_accuracy_chart(</span>
<span id="cb35-31">    scores<span class="op" style="color: #5E5E5E;">=</span>killcaptureraid_scores,</span>
<span id="cb35-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'killcaptureraid' Accuracy by Model"</span>,</span>
<span id="cb35-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'killcaptureraid' Accuracy Score"</span></span>
<span id="cb35-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This is another attribute where it’s clear the lack of signposting in the prompts to the OpenAI models put them at a disadvantage. The term ‘kill-capture raid’ is a term of art and it was used in a specific way for the labelling. OpenAI knows nothing about how I made those calls, which explains why they performed so poorly here.</p>
</section>
<section id="accuracy-for-airstrike" class="level1">
<h1>Accuracy for <code>airstrike</code></h1>
<div class="cell" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb36-2"></span>
<span id="cb36-3">airstrike_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb36-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb36-14">}</span>
<span id="cb36-15"></span>
<span id="cb36-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb36-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb36-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb36-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb36-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb36-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb36-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"airstrike"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb36-23">                pred_airstrike <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"airstrike"</span>]</span>
<span id="cb36-24">                correct_airstrike <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"airstrike"</span>]</span>
<span id="cb36-25">                <span class="cf" style="color: #003B4F;">if</span> pred_airstrike <span class="op" style="color: #5E5E5E;">==</span> correct_airstrike:</span>
<span id="cb36-26">                    airstrike_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb36-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb36-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb36-29"></span>
<span id="cb36-30">create_accuracy_chart(</span>
<span id="cb36-31">    scores<span class="op" style="color: #5E5E5E;">=</span>airstrike_scores,</span>
<span id="cb36-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'airstrike' Accuracy by Model"</span>,</span>
<span id="cb36-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'airstrike' Accuracy Score"</span></span>
<span id="cb36-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-35-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="accuracy-for-noshotsfired" class="level1">
<h1>Accuracy for <code>noshotsfired</code></h1>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb37-2"></span>
<span id="cb37-3">noshotsfired_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb37-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb37-14">}</span>
<span id="cb37-15"></span>
<span id="cb37-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb37-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb37-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb37-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb37-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb37-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb37-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"noshotsfired"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb37-23">                pred_noshotsfired <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"noshotsfired"</span>]</span>
<span id="cb37-24">                correct_noshotsfired <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"noshotsfired"</span>]</span>
<span id="cb37-25">                <span class="cf" style="color: #003B4F;">if</span> pred_noshotsfired <span class="op" style="color: #5E5E5E;">==</span> correct_noshotsfired:</span>
<span id="cb37-26">                    noshotsfired_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb37-27">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb37-28">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb37-29"></span>
<span id="cb37-30">create_accuracy_chart(</span>
<span id="cb37-31">    scores<span class="op" style="color: #5E5E5E;">=</span>noshotsfired_scores,</span>
<span id="cb37-32">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'noshotsfired' Accuracy by Model"</span>,</span>
<span id="cb37-33">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"'noshotsfired' Accuracy Score"</span></span>
<span id="cb37-34">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-36-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I’m not quite sure why the OpenAI models are performing in the reverse order of what you’d expect. Recall that the <code>noshotsfired</code> attribute refers to whether the press release states that no shots were fired during a particular raid / event. (For a certain period the press releases were keen to mention this and it was a metric that was particularly useful for ISAF as a public relations gimmick.)</p>
<p>I can think of some semi-anthropomorphizing ways to explain this around how the GPT-4 class of models were ‘overthinking’ the label, but more investigation would be needed to really understand this.</p>
</section>
<section id="accuracy-for-min_leaders_killed" class="level1">
<h1>Accuracy for <code>min_leaders_killed</code></h1>
<div class="cell" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb38-2"></span>
<span id="cb38-3">min_leaders_killed_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb38-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb38-14">}</span>
<span id="cb38-15"></span>
<span id="cb38-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb38-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb38-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb38-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb38-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb38-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb38-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"min_leaders_killed"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb38-23">                pred_min_leaders_killed <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"min_leaders_killed"</span>]</span>
<span id="cb38-24">                correct_min_leaders_killed <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"min_leaders_killed"</span>]</span>
<span id="cb38-25">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_min_leaders_killed, <span class="bu" style="color: null;">int</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="bu" style="color: null;">isinstance</span>(correct_min_leaders_killed, <span class="bu" style="color: null;">int</span>):</span>
<span id="cb38-26">                    <span class="cf" style="color: #003B4F;">if</span> pred_min_leaders_killed <span class="op" style="color: #5E5E5E;">==</span> correct_min_leaders_killed:</span>
<span id="cb38-27">                        min_leaders_killed_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb38-28">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb38-29">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb38-30"></span>
<span id="cb38-31">total_entries <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(dataset_with_preds)</span>
<span id="cb38-32"></span>
<span id="cb38-33">create_accuracy_chart(</span>
<span id="cb38-34">    scores<span class="op" style="color: #5E5E5E;">=</span>min_leaders_killed_scores,</span>
<span id="cb38-35">    title<span class="op" style="color: #5E5E5E;">=</span><span class="ss" style="color: #20794D;">f"Min Leaders Killed Accuracy by Model (out of </span><span class="sc" style="color: #5E5E5E;">{</span>total_entries<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> entries)"</span>,</span>
<span id="cb38-36">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Min Leaders Killed Accuracy Score"</span></span>
<span id="cb38-37">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-37-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We often hear about how LLMs are bad with numbers, how they <a href="https://gramener.com/llmrandom/">default to certain values</a> and so on, so I was surprised to see such high scores across the board for this task. I imagine this is something that everyone has been trying to improve and it shows. Still, though, our finetuned models do best.</p>
</section>
<section id="accuracy-for-min_leaders_captured" class="level1">
<h1>Accuracy for <code>min_leaders_captured</code></h1>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb39-2"></span>
<span id="cb39-3">min_leaders_captured_scores <span class="op" style="color: #5E5E5E;">=</span> {</span>
<span id="cb39-4">    <span class="st" style="color: #20794D;">"gpt-4o"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-5">    <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-6">    <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-7">    <span class="st" style="color: #20794D;">"tinyllama-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-8">    <span class="st" style="color: #20794D;">"tinyllama-sharegpt"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-9">    <span class="st" style="color: #20794D;">"finetuned-openai-gpt-3.5-turbo-1106"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-10">    <span class="st" style="color: #20794D;">"finetuned-llama3-7b-32k-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-11">    <span class="st" style="color: #20794D;">"mistral-lora-templatefree"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-12">    <span class="st" style="color: #20794D;">"finetuned-mistral-7b-optimised-openpipe"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-13">    <span class="st" style="color: #20794D;">"ft-solar-1-mini-chat-240612-predibase"</span>: <span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb39-14">}</span>
<span id="cb39-15"></span>
<span id="cb39-16"><span class="cf" style="color: #003B4F;">for</span> row <span class="kw" style="color: #003B4F;">in</span> dataset_with_preds:</span>
<span id="cb39-17">    <span class="cf" style="color: #003B4F;">for</span> model_name, pred <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"predictions"</span>].items():</span>
<span id="cb39-18">        <span class="cf" style="color: #003B4F;">if</span> <span class="kw" style="color: #003B4F;">not</span> pred:</span>
<span id="cb39-19">            <span class="cf" style="color: #003B4F;">continue</span></span>
<span id="cb39-20">        <span class="cf" style="color: #003B4F;">try</span>:</span>
<span id="cb39-21">            pred_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(pred)</span>
<span id="cb39-22">            <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_dict, <span class="bu" style="color: null;">dict</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="st" style="color: #20794D;">"min_leaders_captured"</span> <span class="kw" style="color: #003B4F;">in</span> pred_dict:</span>
<span id="cb39-23">                pred_min_leaders_captured <span class="op" style="color: #5E5E5E;">=</span> pred_dict[<span class="st" style="color: #20794D;">"min_leaders_captured"</span>]</span>
<span id="cb39-24">                correct_min_leaders_captured <span class="op" style="color: #5E5E5E;">=</span> row[<span class="st" style="color: #20794D;">"min_leaders_captured"</span>]</span>
<span id="cb39-25">                <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(pred_min_leaders_captured, <span class="bu" style="color: null;">int</span>) <span class="kw" style="color: #003B4F;">and</span> <span class="bu" style="color: null;">isinstance</span>(correct_min_leaders_captured, <span class="bu" style="color: null;">int</span>):</span>
<span id="cb39-26">                    <span class="cf" style="color: #003B4F;">if</span> pred_min_leaders_captured <span class="op" style="color: #5E5E5E;">==</span> correct_min_leaders_captured:</span>
<span id="cb39-27">                        min_leaders_captured_scores[model_name] <span class="op" style="color: #5E5E5E;">+=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb39-28">        <span class="cf" style="color: #003B4F;">except</span> (json.JSONDecodeError, <span class="pp" style="color: #AD0000;">KeyError</span>, <span class="pp" style="color: #AD0000;">TypeError</span>):</span>
<span id="cb39-29">            <span class="cf" style="color: #003B4F;">pass</span></span>
<span id="cb39-30"></span>
<span id="cb39-31">total_entries <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(dataset_with_preds)</span>
<span id="cb39-32"></span>
<span id="cb39-33">create_accuracy_chart(</span>
<span id="cb39-34">    scores<span class="op" style="color: #5E5E5E;">=</span>min_leaders_captured_scores,</span>
<span id="cb39-35">    title<span class="op" style="color: #5E5E5E;">=</span><span class="ss" style="color: #20794D;">f"Min Leaders Captured Accuracy by Model (out of </span><span class="sc" style="color: #5E5E5E;">{</span>total_entries<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;"> entries)"</span>,</span>
<span id="cb39-36">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Min Leaders Captured Accuracy Score"</span></span>
<span id="cb39-37">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-38-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="final-aggregate-scores-for-the-models" class="level1">
<h1>Final aggregate scores for the models</h1>
<p>Let’s add all these individual competency scores up, average them out and get final scores for how well our models do on accuracy.</p>
<div class="cell" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="co" style="color: #5E5E5E;"># adapt the function slightly</span></span>
<span id="cb40-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb40-3"><span class="im" style="color: #00769E;">from</span> matplotlib.patches <span class="im" style="color: #00769E;">import</span> Patch</span>
<span id="cb40-4"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Dict, Union</span>
<span id="cb40-5"></span>
<span id="cb40-6"></span>
<span id="cb40-7"><span class="kw" style="color: #003B4F;">def</span> create_aggregate_accuracy_chart(</span>
<span id="cb40-8">    scores: Dict[<span class="bu" style="color: null;">str</span>, Union[<span class="bu" style="color: null;">int</span>, <span class="bu" style="color: null;">float</span>]], title: <span class="bu" style="color: null;">str</span>, xlabel: <span class="bu" style="color: null;">str</span></span>
<span id="cb40-9">) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb40-10">    <span class="co" style="color: #5E5E5E;"># Separate GPT models and finetuned models</span></span>
<span id="cb40-11">    gpt_models <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"gpt-4o"</span>, <span class="st" style="color: #20794D;">"gpt-4-turbo"</span>, <span class="st" style="color: #20794D;">"gpt-3.5-turbo"</span>]</span>
<span id="cb40-12">    finetuned_models <span class="op" style="color: #5E5E5E;">=</span> [model <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> scores.keys() <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> gpt_models]</span>
<span id="cb40-13"></span>
<span id="cb40-14">    <span class="co" style="color: #5E5E5E;"># Create lists for plotting</span></span>
<span id="cb40-15">    models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(scores.keys())</span>
<span id="cb40-16">    scores_list <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(scores.values())</span>
<span id="cb40-17">    colors <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"#1f77b4"</span> <span class="cf" style="color: #003B4F;">if</span> model <span class="kw" style="color: #003B4F;">in</span> gpt_models <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"#ff7f0e"</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models]</span>
<span id="cb40-18"></span>
<span id="cb40-19">    <span class="co" style="color: #5E5E5E;"># Create the plot</span></span>
<span id="cb40-20">    fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb40-21"></span>
<span id="cb40-22">    <span class="co" style="color: #5E5E5E;"># Plot horizontal bars</span></span>
<span id="cb40-23">    bars <span class="op" style="color: #5E5E5E;">=</span> ax.barh(models, scores_list, color<span class="op" style="color: #5E5E5E;">=</span>colors)</span>
<span id="cb40-24"></span>
<span id="cb40-25">    <span class="co" style="color: #5E5E5E;"># Customize the plot</span></span>
<span id="cb40-26">    ax.set_xlabel(xlabel)</span>
<span id="cb40-27">    ax.set_title(title)</span>
<span id="cb40-28">    ax.set_xlim(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">100</span>)  <span class="co" style="color: #5E5E5E;"># Set x-axis limit to 100</span></span>
<span id="cb40-29"></span>
<span id="cb40-30">    <span class="co" style="color: #5E5E5E;"># Reduce font size for y-axis labels (model names)</span></span>
<span id="cb40-31">    ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"y"</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb40-32"></span>
<span id="cb40-33">    <span class="co" style="color: #5E5E5E;"># Add value labels at the end of each bar</span></span>
<span id="cb40-34">    <span class="cf" style="color: #003B4F;">for</span> bar <span class="kw" style="color: #003B4F;">in</span> bars:</span>
<span id="cb40-35">        width <span class="op" style="color: #5E5E5E;">=</span> bar.get_width()</span>
<span id="cb40-36">        ax.text(</span>
<span id="cb40-37">            width,</span>
<span id="cb40-38">            bar.get_y() <span class="op" style="color: #5E5E5E;">+</span> bar.get_height() <span class="op" style="color: #5E5E5E;">/</span> <span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb40-39">            <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">"</span> <span class="cf" style="color: #003B4F;">if</span> <span class="bu" style="color: null;">isinstance</span>(width, <span class="bu" style="color: null;">float</span>) <span class="cf" style="color: #003B4F;">else</span> <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>width<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>,</span>
<span id="cb40-40">            ha<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"left"</span>,</span>
<span id="cb40-41">            va<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center"</span>,</span>
<span id="cb40-42">        )</span>
<span id="cb40-43"></span>
<span id="cb40-44">    <span class="co" style="color: #5E5E5E;"># Create custom legend handles</span></span>
<span id="cb40-45">    legend_elements <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb40-46">        Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#ff7f0e"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Finetuned Models"</span>),</span>
<span id="cb40-47">        Patch(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"#1f77b4"</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"GPT Models"</span>),</span>
<span id="cb40-48">    ]</span>
<span id="cb40-49"></span>
<span id="cb40-50">    <span class="co" style="color: #5E5E5E;"># Add a legend outside the plot</span></span>
<span id="cb40-51">    ax.legend(handles<span class="op" style="color: #5E5E5E;">=</span>legend_elements, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"center left"</span>, bbox_to_anchor<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb40-52"></span>
<span id="cb40-53">    <span class="co" style="color: #5E5E5E;"># Adjust layout to prevent clipping and make room for the legend</span></span>
<span id="cb40-54">    plt.tight_layout()</span>
<span id="cb40-55">    plt.subplots_adjust(right<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.85</span>)</span>
<span id="cb40-56"></span>
<span id="cb40-57">    <span class="co" style="color: #5E5E5E;"># Show the plot</span></span>
<span id="cb40-58">    plt.show()</span>
<span id="cb40-59"></span>
<span id="cb40-60"><span class="co" style="color: #5E5E5E;"># List of all the score dictionaries</span></span>
<span id="cb40-61">score_dicts <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb40-62">    start_date_scores,</span>
<span id="cb40-63">    province_scores,</span>
<span id="cb40-64">    target_group_scores,</span>
<span id="cb40-65">    event_type_scores,</span>
<span id="cb40-66">    min_killed_scores,</span>
<span id="cb40-67">    min_captured_scores,</span>
<span id="cb40-68">    killq_scores,</span>
<span id="cb40-69">    captureq_scores,</span>
<span id="cb40-70">    killcaptureraid_scores,</span>
<span id="cb40-71">    airstrike_scores,</span>
<span id="cb40-72">    noshotsfired_scores,</span>
<span id="cb40-73">    min_leaders_killed_scores,</span>
<span id="cb40-74">    min_leaders_captured_scores,</span>
<span id="cb40-75">]</span>
<span id="cb40-76"></span>
<span id="cb40-77"><span class="co" style="color: #5E5E5E;"># Get the list of models</span></span>
<span id="cb40-78">models <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(start_date_scores.keys())</span>
<span id="cb40-79"></span>
<span id="cb40-80"><span class="co" style="color: #5E5E5E;"># Initialize the aggregate scores dictionary</span></span>
<span id="cb40-81">aggregate_scores <span class="op" style="color: #5E5E5E;">=</span> {model: <span class="dv" style="color: #AD0000;">0</span> <span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models}</span>
<span id="cb40-82"></span>
<span id="cb40-83"><span class="co" style="color: #5E5E5E;"># Calculate the aggregate score for each model</span></span>
<span id="cb40-84"><span class="cf" style="color: #003B4F;">for</span> model <span class="kw" style="color: #003B4F;">in</span> models:</span>
<span id="cb40-85">    total_score <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb40-86">    <span class="cf" style="color: #003B4F;">for</span> score_dict <span class="kw" style="color: #003B4F;">in</span> score_dicts:</span>
<span id="cb40-87">        total_score <span class="op" style="color: #5E5E5E;">+=</span> score_dict[model]</span>
<span id="cb40-88">    aggregate_scores[model] <span class="op" style="color: #5E5E5E;">=</span> (total_score <span class="op" style="color: #5E5E5E;">/</span> <span class="bu" style="color: null;">len</span>(score_dicts)) <span class="op" style="color: #5E5E5E;">/</span> <span class="bu" style="color: null;">len</span>(dataset_with_preds) <span class="op" style="color: #5E5E5E;">*</span> <span class="dv" style="color: #AD0000;">100</span></span>
<span id="cb40-89"></span>
<span id="cb40-90"><span class="co" style="color: #5E5E5E;"># Create the aggregate score chart</span></span>
<span id="cb40-91">create_aggregate_accuracy_chart(</span>
<span id="cb40-92">    scores<span class="op" style="color: #5E5E5E;">=</span>aggregate_scores,</span>
<span id="cb40-93">    title<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Aggregate Accuracy Score by Model (0-100 Scale)"</span>,</span>
<span id="cb40-94">    xlabel<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Aggregate Accuracy Score"</span>,</span>
<span id="cb40-95">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation_files/figure-html/cell-39-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Surprising even me, the finetuned models beat out the GPT-class models from OpenAI. Actually even TinyLlama beats out GPT 3.5 Turbo!</p>
<p>Our top performer was Mistral-7B (finetuned on OpenPipe), very closely followed by Solar LLM and Llama3-7B. Just looking at the scores above it seems like anyone finetuning models for structured data extraction would do well to start of with Mistral-7B, Solar 7B or Llama3-7B and then see which performs best, though with the caveat that they might all be more or less the same for accuracy. There are probably different tradeoffs when it comes to serving the model and the efficiency and latency there, but still these three do really well.</p>
<p>I think if I were to stuff a few more examples into the prompt (as well a bit more explanation and rules) I could get the OpenAI models to perform even better, but at a certain point you have to remember all the things that having your own finetuned model brings you:</p>
<ul>
<li>data privacy (by not sending your confidential information to OpenAI)</li>
<li>smaller models most likely means better performance (though I still have to test and prove that out)</li>
<li>more control overall</li>
<li>cost improvements</li>
</ul>
<p>On the cost, it’s a bit hard to make that comparison or claim right now, especially given the economies of scale that the large cloud providers can rely on, but in a real-world use case where you were building this model for repeated inference over a long-term time frame, then you would have a better chance of having the cost argument make sense, particularly since the only way to make the OpenAI inference calls better to stuff them full of examples and extra explanation, significantly bloating the cost-per-query.</p>
<p>That said, there are some real tradeoffs that come up when finetuning a model, and I’ll get to some of those in my concluding thoughts.</p>
</section>
<section id="finetuning-works-a-charm-but" class="level1">
<h1>Finetuning works a charm, but…</h1>
<p>First off I’m so pleased that the oft-repeated “finetune your model and get better performance than with GPT-4” actually turned out to be true! And not only was it true, but it was true with <em>relatively</em> little tweaks and adaptations. Remember all the above models are the first finetunes I made with the data I brought. I basically used just the default values for everything and so it worked out of the box.</p>
<p>For any further work I’ll focus on the Solar, Llama3 and Mistral 7B models which performed best. I used cloud finetuning services to finetune the best performing versions of those models, so I’ll want to get that all working locally as well.</p>
<section id="evals-were-a-pain-this-time-round" class="level2">
<h2 class="anchored" data-anchor-id="evals-were-a-pain-this-time-round">Evals were a pain (this time round)…</h2>
<p>Most of the evaluation work is represented here in this notebook, and that was perhaps the seeds of my own misfortune. I had some models that worked locally, and then a bunch of other models deployed in different environments and with different services.</p>
<p>Not only that, but it was pretty slow to iterate through the 724 row so my test data (which the models hadn’t seen during finetuning, just to be clear) since I implemented it fairly naively.</p>
<p>If I were to now make some updates to the models, or get them working locally, I’d really want to make sure that I have a way to run these evals locally as well. Moreover, I’d want a way to run a subset of the evals (i.e.&nbsp;on a slice of the data) and then at some point switch that out so that they could run across all the data.</p>
<p>All of this is completely within the realm of possible, but for this round I was more focused on getting the results than I was about making the process repeatable and/or efficient. I know I can’t run all the models concurrently on the same machine, so maybe the way forward is simply to have a reliable cloud GPU provider like Modal where I can farm out these evaluations. I had a really good experience with them when I used them, so that’s probably the way forward there.</p>
<p>In general, it was also painful having the models living in different places. I had to remember so many things. In any ideal world, you want a standard interface for inference to all your models, especially if they’re for the same use case or project. It’s convenient that my finetuned GPT3.5 is automatically deployed and served by OpenAI, and the same goes for Llama3 and Solar or Mistral, but I want a single place where I can see them all. Until now I hadn’t really seen this project or problem as being so much about MLOps, but when you have multiple models in play and you’re finetuning and updating them and data is changing all the time, then you’ll need a way of managing all this.</p>
<p>This is funny to me since <a href="https://zenml.io">I work at an MLOps company</a> – we build an open-source MLOps framework that helps you set up a platform – but I hadn’t anticipated it’d reach this point where I’d need something like a ZenML so soon. This is, of course, one of the major tradeoffs of finetuning LLMs, in that you have to manage all this <em>stuff</em> in order to make it work reliably and repeatably. Even at this early stage of my project, it’s clear that you need a way to keep everything straight without making mistakes.</p>
</section>
<section id="but-evals-give-me-a-way-to-know-if-im-making-progress" class="level2">
<h2 class="anchored" data-anchor-id="but-evals-give-me-a-way-to-know-if-im-making-progress">…but evals give me a way to know if I’m making progress</h2>
<p>Even though the evaluations were somewhat painful to implement (at least in the form of this Jupyter notebook), they have given me an amazing gift in that I now have a task-specific way to know whether any of the improvements or refinements to either the training data or to the model are helping move me forward. Without this I’d essentially be flying blind.</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<p>I had originally thought and suggested that I’d want to train multiple models to be super-specialists in their field, so for example to have one model that was really good at estimating how many people were captured in a particular event. Seeing the performance of my models, I’m not sure that’s the obvious next step for this project, or if I’d really be able to boost the accuracy by a significant amount by taking that approach.</p>
<p>This project is all about accuracy, so it’s possible that I might want to try that out, but for now I’m still exploring all the different phases of the LLM finetuning process so I’ll put the submodels idea on the backburner.</p>
<p>The first obvious next step is to run some evaluations for the non-accuracy-related tests mentioned <a href="https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html">in my last blog</a>. For example, I’d like to see how it performs with out of domain data (i.e. completely made up data about something completely different).</p>
<p>The other next step is to get into some of the details around model serving. I’d like to take my top three performers and dive into how LLM model serving is done. I’m familiar with non-LLM model serving and some of the ways people do that through my work, but LLM serving has it’s own tricks, tradeoffs and tools and I’m eager to learn more about those.</p>
<p>If this was a problem that I was deeply invested in solving beyond these already excellent results, I’d probably also want to dive into the areas where my LLMs struggled. So I’d take all the places where my LLMs failed to get the answer correct, load them up into some kind of web interface like Lilac or Argilla and really inspect my data further. Understanding the failure scenarios will probably do more for the accuracy than any tweaking of the finetuning parameters or the like.</p>
<p>For now, I’m just happy the finetuned models beat GPT-4!</p>


</section>
</section>

 ]]></description>
  <category>nlp</category>
  <category>afghanistan</category>
  <category>llms</category>
  <category>miniproject</category>
  <category>finetuning</category>
  <category>isafpr</category>
  <category>evaluation</category>
  <guid>https://mlops.systems/posts/2024-07-01-full-finetuned-model-evaluation.html</guid>
  <pubDate>Sun, 30 Jun 2024 22:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/finetuned-model-eval.png" medium="image" type="image/png" height="118" width="144"/>
</item>
<item>
  <title>How to think about creating a dataset for LLM finetuning evaluation</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html</link>
  <description><![CDATA[ 




<p>I <a href="https://mlops.systems/posts/2024-06-17-one-click-finetuning.html">previously</a> experimented with one-click LLM finetuning providers and now is a good time to return to the core of the matter: evaluating how well all these fine-tuned models and experiments are faring. I have a gut feeling that my fine-tuned models did pretty well, but we’re not in the business of gut feeling so I’m hoping to be able to put some real numbers down to either prove or disprove this hypothesis.</p>
<p>As a quick reminder if you didn’t read <a href="https://mlops.systems/#category=isafpr">any of the previous posts in the series</a>, I’m building a model that can take a press release text like this:</p>
<blockquote class="blockquote">
<p>“2011-11-S-011 ISAF Joint Command - Afghanistan For Immediate Release KABUL, Afghanistan (Nov.&nbsp;7, 2011) — A combined Afghan and coalition security force conducted an operation in search of a Haqqani facilitator in Argo district, Badakshan province. The facilitator coordinates suicide attacks with other insurgent leaders in the area. During the operation, a local national male failed to comply with repeated verbal warnings and displayed hostile intent toward the security force. The security force engaged the individual, resulting in his death. The security force confiscated a shotgun and intelligence linking the local national to the Haqqani network. The security force also detained two suspected insurgents during the operation.”</p>
</blockquote>
<p>…and then turn it into structured data (i.e.&nbsp;a JSON object) like this:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><span class="fu" style="color: #4758AB;">{</span></span>
<span id="cb1-2">    <span class="er" style="color: #AD0000;">'name'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">'</span><span class="dv" style="color: #AD0000;">1</span><span class="er" style="color: #AD0000;">'</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-3">    <span class="er" style="color: #AD0000;">'start_date'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">'</span><span class="dv" style="color: #AD0000;">2011-11-07</span><span class="er" style="color: #AD0000;">'</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-4">    <span class="er" style="color: #AD0000;">'event_type'</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span><span class="er" style="color: #AD0000;">'captureandkill'</span><span class="ot" style="color: #003B4F;">]</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-5">    <span class="er" style="color: #AD0000;">'province'</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span><span class="er" style="color: #AD0000;">'badakhshan'</span><span class="ot" style="color: #003B4F;">]</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-6">    <span class="er" style="color: #AD0000;">'target_group'</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span><span class="er" style="color: #AD0000;">'haqqani'</span><span class="ot" style="color: #003B4F;">]</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-7">    <span class="er" style="color: #AD0000;">'min_killed'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">1</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-8">    <span class="er" style="color: #AD0000;">'min_captured'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">2</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-9">    <span class="er" style="color: #AD0000;">'killq'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">True</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-10">    <span class="er" style="color: #AD0000;">'captureq'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">True</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-11">    <span class="er" style="color: #AD0000;">'killcaptureraid'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">True</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-12">    <span class="er" style="color: #AD0000;">'airstrike'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">False</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-13">    <span class="er" style="color: #AD0000;">'noshotsfired'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">False</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-14">    <span class="er" style="color: #AD0000;">'min_leaders_killed'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">0</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb1-15">    <span class="er" style="color: #AD0000;">'min_leaders_captured'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb1-16"><span class="fu" style="color: #4758AB;">}</span></span></code></pre></div>
<p>I’ve now fine-tuned several models and I want to get a sense of how good these are. I showcased <a href="https://mlops.systems/posts/2024-06-03-isafpr-evaluating-baseline.html">some initial baseline evaluations</a> using OpenAI’s <code>gpt-4-turbo</code> but I want to pit model against model now.</p>
<p>I’m also interested in teasing out some of the edge cases where I know I struggled as a human annotator. (I <a href="https://huggingface.co/datasets/strickvl/isafpressreleases">released the dataset</a> for this project publicly on the Hugging Face Hub and also was responsible for annotating every single item so I know the data intimately.) I can even consider using the hard examples to generate some synthetic data to boost performance on those edge cases, but that’s a task for much later on.</p>
<p>This blogpost is a prose overview of some of the evaluations I’m adding to my suite of tests (and why I’m adding them). I learned a lot from <a href="https://hamel.dev/blog/posts/evals/">Hamel Husain’s “Your AI Product Needs Evals” blogpost</a> and if you’re interested in this I’d recommend reading it and then actually implementing his suggestions.</p>
<section id="core-evaluations-for-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="core-evaluations-for-accuracy">Core evaluations for accuracy</h2>
<p>The most important measurement to start with is just a pure “did the LLM make a correct prediction or not?” If I was doing all these evaluations manually myself, I’d take a look at the example above, for example, and ask myself “was the start date of the event mentioned in the blogpost really ‘2011-11-07’ as predicted by the model?” and “did the event take place in Badakhshan province?” and “were the Haqqanis the group targeted?”</p>
<p>It’s fairly straightforward to make these determinations when comparing each property one by one. I can then repeat this over every example in my test slice of my dataset and take an average if I want a single aggregate figure, or I can get individual figures for dates, provinces, target groups and so on (to know if maybe there’s one part of the prediction it struggles with most).</p>
</section>
<section id="out-of-domain-data" class="level2">
<h2 class="anchored" data-anchor-id="out-of-domain-data">Out of domain data</h2>
<p>The ISAF mission has come to an end, so I don’t have to worry too much about new data and having to adapt to a continuously changing world, but it is possible that some smaller groups weren’t well represented in the training data (for predicting the target group, for example) so I want to know how well my model does with data it hasn’t seen.</p>
<p>My prompt passes in the schema for the data and I encourage it to follow the schema in its response, but if there’s a new group will it add the new group to the schema? I can write an evaluation to test this.</p>
<p>Another edge case is the possibility that a press release doesn’t follow the standard format. Having read them all, I know that the vast majority are pretty formulaic, but sometimes there is a special event or incident which caused the author of the press release to depart from the standard formula. I want to know that my model will:</p>
<ol type="a">
<li>not just make something up so as to have <em>some</em> kind of JSON response even if the press release is about someone’s birthday party</li>
<li>even better, produce some sort of error code or blank response when this happens.</li>
</ol>
<p>I can use examples of this out of domain data to see what happens, and put a value to how often my model will just hallucinate something out of nothing. This will be important since the name of the game for this model is accuracy.</p>
</section>
<section id="gradations-of-some-a-few-many" class="level2">
<h2 class="anchored" data-anchor-id="gradations-of-some-a-few-many">Gradations of ‘some’, ‘a few’, ‘many’</h2>
<p>The press releases try to give some information without actually giving too much. Indeed, when I published <a href="https://www.afghanistan-analysts.org/en/special-reports/a-knock-on-the-door-22-months-of-isaf-press-releases/">my report</a> on the press releases back in 2011, ISAF even <a href="https://www.dvidshub.net/news/78455/isaf-responds-use-aan-news-releases-study">issued a press release</a> (!) in which they stated that:</p>
<blockquote class="blockquote">
<p>“Release of information in insurgent warfare is not always made public, so studies based on the use of press releases can be both incomplete and problematic. […] Authoritative research cannot be conducted through mere analysis of press releases, since the release of information through such releases is, by design, incomplete.”</p>
</blockquote>
<p>So reading the press releases is very much an exercise in reading between the lines. In the press release cited earlier, all the numbers are specific (“a facilitator”, “a male”, “two insurgents”) so it’s easy to put numbers to how many were killed or captured. In many of the press releases, particularly during times where raids were being conducted at a very high tempo, you have to just take assumptions about what their words mean and assign minimum values to those words.</p>
<p>So ‘a couple’ meant at least two, but ‘a few’ meant 3 or more. Similarly ‘dozens’ means multiple dozens so that meant a minimum value of at least 24. From the original report:</p>
<blockquote class="blockquote">
<p>“If a press release said that ‘insurgents’ were detained, without further details, we assigned that incident as having a minimum number of 2 detained (since we could not be sure of more). ‘A couple’ we took to mean 2. ‘Several’ we took to mean at least 3, even though on other occasions ‘several’ was used to refer to 7 or 8. Other terms we classified as denoting at least 3 included: ‘a few’, ‘some’, ‘a group’, ‘a small group’ and ‘multiple’; these terms sometimes were used to refer to far larger numbers but we chose the smaller number (if no other information was available in the press release) in order to come up with a minimally acceptable figure. ‘Numerous’ and ‘a handful’ we took to mean at least 4, and ‘a large number’ at least 5.”</p>
</blockquote>
<p>The reports mostly referred to events that had taken place that day or the day before, but occasionally they’d refer to events that took place “last Thursday” or “last week” and so then you’d have to know what day the press release was issued and then make calculations accordingly. For this backwards-referring time assignations, I’m particularly interested (read: concerned!) to know how well my LLMs did. Whatever score we get, it’s probably fixable with a bit of manual parsing and logic, but we need to know if there’s a problem or not first.</p>
<p>Generally speaking there were province names assigned to incidents (all but 23, to be precise) but when they weren’t, then the LLM has to work back from a village name, potentially, or just specify that an incident took place in southern Afghanistan or Afghanistan as a whole. On a few occasions, the press release actually made an error, stating that village X or Y was in a particular province, when this was incorrect and it was in a different province. So for this, would we expect the LLM to assign the event to the correct province for that village, or just retain the error in the press release?</p>
<p>Sometimes a press release might refer to an event having taken place “in the provincial capital of X province” but without mentioning that city by name. So the LLM will have to have some knowledge of these things and I want to test how well it performs with this.</p>
<p>These might seem like tiny errors to get wrong, but for a project like this (where my report was making some strong accusations and drawing certain conclusions based on the data), it wouldn’t do to get things factually wrong. For an internal-facing LLM-powered chatbot, the price of mistakes is minimal, but for a project like this, there are potentially far more serious consequences which is why I’m putting together such detailed evaluations.</p>
</section>
<section id="spelling-variation" class="level2">
<h2 class="anchored" data-anchor-id="spelling-variation">Spelling variation</h2>
<p>Another issue with some of the press releases is that they use a variety of spellings for the same locations or names of individuals. For some things — province names, for example — it makes sense to standardise on a fixed naming convention but for others it’s not always clear what to do. So our evaluation should ensure that common variations of certain provinces or designations or names are captured correctly by the LLM output.</p>
</section>
<section id="complex-stories" class="level2">
<h2 class="anchored" data-anchor-id="complex-stories">Complex stories</h2>
<p>Some stories are very complicated and there may be no correct way to assign numbers, for example, to the text that was published. In those cases when annotating I often just left the minimum numbers at zero even though we know that <em>something</em> happened. Would the LLM also make the same call? What is the threshold for deciding not to take a chance on making a guess?</p>
</section>
<section id="next-step" class="level2">
<h2 class="anchored" data-anchor-id="next-step">Next step</h2>
<p>The obvious next step is to actually code up these evaluation criteria and run those across our fine-tuned LLMs as well as the API-driven proprietary ones. I’ll be working on that over the coming days. Luckily, I did most of the work to identify all of the above when I first wrote the report so there isn’t much ground that needs to be broken so much as just sitting down and getting it done.</p>


</section>

 ]]></description>
  <category>llms</category>
  <category>finetuning</category>
  <category>isafpr</category>
  <category>afghanistan</category>
  <category>datasets</category>
  <category>evaluation</category>
  <category>miniproject</category>
  <guid>https://mlops.systems/posts/2024-06-25-evaluation-finetuning-manual-dataset.html</guid>
  <pubDate>Mon, 24 Jun 2024 22:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/eval-preview.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>One-click LLM finetuning with Predibase, OpenPipe and OpenAI</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2024-06-17-one-click-finetuning.html</link>
  <description><![CDATA[ 




<p>The <a href="https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html">last post in this series</a> showed that finetuning an LLM needn’t be particularly difficult. I used <code>axolotl</code> to produce finetuned versions of Llama3, Mistral and TinyLlama models. During the course we were given a bunch of credits by various companies in the LLM and finetuning space. Among those were credits from some finetuning-as-a-service companies and I thought now might be a good time to try out these services now that I’ve done the process manually a few times.</p>
<p>I picked three to try out: Predibase, OpenPipe and OpenAI. All were surprisingly similar in the approach they took. I’ll give a few details on the experience for each and how they compare to each other. With all the services, the process was roughly the same as when I did it manually:</p>
<ol type="1">
<li>Upload custom data</li>
<li>Select some hyperparameters</li>
<li>Start the finetuning</li>
<li>Try the model</li>
</ol>
<p>The step I had the most trouble with was the custom data upload, since each provider wanted the data in a different format. Converting the data from the Pydantic models I had previously created was not a huge deal, but I wasn’t sure about the tradeoffs that I was making (or that were being made for me) by converting my data into these formats.</p>
<section id="predibase" class="level2">
<h2 class="anchored" data-anchor-id="predibase">Predibase</h2>
<p>I started with <a href="https://predibase.com/">Predibase</a> since I had enjoyed the talk Travis Addair had given during the course. Predibase is famous for their work on LORA adapters, particularly their demonstration of <a href="https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4">Lora Land</a> where they gave some examples of how finetuned LORA models / adapters could outperform GPT-4.</p>
<p>Predibase requires that the data you upload has certain column names depending on the task you select for the finetuning. At the moment they have instruction tuning and text completion as their two tasks, but it wasn’t clear to me which to select. (They also have <a href="https://colab.research.google.com/drive/1r505Aq_SWZdaSkBIs3ovh4F8c36DHwAh?usp=sharing">a Colab notebook</a> to help with constructing splits from your data.)</p>
<p>Once your data is ready and validated, you can select the model you want to finetune along with a few other hyperparameters. This is the full extent of what you can set from the web UI:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/predibase-hyperparameters.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Screenshot of Predibase website and the hyperparameters you can set</figcaption><p></p>
</figure>
</div>
<p>There’s also a helpful dataset preview pane to give a final sanity check for your data, to make sure that the inputs and outputs look what you’d expect:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/predibase-dataset-preview.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Screenshot of Predibase website and the dataset preview pane</figcaption><p></p>
</figure>
</div>
<p>As you’ll read in a little bit, this feature helps catch potentially costly errors before you start the finetuning process.</p>
<p>Once you click the button to start the training, there isn’t a great deal of information available to you beyond (eventually) a loss curve that you can see. I chose to finetune Qwen2 in Predibase and this took about 53 minutes using an A-100 GPU accelerator.</p>
<p>Once your model is ready, you can prompt the model in the UI, or using their REST API / Python SDK. They give code snippets prefilled with some dummy text that you can easily try out locally. Let’s show that here, but before you can run your inference query you have to first deploy the model. I hadn’t expected this extra step, and it takes a while to spin up since it’s deploying the adaptor along with the base model it was finetuned alongside. My Qwen2 model has a context window of 131072 tokens and supposedly would cost $3.90 per hour that it was up (as a dedicated deployment).</p>
<p>Let’s show the results we got:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">pr1 <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"""2011-11-S-011 ISAF Joint Command - Afghanistan For Immediate Release</span></span>
<span id="cb1-2"><span class="st" style="color: #20794D;">      KABUL, Afghanistan (Nov. 7, 2011) — A combined Afghan and coalition</span></span>
<span id="cb1-3"><span class="st" style="color: #20794D;">      security force conducted an operation in search of a Haqqani facilitator</span></span>
<span id="cb1-4"><span class="st" style="color: #20794D;">      in Argo district, Badakshan province. The facilitator coordinates suicide</span></span>
<span id="cb1-5"><span class="st" style="color: #20794D;">      attacks with other insurgent leaders in the area. During the operation, a</span></span>
<span id="cb1-6"><span class="st" style="color: #20794D;">      local national male failed to comply with repeated verbal warnings and</span></span>
<span id="cb1-7"><span class="st" style="color: #20794D;">      displayed hostile intent toward the security force. The security force</span></span>
<span id="cb1-8"><span class="st" style="color: #20794D;">      engaged the individual, resulting in his death. The security force</span></span>
<span id="cb1-9"><span class="st" style="color: #20794D;">      confiscated a shotgun and intelligence linking the local national to the</span></span>
<span id="cb1-10"><span class="st" style="color: #20794D;">      Haqqani network. The security force also detained two suspected insurgents during the operation."""</span></span>
<span id="cb1-11"></span>
<span id="cb1-12">prompt <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f"""You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span></span>
<span id="cb1-13"></span>
<span id="cb1-14"><span class="ss" style="color: #20794D;">### Instruction:</span></span>
<span id="cb1-15"></span>
<span id="cb1-16"><span class="ss" style="color: #20794D;">PRESS RELEASE TEXT: '</span><span class="sc" style="color: #5E5E5E;">{</span>pr1<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span></span>
<span id="cb1-17"></span>
<span id="cb1-18"><span class="ss" style="color: #20794D;">### Response:</span></span>
<span id="cb1-19"><span class="ss" style="color: #20794D;">"""</span></span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">from</span> predibase <span class="im" style="color: #00769E;">import</span> Predibase</span>
<span id="cb2-3"></span>
<span id="cb2-4">pb <span class="op" style="color: #5E5E5E;">=</span> Predibase(api_token<span class="op" style="color: #5E5E5E;">=</span>os.getenv(<span class="st" style="color: #20794D;">"PREDIBASE_API_KEY"</span>))</span>
<span id="cb2-5"><span class="co" style="color: #5E5E5E;"># pb = Predibase(api_token="")</span></span>
<span id="cb2-6"></span>
<span id="cb2-7">lorax_client <span class="op" style="color: #5E5E5E;">=</span> pb.deployments.client(<span class="st" style="color: #20794D;">"isafpr"</span>)</span>
<span id="cb2-8"><span class="bu" style="color: null;">print</span>(lorax_client.generate(prompt, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>).generated_text)</span></code></pre></div>
</div>
<p>Unfortunately my Predibase model deployment was still ‘initializing’ after a couple of hours of spinning up. I didn’t want to leave that dedicated deployment up and running overnight, so I just deleted the deployment and I’ll try to get this going at a later date. So no inference sample to show you for this one. I’m very curious to see how Qwen2 did, though!</p>
</section>
<section id="openai" class="level2">
<h2 class="anchored" data-anchor-id="openai">OpenAI</h2>
<p>I was actually surprised that this is even a thing that people do or that is offered by OpenAI. Currently you’re able to finetune three versions of GPT3.5 as well as <code>babbage-002</code> and <code>davinci-002</code>. In the OpenAI presentation during the course they mentioned that they were working to make it possible to finetune GPT4 as well, but no timeline was given on this.</p>
<p>So why would someone want to finetune GPT3.5? I think there are some problems that are sufficiently complex or of a specific nature where the OpenAI GPT family shines where you might want to squeeze out a final last bit of performance and where the open LLMs just aren’t there yet.</p>
<p>The OpenAI models are sort of the antithesis of an ‘open’ model and nothing about the finetuning process lent itself to disabusing you of that idea. This was the UI to fill in in order to finetune a model and as you can see there aren’t really too many options available to you.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/openai-finetuning-ui.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">OpenAI Finetuning UI</figcaption><p></p>
</figure>
</div>
<p>Supposedly the data you upload (options for train as well as a separate test set here) will never be used by OpenAI to train their models but you have to just trust them on that front.</p>
<p><img src="https://mlops.systems/posts/images/openai-finetuning-ui-2.png" class="img-fluid" alt="UI medatada during finetuning 1"> <img src="https://mlops.systems/posts/images/openai-finetuning-ui-3.png" class="img-fluid" alt="UI medatada during finetuning 2"></p>
<p>As with Predibase, during finetuning you don’t have access to any logs or even too much feedback during training. You get a loss curve and a few scraps of metadata and that’s it. The training took around 90 minutes to run and then you’re able to prompt the model to see how it works, using the standard OpenAI interface and methods you’re used to:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;">from</span> openai <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb3-2"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb3-3"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb3-4"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb3-5"></span>
<span id="cb3-6">client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(api_key<span class="op" style="color: #5E5E5E;">=</span>os.getenv(<span class="st" style="color: #20794D;">"OPENAI_API_KEY"</span>))</span>
<span id="cb3-7"></span>
<span id="cb3-8">response <span class="op" style="color: #5E5E5E;">=</span> client.chat.completions.create(</span>
<span id="cb3-9">    model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"ft:gpt-3.5-turbo-SOME_EXTRA_STUFF_HERE_FOR_MY_MODEL"</span>,</span>
<span id="cb3-10">    messages<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb3-11">        {</span>
<span id="cb3-12">            <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb3-13">            <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span></span>
<span id="cb3-14">        },</span>
<span id="cb3-15">        {</span>
<span id="cb3-16">            <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>,</span>
<span id="cb3-17">            <span class="st" style="color: #20794D;">"content"</span>: pr1</span>
<span id="cb3-18">        }</span>
<span id="cb3-19">    ],</span>
<span id="cb3-20">    temperature<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb3-21">)</span>
<span id="cb3-22"></span>
<span id="cb3-23"><span class="bu" style="color: null;">print</span>(json.loads(response.choices[<span class="dv" style="color: #AD0000;">0</span>].message.content))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'name'</span>: <span style="color: #008000; text-decoration-color: #008000">'1'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'start_date'</span>: <span style="color: #008000; text-decoration-color: #008000">'2011-11-07'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'event_type'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'captureandkill'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'province'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'badakhshan'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'target_group'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'haqqani'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'captureq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killcaptureraid'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'airstrike'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'noshotsfired'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>They also give you an interface to see the response of the base model side-by-side against the finetuned model:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/openai-finetuning-ui-4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Side-by-side UI of base model and finetuned model inference</figcaption><p></p>
</figure>
</div>
<p>As you can see, it’s done pretty well! It stuck to the JSON structure, and the extracted metadata looks good. Of course, since this is a GPT3.5 model, there’s no way to now download this model and run it locally. You’re hostage to OpenAI, to being online, etc etc. Not a scenario I’d like to be in, so I don’t think I’ll pursue this much further and rather use my OpenAI credits for other purposes.</p>
<p>All that said, I do think there might be some scenarios where only the OpenAI models are reliable enough to use (be that in terms of accuracy or with sufficient guardrails) and there were people in the course who were in this boat.</p>
</section>
<section id="openpipe" class="level2">
<h2 class="anchored" data-anchor-id="openpipe">OpenPipe</h2>
<p>This was the last one-click provider I tried. As with the others, you upload your data first. When I tried this, I got a fairly opaque error message but I guess the format I’d used was incompatible. OpenPipe uses the same format as OpenAI does, it turns out, but it handles the train/test split itself so you just have to set your data up in a single file (unlike with OpenAI where they can take two separate files).</p>
<p>The interface for finetuning your model was somehow the most threadbare of all:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/openpipe-finetuning-ui.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">OpenPipe finetuning UI</figcaption><p></p>
</figure>
</div>
<p>Moreover, the selection of base models on which to finetune were also pretty slim: Llama3, Mistral, Mixtral and two OpenAI GPT3.5 models. I was surprised by the estimate of how much it’d cost to finetune the model (around $30 USD) but by limiting the number of options available to the user the path forward really was pretty easy.</p>
<p>You get no single morsel of information during the finetuning process and for me it took a while for the job to even start working, but after an hour or two (I can’t be sure as I left my desk) you get a model out the other end. At this point you can export the weights or just try out the model with a Python call.</p>
<p>Helpfully, the web UI gives you code snippets you can use for Python, Javascript and cURL, and the snippets even have your prompt pre-filled with an example from your dataset. This was a nice touch.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># pip install openpipe</span></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="im" style="color: #00769E;">from</span> openpipe <span class="im" style="color: #00769E;">import</span> OpenAI</span>
<span id="cb4-4"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb4-5"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb4-6"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb4-7"></span>
<span id="cb4-8">client <span class="op" style="color: #5E5E5E;">=</span> OpenAI(</span>
<span id="cb4-9">  openpipe<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"api_key"</span>: os.getenv(<span class="st" style="color: #20794D;">"OPENPIPE_API_KEY"</span>)}</span>
<span id="cb4-10">)</span>
<span id="cb4-11"></span>
<span id="cb4-12">completion <span class="op" style="color: #5E5E5E;">=</span> client.chat.completions.create(</span>
<span id="cb4-13">    model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"openpipe:MY_MODEL_ID_WAS_HERE"</span>,</span>
<span id="cb4-14">    messages<span class="op" style="color: #5E5E5E;">=</span>[</span>
<span id="cb4-15">        {</span>
<span id="cb4-16">            <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb4-17">            <span class="st" style="color: #20794D;">"content"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']."</span></span>
<span id="cb4-18">        },</span>
<span id="cb4-19">        {</span>
<span id="cb4-20">            <span class="st" style="color: #20794D;">"role"</span>: <span class="st" style="color: #20794D;">"user"</span>,</span>
<span id="cb4-21">            <span class="st" style="color: #20794D;">"content"</span>: pr1</span>
<span id="cb4-22">        }</span>
<span id="cb4-23">    ],</span>
<span id="cb4-24">    temperature<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>,</span>
<span id="cb4-25">    openpipe<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb4-26">        <span class="st" style="color: #20794D;">"tags"</span>: {</span>
<span id="cb4-27">            <span class="st" style="color: #20794D;">"prompt_id"</span>: <span class="st" style="color: #20794D;">"counting"</span>,</span>
<span id="cb4-28">            <span class="st" style="color: #20794D;">"any_key"</span>: <span class="st" style="color: #20794D;">"any_value"</span></span>
<span id="cb4-29">        }</span>
<span id="cb4-30">    },</span>
<span id="cb4-31">)</span>
<span id="cb4-32"></span>
<span id="cb4-33"><span class="bu" style="color: null;">print</span>(json.loads(completion.choices[<span class="dv" style="color: #AD0000;">0</span>].message.content))</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'name'</span>: <span style="color: #008000; text-decoration-color: #008000">'3 killed and 2 captured in Badakhshan'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'start_date'</span>: <span style="color: #008000; text-decoration-color: #008000">'2011-11-07'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'event_type'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'captureandkill'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'province'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'badakhshan'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'target_group'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'haqqani'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'captureq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killcaptureraid'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'airstrike'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'noshotsfired'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>Again you can see we have a really nice result here: JSON is good and the content is solid too. This was a finetune of Llama3 so clearly the problem I noted <a href="https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html#finetuning-our-model">in the previous blog post</a> was a problem with how I’d set up my local finetune and not with Llama3 itself.</p>
<p>I liked how OpenPipe automatically deployed my model for me once the finetune was complete. Moreover, there was no extra cost associated with this. (Since their base models are limited, I assume this means that they have lost of customers’ LORA adapters all connected to these base models and that’s how they’re able to keep all these deployments up and cost-effective.)</p>
<p>There was one final trick that OpenPipe had up its sleeve: an ‘evals’ interface. The interface is pretty simple again, but the gist is that you get to select OpenAI models to compare your finetune against a test dataset and get a comparison. You can select multiple models to run at the same time and the cost is pretty reasonable.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/openpipe-eval-ui.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Openpipe eval UI</figcaption><p></p>
</figure>
</div>
<p>The evaluation is parallelised and you get a nice table with the aggregate results:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/openpipe-eval-results.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Openpipe eval results</figcaption><p></p>
</figure>
</div>
<p>You also (in the datasets tab) get a table with the individual responses for the test data:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://mlops.systems/posts/images/openpipe-eval-datasets.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Openpipe eval datasets</figcaption><p></p>
</figure>
</div>
<p>Looking at the results you quickly become aware that this specific evaluation didn’t really make much sense. Comparing the same prompt between the finetuned model and GPT4 could never have been fair since my prompt never asks for the result back in a certain format, or that it should be JSON and so on.</p>
<p>Moreover, you can see that the evaluation prompt itself doesn’t do a good job of picking up that the finetuned model really did a great job on the whole and so the aggregate comparison scores don’t really make much sense here.</p>
<p>That said, I found this feature a useful ‘nice-to-have’ and I can see how someone might find this helpful if they either wanted to run a quick experiment or weren’t particularly technically savvy.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final thoughts</h2>
<p>Overall I found this an interesting experience to do these finetunes in parallel. I suspect that I am not the core audience / market for these services. I was surprised how little customisation they offered, and I actually wonder who is using them. They were easy to use, however, and they do potentially open up the possibility for someone less technical to do something somewhat advanced with LLMs that they wouldn’t otherwise be able to do.</p>
<p>The moment you want to do something slightly custom, with your prompt template or with the architecture or try something new and cutting-edge, then immediately these services aren’t for you. Similarly, even though I think all of the services offer a Python SDK to replicate what I did in the web UI, I think you essentially have the same limited options available to you if you wanted to trigger these jobs programatically as part of a larger pipeline.</p>
<p>For the most part you never had the feeling that you were part of a wider ecosystem of these open models, with new techniques coming out all the time and new models as well. These are some of the things I missed from the experience, but as I mentioned before, I’m not the core audience here.</p>
<p>I do appreciate the opportunity to try these out a few times and the companies for providing credits to do some meaningful attempts at doing something useful. I’ll try these a bit further down the road again and report back if my impression changes or if/when new features are added.</p>


</section>

 ]]></description>
  <category>nlp</category>
  <category>llms</category>
  <category>miniproject</category>
  <category>finetuning</category>
  <category>isafpr</category>
  <guid>https://mlops.systems/posts/2024-06-17-one-click-finetuning.html</guid>
  <pubDate>Sun, 16 Jun 2024 22:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/one-click-finetuning.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Finetuning my first LLM(s) for structured data extraction with axolotl</title>
  <dc:creator>Alex Strick van Linschoten</dc:creator>
  <link>https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html</link>
  <description><![CDATA[ 




<p>We previously <a href="https://mlops.systems/posts/2024-06-03-isafpr-evaluating-baseline.html">looked into how well</a> the top LLMs could do when given press releases and asked to extract structured data from them. I was glad that this clearly wasn’t a task they struggled with, but it was by no means a simple task for them and some basic evaluations that I performed showed that there was room for improvement.</p>
<p>Since writing that post I also heard from readers to say that perhaps I wasn’t using the OpenAI API in a way that would get the best results. In particular, function calling would give a better accuracy over the raw prompting that I was using. I’ll probably return to that in a separate post when I compare how well we’re doing with finetuning.</p>
<p>As a quick reminder, we’re hoping to create something that will allow us to go from an unstructured text (a press release, in our case) to a structured output that accurately extracts certain pieces of metadata from the text. Please give <a href="https://mlops.systems/posts/2024-06-02-isafpr-prompting-baseline.html">the first post</a> in the series a read if you want more of the context of what we’re doing.</p>
<p>This blogpost will be about my first finetune(s) of some models and I’ll showcase how I got the data ready and then some observations about the finetuning process in general.</p>
<section id="preparing-data-for-finetuning" class="level1">
<h1>Preparing data for finetuning</h1>
<p>In my previous posts I’ve already showed how I converted my dataset into <a href="https://github.com/pydantic/pydantic">Pydantic</a> models. This helps ensure a uniformity to the data I’ll be using in my finetuning. We’ll actually want to convert the labelled data into JSON strings from the Pydantic models, but as an interim datastructure Pydantic is useful for the validation.</p>
<section id="loading-the-datasets" class="level2">
<h2 class="anchored" data-anchor-id="loading-the-datasets">Loading the datasets</h2>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;"># Loadthe dataset</span></span>
<span id="cb1-6">train_dataset <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases"</span>, split<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"train"</span>)</span>
<span id="cb1-7">test_dataset <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases"</span>, split<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"test"</span>)</span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;"># Convert the dataset to a pandas DataFrame</span></span>
<span id="cb1-10">train_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(train_dataset)</span>
<span id="cb1-11">test_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(test_dataset)</span></code></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"4b0282eaaa0f458cbeeaf3eb2eae9d5d","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"34595eba125f4fa8b243759c8392f415","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"97c9b72c11614da2aa24d90dc13f3d9c","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bd396a8ddbea43febdd1e2ca132890c1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3aad0332bcc54088872aa9de9c7acc7c","version_major":2,"version_minor":0}
</script>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">train_dataset</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>Dataset({
    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],
    num_rows: 4098
})</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">test_dataset</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>Dataset({
    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],
    num_rows: 724
})</code></pre>
</div>
</div>
<p>We have 4098 training examples and 724 test examples. This seems a good split to me. I experimented a bit with the exact split and found that 15% seemed like a good compromise. We want enough data to get a good evaluation, but we also want to give our model enough examples to learn. In the course people were frequently talking about somewhere in the order of mid hundreds to low thousands as being the sweet spot, so I hope I’m firmly in that range.</p>
<p>It’s also worth reflecting that I’m lucky that I have such a large clean dataset to work with. In a later project I’d like to try working with much less and slowly building up something more complex since that’s a skill in and of itself.</p>
</section>
<section id="setting-up-our-pydantic-models-with-validation" class="level2">
<h2 class="anchored" data-anchor-id="setting-up-our-pydantic-models-with-validation">Setting up our Pydantic models with validation</h2>
<p>There’s a decent amount of code in the next cell, and definitely read the previous posts to understand what all the pieces are about, but in a nutshell we’re setting ourselves up to extract structured data from the text. This Pydantic model is what will hold the data we’re interested in.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;">from</span> enum <span class="im" style="color: #00769E;">import</span> Enum</span>
<span id="cb6-2"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Set, Annotated, Optional</span>
<span id="cb6-3"><span class="im" style="color: #00769E;">from</span> pydantic <span class="im" style="color: #00769E;">import</span> BaseModel, Field, validator, ValidationInfo</span>
<span id="cb6-4"><span class="im" style="color: #00769E;">from</span> datetime <span class="im" style="color: #00769E;">import</span> date</span>
<span id="cb6-5"></span>
<span id="cb6-6"></span>
<span id="cb6-7"><span class="kw" style="color: #003B4F;">class</span> EventType(<span class="bu" style="color: null;">str</span>, Enum):</span>
<span id="cb6-8">    airstrike <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"airstrike"</span></span>
<span id="cb6-9">    detention <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"detention"</span></span>
<span id="cb6-10">    captureandkill <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"captureandkill"</span></span>
<span id="cb6-11">    insurgentskilled <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"insurgentskilled"</span></span>
<span id="cb6-12">    exchangeoffire <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"exchangeoffire"</span></span>
<span id="cb6-13">    civiliancasualty <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"civiliancasualty"</span></span>
<span id="cb6-14"></span>
<span id="cb6-15"></span>
<span id="cb6-16"><span class="kw" style="color: #003B4F;">class</span> Province(<span class="bu" style="color: null;">str</span>, Enum):</span>
<span id="cb6-17">    badakhshan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"badakhshan"</span></span>
<span id="cb6-18">    badghis <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"badghis"</span></span>
<span id="cb6-19">    baghlan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"baghlan"</span></span>
<span id="cb6-20">    balkh <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"balkh"</span></span>
<span id="cb6-21">    bamyan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"bamyan"</span></span>
<span id="cb6-22">    day_kundi <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"day_kundi"</span></span>
<span id="cb6-23">    farah <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"farah"</span></span>
<span id="cb6-24">    faryab <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"faryab"</span></span>
<span id="cb6-25">    ghazni <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"ghazni"</span></span>
<span id="cb6-26">    ghor <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"ghor"</span></span>
<span id="cb6-27">    helmand <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"helmand"</span></span>
<span id="cb6-28">    herat <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"herat"</span></span>
<span id="cb6-29">    jowzjan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"jowzjan"</span></span>
<span id="cb6-30">    kabul <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kabul"</span></span>
<span id="cb6-31">    kandahar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kandahar"</span></span>
<span id="cb6-32">    kapisa <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kapisa"</span></span>
<span id="cb6-33">    khost <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"khost"</span></span>
<span id="cb6-34">    kunar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kunar"</span></span>
<span id="cb6-35">    kunduz <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"kunduz"</span></span>
<span id="cb6-36">    laghman <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"laghman"</span></span>
<span id="cb6-37">    logar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"logar"</span></span>
<span id="cb6-38">    nangarhar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"nangarhar"</span></span>
<span id="cb6-39">    nimroz <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"nimroz"</span></span>
<span id="cb6-40">    nuristan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"nuristan"</span></span>
<span id="cb6-41">    paktya <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"paktya"</span></span>
<span id="cb6-42">    paktika <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"paktika"</span></span>
<span id="cb6-43">    panjshir <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"panjshir"</span></span>
<span id="cb6-44">    parwan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"parwan"</span></span>
<span id="cb6-45">    samangan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"samangan"</span></span>
<span id="cb6-46">    sar_e_pul <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"sar_e_pul"</span></span>
<span id="cb6-47">    takhar <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"takhar"</span></span>
<span id="cb6-48">    uruzgan <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"uruzgan"</span></span>
<span id="cb6-49">    wardak <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"wardak"</span></span>
<span id="cb6-50">    zabul <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"zabul"</span></span>
<span id="cb6-51"></span>
<span id="cb6-52"></span>
<span id="cb6-53"><span class="kw" style="color: #003B4F;">class</span> TargetGroup(<span class="bu" style="color: null;">str</span>, Enum):</span>
<span id="cb6-54">    taliban <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"taliban"</span></span>
<span id="cb6-55">    haqqani <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"haqqani"</span></span>
<span id="cb6-56">    criminals <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"criminals"</span></span>
<span id="cb6-57">    aq <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"aq"</span></span>
<span id="cb6-58">    hig <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"hig"</span></span>
<span id="cb6-59">    let <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"let"</span></span>
<span id="cb6-60">    imu <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"imu"</span></span>
<span id="cb6-61">    judq <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"judq"</span></span>
<span id="cb6-62">    iju <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"iju"</span></span>
<span id="cb6-63">    hik <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"hik"</span></span>
<span id="cb6-64">    ttp <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"ttp"</span></span>
<span id="cb6-65">    other <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb6-66"></span>
<span id="cb6-67"></span>
<span id="cb6-68"><span class="kw" style="color: #003B4F;">def</span> validate_event_type(value: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb6-69">    valid_values <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb6-70">        <span class="st" style="color: #20794D;">"airstrike"</span>,</span>
<span id="cb6-71">        <span class="st" style="color: #20794D;">"detention"</span>,</span>
<span id="cb6-72">        <span class="st" style="color: #20794D;">"captureandkill"</span>,</span>
<span id="cb6-73">        <span class="st" style="color: #20794D;">"insurgentskilled"</span>,</span>
<span id="cb6-74">        <span class="st" style="color: #20794D;">"exchangeoffire"</span>,</span>
<span id="cb6-75">        <span class="st" style="color: #20794D;">"civiliancasualty"</span>,</span>
<span id="cb6-76">    ]</span>
<span id="cb6-77">    <span class="cf" style="color: #003B4F;">if</span> value.lower() <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> valid_values:</span>
<span id="cb6-78">        <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb6-79">    <span class="cf" style="color: #003B4F;">return</span> value.lower()</span>
<span id="cb6-80"></span>
<span id="cb6-81"></span>
<span id="cb6-82"><span class="kw" style="color: #003B4F;">def</span> validate_province(value: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb6-83">    valid_values <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb6-84">        <span class="st" style="color: #20794D;">"badakhshan"</span>,</span>
<span id="cb6-85">        <span class="st" style="color: #20794D;">"badghis"</span>,</span>
<span id="cb6-86">        <span class="st" style="color: #20794D;">"baghlan"</span>,</span>
<span id="cb6-87">        <span class="st" style="color: #20794D;">"balkh"</span>,</span>
<span id="cb6-88">        <span class="st" style="color: #20794D;">"bamyan"</span>,</span>
<span id="cb6-89">        <span class="st" style="color: #20794D;">"day_kundi"</span>,</span>
<span id="cb6-90">        <span class="st" style="color: #20794D;">"farah"</span>,</span>
<span id="cb6-91">        <span class="st" style="color: #20794D;">"faryab"</span>,</span>
<span id="cb6-92">        <span class="st" style="color: #20794D;">"ghazni"</span>,</span>
<span id="cb6-93">        <span class="st" style="color: #20794D;">"ghor"</span>,</span>
<span id="cb6-94">        <span class="st" style="color: #20794D;">"helmand"</span>,</span>
<span id="cb6-95">        <span class="st" style="color: #20794D;">"herat"</span>,</span>
<span id="cb6-96">        <span class="st" style="color: #20794D;">"jowzjan"</span>,</span>
<span id="cb6-97">        <span class="st" style="color: #20794D;">"kabul"</span>,</span>
<span id="cb6-98">        <span class="st" style="color: #20794D;">"kandahar"</span>,</span>
<span id="cb6-99">        <span class="st" style="color: #20794D;">"kapisa"</span>,</span>
<span id="cb6-100">        <span class="st" style="color: #20794D;">"khost"</span>,</span>
<span id="cb6-101">        <span class="st" style="color: #20794D;">"kunar"</span>,</span>
<span id="cb6-102">        <span class="st" style="color: #20794D;">"kunduz"</span>,</span>
<span id="cb6-103">        <span class="st" style="color: #20794D;">"laghman"</span>,</span>
<span id="cb6-104">        <span class="st" style="color: #20794D;">"logar"</span>,</span>
<span id="cb6-105">        <span class="st" style="color: #20794D;">"nangarhar"</span>,</span>
<span id="cb6-106">        <span class="st" style="color: #20794D;">"nimroz"</span>,</span>
<span id="cb6-107">        <span class="st" style="color: #20794D;">"nuristan"</span>,</span>
<span id="cb6-108">        <span class="st" style="color: #20794D;">"paktya"</span>,</span>
<span id="cb6-109">        <span class="st" style="color: #20794D;">"paktika"</span>,</span>
<span id="cb6-110">        <span class="st" style="color: #20794D;">"panjshir"</span>,</span>
<span id="cb6-111">        <span class="st" style="color: #20794D;">"parwan"</span>,</span>
<span id="cb6-112">        <span class="st" style="color: #20794D;">"samangan"</span>,</span>
<span id="cb6-113">        <span class="st" style="color: #20794D;">"sar_e_pul"</span>,</span>
<span id="cb6-114">        <span class="st" style="color: #20794D;">"takhar"</span>,</span>
<span id="cb6-115">        <span class="st" style="color: #20794D;">"uruzgan"</span>,</span>
<span id="cb6-116">        <span class="st" style="color: #20794D;">"wardak"</span>,</span>
<span id="cb6-117">        <span class="st" style="color: #20794D;">"zabul"</span>,</span>
<span id="cb6-118">    ]</span>
<span id="cb6-119">    <span class="cf" style="color: #003B4F;">if</span> value.lower() <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> valid_values:</span>
<span id="cb6-120">        <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb6-121">    <span class="cf" style="color: #003B4F;">return</span> value.lower()</span>
<span id="cb6-122"></span>
<span id="cb6-123"></span>
<span id="cb6-124"><span class="kw" style="color: #003B4F;">def</span> validate_target_group(value: <span class="bu" style="color: null;">str</span>):</span>
<span id="cb6-125">    valid_values <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb6-126">        <span class="st" style="color: #20794D;">"taliban"</span>,</span>
<span id="cb6-127">        <span class="st" style="color: #20794D;">"haqqani"</span>,</span>
<span id="cb6-128">        <span class="st" style="color: #20794D;">"criminals"</span>,</span>
<span id="cb6-129">        <span class="st" style="color: #20794D;">"aq"</span>,</span>
<span id="cb6-130">        <span class="st" style="color: #20794D;">"hig"</span>,</span>
<span id="cb6-131">        <span class="st" style="color: #20794D;">"let"</span>,</span>
<span id="cb6-132">        <span class="st" style="color: #20794D;">"imu"</span>,</span>
<span id="cb6-133">        <span class="st" style="color: #20794D;">"judq"</span>,</span>
<span id="cb6-134">        <span class="st" style="color: #20794D;">"iju"</span>,</span>
<span id="cb6-135">        <span class="st" style="color: #20794D;">"hik"</span>,</span>
<span id="cb6-136">        <span class="st" style="color: #20794D;">"ttp"</span>,</span>
<span id="cb6-137">        <span class="st" style="color: #20794D;">"other"</span>,</span>
<span id="cb6-138">    ]</span>
<span id="cb6-139">    <span class="cf" style="color: #003B4F;">if</span> value.lower() <span class="kw" style="color: #003B4F;">not</span> <span class="kw" style="color: #003B4F;">in</span> valid_values:</span>
<span id="cb6-140">        <span class="cf" style="color: #003B4F;">return</span> <span class="st" style="color: #20794D;">"other"</span></span>
<span id="cb6-141">    <span class="cf" style="color: #003B4F;">return</span> value.lower()</span>
<span id="cb6-142"></span>
<span id="cb6-143"></span>
<span id="cb6-144"><span class="kw" style="color: #003B4F;">class</span> IsafEvent(BaseModel):</span>
<span id="cb6-145">    name: <span class="bu" style="color: null;">str</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-146">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"A title or name for the event which summarises the event as a headline"</span></span>
<span id="cb6-147">    )</span>
<span id="cb6-148">    text: Optional[<span class="bu" style="color: null;">str</span>] <span class="op" style="color: #5E5E5E;">=</span> Field(description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The full text of the press release"</span>)</span>
<span id="cb6-149">    start_date: date <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-150">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The start date of the event in YYYY-MM-DD format"</span></span>
<span id="cb6-151">    )</span>
<span id="cb6-152">    event_type: Set[Annotated[<span class="bu" style="color: null;">str</span>, Field(validator<span class="op" style="color: #5E5E5E;">=</span>validate_event_type)]] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-153">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The event type. Can be multiple types."</span></span>
<span id="cb6-154">    )</span>
<span id="cb6-155">    province: Set[Annotated[<span class="bu" style="color: null;">str</span>, Field(validator<span class="op" style="color: #5E5E5E;">=</span>validate_province)]] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-156">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The province in which the event occurred. Can be multiple provinces."</span></span>
<span id="cb6-157">    )</span>
<span id="cb6-158">    target_group: Set[Annotated[<span class="bu" style="color: null;">str</span>, Field(validator<span class="op" style="color: #5E5E5E;">=</span>validate_target_group)]] <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-159">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The group that was targetted during the event. Can be multiple groups."</span></span>
<span id="cb6-160">    )</span>
<span id="cb6-161">    min_killed: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-162">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of people killed during the event"</span></span>
<span id="cb6-163">    )</span>
<span id="cb6-164">    min_captured: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-165">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of people captured during the event"</span></span>
<span id="cb6-166">    )</span>
<span id="cb6-167">    killq: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-168">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether someone was killed or not during the event"</span></span>
<span id="cb6-169">    )</span>
<span id="cb6-170">    captureq: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-171">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether someone was captured or not during the event"</span></span>
<span id="cb6-172">    )</span>
<span id="cb6-173">    killcaptureraid: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-174">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether the event was a so-called 'kill-capture raid'."</span></span>
<span id="cb6-175">    )</span>
<span id="cb6-176">    airstrike: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-177">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether an airstrike was used during the event"</span></span>
<span id="cb6-178">    )</span>
<span id="cb6-179">    noshotsfired: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-180">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Whether no shots were fired during the event"</span></span>
<span id="cb6-181">    )</span>
<span id="cb6-182">    min_leaders_killed: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-183">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of leaders killed during the event"</span></span>
<span id="cb6-184">    )</span>
<span id="cb6-185">    min_leaders_captured: <span class="bu" style="color: null;">int</span> <span class="op" style="color: #5E5E5E;">=</span> Field(</span>
<span id="cb6-186">        description<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"The minimum number of leaders captured during the event"</span></span>
<span id="cb6-187">    )</span>
<span id="cb6-188"></span>
<span id="cb6-189">    <span class="kw" style="color: #003B4F;">class</span> Config:</span>
<span id="cb6-190">        arbitrary_types_allowed <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span></span></code></pre></div>
</div>
<p>Here’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> List</span>
<span id="cb7-2"></span>
<span id="cb7-3">events: List[IsafEvent] <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb7-4"></span>
<span id="cb7-5"><span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">list</span>(train_df.iterrows()):</span>
<span id="cb7-6">    event_types <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb7-7">        eventtype.strip().lower() <span class="cf" style="color: #003B4F;">for</span> eventtype <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"eventtype"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb7-8">    )</span>
<span id="cb7-9">    provinces <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(province.strip().lower() <span class="cf" style="color: #003B4F;">for</span> province <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"province"</span>].split(<span class="st" style="color: #20794D;">","</span>))</span>
<span id="cb7-10">    target_groups <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb7-11">        target_group.strip().lower() <span class="cf" style="color: #003B4F;">for</span> target_group <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"targetgroup"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb7-12">    )</span>
<span id="cb7-13"></span>
<span id="cb7-14">    events.append(</span>
<span id="cb7-15">        IsafEvent(</span>
<span id="cb7-16">            name<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"name"</span>],</span>
<span id="cb7-17">            text<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"text"</span>],</span>
<span id="cb7-18">            start_date<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"StartDate"</span>].to_pydatetime().date(),</span>
<span id="cb7-19">            event_type<span class="op" style="color: #5E5E5E;">=</span>event_types,</span>
<span id="cb7-20">            province<span class="op" style="color: #5E5E5E;">=</span>provinces,</span>
<span id="cb7-21">            target_group<span class="op" style="color: #5E5E5E;">=</span>target_groups,</span>
<span id="cb7-22">            min_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minkilled"</span>]),</span>
<span id="cb7-23">            min_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"mincaptured"</span>]),</span>
<span id="cb7-24">            killq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb7-25">            captureq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"captureq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb7-26">            killcaptureraid<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killcaptureraid"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb7-27">            airstrike<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"airstrike"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb7-28">            noshotsfired<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"noshotsfired"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb7-29">            min_leaders_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderskilled"</span>]),</span>
<span id="cb7-30">            min_leaders_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderscaptured"</span>]),</span>
<span id="cb7-31">        )</span>
<span id="cb7-32">    )</span>
<span id="cb7-33"></span>
<span id="cb7-34"><span class="bu" style="color: null;">print</span>(events[:<span class="dv" style="color: #AD0000;">2</span>])</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
    <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">IsafEvent</span><span style="font-weight: bold">(</span>
        <span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'Several insurgents killed in Helmand'</span>,
        <span style="color: #808000; text-decoration-color: #808000">text</span>=<span style="color: #008000; text-decoration-color: #008000">'ISAF Joint Command Evening Operational Update Feb. 19, 2011\nISAF Joint Command - </span>
<span style="color: #008000; text-decoration-color: #008000">Afghanistan\u20282011-02-S-143\u2028For Immediate Release \u2028\u2028KABUL, Afghanistan (Feb. 19)\u2028\u2028ISAF </span>
<span style="color: #008000; text-decoration-color: #008000">service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of </span>
<span style="color: #008000; text-decoration-color: #008000">their position talking on radios today. After gaining positive identification of the insurgent positions, the </span>
<span style="color: #008000; text-decoration-color: #008000">coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning </span>
<span style="color: #008000; text-decoration-color: #008000">in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent</span>
<span style="color: #008000; text-decoration-color: #008000">positions, resulting in several more insurgents being killed.'</span>,
        <span style="color: #808000; text-decoration-color: #808000">start_date</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime</span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2011</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span><span style="font-weight: bold">)</span>,
        <span style="color: #808000; text-decoration-color: #808000">event_type</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'insurgentskilled'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">province</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'helmand'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">target_group</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">''</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">killq</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">captureq</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">killcaptureraid</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">airstrike</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">noshotsfired</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
    <span style="font-weight: bold">)</span>,
    <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">IsafEvent</span><span style="font-weight: bold">(</span>
        <span style="color: #808000; text-decoration-color: #808000">name</span>=<span style="color: #008000; text-decoration-color: #008000">'Force Continues Targeting Haqqani Leadership'</span>,
        <span style="color: #808000; text-decoration-color: #808000">text</span>=<span style="color: #008000; text-decoration-color: #008000">'Force Continues Targeting Haqqani Leadership\nISAF Joint Command - Afghanistan\u20282010-09-CA-211 </span>
<span style="color: #008000; text-decoration-color: #008000">For Immediate Release\u2028Download PDF \u2028\u2028\u2028\xa0KABUL, Afghanistan (Sept. 20) - An Afghan and </span>
<span style="color: #008000; text-decoration-color: #008000">coalition security force detained two insurgents, including a Haqqani Network sub-commander operating in Khost </span>
<span style="color: #008000; text-decoration-color: #008000">province, Sunday. \u2028\u2028The commander coordinated and conducted attacks on coalition forces operating in the </span>
<span style="color: #008000; text-decoration-color: #008000">province and was formerly active in Kabul. \u2028\u2028Intelligence reports led the security force to a compound </span>
<span style="color: #008000; text-decoration-color: #008000">northwest of Khost City to search for the commander. Afghan forces called for all occupants to exit the buildings </span>
<span style="color: #008000; text-decoration-color: #008000">peacefully and then the combined force cleared and secured the compound. During the clearance, an armed individual </span>
<span style="color: #008000; text-decoration-color: #008000">came out of an adjacent building toward the security force. The forced engaged the individual and killed him. </span>
<span style="color: #008000; text-decoration-color: #008000">\u2028\u2028After the area was secure, the security force questioned the residents at the scene and detained the </span>
<span style="color: #008000; text-decoration-color: #008000">commander and one of his associates. The security force also found multiple automatic weapons, magazines and </span>
<span style="color: #008000; text-decoration-color: #008000">grenades at the scene. \u2028\u2028The assault force protected the women and children throughout the search.'</span>,
        <span style="color: #808000; text-decoration-color: #808000">start_date</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">datetime</span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">.date</span><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2010</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">9</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19</span><span style="font-weight: bold">)</span>,
        <span style="color: #808000; text-decoration-color: #808000">event_type</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'captureandkill'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">province</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'khost'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">target_group</span>=<span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">'haqqani'</span><span style="font-weight: bold">}</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
        <span style="color: #808000; text-decoration-color: #808000">killq</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">captureq</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">killcaptureraid</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
        <span style="color: #808000; text-decoration-color: #808000">airstrike</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">noshotsfired</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_killed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
        <span style="color: #808000; text-decoration-color: #808000">min_leaders_captured</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
    <span style="font-weight: bold">)</span>
<span style="font-weight: bold">]</span>
</pre>
</div>
</div>
<p>So this is data that we’ve already labelled. You can see the text that we’ll provide as input to our model, and then you can see the various fields that we’re hoping our model can learn to extract. As a JSON string, the prediction that we’re hoping our model will output would look like this:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">json_str <span class="op" style="color: #5E5E5E;">=</span> events[<span class="dv" style="color: #AD0000;">0</span>].model_dump_json(exclude<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"text"</span>})</span>
<span id="cb8-2"><span class="bu" style="color: null;">print</span>(json_str)</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"name"</span>:<span style="color: #008000; text-decoration-color: #008000">"Several insurgents killed in </span>
<span style="color: #008000; text-decoration-color: #008000">Helmand"</span>,<span style="color: #008000; text-decoration-color: #008000">"start_date"</span>:<span style="color: #008000; text-decoration-color: #008000">"2011-02-18"</span>,<span style="color: #008000; text-decoration-color: #008000">"event_type"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"insurgentskilled"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"province"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"helmand"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"target_group"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">""</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"mi</span>
<span style="color: #008000; text-decoration-color: #008000">n_killed"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>,<span style="color: #008000; text-decoration-color: #008000">"min_captured"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,<span style="color: #008000; text-decoration-color: #008000">"killq"</span>:true,<span style="color: #008000; text-decoration-color: #008000">"captureq"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"airstrike"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"noshotsfired"</span>
:false,<span style="color: #008000; text-decoration-color: #008000">"min_leaders_killed"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>If you wish to view more examples of the data, I <a href="https://huggingface.co/datasets/strickvl/isaf_press_releases_ft">created an interim dataset</a> which I uploaded to the Hugging Face Hub, but it’s not completely in the required form for finetuning so I’ll just <a href="https://huggingface.co/datasets/strickvl/isaf_press_releases_ft">link to it here</a> and you can explore it to see the pairings of input and output if you’re interested.</p>
</section>
</section>
<section id="writing-our-data-as-jsonl" class="level1">
<h1>Writing our data as JSONL</h1>
<p><code>axolotl</code> likes its training data as a JSONL file, so that’s what we’ll write to disk to make training possible. We’ll write two different files, one for training and another for evaluation. <code>axolotl</code> actually handles making a train-test split for us, so we’ll actually use the test set as a true held out evaluation set for use later on.</p>
<p>The data needs to be formatted in a certain way for our model to learn to output in JSON format. I’ll show two different ways of doing this below, since I found the process a bit confusing and the first time I did it in a way that technically <em>works</em> but might not be the best.</p>
<section id="writing-alpaca-sharegpt-format-jsonl" class="level2">
<h2 class="anchored" data-anchor-id="writing-alpaca-sharegpt-format-jsonl">Writing Alpaca / ShareGPT format JSONL</h2>
<p>In the course, Hamel shared an example from work he’d done with Honeycomb to output in a particular format. He <a href="https://github.com/parlance-labs/ftcourse/blob/master/04_prep_data.ipynb">shared his notebooks</a> and I found this useful as an initial example to move forward with. He uses the ShareGPT template to format his data, and I followed that format for my data. This looks something like this:</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb9-1"><span class="fu" style="color: #4758AB;">{</span></span>
<span id="cb9-2">    <span class="dt" style="color: #AD0000;">"conversations"</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span></span>
<span id="cb9-3">        <span class="fu" style="color: #4758AB;">{</span></span>
<span id="cb9-4">            <span class="dt" style="color: #AD0000;">"from"</span><span class="fu" style="color: #4758AB;">:</span> <span class="st" style="color: #20794D;">"system"</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb9-5">            <span class="dt" style="color: #AD0000;">"value"</span><span class="fu" style="color: #4758AB;">:</span> <span class="st" style="color: #20794D;">"Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produce a Honeycomb query."</span></span>
<span id="cb9-6">        <span class="fu" style="color: #4758AB;">}</span><span class="ot" style="color: #003B4F;">,</span></span>
<span id="cb9-7">        <span class="fu" style="color: #4758AB;">{</span></span>
<span id="cb9-8">            <span class="dt" style="color: #AD0000;">"from"</span><span class="fu" style="color: #4758AB;">:</span> <span class="st" style="color: #20794D;">"human"</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb9-9">            <span class="dt" style="color: #AD0000;">"value"</span><span class="fu" style="color: #4758AB;">:</span> <span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n\n</span><span class="st" style="color: #20794D;">NLQ: </span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">group by HTTP method</span><span class="ch" style="color: #20794D;">\"\n\n</span><span class="st" style="color: #20794D;">Columns: ['query_string_num_tokens', 'query_string_length', 'data_queries', 'http.target', 'task.id', 'trace_root.http.target', 'topic', 'http.host', 'total_hits', 'db.user', 'domain_types', 'db.name', 'graphql.document', 'history', 'http.scheme', 'http.method', 'frontend.version', 'disposition_for_dBVVysC8x4Ymwg9rtjMckgw9', 'db.system', 'event_name', 'organization', 'auth.logout', 'organizations', 'name', 'net.transport', 'db.operation', 'disposition_for_UvsPPBVUn9FDuzDjsjYCqopq', 'disposition_for_1RUGSd7GdnP5tuKdgqBRZUm2', 'process.pid', 'disposition_for_6uyAoBc3PuvEcTTPFgPM3Rtk', 'exception.stacktrace', 'data_ingestion_individuals_count', 'disposition_for_qrnUBUz8YBfNX7Liekq6nKi3', 'task_type.type', 'disposition_for_JQDNbuUdaQcEbEwQNxUbV5EF', 'disposition_for_rAcWoXfbHw4eWoJFH4ZcY8ue', 'disposition_for_eShqQoC9jUi9VQBidpp2oXHP', 'parent_name', 'template', 'graphql.operation.name', 'span.num_links', 'disposition_for_kNSPtvsCWkDoEyFP2QE6VPmQ', 'disposition_for_UUqf9L1qkFxDNEvcgsVMA2yy', 'disposition_for_vwbbN76HZ7uitLubvkUjPFQE', 'disposition_for_aAto1pGrdF5RunpSX8sY5hvn', 'disposition_for_UbKCMdnkPQ6TuHrfdBo5juZu', 'disposition_for_QfrvmoHxSgLPJXPKZCrZfGo8', 'disposition_for_NoKSSruBRCX6UG28PzmkybUd', 'disposition_for_UZAqvZ5XVBZjKKWuMeRkRayS', 'organization_token', 'duration_ms', 'trace.parent_id', 'db.statement', 'exception.message', 'error', 'service.name', 'http.status_code', 'http.route']"</span></span>
<span id="cb9-10">        <span class="fu" style="color: #4758AB;">}</span><span class="ot" style="color: #003B4F;">,</span></span>
<span id="cb9-11">        <span class="fu" style="color: #4758AB;">{</span></span>
<span id="cb9-12">            <span class="dt" style="color: #AD0000;">"from"</span><span class="fu" style="color: #4758AB;">:</span> <span class="st" style="color: #20794D;">"gpt"</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb9-13">            <span class="dt" style="color: #AD0000;">"value"</span><span class="fu" style="color: #4758AB;">:</span> <span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">{</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">breakdowns</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">: [</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">http.method</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">], </span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">calculations</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">: [{</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">op</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">: </span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">COUNT</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">}], </span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">time_range</span><span class="ch" style="color: #20794D;">\"</span><span class="st" style="color: #20794D;">: 7200}"</span></span>
<span id="cb9-14">        <span class="fu" style="color: #4758AB;">}</span></span>
<span id="cb9-15">    <span class="ot" style="color: #003B4F;">]</span></span>
<span id="cb9-16"><span class="fu" style="color: #4758AB;">}</span></span></code></pre></div>
<p>You can see that a single entry is part of a <code>conversations</code> key, and then you have a series of messages with <code>system</code>, <code>human</code> and <code>gpt</code> roles. I followed this closely for my data, but with a base model I’m pretty sure this isn’t strictly necessary. (The second option is what I’ll show a bit later).</p>
<p>Writing the data is a case of wrangling the data so it fits the format above, and doing it separately for our train and test datasets. You’ll note that in the system call I stuff in the event types, provinces and target groups as a way to provide some context to the model. This was something that Hamel did in his example.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb10-2"><span class="im" style="color: #00769E;">from</span> datasets <span class="im" style="color: #00769E;">import</span> load_dataset</span>
<span id="cb10-3"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb10-4"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb10-5"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> List</span>
<span id="cb10-6"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb10-7"></span>
<span id="cb10-8"><span class="co" style="color: #5E5E5E;"># Load the dataset</span></span>
<span id="cb10-9">dataset <span class="op" style="color: #5E5E5E;">=</span> load_dataset(<span class="st" style="color: #20794D;">"strickvl/isafpressreleases"</span>)</span>
<span id="cb10-10">train_target_file_path <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"../data/sharegpt_isaf_press_releases_ft_train.jsonl"</span></span>
<span id="cb10-11">test_target_file_path <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"../data/sharegpt_isaf_press_releases_ft_test.jsonl"</span></span>
<span id="cb10-12"></span>
<span id="cb10-13"></span>
<span id="cb10-14"><span class="kw" style="color: #003B4F;">def</span> write_data_to_jsonl(df: pd.DataFrame, target_file_path: <span class="bu" style="color: null;">str</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb10-15">    events: List[IsafEvent] <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb10-16"></span>
<span id="cb10-17">    <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">list</span>(df.iterrows()):</span>
<span id="cb10-18">        event_types <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb10-19">            eventtype.strip().lower() <span class="cf" style="color: #003B4F;">for</span> eventtype <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"eventtype"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb10-20">        )</span>
<span id="cb10-21">        provinces <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb10-22">            province.strip().lower() <span class="cf" style="color: #003B4F;">for</span> province <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"province"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb10-23">        )</span>
<span id="cb10-24">        target_groups <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb10-25">            target_group.strip().lower()</span>
<span id="cb10-26">            <span class="cf" style="color: #003B4F;">for</span> target_group <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"targetgroup"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb10-27">        )</span>
<span id="cb10-28"></span>
<span id="cb10-29">        events.append(</span>
<span id="cb10-30">            IsafEvent(</span>
<span id="cb10-31">                name<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"name"</span>],</span>
<span id="cb10-32">                text<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"text"</span>],</span>
<span id="cb10-33">                start_date<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"StartDate"</span>].to_pydatetime().date(),</span>
<span id="cb10-34">                event_type<span class="op" style="color: #5E5E5E;">=</span>event_types,</span>
<span id="cb10-35">                province<span class="op" style="color: #5E5E5E;">=</span>provinces,</span>
<span id="cb10-36">                target_group<span class="op" style="color: #5E5E5E;">=</span>target_groups,</span>
<span id="cb10-37">                min_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minkilled"</span>]),</span>
<span id="cb10-38">                min_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"mincaptured"</span>]),</span>
<span id="cb10-39">                killq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb10-40">                captureq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"captureq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb10-41">                killcaptureraid<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killcaptureraid"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb10-42">                airstrike<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"airstrike"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb10-43">                noshotsfired<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"noshotsfired"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb10-44">                min_leaders_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderskilled"</span>]),</span>
<span id="cb10-45">                min_leaders_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderscaptured"</span>]),</span>
<span id="cb10-46">            )</span>
<span id="cb10-47">        )</span>
<span id="cb10-48"></span>
<span id="cb10-49">    processed_data <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb10-50">        {</span>
<span id="cb10-51">            <span class="st" style="color: #20794D;">"conversations"</span>: [</span>
<span id="cb10-52">                {</span>
<span id="cb10-53">                    <span class="st" style="color: #20794D;">"from"</span>: <span class="st" style="color: #20794D;">"system"</span>,</span>
<span id="cb10-54">                    <span class="st" style="color: #20794D;">"value"</span>: <span class="st" style="color: #20794D;">"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']"</span>,</span>
<span id="cb10-55">                },</span>
<span id="cb10-56">                {<span class="st" style="color: #20794D;">"from"</span>: <span class="st" style="color: #20794D;">"human"</span>, <span class="st" style="color: #20794D;">"value"</span>: <span class="ss" style="color: #20794D;">f"PRESS RELEASE TEXT: </span><span class="sc" style="color: #5E5E5E;">{</span>event<span class="sc" style="color: #5E5E5E;">.</span>text<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>},</span>
<span id="cb10-57">                {<span class="st" style="color: #20794D;">"from"</span>: <span class="st" style="color: #20794D;">"gpt"</span>, <span class="st" style="color: #20794D;">"value"</span>: <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>event<span class="sc" style="color: #5E5E5E;">.</span>model_dump_json(exclude<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">'text'</span>})<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>},</span>
<span id="cb10-58">            ]</span>
<span id="cb10-59">        }</span>
<span id="cb10-60">        <span class="cf" style="color: #003B4F;">for</span> event <span class="kw" style="color: #003B4F;">in</span> events</span>
<span id="cb10-61">    ]</span>
<span id="cb10-62"></span>
<span id="cb10-63">    <span class="co" style="color: #5E5E5E;"># Write the processed data to a JSONL file</span></span>
<span id="cb10-64">    os.makedirs(os.path.dirname(target_file_path), exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb10-65">    <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(target_file_path, <span class="st" style="color: #20794D;">"w"</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb10-66">        <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> processed_data:</span>
<span id="cb10-67">            f.write(json.dumps(item) <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>)</span>
<span id="cb10-68"></span>
<span id="cb10-69"></span>
<span id="cb10-70"><span class="co" style="color: #5E5E5E;"># Write the data to disk</span></span>
<span id="cb10-71">train_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(dataset[<span class="st" style="color: #20794D;">"train"</span>])</span>
<span id="cb10-72">test_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame(dataset[<span class="st" style="color: #20794D;">"test"</span>])</span>
<span id="cb10-73"></span>
<span id="cb10-74">write_data_to_jsonl(train_df, train_target_file_path)</span>
<span id="cb10-75">write_data_to_jsonl(test_df, test_target_file_path)</span></code></pre></div>
</div>
<p>The first line of the training file looks like this now:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(train_target_file_path, <span class="st" style="color: #20794D;">"r"</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb11-2">    <span class="bu" style="color: null;">print</span>(f.readline())</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"conversations"</span>: <span style="font-weight: bold">[{</span><span style="color: #008000; text-decoration-color: #008000">"from"</span>: <span style="color: #008000; text-decoration-color: #008000">"system"</span>, <span style="color: #008000; text-decoration-color: #008000">"value"</span>: <span style="color: #008000; text-decoration-color: #008000">"You are an expert at identifying events in a press release. You are</span>
<span style="color: #008000; text-decoration-color: #008000">precise and always make sure you are correct, drawing inference from the text of the press release. event_types = </span>
<span style="color: #008000; text-decoration-color: #008000">['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces =</span>
<span style="color: #008000; text-decoration-color: #008000">['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', </span>
<span style="color: #008000; text-decoration-color: #008000">'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', </span>
<span style="color: #008000; text-decoration-color: #008000">'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', </span>
<span style="color: #008000; text-decoration-color: #008000">'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', </span>
<span style="color: #008000; text-decoration-color: #008000">'judq', 'iju', 'hik', 'ttp', 'other']"</span><span style="font-weight: bold">}</span>, <span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"from"</span>: <span style="color: #008000; text-decoration-color: #008000">"human"</span>, <span style="color: #008000; text-decoration-color: #008000">"value"</span>: <span style="color: #008000; text-decoration-color: #008000">"PRESS RELEASE TEXT: ISAF Joint Command Evening</span>
<span style="color: #008000; text-decoration-color: #008000">Operational Update Feb. 19, 2011\nISAF Joint Command - Afghanistan\u20282011-02-S-143\u2028For Immediate Release </span>
<span style="color: #008000; text-decoration-color: #008000">\u2028\u2028KABUL, Afghanistan (Feb. 19)\u2028\u2028ISAF service members at a compound in Sangin district, Helmand </span>
<span style="color: #008000; text-decoration-color: #008000">province observed numerous insurgents north and south of their position talking on radios today. After gaining </span>
<span style="color: #008000; text-decoration-color: #008000">positive identification of the insurgent positions, the coalition troops engaged, killing several insurgents. </span>
<span style="color: #008000; text-decoration-color: #008000">Later, the ISAF troops observed more insurgents positioning in the area with weapons. After positive </span>
<span style="color: #008000; text-decoration-color: #008000">identification, coalition forces continued firing on the various insurgent positions, resulting in several more </span>
<span style="color: #008000; text-decoration-color: #008000">insurgents being killed."</span><span style="font-weight: bold">}</span>, <span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"from"</span>: <span style="color: #008000; text-decoration-color: #008000">"gpt"</span>, <span style="color: #008000; text-decoration-color: #008000">"value"</span>: <span style="color: #008000; text-decoration-color: #008000">"{\"name\":\"Several insurgents killed in </span>
<span style="color: #008000; text-decoration-color: #008000">Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_g</span>
<span style="color: #008000; text-decoration-color: #008000">roup\":[\"\"],\"min_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"air</span>
<span style="color: #008000; text-decoration-color: #008000">strike\":false,\"noshotsfired\":false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}"</span><span style="font-weight: bold">}]}</span>

</pre>
</div>
</div>
</section>
<section id="writing-template-free-json" class="level2">
<h2 class="anchored" data-anchor-id="writing-template-free-json">Writing Template-Free JSON</h2>
<p>Another option available to us, especially if we are finetuning a base LLM (as opposed to one that has been instruction-tuned), is to write our data in a different format. Hamel’s <a href="https://hamel.dev/notes/llm/finetuning/09_template_free.html">written a guide for this on his blog</a> and that has also been absorbed into the official <code>axolotl</code> documentation, so <a href="https://hamel.dev/notes/llm/finetuning/09_template_free.html">read the blog</a> if you want more information.</p>
<p>The basic idea is that instead of following a format like the one above, we can essentially create our own that’s custom to our own needs. You want this kind of freedom because to follow one of the standard templates is sometimes to shoot yourself in the food with artifacts of those templates that you don’t need in your output.</p>
<p>The key is to specify <code>train_on_inputs</code> as false in our <code>axolotl</code> config which will allow us to mask certain segments of our input data. This means that our model won’t learn the inputs but only the outputs (which we’ll specify).</p>
<p>All that we have to do is set up the JSONL output in a way that makes sense for our use case:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">template_free_train_target_file_path <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb12-2">    <span class="st" style="color: #20794D;">"../data/templatefree_isaf_press_releases_ft_train.jsonl"</span></span>
<span id="cb12-3">)</span>
<span id="cb12-4">template_free_test_target_file_path <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb12-5">    <span class="st" style="color: #20794D;">"../data/templatefree_isaf_press_releases_ft_test.jsonl"</span></span>
<span id="cb12-6">)</span>
<span id="cb12-7"></span>
<span id="cb12-8"></span>
<span id="cb12-9"><span class="kw" style="color: #003B4F;">def</span> write_data_to_jsonl(df: pd.DataFrame, target_file_path: <span class="bu" style="color: null;">str</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> <span class="va" style="color: #111111;">None</span>:</span>
<span id="cb12-10">    events: List[IsafEvent] <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb12-11"></span>
<span id="cb12-12">    <span class="cf" style="color: #003B4F;">for</span> i, row <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">list</span>(df.iterrows()):</span>
<span id="cb12-13">        event_types <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb12-14">            eventtype.strip().lower() <span class="cf" style="color: #003B4F;">for</span> eventtype <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"eventtype"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb12-15">        )</span>
<span id="cb12-16">        provinces <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb12-17">            province.strip().lower() <span class="cf" style="color: #003B4F;">for</span> province <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"province"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb12-18">        )</span>
<span id="cb12-19">        target_groups <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">set</span>(</span>
<span id="cb12-20">            target_group.strip().lower()</span>
<span id="cb12-21">            <span class="cf" style="color: #003B4F;">for</span> target_group <span class="kw" style="color: #003B4F;">in</span> row[<span class="st" style="color: #20794D;">"targetgroup"</span>].split(<span class="st" style="color: #20794D;">","</span>)</span>
<span id="cb12-22">        )</span>
<span id="cb12-23"></span>
<span id="cb12-24">        events.append(</span>
<span id="cb12-25">            IsafEvent(</span>
<span id="cb12-26">                name<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"name"</span>],</span>
<span id="cb12-27">                text<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"text"</span>],</span>
<span id="cb12-28">                start_date<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"StartDate"</span>].to_pydatetime().date(),</span>
<span id="cb12-29">                event_type<span class="op" style="color: #5E5E5E;">=</span>event_types,</span>
<span id="cb12-30">                province<span class="op" style="color: #5E5E5E;">=</span>provinces,</span>
<span id="cb12-31">                target_group<span class="op" style="color: #5E5E5E;">=</span>target_groups,</span>
<span id="cb12-32">                min_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minkilled"</span>]),</span>
<span id="cb12-33">                min_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"mincaptured"</span>]),</span>
<span id="cb12-34">                killq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb12-35">                captureq<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"captureq"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb12-36">                killcaptureraid<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"killcaptureraid"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb12-37">                airstrike<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"airstrike"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb12-38">                noshotsfired<span class="op" style="color: #5E5E5E;">=</span>row[<span class="st" style="color: #20794D;">"noshotsfired"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">"true"</span>,</span>
<span id="cb12-39">                min_leaders_killed<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderskilled"</span>]),</span>
<span id="cb12-40">                min_leaders_captured<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">int</span>(row[<span class="st" style="color: #20794D;">"minleaderscaptured"</span>]),</span>
<span id="cb12-41">            )</span>
<span id="cb12-42">        )</span>
<span id="cb12-43"></span>
<span id="cb12-44">    processed_data <span class="op" style="color: #5E5E5E;">=</span> [</span>
<span id="cb12-45">        {</span>
<span id="cb12-46">            <span class="st" style="color: #20794D;">"segments"</span>: [</span>
<span id="cb12-47">                {</span>
<span id="cb12-48">                    <span class="st" style="color: #20794D;">"label"</span>: <span class="va" style="color: #111111;">False</span>,</span>
<span id="cb12-49">                    <span class="st" style="color: #20794D;">"text"</span>: <span class="st" style="color: #20794D;">"&lt;s&gt;You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']"</span>,</span>
<span id="cb12-50">                },</span>
<span id="cb12-51">                {<span class="st" style="color: #20794D;">"label"</span>: <span class="va" style="color: #111111;">False</span>, <span class="st" style="color: #20794D;">"text"</span>: <span class="ss" style="color: #20794D;">f"PRESS RELEASE TEXT: </span><span class="sc" style="color: #5E5E5E;">{</span>event<span class="sc" style="color: #5E5E5E;">.</span>text<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>},</span>
<span id="cb12-52">                {</span>
<span id="cb12-53">                    <span class="st" style="color: #20794D;">"label"</span>: <span class="va" style="color: #111111;">True</span>,</span>
<span id="cb12-54">                    <span class="st" style="color: #20794D;">"text"</span>: <span class="ss" style="color: #20794D;">f"</span><span class="sc" style="color: #5E5E5E;">{</span>event<span class="sc" style="color: #5E5E5E;">.</span>model_dump_json(exclude<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">'text'</span>})<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">&lt;/s&gt;"</span>,</span>
<span id="cb12-55">                },</span>
<span id="cb12-56">            ]</span>
<span id="cb12-57">        }</span>
<span id="cb12-58">        <span class="cf" style="color: #003B4F;">for</span> event <span class="kw" style="color: #003B4F;">in</span> events</span>
<span id="cb12-59">    ]</span>
<span id="cb12-60"></span>
<span id="cb12-61">    <span class="co" style="color: #5E5E5E;"># Write the processed data to a JSONL file</span></span>
<span id="cb12-62">    os.makedirs(os.path.dirname(target_file_path), exist_ok<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb12-63">    <span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(target_file_path, <span class="st" style="color: #20794D;">"w"</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb12-64">        <span class="cf" style="color: #003B4F;">for</span> item <span class="kw" style="color: #003B4F;">in</span> processed_data:</span>
<span id="cb12-65">            f.write(json.dumps(item) <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>)</span>
<span id="cb12-66"></span>
<span id="cb12-67"></span>
<span id="cb12-68">write_data_to_jsonl(train_df, template_free_train_target_file_path)</span>
<span id="cb12-69">write_data_to_jsonl(test_df, template_free_test_target_file_path)</span></code></pre></div>
</div>
<p>And you can now see the difference in the format of the JSONL dataset we’ve constructured:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(template_free_train_target_file_path, <span class="st" style="color: #20794D;">"r"</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb13-2">    <span class="bu" style="color: null;">print</span>(f.readline())</span></code></pre></div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"segments"</span>: <span style="font-weight: bold">[{</span><span style="color: #008000; text-decoration-color: #008000">"label"</span>: false, <span style="color: #008000; text-decoration-color: #008000">"text"</span>: <span style="color: #008000; text-decoration-color: #008000">"&lt;s&gt;You are an expert at identifying events in a press release. You are </span>
<span style="color: #008000; text-decoration-color: #008000">precise and always make sure you are correct, drawing inference from the text of the press release. event_types = </span>
<span style="color: #008000; text-decoration-color: #008000">['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces =</span>
<span style="color: #008000; text-decoration-color: #008000">['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', </span>
<span style="color: #008000; text-decoration-color: #008000">'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', </span>
<span style="color: #008000; text-decoration-color: #008000">'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', </span>
<span style="color: #008000; text-decoration-color: #008000">'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', </span>
<span style="color: #008000; text-decoration-color: #008000">'judq', 'iju', 'hik', 'ttp', 'other']"</span><span style="color: #000000; text-decoration-color: #000000; font-weight: bold">}</span><span style="color: #000000; text-decoration-color: #000000">, </span><span style="color: #000000; text-decoration-color: #000000; font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"label"</span><span style="color: #000000; text-decoration-color: #000000">: false, </span><span style="color: #008000; text-decoration-color: #008000">"text"</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">"PRESS RELEASE TEXT: ISAF Joint Command Evening </span>
<span style="color: #008000; text-decoration-color: #008000">Operational Update Feb. 19, 2011\nISAF Joint Command - Afghanistan\u20282011-02-S-143\u2028For Immediate Release </span>
<span style="color: #008000; text-decoration-color: #008000">\u2028\u2028KABUL, Afghanistan (Feb. 19)\u2028\u2028ISAF service members at a compound in Sangin district, Helmand </span>
<span style="color: #008000; text-decoration-color: #008000">province observed numerous insurgents north and south of their position talking on radios today. After gaining </span>
<span style="color: #008000; text-decoration-color: #008000">positive identification of the insurgent positions, the coalition troops engaged, killing several insurgents. </span>
<span style="color: #008000; text-decoration-color: #008000">Later, the ISAF troops observed more insurgents positioning in the area with weapons. After positive </span>
<span style="color: #008000; text-decoration-color: #008000">identification, coalition forces continued firing on the various insurgent positions, resulting in several more </span>
<span style="color: #008000; text-decoration-color: #008000">insurgents being killed."</span><span style="color: #000000; text-decoration-color: #000000; font-weight: bold">}</span><span style="color: #000000; text-decoration-color: #000000">, </span><span style="color: #000000; text-decoration-color: #000000; font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"label"</span><span style="color: #000000; text-decoration-color: #000000">: true, </span><span style="color: #008000; text-decoration-color: #008000">"text"</span><span style="color: #000000; text-decoration-color: #000000">: </span><span style="color: #008000; text-decoration-color: #008000">"{\"name\":\"Several insurgents killed in </span>
<span style="color: #008000; text-decoration-color: #008000">Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_g</span>
<span style="color: #008000; text-decoration-color: #008000">roup\":[\"\"],\"min_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"air</span>
<span style="color: #008000; text-decoration-color: #008000">strike\":false,\"noshotsfired\":false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}&lt;/s&gt;"</span><span style="font-weight: bold">}]}</span>

</pre>
</div>
</div>
</section>
</section>
<section id="finetuning-our-model" class="level1">
<h1>Finetuning our model</h1>
<p>With our datasets ready, finetuning our model is a simple matter of running the following two commands:</p>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb14-1"><span class="co" style="color: #5E5E5E;"># preprocess the data ahead of training</span></span>
<span id="cb14-2"><span class="va" style="color: #111111;">CUDA_VISIBLE_DEVICES</span><span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">""</span> <span class="ex" style="color: null;">python</span> <span class="at" style="color: #657422;">-m</span> axolotl.cli.preprocess configs/tiny-llama/lora.yml</span>
<span id="cb14-3"><span class="co" style="color: #5E5E5E;"># train the model</span></span>
<span id="cb14-4"><span class="ex" style="color: null;">accelerate</span> launch <span class="at" style="color: #657422;">-m</span> axolotl.cli.train configs/tiny-llama/lora.yml</span></code></pre></div>
<p><code>axolotl</code> handles everything else. You’ll note that we’re using a pre-prepared config file which is quite long but I’m basically using a default config with only a few changes. We can use <a href="https://github.com/sharkdp/bat"><code>bat</code></a> to view the config file:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="op" style="color: #5E5E5E;">!</span>bat <span class="op" style="color: #5E5E5E;">/</span>home<span class="op" style="color: #5E5E5E;">/</span>strickvl<span class="op" style="color: #5E5E5E;">/</span>coding<span class="op" style="color: #5E5E5E;">/</span>isafpr_finetune<span class="op" style="color: #5E5E5E;">/</span>configs<span class="op" style="color: #5E5E5E;">/</span>tiny<span class="op" style="color: #5E5E5E;">-</span>llama<span class="op" style="color: #5E5E5E;">/</span>lora.yml</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>───────┬────────────────────────────────────────────────────────────────────────
       │ File: /home/strickvl/coding/isafpr_finetune/configs/tiny-llama/lora.yml
───────┼────────────────────────────────────────────────────────────────────────
   1   │ base_model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
   2   │ model_type: LlamaForCausalLM
   3   │ tokenizer_type: LlamaTokenizer
   4   │ 
   5   │ load_in_8bit: false
   6 + │ # I'm training on 4090 GPUs
   7 + │ # so I'm using 4-bit precision to save on memory
   8   │ load_in_4bit: true
   9   │ strict: false
  10   │ 
  11   │ data_seed: 42
  12   │ seed: 42
  13   │ 
  14   │ datasets:
  15 ~ │   - path: data/templatefree_isaf_press_releases_ft_train.jsonl
  16 ~ │     type: input_output
  17   │ dataset_prepared_path:
  18 ~ │ val_set_size: 0.1
  19   │ output_dir: ./outputs/tiny-llama/lora-out
  20   │ hub_model_id: strickvl/isafpr-tiny-llama-lora
  21   │ 
  22   │ sequence_len: 4096
  23   │ sample_packing: true
  24   │ eval_sample_packing: false
  25   │ pad_to_sequence_len: true
  26   │ 
  27   │ adapter: lora
  28   │ lora_model_dir:
  29   │ lora_r: 32
  30   │ lora_alpha: 16
  31   │ lora_dropout: 0.05
  32   │ lora_target_linear: true
  33   │ lora_fan_in_fan_out:
  34   │ 
  35   │ wandb_project: isaf_pr_ft
  36   │ wandb_entity: strickvl
  37   │ wandb_watch:
  38   │ wandb_name:
  39   │ wandb_log_model:
  40   │ 
  41   │ gradient_accumulation_steps: 4
  42   │ micro_batch_size: 2
  43   │ num_epochs: 4
  44   │ optimizer: adamw_bnb_8bit
  45   │ lr_scheduler: cosine
  46   │ learning_rate: 0.0002
  47   │ 
  48   │ train_on_inputs: false
  49   │ group_by_length: false
  50   │ bf16: auto
  51   │ fp16:
  52   │ tf32: false
  53   │ 
  54   │ gradient_checkpointing: true
  55   │ early_stopping_patience:
  56   │ resume_from_checkpoint:
  57   │ local_rank:
  58   │ logging_steps: 1
  59   │ xformers_attention:
  60   │ flash_attention: true
  61   │ 
  62   │ warmup_steps: 10
  63   │ evals_per_epoch: 4
  64   │ saves_per_epoch: 1
  65   │ debug:
  66   │ deepspeed:
  67   │ weight_decay: 0.0
  68   │ fsdp:
  69   │ fsdp_config:
  70   │ special_tokens:
───────┴────────────────────────────────────────────────────────────────────────</code></pre>
</div>
</div>
<p>You can check out some of the trainings I did with the following links:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Config</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Wandb Report</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Tiny-Llama (Template Free)</td>
<td style="text-align: center;"><a href="https://github.com/strickvl/isafpr_finetune/blob/main/configs/tiny-llama/lora-templatefree.yml">link</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/strickvl/isafpr-tiny-llama-lora-templatefree">link</a></td>
<td style="text-align: center;"><a href="https://wandb.ai/strickvl/isaf_pr_ft/runs/clzgrvww">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">Tiny-Llama (ShareGPT)</td>
<td style="text-align: center;"><a href="https://github.com/strickvl/isafpr_finetune/blob/main/configs/tiny-llama/lora-sharegpt.yml">link</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/strickvl/isafpr-tiny-llama-lora-sharegpt">link</a></td>
<td style="text-align: center;"><a href="https://wandb.ai/strickvl/isaf_pr_ft/runs/d5von2yp">link</a></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Llama-3 (Template Free)</td>
<td style="text-align: center;"><a href="https://github.com/strickvl/isafpr_finetune/blob/main/configs/llama-3/lora-templatefree.yml">link</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/strickvl/isafpr-llama3-lora-templatefree">link</a></td>
<td style="text-align: center;"><a href="https://wandb.ai/strickvl/isaf_pr_ft/runs/pm4obu3k">link</a></td>
</tr>
<tr class="even">
<td style="text-align: left;">Mistral (Template Free)</td>
<td style="text-align: center;"><a href="https://github.com/strickvl/isafpr_finetune/blob/main/configs/mistral/lora-templatefree.yml">link</a></td>
<td style="text-align: center;"><a href="https://huggingface.co/strickvl/isafpr-mistral-lora-templatefree">link</a></td>
<td style="text-align: center;"><a href="https://wandb.ai/strickvl/isaf_pr_ft/runs/rzo0sad4">link</a></td>
</tr>
</tbody>
</table>
<p>Now that we have 4 models, we can try some out to see how they fare with some data they haven’t yet seen (from the test set). I used the code from <a href="https://github.com/parlance-labs/ftcourse/blob/master/06_sanity_check.ipynb">Hamel’s Sanity Check notebook</a> to generate some predictions and evaluate them:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> Union</span>
<span id="cb17-2"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb17-3"><span class="im" style="color: #00769E;">from</span> peft <span class="im" style="color: #00769E;">import</span> AutoPeftModelForCausalLM</span>
<span id="cb17-4"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> AutoTokenizer</span>
<span id="cb17-5"></span>
<span id="cb17-6"></span>
<span id="cb17-7"><span class="kw" style="color: #003B4F;">def</span> prompt(press_release):</span>
<span id="cb17-8">    <span class="cf" style="color: #003B4F;">return</span> <span class="ss" style="color: #20794D;">f"""You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']</span></span>
<span id="cb17-9"></span>
<span id="cb17-10"><span class="ss" style="color: #20794D;">### Instruction:</span></span>
<span id="cb17-11"></span>
<span id="cb17-12"><span class="ss" style="color: #20794D;">PRESS RELEASE TEXT: "</span><span class="sc" style="color: #5E5E5E;">{</span>press_release<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb17-13"></span>
<span id="cb17-14"><span class="ss" style="color: #20794D;">### Response:</span></span>
<span id="cb17-15"><span class="ss" style="color: #20794D;">"""</span></span>
<span id="cb17-16"></span>
<span id="cb17-17"></span>
<span id="cb17-18"><span class="kw" style="color: #003B4F;">def</span> prompt_tok(</span>
<span id="cb17-19">    model: AutoPeftModelForCausalLM,</span>
<span id="cb17-20">    tokenizer: AutoTokenizer,</span>
<span id="cb17-21">    press_release: <span class="bu" style="color: null;">str</span>,</span>
<span id="cb17-22">    return_ids: <span class="bu" style="color: null;">bool</span> <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>,</span>
<span id="cb17-23">) <span class="op" style="color: #5E5E5E;">-&gt;</span> Union[<span class="bu" style="color: null;">str</span>, torch.Tensor]:</span>
<span id="cb17-24">    _p <span class="op" style="color: #5E5E5E;">=</span> prompt(press_release)</span>
<span id="cb17-25">    input_ids <span class="op" style="color: #5E5E5E;">=</span> tokenizer(_p, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>).input_ids.cuda()</span>
<span id="cb17-26">    out_ids <span class="op" style="color: #5E5E5E;">=</span> model.generate(input_ids<span class="op" style="color: #5E5E5E;">=</span>input_ids, max_new_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5000</span>, do_sample<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb17-27">    ids <span class="op" style="color: #5E5E5E;">=</span> out_ids.detach().cpu().numpy()</span>
<span id="cb17-28">    <span class="cf" style="color: #003B4F;">if</span> return_ids:</span>
<span id="cb17-29">        <span class="cf" style="color: #003B4F;">return</span> out_ids</span>
<span id="cb17-30">    <span class="cf" style="color: #003B4F;">return</span> tokenizer.batch_decode(ids, skip_special_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)[<span class="dv" style="color: #AD0000;">0</span>][<span class="bu" style="color: null;">len</span>(_p) :]</span></code></pre></div>
</div>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;">from</span> rich <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">print</span></span>
<span id="cb18-2"><span class="im" style="color: #00769E;">import</span> json</span>
<span id="cb18-3"></span>
<span id="cb18-4">tinyllama_templatefree_model_id <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"strickvl/isafpr-tiny-llama-lora-templatefree"</span></span>
<span id="cb18-5">model <span class="op" style="color: #5E5E5E;">=</span> AutoPeftModelForCausalLM.from_pretrained(tinyllama_templatefree_model_id).cuda()</span>
<span id="cb18-6">tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(tinyllama_templatefree_model_id)</span>
<span id="cb18-7">tokenizer.pad_token <span class="op" style="color: #5E5E5E;">=</span> tokenizer.eos_token</span>
<span id="cb18-8"></span>
<span id="cb18-9">press_release_sample1 <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"""2011-11-S-011 ISAF Joint Command - Afghanistan For Immediate Release KABUL, Afghanistan (Nov. 7, 2011) — A combined Afghan and coalition security force conducted an operation in search of a Haqqani facilitator in Argo district, Badakshan province. The facilitator coordinates suicide attacks with other insurgent leaders in the area. During the operation, a local national male failed to comply with repeated verbal warnings and displayed hostile intent toward the security force. The security force engaged the individual, resulting in his death. The security force confiscated a shotgun and intelligence linking the local national to the Haqqani network. The security force also detained two suspected insurgents during the operation."""</span></span>
<span id="cb18-10"></span>
<span id="cb18-11">out <span class="op" style="color: #5E5E5E;">=</span> prompt_tok(model, tokenizer, press_release_sample1)</span>
<span id="cb18-12"><span class="bu" style="color: null;">print</span>(out)</span>
<span id="cb18-13">out_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(out)</span>
<span id="cb18-14"><span class="bu" style="color: null;">print</span>(out_dict)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span><span style="color: #008000; text-decoration-color: #008000">"name"</span>:<span style="color: #008000; text-decoration-color: #008000">"2011-11-07-airstrike"</span>,<span style="color: #008000; text-decoration-color: #008000">"start_date"</span>:<span style="color: #008000; text-decoration-color: #008000">"2011-11-07"</span>,<span style="color: #008000; text-decoration-color: #008000">"event_type"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"airstrike"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"province"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"badakhshan"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"targ</span>
<span style="color: #008000; text-decoration-color: #008000">et_group"</span>:<span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">"haqqani"</span><span style="font-weight: bold">]</span>,<span style="color: #008000; text-decoration-color: #008000">"min_killed"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,<span style="color: #008000; text-decoration-color: #008000">"min_captured"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,<span style="color: #008000; text-decoration-color: #008000">"killq"</span>:true,<span style="color: #008000; text-decoration-color: #008000">"captureq"</span>:true,<span style="color: #008000; text-decoration-color: #008000">"killcaptureraid"</span>:true,<span style="color: #008000; text-decoration-color: #008000">"airstrik</span>
<span style="color: #008000; text-decoration-color: #008000">e"</span>:true,<span style="color: #008000; text-decoration-color: #008000">"noshotsfired"</span>:false,<span style="color: #008000; text-decoration-color: #008000">"min_leaders_killed"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,<span style="color: #008000; text-decoration-color: #008000">"min_leaders_captured"</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span><span style="font-weight: bold">}</span>
</pre>
</div>
<div class="cell-output cell-output-display">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">{</span>
    <span style="color: #008000; text-decoration-color: #008000">'name'</span>: <span style="color: #008000; text-decoration-color: #008000">'2011-11-07-airstrike'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'start_date'</span>: <span style="color: #008000; text-decoration-color: #008000">'2011-11-07'</span>,
    <span style="color: #008000; text-decoration-color: #008000">'event_type'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'airstrike'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'province'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'badakhshan'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'target_group'</span>: <span style="font-weight: bold">[</span><span style="color: #008000; text-decoration-color: #008000">'haqqani'</span><span style="font-weight: bold">]</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'captureq'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'killcaptureraid'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'airstrike'</span>: <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
    <span style="color: #008000; text-decoration-color: #008000">'noshotsfired'</span>: <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_killed'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,
    <span style="color: #008000; text-decoration-color: #008000">'min_leaders_captured'</span>: <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>
<span style="font-weight: bold">}</span>
</pre>
</div>
</div>
<p>The model has certainly learned to output JSON, and it’s done an <em>ok</em> job at parsing the contents of the text, but it has also made errors. It’s said that this was an airstrike whereas no airstrike is mentioned in the text.</p>
<p>This is only a finetune of <a href="https://github.com/jzhang38/TinyLlama">Tiny-Llama</a>, a much smaller version of a Llama model (with v2 architecture). Let’s maybe check out how our Mistral finetune did in comparison:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">mistral_templatefree_model_id <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"strickvl/isafpr-mistral-lora-templatefree"</span></span>
<span id="cb20-2">mistral_model <span class="op" style="color: #5E5E5E;">=</span> AutoPeftModelForCausalLM.from_pretrained(</span>
<span id="cb20-3">    mistral_templatefree_model_id</span>
<span id="cb20-4">).cuda()</span>
<span id="cb20-5">mistral_tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(mistral_templatefree_model_id)</span>
<span id="cb20-6">mistral_tokenizer.pad_token <span class="op" style="color: #5E5E5E;">=</span> mistral_tokenizer.eos_token</span>
<span id="cb20-7"></span>
<span id="cb20-8">mistral_out <span class="op" style="color: #5E5E5E;">=</span> prompt_tok(mistral_model, mistral_tokenizer, press_release_sample1)</span>
<span id="cb20-9"><span class="bu" style="color: null;">print</span>(mistral_out)</span>
<span id="cb20-10">mistral_out_dict <span class="op" style="color: #5E5E5E;">=</span> json.loads(mistral_out)</span>
<span id="cb20-11"><span class="bu" style="color: null;">print</span>(mistral_out_dict)</span></code></pre></div>
</div>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb21-1"><span class="fu" style="color: #4758AB;">{</span></span>
<span id="cb21-2">    <span class="er" style="color: #AD0000;">'name'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">'</span><span class="dv" style="color: #AD0000;">1</span><span class="er" style="color: #AD0000;">'</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-3">    <span class="er" style="color: #AD0000;">'start_date'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">'</span><span class="dv" style="color: #AD0000;">2011-11-07</span><span class="er" style="color: #AD0000;">'</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-4">    <span class="er" style="color: #AD0000;">'event_type'</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span><span class="er" style="color: #AD0000;">'captureandkill'</span><span class="ot" style="color: #003B4F;">]</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-5">    <span class="er" style="color: #AD0000;">'province'</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span><span class="er" style="color: #AD0000;">'badakhshan'</span><span class="ot" style="color: #003B4F;">]</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-6">    <span class="er" style="color: #AD0000;">'target_group'</span><span class="fu" style="color: #4758AB;">:</span> <span class="ot" style="color: #003B4F;">[</span><span class="er" style="color: #AD0000;">'haqqani'</span><span class="ot" style="color: #003B4F;">]</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-7">    <span class="er" style="color: #AD0000;">'min_killed'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">1</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-8">    <span class="er" style="color: #AD0000;">'min_captured'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">2</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-9">    <span class="er" style="color: #AD0000;">'killq'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">True</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-10">    <span class="er" style="color: #AD0000;">'captureq'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">True</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-11">    <span class="er" style="color: #AD0000;">'killcaptureraid'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">True</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-12">    <span class="er" style="color: #AD0000;">'airstrike'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">False</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-13">    <span class="er" style="color: #AD0000;">'noshotsfired'</span><span class="fu" style="color: #4758AB;">:</span> <span class="er" style="color: #AD0000;">False</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-14">    <span class="er" style="color: #AD0000;">'min_leaders_killed'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">0</span><span class="fu" style="color: #4758AB;">,</span></span>
<span id="cb21-15">    <span class="er" style="color: #AD0000;">'min_leaders_captured'</span><span class="fu" style="color: #4758AB;">:</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb21-16"><span class="fu" style="color: #4758AB;">}</span></span></code></pre></div>
<p>I actually had to cheat a bit to get this output. I was getting an out-of-memory (OOM) error when trying to run the inference locally, so I ran the inference using Modal’s compute platform. You can see the script where I ran the one-off inference <a href="https://github.com/strickvl/isafpr_finetune/blob/main/notebooks/sanity_check.py">here</a>. Note that I had to pass in my Hugging Face write token ahead of running the script since Mistral is a gated model. (So the final command was <code>HF_TOKEN="hf_MY_TOKEN_VALUE_WENT_HERE" modal run notebooks/sanity_check.py</code>)</p>
<p>The output it produced, however, is pretty spot on. Actually I noticed that it outperformed the original data since it was able to correctly identify the event type as <code>captureandkill</code> and specify the boolean <code>killcaptureraid</code> as <code>True</code> even though in the ground truth dataset it seems I mislabelled the data and stated that the <code>killcaptureraid</code> value was <code>False</code>.</p>
<p>One thing you might also have noticed is that the <code>name</code> attribute was predicted as being <code>1</code>. Actually, this is not really a problem. When I was labelling the dataset there were sometimes press releases that I needed to split up into separate reports, so I’d give them a numerical name where these were split up. The name is really just a summary of the event, but there are a non-trivial number of events which have numbers as their <code>name</code>, so it’s probably not even a useful field to be training on or trying to predict. Rather, if this was really necessary I could train a model to summarise the content as a headline, but for my specific use case I’m not sure it’s even useful to have this information at all.</p>
<p>I tried getting my Llama3 model to make its predictions, but all I got out was an endless stream of JSON content.</p>
<p><img src="https://mlops.systems/posts/images/llama3-inference.png" class="img-fluid"></p>
<p>I suspect it has something to do with the presence or absence of an <code>&lt;s&gt;</code> tag, which I was using for EOS or ‘end of stream’. I recall there were some error messages during training around these lines, so potentially I’ll want to look into that. I also ran into the same problem as with the Mistral model, i.e having to run it on Modal, and was only able to make it work by specifying a different datatype when loading the model:</p>
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">model <span class="op" style="color: #5E5E5E;">=</span> AutoPeftModelForCausalLM.from_pretrained(</span>
<span id="cb22-2">        model_id,</span>
<span id="cb22-3">        torch_dtype<span class="op" style="color: #5E5E5E;">=</span>torch.bfloat16,</span>
<span id="cb22-4">        device_map<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"auto"</span>,</span>
<span id="cb22-5">    ).cuda()</span></code></pre></div>
<p>Thanks to <a href="https://drchrislevy.github.io/">Chris Levy</a> over on the course forum for suggesting this approach. While this works for getting it to run on Modal, I still am looking for a way to get my Mistral and Llama3 models to run locally, so I’ll probably have to investigate how to optimise the model’s memory usage further.</p>
</section>
<section id="next-steps" class="level1">
<h1>Next Steps</h1>
<p>I’m pretty happy with this set of experiments. It was exciting to see that it’s relatively quick to do experiments with finetuning LLMs: a finetune of Llama3 or Mistral over four epochs only took about 35 minutes on my local machine.</p>
<p>Some obvious next steps for this project are:</p>
<ol type="1">
<li>Figure out the model loading issue mentioned at the end of this post: how to load my models locally and what are the tradeoffs of whatever approaches are possible?</li>
<li>Get more deliberate about adding in some manual evaluations: figure out some examples where I’m deliberately testing some known edge cases and hard-coded outputs. This feels like something that ought to be done sooner than later.</li>
<li>Run the evaluations I had for GPT-4 in the previous blog using my new model(s). Let’s see how well my finetuned models do in comparison.</li>
<li>Run training jobs on all the different platforms where we have course credits in order to get a sense for hwo they work. I’ve only tried out Modal so far, and not even for training, just inference.</li>
<li>Pick one of the base models and try some hyperparameter tuning to see which combination of parameters and config values gives the best performance.</li>
<li>Think about model deployment for whichever candidate I choose as being the best, then run some benchmarking / tests to see how well it performs and whether we can ever compete with the price point of something like GPT-4 (esp when we factor in the accuracy scores across my evaluations).</li>
</ol>
<p>Having written these down, the order in which I wrote them seems like a sensible way to keep going. So my next step will be to read up on the model loading a bit and to try out some possible solutions for loading my models locally.</p>
<p>I’ll also add one personal note on the experience so far. I’m really enjoying the experience of being very hands-on. There are some parts of what I did so far that perhaps require a bit of experience to quickly move past some boring work (converting data from one format to the other and so on), but for the most part I’ve found the work of finetuning models to be really accessible to someone without much technical background. Even the conversion of datasets and construction of templates is all relatively straightforward and (beyond needing to have the intuition to <em>know</em> that that’s a thing you have to do) you could accomplish most of it using Claude or GPT-4 without even any technical background at all.</p>
<p>It’s also quite empowering to see all these vistas open up before me, especially the ones enabled by being able to finetune these models on my local machine. I’m really excited about the next experiments and stages of this project to come, in particular how much there is to learn!</p>


</section>

 ]]></description>
  <category>nlp</category>
  <category>afghanistan</category>
  <category>llms</category>
  <category>miniproject</category>
  <category>finetuning</category>
  <category>isafpr</category>
  <guid>https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html</guid>
  <pubDate>Fri, 14 Jun 2024 22:00:00 GMT</pubDate>
  <media:content url="https://mlops.systems/posts/images/first-finetune.png" medium="image" type="image/png" height="116" width="144"/>
</item>
</channel>
</rss>
