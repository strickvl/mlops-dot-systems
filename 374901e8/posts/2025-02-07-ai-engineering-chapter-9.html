<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Strick van Linschoten">
<meta name="dcterms.date" content="2025-02-07">
<meta name="description" content="Chapter 9 is a guide to ML inference optimization covering compute and memory bottlenecks, performance metrics, and practical implementation strategies. This technical summary explores model-level, hardware-level, and service-level optimizations, with detailed explanations of batching strategies, parallelism approaches, and attention mechanisms - essential knowledge for ML engineers working to reduce inference costs and improve system performance.">

<title>Alex Strick van Linschoten - Notes on ‘AI Engineering’ chapter 9: Inference Optimisation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script defer="" data-domain="mlops.systems" src="https://plausible.io/js/script.js"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Alex Strick van Linschoten - Notes on ‘AI Engineering’ chapter 9: Inference Optimisation">
<meta property="og:description" content="Chapter 9 is a guide to ML inference optimization covering compute and memory bottlenecks, performance metrics, and practical implementation strategies.">
<meta property="og:image" content="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/flash-attention.png">
<meta property="og:site-name" content="Alex Strick van Linschoten">
<meta property="og:image:height" content="808">
<meta property="og:image:width" content="1404">
<meta name="twitter:title" content="Alex Strick van Linschoten - Notes on ‘AI Engineering’ chapter 9: Inference Optimisation">
<meta name="twitter:description" content="Chapter 9 is a guide to ML inference optimization covering compute and memory bottlenecks, performance metrics, and practical implementation strategies.">
<meta name="twitter:image" content="https://mlops.systems/posts/images/2025-02-07-ai-engineering-chapter-9/flash-attention.png">
<meta name="twitter:creator" content="@strickvl">
<meta name="twitter:image-height" content="808">
<meta name="twitter:image-width" content="1404">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Alex Strick van Linschoten</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../til.html">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/strickvl"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://sigmoid.social/web/@alexstrick"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/strickvl"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mlops.systems/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Notes on ‘AI Engineering’ chapter 9: Inference Optimisation</h1>
                  <div>
        <div class="description">
          Chapter 9 is a guide to ML inference optimization covering compute and memory bottlenecks, performance metrics, and practical implementation strategies. This technical summary explores model-level, hardware-level, and service-level optimizations, with detailed explanations of batching strategies, parallelism approaches, and attention mechanisms - essential knowledge for ML engineers working to reduce inference costs and improve system performance.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">books-i-read</div>
                <div class="quarto-category">inference</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">llms</div>
                <div class="quarto-category">hardware</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Strick van Linschoten </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 7, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-9-overview" id="toc-chapter-9-overview" class="nav-link active" data-scroll-target="#chapter-9-overview">Chapter 9: Overview</a></li>
  <li><a href="#core-concepts-and-bottlenecks" id="toc-core-concepts-and-bottlenecks" class="nav-link" data-scroll-target="#core-concepts-and-bottlenecks">Core Concepts and Bottlenecks</a></li>
  <li><a href="#inference-apis-and-service-patterns" id="toc-inference-apis-and-service-patterns" class="nav-link" data-scroll-target="#inference-apis-and-service-patterns">Inference APIs and Service Patterns</a></li>
  <li><a href="#inference-performance-metrics" id="toc-inference-performance-metrics" class="nav-link" data-scroll-target="#inference-performance-metrics">Inference Performance Metrics</a>
  <ul class="collapse">
  <li><a href="#latency-components" id="toc-latency-components" class="nav-link" data-scroll-target="#latency-components">Latency Components</a></li>
  <li><a href="#throughput-and-goodput-metrics" id="toc-throughput-and-goodput-metrics" class="nav-link" data-scroll-target="#throughput-and-goodput-metrics">Throughput and Goodput Metrics</a></li>
  <li><a href="#resource-utilization-metrics" id="toc-resource-utilization-metrics" class="nav-link" data-scroll-target="#resource-utilization-metrics">Resource Utilization Metrics</a></li>
  </ul></li>
  <li><a href="#hardware-considerations-and-ai-accelerators" id="toc-hardware-considerations-and-ai-accelerators" class="nav-link" data-scroll-target="#hardware-considerations-and-ai-accelerators">Hardware Considerations and AI Accelerators</a>
  <ul class="collapse">
  <li><a href="#popular-ai-accelerators" id="toc-popular-ai-accelerators" class="nav-link" data-scroll-target="#popular-ai-accelerators">Popular AI Accelerators</a></li>
  </ul></li>
  <li><a href="#model-optimization-techniques" id="toc-model-optimization-techniques" class="nav-link" data-scroll-target="#model-optimization-techniques">Model Optimization Techniques</a>
  <ul class="collapse">
  <li><a href="#core-approaches" id="toc-core-approaches" class="nav-link" data-scroll-target="#core-approaches">Core Approaches</a></li>
  <li><a href="#advanced-decoding-strategies" id="toc-advanced-decoding-strategies" class="nav-link" data-scroll-target="#advanced-decoding-strategies">Advanced Decoding Strategies</a></li>
  </ul></li>
  <li><a href="#service-level-optimization" id="toc-service-level-optimization" class="nav-link" data-scroll-target="#service-level-optimization">Service-Level Optimization</a>
  <ul class="collapse">
  <li><a href="#batching-strategies" id="toc-batching-strategies" class="nav-link" data-scroll-target="#batching-strategies">Batching Strategies</a></li>
  <li><a href="#prefill-decode-decoupling" id="toc-prefill-decode-decoupling" class="nav-link" data-scroll-target="#prefill-decode-decoupling">Prefill-Decode Decoupling</a></li>
  <li><a href="#prompt-caching" id="toc-prompt-caching" class="nav-link" data-scroll-target="#prompt-caching">Prompt Caching</a></li>
  <li><a href="#parallelism-strategies" id="toc-parallelism-strategies" class="nav-link" data-scroll-target="#parallelism-strategies">Parallelism Strategies</a></li>
  </ul></li>
  <li><a href="#implementation-considerations" id="toc-implementation-considerations" class="nav-link" data-scroll-target="#implementation-considerations">Implementation Considerations</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>What follows are my notes on chapter 9 of Chip Huyen’s ‘AI Engineering’ book. This chapter was on optimising your inference and I learned a lot while reading it! There are interesting techniques like prompt caching and architectural considerations that I was vaguely aware of but hadn’t fully appreciated how they might work in real inference systems.</p>
<section id="chapter-9-overview" class="level2">
<h2 class="anchored" data-anchor-id="chapter-9-overview">Chapter 9: Overview</h2>
<p>Machine learning inference optimization operates across three fundamental domains: model optimization, hardware optimization, and service optimization. While hardware optimization often requires significant investment and may offer limited individual leverage, model and service optimizations provide substantial opportunities for AI engineers to improve performance.</p>
<blockquote class="blockquote">
<p><strong>Critical Cost Insight</strong>: A 2023 survey revealed that inference can account for up to 90% of machine learning costs in deployed AI systems, often exceeding training costs. This emphasizes why inference optimization isn’t just an engineering challenge - it’s a critical business necessity.</p>
</blockquote>
</section>
<section id="core-concepts-and-bottlenecks" class="level2">
<h2 class="anchored" data-anchor-id="core-concepts-and-bottlenecks">Core Concepts and Bottlenecks</h2>
<p>Understanding inference bottlenecks is essential for effective optimization. Two primary types of computational bottlenecks impact inference performance:</p>
<blockquote class="blockquote">
<p><strong>Compute-Bound Bottlenecks</strong>: Tasks that are limited by raw computational capacity, typically involving complex mathematical operations that take significant time to complete. These bottlenecks are particularly evident in computationally intensive operations within neural networks.</p>
<p><strong>Memory Bandwidth-Bound Bottlenecks</strong>: Limitations arising from data transfer requirements between system components, particularly between memory and processors. This becomes especially relevant in Large Language Models where significant amounts of data need to be moved between different memory hierarchies.</p>
</blockquote>
<p>In Large Language Models (LLMs), different operations exhibit varying profiles of these bottlenecks. This understanding has led to architectural decisions such as decoupling the prefilling step from the decode step in production environments - a practice that has become increasingly common as organizations optimize their inference pipelines.</p>
</section>
<section id="inference-apis-and-service-patterns" class="level2">
<h2 class="anchored" data-anchor-id="inference-apis-and-service-patterns">Inference APIs and Service Patterns</h2>
<p>Two fundamental approaches to inference deployment exist:</p>
<ol type="1">
<li><strong>Online Inference APIs</strong>
<ul>
<li>Optimized for minimal latency</li>
<li>Designed for real-time responses</li>
<li>Typically more expensive per inference</li>
<li>Critical for interactive applications</li>
</ul></li>
<li><strong>Batch Inference APIs</strong>
<ul>
<li>Optimized for cost efficiency</li>
<li>Can tolerate longer processing times (potentially hours)</li>
<li>Allows providers to optimize resource utilization</li>
<li>Ideal for bulk processing tasks</li>
</ul></li>
</ol>
</section>
<section id="inference-performance-metrics" class="level2">
<h2 class="anchored" data-anchor-id="inference-performance-metrics">Inference Performance Metrics</h2>
<p>Several key metrics help quantify inference performance:</p>
<section id="latency-components" class="level3">
<h3 class="anchored" data-anchor-id="latency-components">Latency Components</h3>
<ol type="1">
<li><strong>Time to First Token</strong>
<ul>
<li>Measures duration between query submission and initial response</li>
<li>Critical for user experience in interactive applications</li>
<li>Often a key optimization target for real-time systems</li>
</ul></li>
<li><strong>Time per Output Token</strong>
<ul>
<li>Generation speed after the first token</li>
<li>Impacts overall completion time</li>
<li>Can vary based on model architecture and optimization</li>
</ul></li>
<li><strong>Inter-token Latency</strong>
<ul>
<li>Time intervals between consecutive tokens</li>
<li>Affects perceived smoothness of generation</li>
<li>Important for streaming applications</li>
</ul></li>
</ol>
<p>Total latency can be expressed as: <code>time_to_first_token + (time_per_token × number_of_tokens)</code></p>
</section>
<section id="throughput-and-goodput-metrics" class="level3">
<h3 class="anchored" data-anchor-id="throughput-and-goodput-metrics">Throughput and Goodput Metrics</h3>
<blockquote class="blockquote">
<p><strong>Throughput</strong>: The number of output tokens per second an inference service can generate across all users and requests. This raw metric provides insight into system capacity.</p>
</blockquote>
<blockquote class="blockquote">
<p><strong>Goodput</strong>: The number of requests per second that successfully meet the Service Level Objective (SLO). This metric offers a more realistic view of useful system capacity.</p>
</blockquote>
</section>
<section id="resource-utilization-metrics" class="level3">
<h3 class="anchored" data-anchor-id="resource-utilization-metrics">Resource Utilization Metrics</h3>
<ol type="1">
<li><strong>Model FLOPS Utilization (MFU)</strong>
<ul>
<li>Ratio of actual to theoretical FLOPS</li>
<li>Indicates computational efficiency</li>
<li>Key metric for hardware optimization</li>
</ul></li>
<li><strong>Model Bandwidth Utilization (MBU)</strong>
<ul>
<li>Percentage of achievable memory bandwidth utilized</li>
<li>Critical for memory-intensive operations</li>
<li>Helps identify memory bottlenecks</li>
</ul></li>
</ol>
</section>
</section>
<section id="hardware-considerations-and-ai-accelerators" class="level2">
<h2 class="anchored" data-anchor-id="hardware-considerations-and-ai-accelerators">Hardware Considerations and AI Accelerators</h2>
<p>While NVIDIA GPUs dominate the market, various specialized chips exist for inference:</p>
<section id="popular-ai-accelerators" class="level3">
<h3 class="anchored" data-anchor-id="popular-ai-accelerators">Popular AI Accelerators</h3>
<ul>
<li>NVIDIA GPUs (market leader)</li>
<li>AMD accelerators</li>
<li>Google TPUs</li>
<li>Various emerging specialized chips</li>
</ul>
<blockquote class="blockquote">
<p><strong>Inference vs Training Hardware</strong>: Inference-optimized chips prioritize lower precision and faster memory access over large memory capacity, contrasting with training-focused hardware that requires substantial memory capacity.</p>
</blockquote>
<p>Key hardware optimization considerations include:</p>
<ul>
<li>Memory size and bandwidth requirements</li>
<li>Chip architecture specifics</li>
<li>Power consumption profiles</li>
<li>Physical chip architecture variations</li>
<li>Cost-performance ratios</li>
</ul>
</section>
</section>
<section id="model-optimization-techniques" class="level2">
<h2 class="anchored" data-anchor-id="model-optimization-techniques">Model Optimization Techniques</h2>
<p><img src="images/2025-02-07-ai-engineering-chapter-9/inference-optimization-differences.png" class="img-fluid"></p>
<section id="core-approaches" class="level3">
<h3 class="anchored" data-anchor-id="core-approaches">Core Approaches</h3>
<ol type="1">
<li><strong>Quantization</strong>
<ul>
<li>Reduces numerical precision (e.g., 32-bit to 16-bit)</li>
<li>Decreases memory footprint</li>
<li>Weight-only quantization is particularly common</li>
<li>Can halve model size with minimal performance impact</li>
</ul></li>
<li><strong>Pruning</strong>
<ul>
<li>Removes non-essential parameters</li>
<li>Preserves core model behavior</li>
<li>Multiple techniques available</li>
<li>Requires careful validation</li>
</ul></li>
<li><strong>Distillation</strong>
<ul>
<li>Creates smaller, more efficient models</li>
<li>Maintains key capabilities</li>
<li>Covered extensively in Chapter 8</li>
</ul></li>
</ol>
</section>
<section id="advanced-decoding-strategies" class="level3">
<h3 class="anchored" data-anchor-id="advanced-decoding-strategies">Advanced Decoding Strategies</h3>
<p><img src="images/2025-02-07-ai-engineering-chapter-9/pytorch-llama3-optimization.png" class="img-fluid"></p>
<section id="speculative-decoding" class="level4">
<h4 class="anchored" data-anchor-id="speculative-decoding">Speculative Decoding</h4>
<p>This approach combines a large model with a smaller, faster model:</p>
<ul>
<li>Small model generates rapid initial outputs</li>
<li>Large model verifies and corrects as needed</li>
<li>Provides faster token generation</li>
<li>Easy to implement</li>
<li>Integrated into frameworks like VLLM and LamaCPU</li>
</ul>
</section>
<section id="inference-with-reference" class="level4">
<h4 class="anchored" data-anchor-id="inference-with-reference">Inference with Reference</h4>
<p><img src="images/2025-02-07-ai-engineering-chapter-9/inference_with_reference.png" class="img-fluid"></p>
<ul>
<li>Performs mini-RAG operations during decoding</li>
<li>Retrieves relevant context from input query</li>
<li>Requires additional memory overhead</li>
<li>Useful for maintaining context accuracy</li>
</ul>
</section>
<section id="parallel-decoding" class="level4">
<h4 class="anchored" data-anchor-id="parallel-decoding">Parallel Decoding</h4>
<p>Rather than strictly sequential token generation, this method:</p>
<ul>
<li>Generates multiple tokens simultaneously</li>
<li>Uses resolution mechanisms to maintain coherence</li>
<li>Implements look-ahead techniques</li>
<li>Algorithmically complex but offers significant speed benefits</li>
<li>Demonstrated success with look-ahead decoding method</li>
</ul>
</section>
<section id="attention-optimization" class="level4">
<h4 class="anchored" data-anchor-id="attention-optimization">Attention Optimization</h4>
<p>Several strategies exist for optimizing attention mechanisms:</p>
<ol type="1">
<li><strong>Key-Value Cache Optimization</strong>
<ul>
<li>Critical for large context windows</li>
<li>Requires substantial memory</li>
<li>Various techniques for size reduction</li>
</ul></li>
<li><strong>Specialized Attention Kernels</strong>
<ul>
<li>Flash Attention as leading example</li>
<li>Hardware-specific implementations</li>
<li>Flash Attention 3 for H100 GPUs</li>
</ul></li>
</ol>
<p><img src="images/2025-02-07-ai-engineering-chapter-9/flash-attention.png" class="img-fluid"></p>
</section>
</section>
</section>
<section id="service-level-optimization" class="level2">
<h2 class="anchored" data-anchor-id="service-level-optimization">Service-Level Optimization</h2>
<section id="batching-strategies" class="level3">
<h3 class="anchored" data-anchor-id="batching-strategies">Batching Strategies</h3>
<ol type="1">
<li><strong>Static Batching</strong>
<ul>
<li>Processes fixed-size batches</li>
<li>Waits for complete batch (e.g., 100 requests)</li>
<li>Simple but potentially inefficient</li>
</ul></li>
<li><strong>Dynamic Batching</strong>
<ul>
<li>Uses time windows for batch formation</li>
<li>Processes incomplete batches after timeout</li>
<li>Balances latency and throughput</li>
</ul></li>
<li><strong>Continuous Batching</strong>
<ul>
<li>Returns completed responses immediately</li>
<li>Dynamically manages resource utilization</li>
<li>Similar to a bus route that continuously picks up new passengers</li>
<li>Optimizes occupation rate</li>
<li>Based on Orca paper’s findings</li>
</ul></li>
</ol>
</section>
<section id="prefill-decode-decoupling" class="level3">
<h3 class="anchored" data-anchor-id="prefill-decode-decoupling">Prefill-Decode Decoupling</h3>
<ul>
<li>Separates prefill and decode operations</li>
<li>Essential for large-scale inference providers</li>
<li>Allows optimal resource allocation</li>
<li>Improves overall system efficiency</li>
</ul>
</section>
<section id="prompt-caching" class="level3">
<h3 class="anchored" data-anchor-id="prompt-caching">Prompt Caching</h3>
<p><img src="images/2025-02-07-ai-engineering-chapter-9/prompt_caching.png" class="img-fluid"></p>
<ul>
<li>Stores computations for overlapping text segments</li>
<li>Offered by providers like Gemini and Anthropic</li>
<li>May incur storage costs</li>
<li>Requires careful cost-benefit analysis</li>
<li>Must be explicitly enabled</li>
</ul>
</section>
<section id="parallelism-strategies" class="level3">
<h3 class="anchored" data-anchor-id="parallelism-strategies">Parallelism Strategies</h3>
<ol type="1">
<li><strong>Replica Parallelism</strong>
<ul>
<li>Creates multiple copies of the model</li>
<li>Distributes requests across replicas</li>
<li>Simplest form of parallelism</li>
</ul></li>
<li><strong>Tensor Parallelism</strong>
<ul>
<li>Splits individual tensors across devices</li>
<li>Enables processing of larger models</li>
<li>Requires careful coordination</li>
</ul></li>
<li><strong>Pipeline Parallelism</strong>
<ul>
<li>Divides model computation into stages</li>
<li>Assigns stages to different devices</li>
<li>Optimizes resource utilization</li>
<li>Reduces memory requirements</li>
</ul></li>
<li><strong>Context Parallelism</strong>
<ul>
<li>Processes different parts of input context in parallel</li>
<li>Particularly useful for long sequences</li>
<li>Can significantly reduce latency</li>
</ul></li>
<li><strong>Sequence Parallelism</strong>
<ul>
<li>Processes multiple sequences simultaneously</li>
<li>Leverages hardware-specific features</li>
<li>Requires careful implementation</li>
</ul></li>
</ol>
</section>
</section>
<section id="implementation-considerations" class="level2">
<h2 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h2>
<p>When implementing inference optimizations:</p>
<ul>
<li>Multiple optimization techniques are typically combined in production</li>
<li>Hardware-specific optimizations require careful testing</li>
<li>Service-level optimizations often provide significant gains with minimal model modifications</li>
<li>Optimization choices depend heavily on specific use cases and requirements</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="strickvl/mlops-dot-systems" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>