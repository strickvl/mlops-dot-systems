{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /2022/05/12/seven-steps-gradient-calculations\n",
    "date: '2022-05-12'\n",
    "description: I outline the basic process that a computer uses when training a model,\n",
    "  greatly simplified and all explained through the lens of PyTorch and how it calculates\n",
    "  gradients. These are some pre-requisite foundations that we will later apply to\n",
    "  our Fashion MNIST dataset.\n",
    "output-file: 2022-05-12-seven-steps-gradient-calculations.html\n",
    "title: Some foundations for machine learning with PyTorch\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9976ac80-48c6-473e-8096-01759c7321ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "!pip install -Uqq fastbook nbdev torch\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d3a5f-6293-4ce1-800e-021a194d72d4",
   "metadata": {},
   "source": [
    "In the [previous post](https://mlops.systems/fastai/computervision/pytorch/2022/05/12/fashion-mnist-pixel-similarity.html) I showed a naive approach to calculating the similarity or difference between images, and how that could be used to create a function that did pretty well at estimating whether any particular image was a pullover or a dress.\n",
    "\n",
    "[Chapter 4 of the fastbook](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb) then takes us on a journey showing a smarter approach where the computer can make even better estimations and predictions. The broad strokes of this approach are simple to grasp, but of course the individual details are where the nuances of machine learning are to be found.\n",
    "\n",
    "# Seven steps for a machine to learn\n",
    "\n",
    "![](nb_images/ml-training-big-picture/seven-steps.png)\n",
    "\n",
    "**1. Initialise a set of weights**\n",
    "\n",
    "We use random values start with. There could potentially be more elaborate /\n",
    "fancy ways to calculate some starting values that are closer to our end goal,\n",
    "but in practice it is unnecessary because we have a process that we are going to\n",
    "use to update our values.\n",
    "\n",
    "**2. Use the weights to make a prediction**\n",
    "\n",
    "We check whether the sets of weights we have currently set as part of our\n",
    "function are the right ones. We check the prediction (i.e. what came out the\n",
    "other end of our model / function) and get a sense of how well our model did.\n",
    "\n",
    "**3. Loss: see how well we did with our predictions**\n",
    "\n",
    "We calculate the loss for our data. How well did we do at predicting what we\n",
    "were trying to predict? We use a number that will be small if our function is\n",
    "doing well. (Note: this is just a convention and otherwise there's no special\n",
    "reason for this.)\n",
    "\n",
    "**4. Calculate the gradients across all the weights**\n",
    "\n",
    "We are calculating the gradient for lots of numbers, not just a single value.\n",
    "For every stage, we get back the gradient for all of the numbers. This is done\n",
    "sequentially, where we calculate the gradient for one weight / number, keeping\n",
    "all the other numbers constant, and then we repeat this for all the other\n",
    "weights.\n",
    "\n",
    "Gradient calculation relates to calculating derivatives and with this we are\n",
    "stepping firmly into the space of calculus. I don't fully understand how all\n",
    "this works, but intuitively, the important thing to know is this: we are\n",
    "calculating the change of the value, not the value itself. I.e. we want to know\n",
    "how things will change (by how much, and in what direction) if we shift this\n",
    "value slightly.\n",
    "\n",
    "Note that the process of calculating the gradients is a performance\n",
    "optimisation. We could just as well have done this with a (slower) manual\n",
    "process where we adjust a little bit each time. With the gradient calculations\n",
    "we can take bigger steps in the direction we want, with more precision guiding\n",
    "our guesses about the direction and distance we want to go.\n",
    "\n",
    "**5. 'Step': Update the weights**\n",
    "\n",
    "This is where we increase or decrease our own weights by a small amount and see\n",
    "whether the loss goes up or down. As hinted in step 4, we use calculus to figure\n",
    "out:\n",
    "\n",
    "- which direction to go in\n",
    "- how much we should increase or decrease our own weights\n",
    "\n",
    "**6. Repeat starting at step 2**\n",
    "\n",
    "This is an iterative process.\n",
    "\n",
    "**7. Iterate until we decide to stop**\n",
    "\n",
    "There are various criteria determining when we should stop. Some possible\n",
    "stopping points might include:\n",
    "\n",
    "- when the model is 'good enough' for our use case\n",
    "- when we have run out of time (or money!)\n",
    "- when the accuracy starts getting worse (i.e. the model isn't performing as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29c762-4f8e-4912-80f3-e2f55b38c11c",
   "metadata": {},
   "source": [
    "# Calculating gradients with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e3518",
   "metadata": {},
   "source": [
    "For those of us who don't bring a strong mathematics foundation into this space,\n",
    "the mention of calculus, derivatives and gradients isn't especially reassuring,\n",
    "but rest assured that PyTorch can help us out during this process.\n",
    "\n",
    "There are three main parts to using PyTorch to calculate gradients (i.e. step\n",
    "four of the seven steps listed above.)\n",
    "\n",
    "## 1. Setup: add `.requires_grad_()` to a tensor\n",
    "\n",
    "For any Tensor where we know we're going to want to calculate the gradients of\n",
    "values, we call `.require_grad()` on that Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55343b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define a simple function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "x_tensor = torch.tensor(3.).requires_grad_()\n",
    "\n",
    "y_tensor = f(x_tensor)\n",
    "\n",
    "y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062f548",
   "metadata": {},
   "source": [
    "Here we can see that 3 squared is indeed 9, and we can see the `grad_fn` as part\n",
    "of the Tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12239cfd",
   "metadata": {},
   "source": [
    "## 2. Use `.backward()` to calculate the gradient\n",
    "\n",
    "This actually refers to backpropagation, something which is explained much later\n",
    "in the book. This step is also known as the 'backward pass'. Note, that this is\n",
    "again another piece of jargon that we just have to learn. In reality this method\n",
    "might as well have been called `.calculate_gradients()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bedb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1f9bc",
   "metadata": {},
   "source": [
    "## 3. Access the gradient via the `.grad` attribute\n",
    "\n",
    "We view the gradient by checking this `.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15cf5b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd072b21",
   "metadata": {},
   "source": [
    "I can't explain why this is the case, since I've never learned how to calculate\n",
    "gradients or derivatives (or anything about calculus, for that matter!) but in\n",
    "any case it's not really important.\n",
    "\n",
    "Note that we can do this whole process over Tensors that are more complex than\n",
    "illustrated in the above simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b7cae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6., 10., 24.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_x = tensor([3., 5., 12.]).requires_grad_()\n",
    "\n",
    "def f(x):\n",
    "    return (x**2).sum()\n",
    "\n",
    "complex_y = f(complex_x)\n",
    "complex_y.backward()\n",
    "complex_x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7db548",
   "metadata": {},
   "source": [
    "Something else I discovered while doing this was that gradients can only be\n",
    "calculated on floating point values, so this is why when we create `x_tensor`\n",
    "and `complex_x` we create them with floating point values (`3.` etc) instead of\n",
    "just integers. In reality, I think there will be some kind of normalisation of\n",
    "our values as part of the process, so they would probably *already* be floats,\n",
    "but it's worth noting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7576e-79d9-4b03-8686-668628548d6f",
   "metadata": {},
   "source": [
    "# 'Stepping': using learning rates to figure out how much to step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f1b2a",
   "metadata": {},
   "source": [
    "Now that we know how to calculate gradients, the key question for the fifth step\n",
    "in our seven-step process is the following:\n",
    "\n",
    "> \"How much should we shift our values based on what we get back from our\n",
    "> gradient calculations?\n",
    "\n",
    "We call this amount the 'learning rate', and it is usually a value between 0.001\n",
    "and 0.1. Very small, in other words :)\n",
    "\n",
    "We adjust our weights / parameters by this basic equation:\n",
    "\n",
    "```python\n",
    "weights -= weights.grad * learning_rate\n",
    "```\n",
    "\n",
    "This process is called \"stepping the parameters\" and it uses an optimisation\n",
    "step.\n",
    "\n",
    "The learning rate shouldn't be too high, else our loss can get higher / worse or\n",
    "otherwise we can just bounce around within the boundaries of our function\n",
    "without ever reaching the optimum (== lowest) loss.\n",
    "\n",
    "At the same time, we shouldn't use a very tiny learning rate (i.e. even tinier\n",
    "than the 0.001-0.1 mentioned above) since then the process will take a really\n",
    "long time and while we might reach the optimum loss at some point, it might not\n",
    "be the fastest way to get there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135a08a-94cd-4189-96db-a8ed9edc464c",
   "metadata": {},
   "source": [
    "# Takeaways from the seven-step process\n",
    "\n",
    "Most of this makes a lot of intuitive sense to me. The parts that don't are what\n",
    "is going on with the gradient calculations and finding of derivatives and so on.\n",
    "For now, it appears that I can get away without understanding precisely how that\n",
    "works. It is enough to appreciate that we have a way to make these calculations,\n",
    "and those calculations are optimised for us "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e0a4c679d9f6b5b7f0e3346667179a21ad75e9d85922afed17d29bd73315778"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
