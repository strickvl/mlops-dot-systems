{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: '2023-06-03'\n",
    "description: \"I explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language. I share the challenges I faced due to language-specific limitations, my initiative to expand language metadata, and my plans to assess and enhance tokenization efficiency.\"\n",
    "output-file: 2023-06-03-training-custom-balochi-tokenizer.html\n",
    "title: \"Tokenizing Balochi with HuggingFace's Tokenizer and FastAI/Spacy\"\n",
    "image: 'images/tokenisation/hf-tokenizer-small.png'\n",
    "author: Alex Strick van Linschoten\n",
    "categories:\n",
    "  - nlp\n",
    "  - balochi-language-model\n",
    "  - tokenisation\n",
    "  - balochi\n",
    "include-before-body: '<script defer data-domain=\"mlops.systems\" src=\"https://plausible.io/js/script.js\"></script>'\n",
    "toc: true\n",
    "layout: post\n",
    "comments:\n",
    "    utterances:\n",
    "        repo: strickvl/mlops-dot-systems\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this blog I want to walk through how I trained my first tokenizer(s) on a\n",
    "small Balochi language corpus. I used the Huggingface Tokenizers library and\n",
    "FastAI / Spacy to get a sense of the interfaces involved. There's also some\n",
    "naive pre-processing I did to get the corpus into a format that the tokenizer\n",
    "could handle. I'm not sure if this is the best way to do it, but it worked for\n",
    "this first iteration.\n",
    "\n",
    "We can get straight into the implementation details, but the general process\n",
    "was:\n",
    "\n",
    "1. Load in our data corpus\n",
    "2. Pre-process the data (remove non-Balochi characters and numbers, etc.)\n",
    "3. Load the algorithm we want to use for tokenisation (using BPE here)\n",
    "4. Tokenise the text\n",
    "\n",
    "I'll go through each of these steps in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# !pip install datasets\n",
    "# !huggingface-cli login\n",
    "# from datasets import load_dataset\n",
    "# load_dataset(\"balochiml/balochi-language-data\", data_dir=\"data\", cache_dir=\"../data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our text corpus\n",
    "\n",
    "Here I walk through my `.txt` files and load the paths into a list. You can see\n",
    "we have 4294 files to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def get_txt_file_paths(directory):\n",
    "    txt_file_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                txt_file_paths.append(file_path)\n",
    "    return txt_file_paths\n",
    "\n",
    "\n",
    "# Replace \"directory_path\" with the actual path of the directory you want to search\n",
    "directory_path = \"../data/raw_text\"\n",
    "txt_paths = get_txt_file_paths(directory_path)\n",
    "\n",
    "len(txt_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the texts\n",
    "\n",
    "I still don't fully have a good sense of the best ways to do this, not least of\n",
    "all because I'm not sure of the tradeoffs for decisions I take. For example, I\n",
    "frequently hear that people remove punctuation during pre-processing, but I'm\n",
    "not sure how that's helpful. It feels like you'd be removing context more than\n",
    "anything else.\n",
    "\n",
    "I had similar thoughts on the removal of numbers, but in the end I removed them\n",
    "along with any a-z or A-Z English-language characters. I also removed excess whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(file_path):\n",
    "    # Open the file and read it into memory\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Remove English-language characters and numbers\n",
    "    text = re.sub(r\"[a-zA-Z0-9]\", \"\", text)\n",
    "\n",
    "    # Remove any excess whitespace\n",
    "    text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in txt_paths:\n",
    "    cleaned_text = clean_text(path)\n",
    "\n",
    "    # write the cleaned text to a new file with an incremented filename\n",
    "    # write the files all into the '../data/processed_text' directory\n",
    "    with open(\n",
    "        f'../data/processed_text/{path.split(\"/\")[-1]}', \"w\", encoding=\"utf-8\"\n",
    "    ) as file:\n",
    "        file.write(cleaned_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer using 🤗 Tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of 'training' a tokeniser using the Huggingface Tokenizers library\n",
    "was pretty straightforward. There are some nuances and parameters where -- again\n",
    "-- I'm not sure of the tradeoffs I'm making. I'll mention those when I get to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for example, I'm pretty sure that the vocabulary size is an important\n",
    "hyperparameter to tune, as is the minimum frequency of tokens. The values here\n",
    "are the defaults in the library. I've read that a higher vocab size might be\n",
    "warranted in a language that is morphologically complex, but I don't think that\n",
    "Balochi qualifies for that. Also, a larger vocabulary size might be warranted\n",
    "for a language for which I have a larger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "vocab_size = 30000\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    min_frequency=2,\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of all the txt files in\n",
    "# '/Users/strickvl/balochi/balochi-tokenizer/data/processed_text'\n",
    "\n",
    "processed_files = get_txt_file_paths(\"../data/processed_text\")\n",
    "assert len(processed_files) == len(txt_paths)\n",
    "len(processed_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process itself was a matter of passing the files and the\n",
    "(configured) trainer into the `.train()` method. It was extremely quick to run,\n",
    "taking only around minutes to crunch through my corpus. (For reference, I'm now\n",
    "up to around 2.8 million words of Balochi text in the corpus, a drop in the\n",
    "ocean compared to the datasets used to trained English-language LLMs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(processed_files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.models.BPE at 0x108eaa830>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert tokenizer.get_vocab_size() == vocab_size\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also saved the tokenizer to disk so that I (or others) can load it in without\n",
    "needing the dataset at a later date. This saves a JSON file which contains all\n",
    "the information needed to load the tokenizer separately from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../models/30k-balochi-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"../models/30k-balochi-tokenizer.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here you can see the results on a sample from some Balochi text I found\n",
    "somewhere on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'گوں ھر کس ءَ جنگ ء ُ مڑ بیت'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"      آیک  جناورے اَت۔  لھتے گشیت آ سکیں کارزوالے ات کہ اگاں آزاتی دیگ بہ بیت، بازارءَ، لوگے ءَ، جاگاہ یے  ءَ،دپتر ء ُ کارگس یے  ءَ یا ھر ھما جاگاہ ءَ کہ شُت کنت مزنیں کارزوالی کنت۔گوں ھر کس ءَ جنگ ء ُ مڑ بیت۔گدء ُ پچاں  چنڈ چنڈ ء ُ راڑ راڑ کنت،کاگد ء ُ وانگیاں وارت ء ُ آدراہ کنت۔ورگی چیزاں اگاں وارت نکنت آھاں گٹ پاچیت ھراب کنت۔ایندگہ جناور چہ بندات ء َ ایشی ءِ کازوالیاں چہ وتا دیر دارگ ءِ کوشست کن اَنت۔ چیا کہ آ بازیں دگہ ھرابی ء ُ کارزوالی ھم کنت،پمیشکا کسانیں جناور  بالی مُرگ،کوہ پاچن،آسک ء ُ ایندگہ کسان کسانیں جناورچر آئی ءِ کارزوالیانی سوب ءَ آئی ءَ چہ سک باز شزار اَنت ۔\".replace(\n",
    "    \"\\xa0\", \"\"\n",
    ")\n",
    "sample_sentence = sample_text.split(\"۔\")[2]\n",
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['گوں', 'ھر', 'کس', 'ءَ', 'جنگ', 'ء', 'ُ', 'مڑ', 'بیت']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(sample_sentence).tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by this tiny example, actually the tokenization process doesn't seem to\n",
    "have saved us much in terms of space. The tokens from the encoded text are\n",
    "basically just the words from the original text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a tokenizer using Spacy and FastAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FastAI [course and\n",
    "book](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) have a whole\n",
    "chapter that deals with NLP and a section that deals with tokenization and\n",
    "subwords so I thought I'd follow through that process as well to get a sense of\n",
    "the higher-level API that FastAI provides as well as the implementation under\n",
    "the hood provided by [Spacy](https://spacy.io).\n",
    "\n",
    "When you install FastAI, you'll probably notice that it has Spacy as a\n",
    "dependency. This is because it uses Spacy under the hood for tokenization (along\n",
    "with a lot of other NLP tasks). FastAI provides a wrapper around Spacy's\n",
    "`Tokenizer` object along with some helper functions and other bits and pieces.\n",
    "\n",
    "I'll admit to not finding the FastAI interface as intuitive or useful as the 🤗\n",
    "Tokenizers library, in part because it was harder to get at some of the Spacy\n",
    "primitives when it became necessary to do so. More on this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "# a built-in helper function from fastai\n",
    "files = get_text_files(\"../data/processed_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4294"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*آمیتگءِ جُستءَمکن* لچّہ: *آمیتگءِ جُستءَمکن* آ میتگءَکہ من وتی شوکیں کسانی'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some sample text from the first file\n",
    "txt = files[0].open().read(); txt[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#146) ['*','آمیتگءِ','جُستءَمکن','*','لچّہ',':','*','آمیتگءِ','جُستءَمکن','*','آ','میتگءَکہ','من','وتی','شوکیں','کسانی','پیر','کُت','آ','میتگءِ','جسُتءَمکن','آ','میتگءِ','گیراں','مبو','بے','اوستیں','تاهیراں','مبو','آ'...]\n"
     ]
    }
   ],
   "source": [
    "# using the `SpacyTokenizer` from fastai\n",
    "# see https://docs.fast.ai/text.core.html#spacytokenizer\n",
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt]))\n",
    "print(coll_repr(toks, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#147) ['xxbos','*','آمیتگءِ','جُستءَمکن','*','لچّہ',':','*','آمیتگءِ','جُستءَمکن','*','آ','میتگءَکہ','من','وتی','شوکیں','کسانی','پیر','کُت','آ','میتگءِ','جسُتءَمکن','آ','میتگءِ','گیراں','مبو','بے','اوستیں','تاهیراں','مبو','آ'...]\n"
     ]
    }
   ],
   "source": [
    "tkn = Tokenizer(spacy)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = L(o.open().read() for o in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sense for the subwords generated from a\n",
    "# small slice of our text data\n",
    "def subword(size: int):\n",
    "    sp = SubwordTokenizer(vocab_sz=size)\n",
    "    sp.setup(txts)\n",
    "    return \" \".join(first(sp([txt]))[:40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁* آ می تگ ءِ ▁جُست ءَ م ک ن * ▁لچّہ : ▁* آ می تگ ءِ ▁جُست ءَ م ک ن * ▁آ ▁میتگ ءَ کہ ▁من ▁وتی ▁ش وکیں ▁کس انی ▁پیر ▁کُت ▁آ ▁میتگ ءِ ▁ج'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁ * آ م ی ت گ ء ِ ▁ ج ُ س ت ء َ م ک ن * ▁ ل چ ّ ہ : ▁ * آ م ی ت گ ء ِ ▁ ج ُ س ت'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#147) ['xxbos','*','آمیتگءِ','جُستءَمکن','*','لچّہ',':','*','آمیتگءِ','جُستءَمکن'...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks200 = txts[:200].map(tkn)\n",
    "toks200[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, once we've seen a bit how FastAI and Spacy are able to tokenize\n",
    "the text, we can switch into the numericalisation process and see what we get\n",
    "from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#4096) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','ءَ','ءِ','ءُ','۔','کہ','،','انت','من','اے','نہ','وتی','بیت','”','ات','چہ','گوں','اَنت','اِنت','پہ','بہ','‘','یک','آئی','.','آ','منی','ھم',')','کنت','بلوچی','3','تو','بلے','ئے',':','کنگ','(','بوتگ','آں','کن','؟'...]\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize()\n",
    "num.setup(toks200)\n",
    "coll_repr(num.vocab,50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some of the meta-tokens [mentioned in my last\n",
    "blog](https://mlops.systems/posts/2023-06-01-why-tokenisation.html) are also\n",
    "represented here, and then the rest of the words are sorted by frequency order.\n",
    "\n",
    "We can represent a sample of text as the token ids at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([ 156, 2340,    0,  156,  563,   43,  156, 2340,    0,  156,   33,\n",
       "               0,   16,   19, 1490,  831,  457,  102,   33, 1031])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]; nums"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we convert this back, you'll see we get the meta-tokens as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* آمیتگءِ xxunk * لچّہ : * آمیتگءِ xxunk * آ xxunk من وتی شوکیں کسانی پیر کُت آ میتگءِ'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons learned\n",
    "\n",
    "This first attempt at tokenisation was instructive in a number of respects.\n",
    "\n",
    "I didn't show what was going on under the hood with the FastAI wrapper, but if\n",
    "you look at the source code you'll see that the line `spacy = WordTokenizer()`\n",
    "assumes that the base language we're dealing with is English. You can of course\n",
    "pass in a language code to the `WordTokenizer` initialization, but since it uses\n",
    "Spacy under the hood here and since Balochi isn't represented as an official\n",
    "language supported by Spacy, when you're basically out of luck. You hit an error\n",
    "and you can either continue using simplistic algorithms like the ones\n",
    "demonstrated above (essentially splitting on word delimiters) or you can abandon\n",
    "FastAI and dive into Spacy.\n",
    "\n",
    "At that point, you'll have to start implementing a whole bunch of things\n",
    "yourself in order to get going quickly. For example, you'll ideally want to come\n",
    "up with all the list of punctuation marks, stop words, stemming rules and so on\n",
    "that I mentioned last time. (It might well be that it's possible to get up and\n",
    "running faster for a non-standard language with Spacy, but it wasn't clear to me\n",
    "how to do that.)\n",
    "\n",
    "I do actually now intend to make a contribution to the Spacy repo to have\n",
    "Balochi represented there, and to open the window for others to contribute to\n",
    "the language metadata directly, but that didn't help me in the moment. You'll\n",
    "notice that I didn't show how you can save a serialized version of the\n",
    "Spacy/FastAI tokeniser because I wasn't able to figure out how to get access to\n",
    "the underlying Spacy object. I'm sure it's possible since I can [read the Spacy\n",
    "API documentation showing which method to\n",
    "use]((https://spacy.io/api/tokenizer#to_disk) but FastAI didn't itself expose\n",
    "this functionality directly.\n",
    "\n",
    "My initial impression from working with both libraries and spending some time\n",
    "with their documentation is that Spacy might end up being more useful for\n",
    "low-resource languages given the extent to which they support a more complete\n",
    "range of old-school NLP methods and techniques. That said, the 🤗 Tokenizers\n",
    "library was much easier to get up and running with and I think it's a great\n",
    "option for anyone who wants to get started quickly with tokenization. They\n",
    "support most of the major algorithms you'd ever need to use and if they don't\n",
    "you can always implement something yourself to extend it.\n",
    "\n",
    "## Balochi Tokenizers on Huggingface Hub\n",
    "\n",
    "I'm still working through a way to open up the core dataset (along with\n",
    "constructing as I work), but this first iteration of the tokenizer is now\n",
    "[available over on the Huggingface\n",
    "Hub](https://huggingface.co/balochiml/balochi-tokenizer). You can load it for\n",
    "use with the single line:\n",
    "\n",
    "```python\n",
    "tokenizer = Tokenizer.from_file(\"../models/30k-balochi-tokenizer.json\")\n",
    "```\n",
    "\n",
    "The organisation is something I created together with some Balochi colleagues\n",
    "who expressed an interest in working together on this effort. I'm really happy\n",
    "to have made their acquaintance and I hope I'll be able to make steady progress\n",
    "on this project with their help. (If you're interested in contributing, please\n",
    "request access to the organization and/or contact me for more information.)\n",
    "\n",
    "While creating the tokenizer repository, I also noted how Balochi (as with\n",
    "Spacy) is not represented as a language recognised by the metadata tracking\n",
    "languages used on the Hub. Frustratingly, you're asked to input [an ISO-639-1\n",
    "two-letter code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) to\n",
    "represent the language of the model, but of course Balochi doesn't have one of\n",
    "those. Balochi only has an ISO-693-2 and ISO-693-3 code. I'll have to see how we\n",
    "can get Balochi represented on the Hub given all this. It can't be the first\n",
    "time that this has happened.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Now that I have this first iteration complete, I want to reflect a bit on how to\n",
    "know when the tokenizer is 'good enough'. In particular, how do you evaluate\n",
    "tokenisers? Are there ways of benchmarking this? There must have been work done\n",
    "on this and I want to understand both what the start of the art is as well as\n",
    "how to know when I've reached it.\n",
    "\n",
    "I also watched [an extremely rewarding talk on low-resource\n",
    "languages](https://www.youtube.com/watch?v=X7c0T7uwtkM) (blog notes to follow!)\n",
    "and there was a section in that which stressed the foundational\n",
    "nature of tokenisation as part of language models. It also highlighted a failure\n",
    "mode where bad tokenisation made a model perform very badly on a certain kind of\n",
    "task. So based on this context I would like to understand how to evaluate\n",
    "tokenisers and how to know when I've reached a good enough point.\n",
    "\n",
    "I also have a grab-bag of odds and ends relating to tokenization (GPU-based\n",
    "tokenization! tiktoken! etc.) that I mean to write up alongside the above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "balochi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
