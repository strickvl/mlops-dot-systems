{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "date: '2024-06-17'\n",
        "description: \"I tried out some services that promise to simplify the process of\n",
        "finetuning open models. I describe my experiences with Predibase, OpenPipe and OpenAI.\"\n",
        "output-file: 2024-06-17-one-click-finetuning.html\n",
        "title: \"One-click LLM finetuning with Predibase, OpenPipe and OpenAI\"\n",
        "image: 'images/one-click-finetuning.png'\n",
        "author: Alex Strick van Linschoten\n",
        "categories:\n",
        "  - nlp\n",
        "  - llms\n",
        "  - miniproject\n",
        "  - finetuning\n",
        "  - isafpr\n",
        "toc: true\n",
        "layout: post\n",
        "comments:\n",
        "    utterances:\n",
        "        repo: strickvl/mlops-dot-systems\n",
        "aliases:\n",
        "  - \"/posts/2024-06-17-one-click-finetuning.html\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [last post in this series](https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html) showed that finetuning an LLM needn't be\n",
        "particularly difficult. I used `axolotl` to produce finetuned versions of\n",
        "Llama3, Mistral and TinyLlama models. During the course we were given a bunch of\n",
        "credits by various companies in the LLM and finetuning space. Among those were\n",
        "credits from some finetuning-as-a-service companies and I thought now might be a\n",
        "good time to try out these services now that I've done the process manually a\n",
        "few times.\n",
        "\n",
        "I picked three to try out: Predibase, OpenPipe and OpenAI. All were surprisingly\n",
        "similar in the approach they took. I'll give a few details on the experience for\n",
        "each and how they compare to each other. With all the services, the process was\n",
        "roughly the same as when I did it manually:\n",
        "\n",
        "1. Upload custom data\n",
        "2. Select some hyperparameters\n",
        "3. Start the finetuning\n",
        "4. Try the model\n",
        "\n",
        "The step I had the most trouble with was the custom data upload, since each\n",
        "provider wanted the data in a different format. Converting the data from the\n",
        "Pydantic models I had previously created was not a huge deal, but I wasn't sure\n",
        "about the tradeoffs that I was making (or that were being made for me) by\n",
        "converting my data into these formats.\n",
        "\n",
        "## Predibase\n",
        "\n",
        "I started with [Predibase](https://predibase.com/) since I had enjoyed the talk\n",
        "Travis Addair had given during the course. Predibase is famous for their work on\n",
        "LORA adapters, particularly their demonstration of [Lora\n",
        "Land](https://predibase.com/blog/lora-land-fine-tuned-open-source-llms-that-outperform-gpt-4)\n",
        "where they gave some examples of how finetuned LORA models / adapters could\n",
        "outperform GPT-4.\n",
        "\n",
        "Predibase requires that the data you upload has certain column names depending\n",
        "on the task you select for the finetuning. At the moment they have instruction\n",
        "tuning and text completion as their two tasks, but it wasn't clear to me which\n",
        "to select. (They also have [a Colab notebook](https://colab.research.google.com/drive/1r505Aq_SWZdaSkBIs3ovh4F8c36DHwAh?usp=sharing) to help with constructing splits from your\n",
        "data.)\n",
        "\n",
        "Once your data is ready and validated, you can select the model you want to\n",
        "finetune along with a few other hyperparameters. This is the full extent of what\n",
        "you can set from the web UI:\n",
        "\n",
        "![Screenshot of Predibase website and the hyperparameters you can set](images/predibase-hyperparameters.png)\n",
        "\n",
        "There's also a helpful dataset preview pane to give a final sanity check for\n",
        "your data, to make sure that the inputs and outputs look what you'd expect:\n",
        "\n",
        "![Screenshot of Predibase website and the dataset preview pane](images/predibase-dataset-preview.png)\n",
        "\n",
        "As you'll read in a little bit, this feature helps catch potentially costly\n",
        "errors before you start the finetuning process.\n",
        "\n",
        "Once you click the button to start the training, there isn't a great deal of\n",
        "information available to you beyond (eventually) a loss curve that you can see.\n",
        "I chose to finetune Qwen2 in Predibase and this took about 53 minutes using an\n",
        "A-100 GPU accelerator.\n",
        "\n",
        "Once your model is ready, you can prompt the model in the UI, or using their\n",
        "REST API / Python SDK. They give code snippets prefilled with some dummy text\n",
        "that you can easily try out locally. Let's show that here, but before you can\n",
        "run your inference query you have to first deploy the model. I hadn't expected\n",
        "this extra step, and it takes a while to spin up since it's deploying the\n",
        "adaptor along with the base model it was finetuned alongside. My Qwen2 model has\n",
        "a context window of 131072 tokens and supposedly would cost $3.90 per hour that\n",
        "it was up (as a dedicated deployment).\n",
        "\n",
        "Let's show the results we got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pr1 = \"\"\"2011-11-S-011 ISAF Joint Command - Afghanistan For Immediate Release\n",
        "      KABUL, Afghanistan (Nov. 7, 2011) â€” A combined Afghan and coalition\n",
        "      security force conducted an operation in search of a Haqqani facilitator\n",
        "      in Argo district, Badakshan province. The facilitator coordinates suicide\n",
        "      attacks with other insurgent leaders in the area. During the operation, a\n",
        "      local national male failed to comply with repeated verbal warnings and\n",
        "      displayed hostile intent toward the security force. The security force\n",
        "      engaged the individual, resulting in his death. The security force\n",
        "      confiscated a shotgun and intelligence linking the local national to the\n",
        "      Haqqani network. The security force also detained two suspected insurgents during the operation.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\n",
        "\n",
        "### Instruction:\n",
        "\n",
        "PRESS RELEASE TEXT: '{pr1}'\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from predibase import Predibase\n",
        "\n",
        "pb = Predibase(api_token=os.getenv(\"PREDIBASE_API_KEY\"))\n",
        "# pb = Predibase(api_token=\"\")\n",
        "\n",
        "lorax_client = pb.deployments.client(\"isafpr\")\n",
        "print(lorax_client.generate(prompt, max_new_tokens=100).generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unfortunately my Predibase model deployment was still 'initializing' after a\n",
        "couple of hours of spinning up. I didn't want to leave that dedicated deployment\n",
        "up and running overnight, so I just deleted the deployment and I'll try to get\n",
        "this going at a later date. So no inference sample to show you for this one. I'm\n",
        "very curious to see how Qwen2 did, though!\n",
        "\n",
        "## OpenAI\n",
        "\n",
        "I was actually surprised that this is even a thing that people do or that is\n",
        "offered by OpenAI. Currently you're able to finetune three versions of GPT3.5 as\n",
        "well as `babbage-002` and `davinci-002`. In the OpenAI presentation during the\n",
        "course they mentioned that they were working to make it possible to finetune\n",
        "GPT4 as well, but no timeline was given on this.\n",
        "\n",
        "So why would someone want to finetune GPT3.5? I think there are some problems\n",
        "that are sufficiently complex or of a specific nature where the OpenAI GPT\n",
        "family shines where you might want to squeeze out a final last bit of\n",
        "performance and where the open LLMs just aren't there yet.\n",
        "\n",
        "The OpenAI models are sort of the antithesis of an\n",
        "'open' model and nothing about the finetuning process lent itself to disabusing\n",
        "you of that idea. This was the UI to fill in in order to finetune a model and as\n",
        "you can see there aren't really too many options available to you.\n",
        "\n",
        "![OpenAI Finetuning UI](images/openai-finetuning-ui.png)\n",
        "\n",
        "Supposedly the data you upload (options for train as well as a separate test set\n",
        "here) will never be used by OpenAI to train their models but you have to just\n",
        "trust them on that front.\n",
        "\n",
        "![UI medatada during finetuning 1](images/openai-finetuning-ui-2.png)\n",
        "![UI medatada during finetuning 2](images/openai-finetuning-ui-3.png)\n",
        "\n",
        "As with Predibase, during finetuning you don't have access to any logs or even\n",
        "too much feedback during training. You get a loss curve and a few scraps of\n",
        "metadata and that's it. The training took around 90 minutes to run and then\n",
        "you're able to prompt the model to see how it works, using the standard OpenAI\n",
        "interface and methods you're used to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'start_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2011-11-07'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'event_type'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'captureandkill'</span><span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'province'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'badakhshan'</span><span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'target_group'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'haqqani'</span><span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_killed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_captured'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'killq'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'captureq'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'killcaptureraid'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'airstrike'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'noshotsfired'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_leaders_killed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_leaders_captured'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'name'\u001b[0m: \u001b[32m'1'\u001b[0m,\n",
              "    \u001b[32m'start_date'\u001b[0m: \u001b[32m'2011-11-07'\u001b[0m,\n",
              "    \u001b[32m'event_type'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'captureandkill'\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'province'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'badakhshan'\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'target_group'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'haqqani'\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'min_killed'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
              "    \u001b[32m'min_captured'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
              "    \u001b[32m'killq'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[32m'captureq'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[32m'killcaptureraid'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[32m'airstrike'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[32m'noshotsfired'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[32m'min_leaders_killed'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "    \u001b[32m'min_leaders_captured'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from rich import print\n",
        "import json\n",
        "import os\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"ft:gpt-3.5-turbo-SOME_EXTRA_STUFF_HERE_FOR_MY_MODEL\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": pr1\n",
        "        }\n",
        "    ],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(json.loads(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They also give you an interface to see the response of the base model\n",
        "side-by-side against the finetuned model:\n",
        "\n",
        "![Side-by-side UI of base model and finetuned model inference](images/openai-finetuning-ui-4.png)\n",
        "\n",
        "As you can see, it's done pretty well! It stuck to the JSON structure, and the\n",
        "extracted metadata looks good. Of course, since this is a GPT3.5 model, there's\n",
        "no way to now download this model and run it locally. You're hostage to OpenAI,\n",
        "to being online, etc etc. Not a scenario I'd like to be in, so I don't think\n",
        "I'll pursue this much further and rather use my OpenAI credits for other\n",
        "purposes.\n",
        "\n",
        "All that said, I do think there might be some scenarios where only the OpenAI\n",
        "models are reliable enough to use (be that in terms of accuracy or with\n",
        "sufficient guardrails) and there were people in the course who were in this\n",
        "boat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenPipe\n",
        "\n",
        "This was the last one-click provider I tried. As with the others, you upload\n",
        "your data first. When I tried this, I got a fairly opaque error message but I\n",
        "guess the format I'd used was incompatible. OpenPipe uses the same format as\n",
        "OpenAI does, it turns out, but it handles the train/test split itself so you\n",
        "just have to set your data up in a single file (unlike with OpenAI where they\n",
        "can take two separate files).\n",
        "\n",
        "The interface for finetuning your model was somehow the most threadbare of all:\n",
        "\n",
        "![OpenPipe finetuning UI](images/openpipe-finetuning-ui.png)\n",
        "\n",
        "Moreover, the selection of base models on which to finetune were also pretty\n",
        "slim: Llama3, Mistral, Mixtral and two OpenAI GPT3.5 models. I was surprised by\n",
        "the estimate of how much it'd cost to finetune the model (around $30 USD) but by\n",
        "limiting the number of options available to the user the path forward really was\n",
        "pretty easy.\n",
        "\n",
        "You get no single morsel of information during the finetuning process and for me\n",
        "it took a while for the job to even start working, but after an hour or two (I\n",
        "can't be sure as I left my desk) you get a model out the other end. At this\n",
        "point you can export the weights or just try out the model with a Python call.\n",
        "\n",
        "Helpfully, the web UI gives you code snippets you can use for Python, Javascript\n",
        "and cURL, and the snippets even have your prompt pre-filled with an example from\n",
        "your dataset. This was a nice touch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'3 killed and 2 captured in Badakhshan'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'start_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'2011-11-07'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'event_type'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'captureandkill'</span><span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'province'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'badakhshan'</span><span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'target_group'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'haqqani'</span><span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_killed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_captured'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'killq'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'captureq'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'killcaptureraid'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'airstrike'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'noshotsfired'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_leaders_killed'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'min_leaders_captured'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'name'\u001b[0m: \u001b[32m'3 killed and 2 captured in Badakhshan'\u001b[0m,\n",
              "    \u001b[32m'start_date'\u001b[0m: \u001b[32m'2011-11-07'\u001b[0m,\n",
              "    \u001b[32m'event_type'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'captureandkill'\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'province'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'badakhshan'\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'target_group'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'haqqani'\u001b[0m\u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'min_killed'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
              "    \u001b[32m'min_captured'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
              "    \u001b[32m'killq'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[32m'captureq'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[32m'killcaptureraid'\u001b[0m: \u001b[3;92mTrue\u001b[0m,\n",
              "    \u001b[32m'airstrike'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[32m'noshotsfired'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
              "    \u001b[32m'min_leaders_killed'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
              "    \u001b[32m'min_leaders_captured'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# pip install openpipe\n",
        "\n",
        "from openpipe import OpenAI\n",
        "from rich import print\n",
        "import json\n",
        "import os\n",
        "\n",
        "client = OpenAI(\n",
        "  openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")}\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"openpipe:MY_MODEL_ID_WAS_HERE\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": pr1\n",
        "        }\n",
        "    ],\n",
        "    temperature=0,\n",
        "    openpipe={\n",
        "        \"tags\": {\n",
        "            \"prompt_id\": \"counting\",\n",
        "            \"any_key\": \"any_value\"\n",
        "        }\n",
        "    },\n",
        ")\n",
        "\n",
        "print(json.loads(completion.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again you can see we have a really nice result here: JSON is good and the\n",
        "content is solid too. This was a finetune of\n",
        "Llama3 so clearly the problem I noted [in the previous blog\n",
        "post](https://mlops.systems/posts/2024-06-15-isafpr-first-finetune.html#finetuning-our-model)\n",
        "was a problem with how I'd set up my local finetune and not with Llama3 itself.\n",
        "\n",
        "I liked how OpenPipe automatically deployed my model for me once the finetune\n",
        "was complete. Moreover, there was no extra cost associated with this. (Since\n",
        "their base models are limited, I assume this means that they have lost of\n",
        "customers' LORA adapters all connected to these base models and that's how\n",
        "they're able to keep all these deployments up and cost-effective.)\n",
        "\n",
        "There was one final trick that OpenPipe had up its sleeve: an 'evals' interface.\n",
        "The interface is pretty simple again, but the gist is that you get to select\n",
        "OpenAI models to compare your finetune against a test dataset and get a\n",
        "comparison. You can select multiple models to run at the same time and the cost\n",
        "is pretty reasonable.\n",
        "\n",
        "![Openpipe eval UI](images/openpipe-eval-ui.png)\n",
        "\n",
        "The evaluation is parallelised and you get a nice table with the aggregate results:\n",
        "\n",
        "![Openpipe eval results](images/openpipe-eval-results.png)\n",
        "\n",
        "You also (in the datasets tab) get a table with the individual responses for the\n",
        "test data:\n",
        "\n",
        "![Openpipe eval datasets](images/openpipe-eval-datasets.png)\n",
        "\n",
        "Looking at the results you quickly become aware that this specific evaluation\n",
        "didn't really make much sense. Comparing the same prompt between the finetuned\n",
        "model and GPT4 could never have been fair since my prompt never asks for the\n",
        "result back in a certain format, or that it should be JSON and so on.\n",
        "\n",
        "Moreover, you can see that the evaluation prompt itself doesn't do a good job of\n",
        "picking up that the finetuned model really did a great job on the whole and so\n",
        "the aggregate comparison scores don't really make much sense here.\n",
        "\n",
        "That said, I found this feature a useful 'nice-to-have' and I can see how\n",
        "someone might find this helpful if they either wanted to run a quick experiment\n",
        "or weren't particularly technically savvy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final thoughts\n",
        "\n",
        "Overall I found this an interesting experience to do these finetunes in\n",
        "parallel. I suspect that I am not the core audience / market for these services.\n",
        "I was surprised how little customisation they offered, and I actually wonder who\n",
        "is using them. They were easy to use, however, and they do potentially open up\n",
        "the possibility for someone less technical to do something somewhat advanced\n",
        "with LLMs that they wouldn't otherwise be able to do.\n",
        "\n",
        "The moment you want to do something slightly custom, with your prompt template\n",
        "or with the architecture or try something new and cutting-edge, then immediately\n",
        "these services aren't for you. Similarly, even though I think all of the\n",
        "services offer a Python SDK to replicate what I did in the web UI, I think you\n",
        "essentially have the same limited options available to you if you wanted to\n",
        "trigger these jobs programatically as part of a larger pipeline.\n",
        "\n",
        "For the most part you never had the feeling that you were part of a wider\n",
        "ecosystem of these open models, with new techniques coming out all the time and\n",
        "new models as well. These are some of the things I missed from the experience,\n",
        "but as I mentioned before, I'm not the core audience here.\n",
        "\n",
        "I do appreciate the opportunity to try these out a few times and the companies\n",
        "for providing credits to do some meaningful attempts at doing something useful.\n",
        "I'll try these a bit further down the road again and report back if my\n",
        "impression changes or if/when new features are added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
