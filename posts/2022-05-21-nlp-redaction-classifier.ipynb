{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /fastai/nlp/redactionmodel/computervision/huggingface/2022/05/21/nlp-redaction-classifier.html\n",
    "date: '2022-05-21'\n",
    "description: I train an NLP model to see how well it does at predicting whether an\n",
    "  OCRed text contains a redaction or not. I run into a bunch of issues when training,\n",
    "  leading me to conclude that training NLP models is more complicated than I'd at\n",
    "  first suspected.\n",
    "output-file: 2022-05-21-nlp-redaction-classifier.html\n",
    "title: 'Redaction Image Classifier: NLP Edition'\n",
    "image: nlp-redaction-classifier/filenames.png\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a60561-6fd9-4942-a263-603453f36903",
   "metadata": {},
   "source": [
    "I've [previously\n",
    "written](https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html)\n",
    "about my use of fastai's `vision_learner` to create a classification model that\n",
    "was pretty good (> 95% accuracy) at detecting whether an image contained\n",
    "redactions or not.\n",
    "\n",
    "This week in the course we switched domains and got to know HuggingFace's\n",
    "[`transformers` library](https://github.com/huggingface/transformers) as a\n",
    "pathway into NLP (natural language processing) which is all about text inputs. I\n",
    "struggled quite a bit trying to think of interesting yet self-contained / small\n",
    "uses of NLP that I could try out this week. A lot of the common uses for simple\n",
    "NLP modelling seem to be in the area of things like 'sentiment analysis' where I\n",
    "couldn't really see something I could build. Also there are a lot of NLP uses\n",
    "cases which feel unethical or creepy (perhaps more so than in the computer\n",
    "vision, it felt to me).\n",
    "\n",
    "I emerged at the end of this thought process with the idea to try to pit image\n",
    "classification and text classification against one another: could I train an NLP\n",
    "model that would outperform my image classifier in detecting whether a specific\n",
    "document or page contains a redaction or not?\n",
    "\n",
    "Of course, the first thing I had to do was to OCR all the pages in my image\n",
    "dataset and convert this all into a text dataset. When it comes to OCR tools,\n",
    "there are a number of different options available and I'd luckily experimented\n",
    "around with them. (A pretty useful overview of three leading options can be\n",
    "found in [this\n",
    "blogpost](https://francescopochetti.com/easyocr-vs-tesseract-vs-amazon-textract-an-ocr-engine-comparison/)\n",
    "by Francesco Pochetti.) I went with Tesseract as I knew had pretty good\n",
    "performance and accuracy for English-language documents.\n",
    "\n",
    "My process for converting the documents wasn't particularly inspired.\n",
    "Essentially I just loop over the image files one by one, run the OCR engine over\n",
    "them to extract the text and then create a new `.txt` file with the extracted\n",
    "text. At the end, I had two folders with my data, one containing texts whose\n",
    "corresponding images I knew had contained redactions, and one where there were\n",
    "no redactions.\n",
    "\n",
    "I had two hunches that I hoped would help my NLP model.\n",
    "\n",
    "1. I hoped that the redactions would maybe create some kind of noise in the\n",
    "   extracted text that the training process could leverage to learn to\n",
    "   distinguish redacted from unredacted.\n",
    "2. I knew that certain kinds of subjects were more likely to warrant redaction\n",
    "   than others, so perhaps even the noise of the OCR trying to deal with a\n",
    "   missing chunk of the image wouldn't be as important as just grasping the\n",
    "   contents of the document.\n",
    "\n",
    "What follows is my attempt to follow steps initially outlined in Jeremy Howard's\n",
    "[Kaggle\n",
    "notebook](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners)\n",
    "that the course reviewed this week in the live lesson. My code doesn't depart\n",
    "from the original notebook much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b917c6-807d-4449-99e4-8b9b28e7bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "!pip install datasets transformers tokenizers -Uqq\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6d7e8",
   "metadata": {},
   "source": [
    "I save my `.txt` files on the machine and I get a list of all the paths of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0eec15a-a780-48de-93e5-d68051d066ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"redaction_texts\")\n",
    "p = path.glob(\"**/*.txt\")\n",
    "files = [x for x in p if x.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684a53a",
   "metadata": {},
   "source": [
    "I iterate through all the paths, making of list of all the redacted texts as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b2293d-24a4-47de-a7a5-e959f0a9f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file_path in files:\n",
    "    with open(file_path) as file:\n",
    "        texts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c518e1c-a170-43e9-89db-c6ccd9c51522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mredacted\u001b[m\u001b[m   \u001b[1m\u001b[36munredacted\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91056d-41f3-4bb4-85c2-8d2f3bdf6160",
   "metadata": {},
   "source": [
    "## Converting text files into a Pandas DataFrame\n",
    "\n",
    "I needed a way of obtaining the labels for my dataset. These labels were the\n",
    "parent label for each path name. The training process below needed the labels to\n",
    "be floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d22482d-3edd-472a-8c8b-f87916198dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_redacted(path):\n",
    "    \"Extracts the label for a specific filepath\"\n",
    "    if str(path.parent).split(\"/\")[-1] == \"redacted\":\n",
    "        return float(1)\n",
    "    else:\n",
    "        return float(0)\n",
    "\n",
    "is_redacted(files[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735856df",
   "metadata": {},
   "source": [
    "Converting a Python `dict` into a Pandas DataFrame is pretty simple as long as\n",
    "you provide the data in the right formats. I had to play around with this a\n",
    "little when I was getting this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd9b41f-c85c-4982-a12e-c882d75214ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"input\": texts,\n",
    "    \"labels\": [is_redacted(path) for path in files],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "060a92f3-b130-492f-8a51-7a47193a6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"input\", \"labels\"], data=data)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be46903-5349-47ee-addc-58a0038a8363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       input\n",
       "count   3886\n",
       "unique  3830\n",
       "top         \n",
       "freq      35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40593af6",
   "metadata": {},
   "source": [
    "We now have a DataFrame containing 3886 rows of data. You can see here that 35\n",
    "rows have no visible text. Potentially something went wrong with the OCR\n",
    "extraction, or the redaction covered the entire image. I didn't really know or\n",
    "want to fiddle around with that too much, so I left those rows in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37595827-1567-436a-a72e-2ad66fb994b0",
   "metadata": {},
   "source": [
    "## Moving into HF Transformers Land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ca41e",
   "metadata": {},
   "source": [
    "We create a `Dataset` object from our DataFrame. It requires that our targets\n",
    "have the column name `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa2ac90-77e6-4e28-9efa-7575adae1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e158d2c-65b9-4681-8bfd-5d5b1adec80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'labels'],\n",
       "    num_rows: 3886\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c1af8",
   "metadata": {},
   "source": [
    "We're finetuning a pre-trained model here, so I start with the small version of\n",
    "Deberta which will allow me (I hope!) to iterate quickly and come up with an\n",
    "initial baseline and sense of whether this is even a viable approach to solving\n",
    "the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "656c4c34-57f4-4772-8e81-9f89ccd47771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7babc",
   "metadata": {},
   "source": [
    "Before we finetune our model, we have to do two things to our text data in order\n",
    "that it works within our gradient descent powered training process:\n",
    "\n",
    "- we have to tokenise our text data\n",
    "- we have to turn those tokens into numbers so they can be crunched within our\n",
    "  GPU as numbers.\n",
    "\n",
    "Tokenisation is the process of splitting our words into shorter stubs of text --\n",
    "there are varying schools of thought and use cases on the extent to which you\n",
    "break the words down. We have to use the same tokenisation process that was used\n",
    "by our pretrained model, so we let `transformers` grab the original tokenisers\n",
    "that was used with `deberta-v3-small`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acd4f836-cb20-4058-8cc0-69118462cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3045dd68-5721-4738-9609-934b5db2a0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x): return tokz(x[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1e36814-3438-403a-8e02-7a67ad068a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62a991692f84ba2adac10b2f8e763ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10214dfa",
   "metadata": {},
   "source": [
    "We split our data into training and validation subsets as per usual so that we\n",
    "know how our model is doing while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc83e01-8623-4f2a-ac78-1046399b8504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 2914\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 972\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "dds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd2831c",
   "metadata": {},
   "source": [
    "We define our metric as Pearson's `r` AKA [the Pearson correlation\n",
    "coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), a\n",
    "metric I don't feel an immense instinctual understanding for, but suffice it for\n",
    "this blogpost to know that a higher value (up to a maximum of 1) is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e33327f4-68bb-4f09-8b04-7a0605f197fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x, y):\n",
    "    return np.corrcoef(x, y)[0][1]\n",
    "\n",
    "\n",
    "def corr_d(eval_pred):\n",
    "    return {\"pearson\": corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff53a621-50fa-41ce-92a0-2caf577f7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf2e66",
   "metadata": {},
   "source": [
    "Here we define our batch size, the number of epochs we want to train for as well\n",
    "as the learning rate. The defaults in Jeremy's NLP notebook were far higher than\n",
    "what you see here. His batch size was 128. When I ran the cells that follow, I\n",
    "hit the infamous \"CUDA out of memory\" error more or less immediately. I was\n",
    "running on a machine with a 16GB RAM GPU, but this apparently wasn't enough and\n",
    "the batch size was **far** too large. I had to reduce it down to 4, as you can\n",
    "see, in order to even be able to train the model. There are tradeoffs to this in\n",
    "terms of how well the model learns, but without spending lots of money on fancy\n",
    "machines this was the compromise I had to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86947f8d-c599-4c98-9c24-77c6febff3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "epochs = 5\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4356736-91e8-4ca7-9fbe-bfd48dcf0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"outputs\",\n",
    "    learning_rate=lr,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs * 2,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12658302-74ae-4cec-ab6a-1f097e2059cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_nm, num_labels=1\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dds[\"train\"],\n",
    "    eval_dataset=dds[\"test\"],\n",
    "    tokenizer=tokz,\n",
    "    compute_metrics=corr_d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a257e72-25dd-45ed-8054-d7d1ce976ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2914\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3645' max='3645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3645/3645 09:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Pearson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.168366</td>\n",
       "      <td>0.705429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.134761</td>\n",
       "      <td>0.748499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.114869</td>\n",
       "      <td>0.784274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.093946</td>\n",
       "      <td>0.818484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.091717</td>\n",
       "      <td>0.822977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to outputs/checkpoint-500\n",
      "Configuration saved in outputs/checkpoint-500/config.json\n",
      "Model weights saved in outputs/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 972\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/checkpoint-1000\n",
      "Configuration saved in outputs/checkpoint-1000/config.json\n",
      "Model weights saved in outputs/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 972\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/checkpoint-1500\n",
      "Configuration saved in outputs/checkpoint-1500/config.json\n",
      "Model weights saved in outputs/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to outputs/checkpoint-2000\n",
      "Configuration saved in outputs/checkpoint-2000/config.json\n",
      "Model weights saved in outputs/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 972\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/checkpoint-2500\n",
      "Configuration saved in outputs/checkpoint-2500/config.json\n",
      "Model weights saved in outputs/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 972\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to outputs/checkpoint-3000\n",
      "Configuration saved in outputs/checkpoint-3000/config.json\n",
      "Model weights saved in outputs/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to outputs/checkpoint-3500\n",
      "Configuration saved in outputs/checkpoint-3500/config.json\n",
      "Model weights saved in outputs/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 972\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa7641-773f-4993-bf50-6ea1ae907ac7",
   "metadata": {},
   "source": [
    "At the end of all this, we have a Pearson's score of 0.82 on our validation set\n",
    "which doesn't seem to be as good as our image classifier. I'm not sure how I\n",
    "would go about comparing these two different metrics. I imagine I'd want to\n",
    "ensure that both my metrics were identical to make a like-for-like comparison.\n",
    "\n",
    "My model is available on the Huggingface Hub\n",
    "[here](https://huggingface.co/strickvl/nlp-redaction-classifier).\n",
    "\n",
    "## What did I learn?\n",
    "\n",
    "- Training NLP models feels like a bit of a different world from that of\n",
    "  computer vision. There are different constraints in the process that I wasn't\n",
    "  previously aware of and working with the `transformers` library exposed me to\n",
    "  a bunch of new errors and hoops I had to jump through.\n",
    "- It seems that the RAM needed on the GPU is directly correlated with the length\n",
    "  of the text documents. Mine were on the long-ish end of the scale\n",
    "  (particularly when compared with tweets which was what Jeremy was training on\n",
    "  in his notebook). I wonder how people solve this problem, since mine by were\n",
    "  by no means incredibly long.\n",
    "- NLP models take longer to train than computer vision models; at least, the\n",
    "  transformer-based models that I was working with.\n",
    "- It's hard to compare two models together that don't share the same metric or\n",
    "  loss function.\n",
    "- There are MANY fiddly knobs to twist with NLP, particularly around the\n",
    "  pre-processing of text samples, tokenisation strategies and so on. I wonder\n",
    "  how much of those will be abstracted away from the high-level fastai\n",
    "  abstraction when the library integrates with `transformers` in the coming\n",
    "  months.\n",
    "- The end-to-end process is *broadly* the same, however, and it was good to have\n",
    "  the foundation that we've been building up over the previous weeks in the\n",
    "  course.\n",
    "\n",
    "The next model I train hopefully will not be relating to redactions, I promise!\n",
    "\n",
    "UPDATE: I read a bit in the new O'Reilly book by the `transformers` team,\n",
    "[*Natural Language Processing with Transformers*](https://transformersbook.com),\n",
    "which seems to address the issue of the same text size:\n",
    "\n",
    "> \"Transformer models have a maximum input sequence length that is referred to\n",
    "> as the *maximum context size*. For applications using DistilBERT, the maximum\n",
    "> context size is 512 tokens, which amounts to a few paragraphs of text. [...]\n",
    "> Texts that are longer than a model's context size need to be truncated, which\n",
    "> can lead to a loss in performance if the truncated text contains crucial\n",
    "> information.\" (pages 28-29 of the paperback edition)\n",
    "\n",
    "The book suggests plotting out the number of tokens to get a sense of the\n",
    "distribution of the data by size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "313de8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"Tokens per document\"] = df[\"input\"].apply(lambda x: len(x.split()))\n",
    "df.boxplot(\n",
    "    \"Tokens per document\",\n",
    "    by=\"labels\",\n",
    "    grid=False,\n",
    "    showfliers=False,\n",
    ")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d74233",
   "metadata": {},
   "source": [
    "Here we can see that we have a fairly wide distribution, with quite a few texts\n",
    "going all the way up to 800 tokens in length, so that is probably responsible\n",
    "for the large amounts of RAM needed, but perhaps the truncation of texts is also\n",
    "harming our performance.\n",
    "\n",
    "When I visit [the `deberta-v3-small` model\n",
    "card](https://huggingface.co/microsoft/deberta-v3-small) on Huggingface, I also\n",
    "see reference to a maximum sequence length of 256 which would indeed harm my\n",
    "model and its ability to learn, I reckon."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e0a4c679d9f6b5b7f0e3346667179a21ad75e9d85922afed17d29bd73315778"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
