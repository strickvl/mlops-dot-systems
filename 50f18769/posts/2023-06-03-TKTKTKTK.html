<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Strick van Linschoten">
<meta name="dcterms.date" content="2023-06-03">
<meta name="description" content="I explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language. I share the challenges I faced due to language-specific limitations, my initiative to expand language metadata, and my plans to assess and enhance tokenization efficiency.">

<title>Alex Strick van Linschoten - Tokenizing Balochi with HuggingFaceâ€™s Tokenizer and FastAI/Spacy</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script defer="" data-domain="mlops.systems" src="https://plausible.io/js/script.js"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Alex Strick van Linschoten - Tokenizing Balochi with HuggingFaceâ€™s Tokenizer and FastAI/Spacy">
<meta property="og:description" content="I explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language.">
<meta property="og:image" content="https://mlops.systems/posts/images/tokenisation/hf-tokenizer-small.png">
<meta property="og:site-name" content="Alex Strick van Linschoten">
<meta property="og:image:height" content="511">
<meta property="og:image:width" content="669">
<meta name="twitter:title" content="Alex Strick van Linschoten - Tokenizing Balochi with HuggingFaceâ€™s Tokenizer and FastAI/Spacy">
<meta name="twitter:description" content="I explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language.">
<meta name="twitter:image" content="https://mlops.systems/posts/images/tokenisation/hf-tokenizer-small.png">
<meta name="twitter:creator" content="@strickvl">
<meta name="twitter:image-height" content="511">
<meta name="twitter:image-width" content="669">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Alex Strick van Linschoten</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/strickvl"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://sigmoid.social/web/@alexstrick"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/strickvl"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mlops.systems/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Tokenizing Balochi with HuggingFaceâ€™s Tokenizer and FastAI/Spacy</h1>
                  <div>
        <div class="description">
          I explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language. I share the challenges I faced due to language-specific limitations, my initiative to expand language metadata, and my plans to assess and enhance tokenization efficiency.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">nlp</div>
                <div class="quarto-category">balochi-language-model</div>
                <div class="quarto-category">tokenisation</div>
                <div class="quarto-category">balochi</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Strick van Linschoten </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 3, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#load-our-text-corpus" id="toc-load-our-text-corpus" class="nav-link active" data-scroll-target="#load-our-text-corpus">Load our text corpus</a></li>
  <li><a href="#pre-process-the-texts" id="toc-pre-process-the-texts" class="nav-link" data-scroll-target="#pre-process-the-texts">Pre-process the texts</a></li>
  <li><a href="#training-a-tokenizer-using-tokenizers" id="toc-training-a-tokenizer-using-tokenizers" class="nav-link" data-scroll-target="#training-a-tokenizer-using-tokenizers">Training a Tokenizer using ğŸ¤— Tokenizers</a></li>
  <li><a href="#training-a-custom-tokenizer-using-spacy-and-fastai" id="toc-training-a-custom-tokenizer-using-spacy-and-fastai" class="nav-link" data-scroll-target="#training-a-custom-tokenizer-using-spacy-and-fastai">Training a custom tokenizer using Spacy and FastAI</a>
  <ul class="collapse">
  <li><a href="#lessons-learned" id="toc-lessons-learned" class="nav-link" data-scroll-target="#lessons-learned">Lessons learned</a></li>
  <li><a href="#balochi-tokenizers-on-huggingface-hub" id="toc-balochi-tokenizers-on-huggingface-hub" class="nav-link" data-scroll-target="#balochi-tokenizers-on-huggingface-hub">Balochi Tokenizers on Huggingface Hub</a></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next steps</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>In this blog I want to walk through how I trained my first tokenizer(s) on a small Balochi language corpus. I used the Huggingface Tokenizers library and FastAI / Spacy to get a sense of the interfaces involved. Thereâ€™s also some naive pre-processing I did to get the corpus into a format that the tokenizer could handle. Iâ€™m not sure if this is the best way to do it, but it worked for this first iteration.</p>
<p>We can get straight into the implementation details, but the general process was:</p>
<ol type="1">
<li>Load in our data corpus</li>
<li>Pre-process the data (remove non-Balochi characters and numbers, etc.)</li>
<li>Load the algorithm we want to use for tokenisation (using BPE here)</li>
<li>Tokenise the text</li>
</ol>
<p>Iâ€™ll go through each of these steps in turn.</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># !pip install datasets</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># !huggingface-cli login</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># from datasets import load_dataset</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># load_dataset("balochiml/balochi-language-data", data_dir="data", cache_dir="../data")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="load-our-text-corpus" class="level2">
<h2 class="anchored" data-anchor-id="load-our-text-corpus">Load our text corpus</h2>
<p>Here I walk through my <code>.txt</code> files and load the paths into a list. You can see we have 4294 files to work with.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_txt_file_paths(directory):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    txt_file_paths <span class="op">=</span> []</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> root, dirs, files <span class="kw">in</span> os.walk(directory):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> files:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">file</span>.endswith(<span class="st">".txt"</span>):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                file_path <span class="op">=</span> os.path.join(root, <span class="bu">file</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                txt_file_paths.append(file_path)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> txt_file_paths</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace "directory_path" with the actual path of the directory you want to search</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>directory_path <span class="op">=</span> <span class="st">"../data/raw_text"</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>txt_paths <span class="op">=</span> get_txt_file_paths(directory_path)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(txt_paths)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>4294</code></pre>
</div>
</div>
</section>
<section id="pre-process-the-texts" class="level2">
<h2 class="anchored" data-anchor-id="pre-process-the-texts">Pre-process the texts</h2>
<p>I still donâ€™t fully have a good sense of the best ways to do this, not least of all because Iâ€™m not sure of the tradeoffs for decisions I take. For example, I frequently hear that people remove punctuation during pre-processing, but Iâ€™m not sure how thatâ€™s helpful. It feels like youâ€™d be removing context more than anything else.</p>
<p>I had similar thoughts on the removal of numbers, but in the end I removed them along with any a-z or A-Z English-language characters. I also removed excess whitespace.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(file_path):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Open the file and read it into memory</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">"r"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="bu">file</span>.read()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove English-language characters and numbers</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"[a-zA-Z0-9]"</span>, <span class="st">""</span>, text)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Remove any excess whitespace</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r"[^\S\n]+"</span>, <span class="st">" "</span>, text)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> path <span class="kw">in</span> txt_paths:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    cleaned_text <span class="op">=</span> clean_text(path)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># write the cleaned text to a new file with an incremented filename</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># write the files all into the '../data/processed_text' directory</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f'../data/processed_text/</span><span class="sc">{</span>path<span class="sc">.</span>split(<span class="st">"/"</span>)[<span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">'</span>, <span class="st">"w"</span>, encoding<span class="op">=</span><span class="st">"utf-8"</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">file</span>.write(cleaned_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-a-tokenizer-using-tokenizers" class="level1">
<h1>Training a Tokenizer using ğŸ¤— Tokenizers</h1>
<p>The process of â€˜trainingâ€™ a tokeniser using the Huggingface Tokenizers library was pretty straightforward. There are some nuances and parameters where â€“ again â€“ Iâ€™m not sure of the tradeoffs Iâ€™m making. Iâ€™ll mention those when I get to them.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers <span class="im">import</span> Tokenizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.models <span class="im">import</span> BPE</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer(BPE(unk_token<span class="op">=</span><span class="st">"[UNK]"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.pre_tokenizers <span class="im">import</span> Whitespace</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>tokenizer.pre_tokenizer <span class="op">=</span> Whitespace()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, for example, Iâ€™m pretty sure that the vocabulary size is an important hyperparameter to tune, as is the minimum frequency of tokens. The values here are the defaults in the library. Iâ€™ve read that a higher vocab size might be warranted in a language that is morphologically complex, but I donâ€™t think that Balochi qualifies for that. Also, a larger vocabulary size might be warranted for a language for which I have a larger corpus.</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tokenizers.trainers <span class="im">import</span> BpeTrainer</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">30000</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> BpeTrainer(</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    min_frequency<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    vocab_size<span class="op">=</span>vocab_size,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    special_tokens<span class="op">=</span>[<span class="st">"[UNK]"</span>, <span class="st">"[CLS]"</span>, <span class="st">"[SEP]"</span>, <span class="st">"[PAD]"</span>, <span class="st">"[MASK]"</span>],</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    show_progress<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a list of all the txt files in</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># '/Users/strickvl/balochi/balochi-tokenizer/data/processed_text'</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>processed_files <span class="op">=</span> get_txt_file_paths(<span class="st">"../data/processed_text"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">len</span>(processed_files) <span class="op">==</span> <span class="bu">len</span>(txt_paths)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(processed_files)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>4294</code></pre>
</div>
</div>
<p>The training process itself was a matter of passing the files and the (configured) trainer into the <code>.train()</code> method. It was extremely quick to run, taking only around minutes to crunch through my corpus. (For reference, Iâ€™m now up to around 2.8 million words of Balochi text in the corpus, a drop in the ocean compared to the datasets used to trained English-language LLMs.)</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>tokenizer.train(processed_files, trainer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>

</code></pre>
</div>
</div>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>tokenizer.model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>&lt;tokenizers.models.BPE at 0x108eaa830&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> tokenizer.get_vocab_size() <span class="op">==</span> vocab_size</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>tokenizer.get_vocab_size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>30000</code></pre>
</div>
</div>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tokenizer.get_vocab()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>I also saved the tokenizer to disk so that I (or others) can load it in without needing the dataset at a later date. This saves a JSON file which contains all the information needed to load the tokenizer separately from the data.</p>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>tokenizer.save(<span class="st">"../models/30k-balochi-tokenizer.json"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer.from_file(<span class="st">"../models/30k-balochi-tokenizer.json"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And here you can see the results on a sample from some Balochi text I found somewhere on the internet.</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>sample_text <span class="op">=</span> <span class="st">"&nbsp; &nbsp; &nbsp; Ø¢ÛŒÚ©&nbsp; Ø¬Ù†Ø§ÙˆØ±Û’ Ø§ÙØªÛ”&nbsp; Ù„Ú¾ØªÛ’ Ú¯Ø´ÛŒØª Ø¢ Ø³Ú©ÛŒÚº Ú©Ø§Ø±Ø²ÙˆØ§Ù„Û’ Ø§Øª Ú©Û Ø§Ú¯Ø§Úº Ø¢Ø²Ø§ØªÛŒ Ø¯ÛŒÚ¯ Ø¨Û Ø¨ÛŒØªØŒ Ø¨Ø§Ø²Ø§Ø±Ø¡ÙØŒ Ù„ÙˆÚ¯Û’ Ø¡ÙØŒ Ø¬Ø§Ú¯Ø§Û ÛŒÛ’&nbsp; Ø¡ÙØŒØ¯Ù¾ØªØ± Ø¡ Ù Ú©Ø§Ø±Ú¯Ø³ ÛŒÛ’&nbsp; Ø¡Ù ÛŒØ§ Ú¾Ø± Ú¾Ù…Ø§ Ø¬Ø§Ú¯Ø§Û Ø¡Ù Ú©Û Ø´ÙØª Ú©Ù†Øª Ù…Ø²Ù†ÛŒÚº Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒ Ú©Ù†ØªÛ”Ú¯ÙˆÚº Ú¾Ø± Ú©Ø³ Ø¡Ù Ø¬Ù†Ú¯ Ø¡ Ù Ù…Ú‘ Ø¨ÛŒØªÛ”Ú¯Ø¯Ø¡ Ù Ù¾Ú†Ø§Úº&nbsp; Ú†Ù†Úˆ Ú†Ù†Úˆ Ø¡ Ù Ø±Ø§Ú‘ Ø±Ø§Ú‘ Ú©Ù†ØªØŒÚ©Ø§Ú¯Ø¯ Ø¡ Ù ÙˆØ§Ù†Ú¯ÛŒØ§Úº ÙˆØ§Ø±Øª Ø¡ Ù Ø¢Ø¯Ø±Ø§Û Ú©Ù†ØªÛ”ÙˆØ±Ú¯ÛŒ Ú†ÛŒØ²Ø§Úº Ø§Ú¯Ø§Úº ÙˆØ§Ø±Øª Ù†Ú©Ù†Øª Ø¢Ú¾Ø§Úº Ú¯Ù¹ Ù¾Ø§Ú†ÛŒØª Ú¾Ø±Ø§Ø¨ Ú©Ù†ØªÛ”Ø§ÛŒÙ†Ø¯Ú¯Û Ø¬Ù†Ø§ÙˆØ± Ú†Û Ø¨Ù†Ø¯Ø§Øª Ø¡ Ù Ø§ÛŒØ´ÛŒ Ø¡Ù Ú©Ø§Ø²ÙˆØ§Ù„ÛŒØ§Úº Ú†Û ÙˆØªØ§ Ø¯ÛŒØ± Ø¯Ø§Ø±Ú¯ Ø¡Ù Ú©ÙˆØ´Ø³Øª Ú©Ù† Ø§ÙÙ†ØªÛ” Ú†ÛŒØ§ Ú©Û Ø¢ Ø¨Ø§Ø²ÛŒÚº Ø¯Ú¯Û Ú¾Ø±Ø§Ø¨ÛŒ Ø¡ Ù Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒ Ú¾Ù… Ú©Ù†ØªØŒÙ¾Ù…ÛŒØ´Ú©Ø§ Ú©Ø³Ø§Ù†ÛŒÚº Ø¬Ù†Ø§ÙˆØ±&nbsp; Ø¨Ø§Ù„ÛŒ Ù…ÙØ±Ú¯ØŒÚ©ÙˆÛ Ù¾Ø§Ú†Ù†ØŒØ¢Ø³Ú© Ø¡ Ù Ø§ÛŒÙ†Ø¯Ú¯Û Ú©Ø³Ø§Ù† Ú©Ø³Ø§Ù†ÛŒÚº Ø¬Ù†Ø§ÙˆØ±Ú†Ø± Ø¢Ø¦ÛŒ Ø¡Ù Ú©Ø§Ø±Ø²ÙˆØ§Ù„ÛŒØ§Ù†ÛŒ Ø³ÙˆØ¨ Ø¡Ù Ø¢Ø¦ÛŒ Ø¡Ù Ú†Û Ø³Ú© Ø¨Ø§Ø² Ø´Ø²Ø§Ø± Ø§ÙÙ†Øª Û”"</span>.replace(</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"</span><span class="ch">\xa0</span><span class="st">"</span>, <span class="st">""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>sample_sentence <span class="op">=</span> sample_text.split(<span class="st">"Û”"</span>)[<span class="dv">2</span>]</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>sample_sentence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>'Ú¯ÙˆÚº Ú¾Ø± Ú©Ø³ Ø¡Ù Ø¬Ù†Ú¯ Ø¡ Ù Ù…Ú‘ Ø¨ÛŒØª'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>tokenizer.encode(sample_sentence).tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>['Ú¯ÙˆÚº', 'Ú¾Ø±', 'Ú©Ø³', 'Ø¡Ù', 'Ø¬Ù†Ú¯', 'Ø¡', 'Ù', 'Ù…Ú‘', 'Ø¨ÛŒØª']</code></pre>
</div>
</div>
<p>Judging by this tiny example, actually the tokenization process doesnâ€™t seem to have saved us much in terms of space. The tokens from the encoded text are basically just the words from the original text.</p>
</section>
<section id="training-a-custom-tokenizer-using-spacy-and-fastai" class="level1">
<h1>Training a custom tokenizer using Spacy and FastAI</h1>
<p>The FastAI <a href="https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb">course and book</a> have a whole chapter that deals with NLP and a section that deals with tokenization and subwords so I thought Iâ€™d follow through that process as well to get a sense of the higher-level API that FastAI provides as well as the implementation under the hood provided by <a href="https://spacy.io">Spacy</a>.</p>
<p>When you install FastAI, youâ€™ll probably notice that it has Spacy as a dependency. This is because it uses Spacy under the hood for tokenization (along with a lot of other NLP tasks). FastAI provides a wrapper around Spacyâ€™s <code>Tokenizer</code> object along with some helper functions and other bits and pieces.</p>
<p>Iâ€™ll admit to not finding the FastAI interface as intuitive or useful as the ğŸ¤— Tokenizers library, in part because it was harder to get at some of the Spacy primitives when it became necessary to do so. More on this below.</p>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.text.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a built-in helper function from fastai</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>files <span class="op">=</span> get_text_files(<span class="st">"../data/processed_text"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(files)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>4294</code></pre>
</div>
</div>
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get some sample text from the first file</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>txt <span class="op">=</span> files[<span class="dv">0</span>].<span class="bu">open</span>().read()<span class="op">;</span> txt[:<span class="dv">75</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>'*Ø¢Ù…ÛŒØªÚ¯Ø¡Ù Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†* Ù„Ú†Ù‘Û: *Ø¢Ù…ÛŒØªÚ¯Ø¡Ù Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†* Ø¢ Ù…ÛŒØªÚ¯Ø¡ÙÚ©Û Ù…Ù† ÙˆØªÛŒ Ø´ÙˆÚ©ÛŒÚº Ú©Ø³Ø§Ù†ÛŒ'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using the `SpacyTokenizer` from fastai</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="co"># see https://docs.fast.ai/text.core.html#spacytokenizer</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>spacy <span class="op">=</span> WordTokenizer()</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>toks <span class="op">=</span> first(spacy([txt]))</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coll_repr(toks, <span class="dv">30</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(#146) ['*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ù„Ú†Ù‘Û',':','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ø¢','Ù…ÛŒØªÚ¯Ø¡ÙÚ©Û','Ù…Ù†','ÙˆØªÛŒ','Ø´ÙˆÚ©ÛŒÚº','Ú©Ø³Ø§Ù†ÛŒ','Ù¾ÛŒØ±','Ú©ÙØª','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬Ø³ÙØªØ¡ÙÙ…Ú©Ù†','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ú¯ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¨Û’','Ø§ÙˆØ³ØªÛŒÚº','ØªØ§Ù‡ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¢'...]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>tkn <span class="op">=</span> Tokenizer(spacy)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coll_repr(tkn(txt), <span class="dv">31</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(#147) ['xxbos','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ù„Ú†Ù‘Û',':','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ø¢','Ù…ÛŒØªÚ¯Ø¡ÙÚ©Û','Ù…Ù†','ÙˆØªÛŒ','Ø´ÙˆÚ©ÛŒÚº','Ú©Ø³Ø§Ù†ÛŒ','Ù¾ÛŒØ±','Ú©ÙØª','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬Ø³ÙØªØ¡ÙÙ…Ú©Ù†','Ø¢','Ù…ÛŒØªÚ¯Ø¡Ù','Ú¯ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¨Û’','Ø§ÙˆØ³ØªÛŒÚº','ØªØ§Ù‡ÛŒØ±Ø§Úº','Ù…Ø¨Ùˆ','Ø¢'...]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>txts <span class="op">=</span> L(o.<span class="bu">open</span>().read() <span class="cf">for</span> o <span class="kw">in</span> files)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a sense for the subwords generated from a</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co"># small slice of our text data</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> subword(size: <span class="bu">int</span>):</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    sp <span class="op">=</span> SubwordTokenizer(vocab_sz<span class="op">=</span>size)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    sp.setup(txts)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">" "</span>.join(first(sp([txt]))[:<span class="dv">40</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>subword(<span class="dv">1000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="83">
<pre><code>'â–* Ø¢ Ù…ÛŒ ØªÚ¯ Ø¡Ù â–Ø¬ÙØ³Øª Ø¡Ù Ù… Ú© Ù† * â–Ù„Ú†Ù‘Û : â–* Ø¢ Ù…ÛŒ ØªÚ¯ Ø¡Ù â–Ø¬ÙØ³Øª Ø¡Ù Ù… Ú© Ù† * â–Ø¢ â–Ù…ÛŒØªÚ¯ Ø¡Ù Ú©Û â–Ù…Ù† â–ÙˆØªÛŒ â–Ø´ ÙˆÚ©ÛŒÚº â–Ú©Ø³ Ø§Ù†ÛŒ â–Ù¾ÛŒØ± â–Ú©ÙØª â–Ø¢ â–Ù…ÛŒØªÚ¯ Ø¡Ù â–Ø¬'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>subword(<span class="dv">275</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="86">
<pre><code>'â– * Ø¢ Ù… ÛŒ Øª Ú¯ Ø¡ Ù â– Ø¬ Ù Ø³ Øª Ø¡ Ù Ù… Ú© Ù† * â– Ù„ Ú† Ù‘ Û : â– * Ø¢ Ù… ÛŒ Øª Ú¯ Ø¡ Ù â– Ø¬ Ù Ø³ Øª'</code></pre>
</div>
</div>
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>toks200 <span class="op">=</span> txts[:<span class="dv">200</span>].<span class="bu">map</span>(tkn)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>toks200[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="87">
<pre><code>(#147) ['xxbos','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†','*','Ù„Ú†Ù‘Û',':','*','Ø¢Ù…ÛŒØªÚ¯Ø¡Ù','Ø¬ÙØ³ØªØ¡ÙÙ…Ú©Ù†'...]</code></pre>
</div>
</div>
<p>At this point, once weâ€™ve seen a bit how FastAI and Spacy are able to tokenize the text, we can switch into the numericalisation process and see what we get from our dataset.</p>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> Numericalize()</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>num.setup(toks200)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>coll_repr(num.vocab,<span class="dv">50</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="89">
<pre><code>"(#4096) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','Ø¡Ù','Ø¡Ù','Ø¡Ù','Û”','Ú©Û','ØŒ','Ø§Ù†Øª','Ù…Ù†','Ø§Û’','Ù†Û','ÙˆØªÛŒ','Ø¨ÛŒØª','â€','Ø§Øª','Ú†Û','Ú¯ÙˆÚº','Ø§ÙÙ†Øª','Ø§ÙÙ†Øª','Ù¾Û','Ø¨Û','â€˜','ÛŒÚ©','Ø¢Ø¦ÛŒ','.','Ø¢','Ù…Ù†ÛŒ','Ú¾Ù…',')','Ú©Ù†Øª','Ø¨Ù„ÙˆÚ†ÛŒ','3','ØªÙˆ','Ø¨Ù„Û’','Ø¦Û’',':','Ú©Ù†Ú¯','(','Ø¨ÙˆØªÚ¯','Ø¢Úº','Ú©Ù†','ØŸ'...]"</code></pre>
</div>
</div>
<p>You can see that some of the meta-tokens <a href="https://mlops.systems/posts/2023-06-01-why-tokenisation.html">mentioned in my last blog</a> are also represented here, and then the rest of the words are sorted by frequency order.</p>
<p>We can represent a sample of text as the token ids at this point:</p>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>nums <span class="op">=</span> num(toks)[:<span class="dv">20</span>]<span class="op">;</span> nums</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>TensorText([ 156, 2340,    0,  156,  563,   43,  156, 2340,    0,  156,   33,
               0,   16,   19, 1490,  831,  457,  102,   33, 1031])</code></pre>
</div>
</div>
<p>When we convert this back, youâ€™ll see we get the meta-tokens as well.</p>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co">' '</span>.join(num.vocab[o] <span class="cf">for</span> o <span class="kw">in</span> nums)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>'* Ø¢Ù…ÛŒØªÚ¯Ø¡Ù xxunk * Ù„Ú†Ù‘Û : * Ø¢Ù…ÛŒØªÚ¯Ø¡Ù xxunk * Ø¢ xxunk Ù…Ù† ÙˆØªÛŒ Ø´ÙˆÚ©ÛŒÚº Ú©Ø³Ø§Ù†ÛŒ Ù¾ÛŒØ± Ú©ÙØª Ø¢ Ù…ÛŒØªÚ¯Ø¡Ù'</code></pre>
</div>
</div>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons learned</h2>
<p>This first attempt at tokenisation was instructive in a number of respects.</p>
<p>I didnâ€™t show what was going on under the hood with the FastAI wrapper, but if you look at the source code youâ€™ll see that the line <code>spacy = WordTokenizer()</code> assumes that the base language weâ€™re dealing with is English. You can of course pass in a language code to the <code>WordTokenizer</code> initialization, but since it uses Spacy under the hood here and since Balochi isnâ€™t represented as an official language supported by Spacy, when youâ€™re basically out of luck. You hit an error and you can either continue using simplistic algorithms like the ones demonstrated above (essentially splitting on word delimiters) or you can abandon FastAI and dive into Spacy.</p>
<p>At that point, youâ€™ll have to start implementing a whole bunch of things yourself in order to get going quickly. For example, youâ€™ll ideally want to come up with all the list of punctuation marks, stop words, stemming rules and so on that I mentioned last time. (It might well be that itâ€™s possible to get up and running faster for a non-standard language with Spacy, but it wasnâ€™t clear to me how to do that.)</p>
<p>I do actually now intend to make a contribution to the Spacy repo to have Balochi represented there, and to open the window for others to contribute to the language metadata directly, but that didnâ€™t help me in the moment. Youâ€™ll notice that I didnâ€™t show how you can save a serialized version of the Spacy/FastAI tokeniser because I wasnâ€™t able to figure out how to get access to the underlying Spacy object. Iâ€™m sure itâ€™s possible since I can [read the Spacy API documentation showing which method to use]((https://spacy.io/api/tokenizer#to_disk) but FastAI didnâ€™t itself expose this functionality directly.</p>
<p>My initial impression from working with both libraries and spending some time with their documentation is that Spacy might end up being more useful for low-resource languages given the extent to which they support a more complete range of old-school NLP methods and techniques. That said, the ğŸ¤— Tokenizers library was much easier to get up and running with and I think itâ€™s a great option for anyone who wants to get started quickly with tokenization. They support most of the major algorithms youâ€™d ever need to use and if they donâ€™t you can always implement something yourself to extend it.</p>
</section>
<section id="balochi-tokenizers-on-huggingface-hub" class="level2">
<h2 class="anchored" data-anchor-id="balochi-tokenizers-on-huggingface-hub">Balochi Tokenizers on Huggingface Hub</h2>
<p>Iâ€™m still working through a way to open up the core dataset (along with constructing as I work), but this first iteration of the tokenizer is now <a href="https://huggingface.co/balochiml/balochi-tokenizer">available over on the Huggingface Hub</a>. You can load it for use with the single line:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer.from_file(<span class="st">"../models/30k-balochi-tokenizer.json"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The organisation is something I created together with some Balochi colleagues who expressed an interest in working together on this effort. Iâ€™m really happy to have made their acquaintance and I hope Iâ€™ll be able to make steady progress on this project with their help. (If youâ€™re interested in contributing, please request access to the organization and/or contact me for more information.)</p>
<p>While creating the tokenizer repository, I also noted how Balochi (as with Spacy) is not represented as a language recognised by the metadata tracking languages used on the Hub. Frustratingly, youâ€™re asked to input <a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">an ISO-639-1 two-letter code</a> to represent the language of the model, but of course Balochi doesnâ€™t have one of those. Balochi only has an ISO-693-2 and ISO-693-3 code. Iâ€™ll have to see how we can get Balochi represented on the Hub given all this. It canâ€™t be the first time that this has happened.</p>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next steps</h2>
<p>Now that I have this first iteration complete, I want to reflect a bit on how to know when the tokenizer is â€˜good enoughâ€™. In particular, how do you evaluate tokenisers? Are there ways of benchmarking this? There must have been work done on this and I want to understand both what the start of the art is as well as how to know when Iâ€™ve reached it.</p>
<p>I also watched <a href="https://www.youtube.com/watch?v=X7c0T7uwtkM">an extremely rewarding talk on low-resource languages</a> (blog notes to follow!) and there was a section in that which stressed the foundational nature of tokenisation as part of language models. It also highlighted a failure mode where bad tokenisation made a model perform very badly on a certain kind of task. So based on this context I would like to understand how to evaluate tokenisers and how to know when Iâ€™ve reached a good enough point.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="strickvl/mlops-dot-systems" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>