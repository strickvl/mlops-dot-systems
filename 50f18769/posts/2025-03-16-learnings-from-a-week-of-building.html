<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Strick van Linschoten">
<meta name="dcterms.date" content="2025-03-16">
<meta name="description" content="Insights from a week of building an LLM-based knowledge database, highlighting experiences with local models, prompt engineering patterns, development tools like Ollama and RepoPrompt, and software engineering principles that enhance AI-assisted development workflows.">

<title>Alex Strick van Linschoten - Learnings from a week of building with local LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script defer="" data-domain="mlops.systems" src="https://plausible.io/js/script.js"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Alex Strick van Linschoten - Learnings from a week of building with local LLMs">
<meta property="og:description" content="Insights from a week of building an LLM-based knowledge database, highlighting experiences with local models, prompt engineering patterns, development tools like Ollama and RepoPrompt, and software‚Ä¶">
<meta property="og:image" content="https://mlops.systems/posts/images/2025-03-16-learnings-from-a-week-of-building/cover.png">
<meta property="og:site-name" content="Alex Strick van Linschoten">
<meta property="og:image:height" content="984">
<meta property="og:image:width" content="1242">
<meta name="twitter:title" content="Alex Strick van Linschoten - Learnings from a week of building with local LLMs">
<meta name="twitter:description" content="Insights from a week of building an LLM-based knowledge database, highlighting experiences with local models, prompt engineering patterns, development tools like Ollama and RepoPrompt, and software‚Ä¶">
<meta name="twitter:image" content="https://mlops.systems/posts/images/2025-03-16-learnings-from-a-week-of-building/cover.png">
<meta name="twitter:creator" content="@strickvl">
<meta name="twitter:image-height" content="984">
<meta name="twitter:image-width" content="1242">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Alex Strick van Linschoten</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../til.html">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/strickvl"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://sigmoid.social/web/@alexstrick"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/strickvl"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mlops.systems/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Learnings from a week of building with local LLMs</h1>
                  <div>
        <div class="description">
          Insights from a week of building an LLM-based knowledge database, highlighting experiences with local models, prompt engineering patterns, development tools like Ollama and RepoPrompt, and software engineering principles that enhance AI-assisted development workflows.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">claude</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">llms</div>
                <div class="quarto-category">miniproject</div>
                <div class="quarto-category">openai</div>
                <div class="quarto-category">prompt-engineering</div>
                <div class="quarto-category">softwareengineering</div>
                <div class="quarto-category">tools</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Strick van Linschoten </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 16, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#local-models" id="toc-local-models" class="nav-link active" data-scroll-target="#local-models">ü§ñ Local Models</a></li>
  <li><a href="#prompting-instruction-following" id="toc-prompting-instruction-following" class="nav-link" data-scroll-target="#prompting-instruction-following">üí¨ Prompting &amp; Instruction Following</a></li>
  <li><a href="#process-tools" id="toc-process-tools" class="nav-link" data-scroll-target="#process-tools">üß∞ Process &amp; Tools</a></li>
  <li><a href="#software-engineering-patterns" id="toc-software-engineering-patterns" class="nav-link" data-scroll-target="#software-engineering-patterns">üßë‚Äçüî¨ Software Engineering Patterns</a></li>
  <li><a href="#appendix-1-fasthtml" id="toc-appendix-1-fasthtml" class="nav-link" data-scroll-target="#appendix-1-fasthtml">üåê Appendix 1: FastHTML</a></li>
  <li><a href="#appendix-2-ocr-translation" id="toc-appendix-2-ocr-translation" class="nav-link" data-scroll-target="#appendix-2-ocr-translation">üìÉ Appendix 2: OCR + Translation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>I took the past week off to work on a little side project. More on that at some point, but at its heart it‚Äôs an extension of what I worked on with my translation package <a href="https://mlops.systems/posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html"><code>tinbox</code></a>. (The new project uses translated sources to bootstrap a knowledge database.) Building in an environment which has less pressure / deadlines gives you space to experiment, so I both tried out a bunch of new tools and also experimented with different ways of using my tried-and-tested development tools/processes.</p>
<p>Along the way, there were a bunch of small insights which occurred to me so I thought I‚Äôd write them down. As usual with this blog, I‚Äôm mainly writing for my future self but I think there might be parts that are useful for others! Apologies for the somewhat rushed nature of these observations; better I get the blog finished and published than not at all!</p>
<section id="local-models" class="level2">
<h2 class="anchored" data-anchor-id="local-models">ü§ñ Local Models</h2>
<p>During this project, I experimented with several local models, which continue to impress me with their evolving capabilities. The recent launch of <code>gemma3</code> was particularly timely - I found myself regularly using the 27B version, which performed admirably across various tasks.</p>
<p>There are three or four models I keep returning to. <code>mistral-small</code> stands out as an exceptional model that‚Äôs been relatively recently updated and seems a bit underrated / underappreciated. The original <code>mistral</code> model continues to hold up remarkably well, particularly for structured extraction tasks and general writing needs like summarization.</p>
<p>One important realization when working with real-world use cases: benchmarks can be deceptive. While helpful as general indicators, each model has its own strengths and quirks. Many newer models are heavily optimized for structured data extraction, but their performance ultimately depends on whether their training documents align with your specific use case. It‚Äôs crucial to test models against your actual requirements rather than relying solely on published benchmarks.</p>
<p>For robust results with local models, I‚Äôve found that implementing a ‚Äúreflection, iterate and improve‚Äù pattern significantly enhances performance. When you need a model to summarize or analyze content in a particular format, having a secondary model (or even the same model!) review the output against the original prompt requirements is incredibly valuable. This reviewer model can suggest improvements to better fulfill the original request. Running this loop for 2-5 iterations (depending on complexity) can yield results approaching those of proprietary models like Claude or GPT-4, which might achieve similar quality in a single pass. For local deployments, this iterative improvement pattern is essentially non-negotiable.</p>
<p>I also explored vision models, particularly <code>llava</code> and <code>llama-3.2-vision</code>. These were my primary tools for extracting context from images, generating captions, and analyzing visual content. Their effectiveness varies based on content type and language, but they represent impressive capabilities that can run entirely on local systems.</p>
<p>A significant portion of my work involved non-English languages, including some relatively rare ones. This is another area where benchmark claims about supporting ‚Äúhundreds of languages‚Äù often don‚Äôt align with real-world performance. Models might list impressive language coverage in their specifications, but actual proficiency varies dramatically. It reinforces my earlier point - always verify benchmark claims against your specific use case before committing to a particular model.</p>
</section>
<section id="prompting-instruction-following" class="level2">
<h2 class="anchored" data-anchor-id="prompting-instruction-following">üí¨ Prompting &amp; Instruction Following</h2>
<p>Working extensively with various models during this project reinforced some fundamental insights about prompting that might seem basic, but prove critical in practical applications. These observations are particularly relevant when working with local models, though they apply to cloud-based systems as well.</p>
<p>Context matters significantly more than we might assume. While we‚Äôve grown accustomed to proprietary models like Claude or GPT-4o performing admirably with minimal guidance, local models require more deliberate direction. The more relevant context you can provide (within reasonable token limits), the better your results will be. If you would naturally provide certain background information to a human performing the task, make sure to include it in your prompt to the model as well.</p>
<p>Another key insight: every model has its unique characteristics. Techniques that work brilliantly with one model might fall flat with another, especially in the local model ecosystem. They each require slightly different prompting approaches, specific phrasing patterns, and tailored guidance. This necessitates running small experiments to understand how different models respond to various prompting styles. It‚Äôs still more art than science, but this experimentation phase is crucial when implementing local models effectively.</p>
<p>Perhaps the most valuable lesson I rediscovered is that breaking complex tasks into smaller components yields superior results compared to using a single comprehensive prompt. This is particularly true with local models. When performing extensive data extraction or when dealing with structured data where the extraction targets differ significantly from each other, don‚Äôt expect the model to handle everything in one pass ‚Äì even a human might struggle with such an approach.</p>
<p>Instead, break down the task into logical components, create targeted mini-prompts for each aspect, and then recombine the results once all the separate LLM calls are completed. Yes, this approach adds processing time and complexity, but the quality improvement is well worth the trade-off. When accuracy matters more than speed, this decomposition strategy consistently delivers better outcomes.</p>
</section>
<section id="process-tools" class="level2">
<h2 class="anchored" data-anchor-id="process-tools">üß∞ Process &amp; Tools</h2>
<p>My development environment during this project provided plenty of opportunities to evaluate various tools and workflows. As context, I primarily work on a Mac while maintaining access to a separate (local) machine with GPU capabilities for more intensive tasks. This setup allows me to flexibly experiment with both local and cloud-based models.</p>
<p>For managing local models, <a href="https://ollama.ai/">Ollama</a> continues to be my go-to solution for downloading, running, and interfacing with these models. A recent discovery that significantly improved my workflow is <a href="https://boltai.com/">Bolt AI</a>, an excellent Mac interface that provides seamless switching between local Ollama models and cloud-based alternatives. If you‚Äôre working in a hybrid model environment, Bolt AI is definitely worth exploring.</p>
<p>I‚Äôve also recently integrated <a href="https://openrouter.ai/">OpenRouter</a> into my toolkit, which solves the problem of managing countless API keys across different inference providers. OpenRouter not only offers native connections to many cloud providers but also allows you to incorporate your own API keys, streamlining access to a diverse model ecosystem through a unified interface. It also helps with setting spend limits on various models or projects.</p>
<p>In terms of development insights, I was impressed by how rapidly front-end development can progress with the assistance of models like Claude 3.7 and OpenAI‚Äôs O1-Pro. These models perform exceptionally well when supplemented with documentation (such as an <a href="https://llmstxt.org"><code>llms.txt</code> file</a>) alongside your prompts. While I can‚Äôt speak to their effectiveness with extremely complex applications or massive frontend codebases, they demonstrate remarkable proficiency with small to medium-sized projects.</p>
<p>A significant portion of my experimentation involved <a href="https://repoprompt.com">RepoPrompt</a>, a tool that recently transitioned from free beta to a paid license model. RepoPrompt addresses the challenge of getting your codebase into an LLM-friendly format. Unlike standard CLI tools that simply export code to clipboard or text files, RepoPrompt generates a structured XML representation that, when modified by an LLM and pasted back, creates a reviewable diff of the proposed changes. At least, that‚Äôs one of the things it allows you to do! It‚Äôs actually a bit more powerful / flexible than that and here‚Äôs a video so you can see it in action:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://www.youtube.com/watch?v=8zIY0zxcafE%20%22RepoPrompt%20Demo%20Video%20using%20O1%20Pro%22"><img src="https://img.youtube.com/vi/8zIY0zxcafE/0.jpg" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">RepoPrompt Demo Video</figcaption><p></p>
</figure>
</div>
<p>While tools like Cursor and Windsurf offer similar functionality, they tend to become less reliable as project complexity increases. RepoPrompt shines when paired with an OpenAI Pro subscription, enabling effective integration of models like O1 Pro and <code>o3-mini-high</code> into your development lifecycle. In my testing, the RepoPrompt + O1 Pro/O3 Mini High combination consistently delivered superior results compared to using Cursor with Claude 3.7 (even with ‚ÄòThinking Mode‚Äô enabled). Despite the occasional pauses while these models process complex problems, the quality improvement justifies the wait.</p>
<p>Additionally, I continued working with <a href="https://claude.ai/code">Claude Code</a> and <a href="https://codebuff.com/">CodeBuff</a>, both CLI-driven tools focused on code improvement. Of the two, CodeBuff has become my preferred option. Both tools require careful supervision‚ÄîI typically keep Cursor open to monitor changes in real-time, occasionally needing to revert modifications or redirect the approach. These tools excel when you clearly articulate your objectives and maintain oversight of the implementation process. CodeBuff particularly impresses with larger codebases and demonstrates superior stability overall.</p>
<p>An interesting pattern emerged during development: whenever files approached 800-900 lines, it signaled the need to refactor into smaller submodules to maintain LLM comprehension, especially when using agent mode in Cursor. The modular approach significantly improved model performance.</p>
<p>I was genuinely surprised by the effectiveness of the RepoPrompt and O1 Pro combination. For smaller, targeted modifications, CodeBuff continues to demonstrate remarkable capability. While I didn‚Äôt evaluate these tools in conjunction with local models, I suspect such combinations would require more iterative refinement to achieve comparable results.</p>
</section>
<section id="software-engineering-patterns" class="level2">
<h2 class="anchored" data-anchor-id="software-engineering-patterns">üßë‚Äçüî¨ Software Engineering Patterns</h2>
<p>Throughout this experimental project, several software engineering principles proved particularly valuable when working with LLM-assisted development. These patterns aren‚Äôt revolutionary, but their importance amplifies in the context of AI-augmented workflows.</p>
<p>The principle of simplicity served as a cornerstone approach. Breaking development into the smallest logical next task repeatedly demonstrated its value, especially during the exploratory phases when project architecture was still taking shape. While some engineers might possess the cognitive bandwidth to fully conceptualize complex systems with perfect abstractions from the outset, I‚Äôve found incremental development leads to more robust outcomes. This approach aligns naturally with how most developers actually think through problems and provides clear checkpoints for evaluating progress.</p>
<p>Data visibility emerged as another critical factor. When leveraging LLM-assisted coding, comprehensive logging becomes even more essential than in traditional development. Strategically placed log outputs create a diagnostic trail that proves invaluable when troubleshooting unexpected behaviors. This practice creates a feedback loop that strengthens both your understanding of the system and the LLM‚Äôs ability to assist effectively.</p>
<p>A particularly underappreciated practice I haven‚Äôt seen widely discussed is the importance of dead code detection. When working with LLM-assisted development, code cruft tends to accumulate more rapidly than in conventional programming. Tools like <a href="https://github.com/albertas/deadcode"><code>deadcode</code></a> and <a href="https://github.com/jendrikseipp/vulture"><code>vulture</code></a> provide static analysis of Python projects to identify unused functions and variables. Running these tools periodically helps maintain codebase clarity by flagging remnants that might otherwise cause confusion during review. I‚Äôm not certain whether newer tools like <code>ruff</code> from Astral include this functionality (particularly for function calls), but the capability is invaluable for maintaining a clean, navigable codebase.</p>
<p>Taking time to think offline‚Äîaway from the keyboard‚Äîoften yields surprising clarity. This deliberate pause creates space to articulate precisely what you need for the next development increment. When you can express your requirements with precision, the LLM‚Äôs output improves proportionally. Ambiguous instructions inevitably produce suboptimal results, whereas clarity fosters efficiency.</p>
<p>A final observation worth emphasizing: having experience as an engineer in the pre-LLM era remains tremendously advantageous. When confronting complex workflows involving chained LLM calls with interdependencies and reflection patterns, traditional debugging skills become indispensable. Knowing when to step away from AI assistance and dive into manual debugging with tools like <code>pdb</code>, stepping through code execution and inspecting variables directly, represents a crucial judgment call.</p>
<p>LLMs and coding agents often demonstrate a bias toward generating new code rather than methodically analyzing existing problems. Recognizing the moment when direct human intervention becomes more efficient than continually prompting an AI is a skill that comes with experience. Once you‚Äôve manually identified the underlying issue, you can return to the LLM with precisely targeted prompts that yield superior results.</p>
</section>
<section id="appendix-1-fasthtml" class="level2">
<h2 class="anchored" data-anchor-id="appendix-1-fasthtml">üåê Appendix 1: FastHTML</h2>
<p>As a practical addition to my experimentation, I implemented FastHTML for the first time to build a frontend for my knowledge base extraction assistant. The experience was remarkably frictionless, particularly when leveraging their <code>llms.txt</code> file‚Äîa markdown-formatted documentation set that integrates seamlessly with your frontend codebase when provided alongside prompts.</p>
<p>This approach works exceptionally well with models like O1 Pro or O3 Mini High, creating a development workflow that feels intuitive and responsive. Despite having substantial JavaScript experience from previous roles, I found FastHTML significantly more manageable than complex JavaScript frameworks that dominate the ecosystem today.</p>
<p>The reduced cognitive overhead and natural integration with Python-based workflows makes FastHTML a compelling choice for ML practitioners who prefer to minimize context-switching between languages and paradigms. The framework strikes an excellent balance between capability and simplicity that aligns perfectly with rapid prototyping and iterative development cycles common in ML projects. For those building interfaces to ML systems, it‚Äôs definitely worth considering as your frontend solution.</p>
</section>
<section id="appendix-2-ocr-translation" class="level2">
<h2 class="anchored" data-anchor-id="appendix-2-ocr-translation">üìÉ Appendix 2: OCR + Translation</h2>
<p>Another interesting challenge I tackled involved OCR and translation of handwritten documents in non-English languages‚Äîa task that proved impossible to accomplish in a single pass with local models, particularly for less common languages.</p>
<p>The solution emerged through methodical problem decomposition:</p>
<ol type="1">
<li>Breaking down PDFs into individual page images</li>
<li>Segmenting each page into overlapping image chunks (critical for handwriting where text may slant across traditional line boundaries)</li>
<li>Applying OCR to extract text in the original source language from each image segment</li>
<li>Using translation models to convert the extracted text to English</li>
</ol>
<p>This multi-stage pipeline allowed me to overcome the limitations of local models when confronted with the combined complexity of handwriting recognition and translation. Both <code>gemma3</code> and <code>llama-3.3</code> performed admirably within this decomposed workflow, demonstrating that even resource-constrained local deployments can achieve impressive results when problems are thoughtfully restructured.</p>
<p>This case exemplifies a core principle of effective ML implementation: when dealing with complex, multi-faceted challenges, breaking them into targeted sub-problems often yields better outcomes than attempting end-to-end solutions‚Äîespecially when working with constrained computational resources. While this approach may increase processing time, the quality improvement justifies the trade-off for many practical applications.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="strickvl/mlops-dot-systems" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>