<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Strick van Linschoten">
<meta name="dcterms.date" content="2022-03-12">
<meta name="description" content="I used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming. For computer vision problems, it’s really useful to have visual debugging aids and FiftyOne is a well-documented and solid tool to help with that.">

<title>Alex Strick van Linschoten - Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script defer="" data-domain="mlops.systems" src="https://plausible.io/js/script.js"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Alex Strick van Linschoten - Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of">
<meta property="og:description" content="I used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming.">
<meta property="og:image" content="https://mlops.systems/posts/fiftyone-computervision/fiftyone-overview.png">
<meta property="og:site-name" content="Alex Strick van Linschoten">
<meta property="og:image:height" content="331">
<meta property="og:image:width" content="600">
<meta name="twitter:title" content="Alex Strick van Linschoten - Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of">
<meta name="twitter:description" content="I used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming.">
<meta name="twitter:image" content="https://mlops.systems/posts/fiftyone-computervision/fiftyone-overview.png">
<meta name="twitter:creator" content="@strickvl">
<meta name="twitter:image-height" content="331">
<meta name="twitter:image-width" content="600">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Alex Strick van Linschoten</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../til.html">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/strickvl"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://sigmoid.social/web/@alexstrick"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/strickvl"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mlops.systems/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of</h1>
                  <div>
        <div class="description">
          I used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming. For computer vision problems, it’s really useful to have visual debugging aids and FiftyOne is a well-documented and solid tool to help with that.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">redactionmodel</div>
                <div class="quarto-category">computervision</div>
                <div class="quarto-category">tools</div>
                <div class="quarto-category">debugging</div>
                <div class="quarto-category">jupyter</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Strick van Linschoten </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 12, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#fiftyone-basics" id="toc-fiftyone-basics" class="nav-link active" data-scroll-target="#fiftyone-basics">FiftyOne Basics</a></li>
  <li><a href="#comparing-ground-truth-with-predictions" id="toc-comparing-ground-truth-with-predictions" class="nav-link" data-scroll-target="#comparing-ground-truth-with-predictions">Comparing ground truth with predictions</a>
  <ul class="collapse">
  <li><a href="#viewing-only-high-confidence-predictions" id="toc-viewing-only-high-confidence-predictions" class="nav-link" data-scroll-target="#viewing-only-high-confidence-predictions">Viewing only high-confidence predictions</a></li>
  <li><a href="#patches-detailed-views-for-detected-objects" id="toc-patches-detailed-views-for-detected-objects" class="nav-link" data-scroll-target="#patches-detailed-views-for-detected-objects">‘Patches’: detailed views for detected objects</a></li>
  </ul></li>
  <li><a href="#understanding-how-our-model-performs-for-separate-classes" id="toc-understanding-how-our-model-performs-for-separate-classes" class="nav-link" data-scroll-target="#understanding-how-our-model-performs-for-separate-classes">Understanding how our model performs for separate classes</a></li>
  <li><a href="#viewing-the-false-positives-and-false-negatives" id="toc-viewing-the-false-positives-and-false-negatives" class="nav-link" data-scroll-target="#viewing-the-false-positives-and-false-negatives">Viewing the false positives and false negatives</a></li>
  <li><a href="#finding-detection-mistakes-with-fiftyone-brains-mistakenness-calculation" id="toc-finding-detection-mistakes-with-fiftyone-brains-mistakenness-calculation" class="nav-link" data-scroll-target="#finding-detection-mistakes-with-fiftyone-brains-mistakenness-calculation">Finding detection mistakes with FiftyOne Brain’s <code>mistakenness</code> calculation</a></li>
  <li><a href="#finding-missing-annotations" id="toc-finding-missing-annotations" class="nav-link" data-scroll-target="#finding-missing-annotations">Finding missing annotations</a></li>
  <li><a href="#conclusions-and-next-steps" id="toc-conclusions-and-next-steps" class="nav-link" data-scroll-target="#conclusions-and-next-steps">Conclusions and Next Steps</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><em>(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out <a href="https://mlops.systems/#category=redactionmodel">the <code>redactionmodel</code> taglist</a>.)</em></p>
<p>So you’ve trained a computer vision model, but you think it could do better. What do you do next? This is a common scenario, especially for computer vision problems where fine-tuning someone else’s pre-trained model is a pretty normal initial step that gets taken. You emerge with a decent score on whatever metric you care about, but it also isn’t great.</p>
<p>One part of the solution is certainly ‘more data’. This approach was recently highlighted by Boris Dayma on Twitter:</p>
<p>{% twitter https://twitter.com/borisdayma/status/1502317249423679495 %}</p>
<p>In my case, I currently have a little over 1200 images that have been annotated, but of those some 600 of them don’t contain any redactions at all (i.e.&nbsp;they just have content boxes). I did mention that I was using a similar approach early on, where I’d use the model to help pre-annotate images, but I haven’t been using that recently.</p>
<p>I’m realising that more important than pure volume of data is to annotate types of images that are the hardest for the model to learn. So what I really want to know at this point is where I should place my focus when it comes to supplementing the training data. My images aren’t currently divided into separate classes, but I have a proxy (the filename) which will be really helpful once I’ve identified which types I need to supplement.</p>
<p>When seeking to improve computer vision models with error analysis, some kind of visual inspection is essential. <code>fastai</code> had <a href="https://docs.fast.ai/interpret.html">a number of utility methods</a> that helped in the interpretation of where a model was underperforming, but for object detection I think you do need something that was built to purpose, where you can really dive into the specific ways each object was or wasn’t detected.</p>
<p>Enter <a href="https://voxel51.com/docs/fiftyone/">FiftyOne</a>.</p>
<p><a href="https://voxel51.com/docs/fiftyone/">FiftyOne</a> is an open-source tool built specifically to support the curation and creation of datasets for computer vision models. It is almost two years old in its open-source incarnation, and (or but?) it feels very solid and robust in its implementation. <a href="https://voxel51.com">Voxel51</a>, the company behind it, has taken great pains to write excellent documentation and guides, and they have <a href="https://join.slack.com/t/fiftyone-users/shared_invite/zt-gtpmm76o-9AjvzNPBOzevBySKzt02gg">a supportive community</a> behind the scenes, too.</p>
<section id="fiftyone-basics" class="level1">
<h1>FiftyOne Basics</h1>
<p>FiftyOne is a Python library that offers a visual interface to your data. For my redaction model, the base interface looks something like this:</p>
<p><img src="fiftyone-computervision/fiftyone-overview.png" title="The basic view of the FiftyOne app" class="img-fluid"></p>
<p>You need to convert your dataset such that FiftyOne can interpret the structure of where images are stored as well as the annotations themselves, but many commonly-used formats are supported. In my case, COCO annotations are supported out of the box, so it was trivial to import the data to generate the above visualisation.</p>
<p>You can use the FiftyOne application inside a Jupyter Notebook, or you can have it open in a separate tab. A separate tab is my preference as it allows for a larger interface. (There is also a completely separate Desktop app interface you can use, but I think not all functionality works there so you might want to stick to a separate tab).</p>
<p>Luckily for me, my computer vision framework of choice is IceVision, and <a href="https://airctic.com/0.12.0/using_fiftyone_in_icevision/">they recently integrated</a> with FiftyOne which makes creating datasets a breeze.</p>
<p>So how did FiftyOne help me understand how my model was performing? (Note: the sections that follow were significantly helped by following <a href="https://voxel51.com/docs/fiftyone/tutorials/evaluate_detections.html">this</a>, <a href="https://voxel51.com/docs/fiftyone/tutorials/uniqueness.html">this</a> and <a href="https://voxel51.com/docs/fiftyone/tutorials/detection_mistakes.html">this</a> part of the <a href="https://voxel51.com/docs/fiftyone/">FiftyOne docs</a>.)</p>
</section>
<section id="comparing-ground-truth-with-predictions" class="level1">
<h1>Comparing ground truth with predictions</h1>
<p>The first thing I did was visualise the ground truth annotations alongside the predictions of my model. (This is the model mentioned in <a href="https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html">my last blogpost</a>, which had a COCO score of almost 80%.)</p>
<p>This requires performing inference on a slice of our images. Unfortunately, I had to do that inference on my local (CPU) machine because FiftyOne doesn’t work on Paperspace cloud machines on account of port forwarding choices that Paperspace make. This makes for a slightly slower iteration cycle, but once the inference is done you don’t have to do it again.</p>
<p><img src="fiftyone-computervision/groundtruth-predictions.gif" title="Comparing ground truth with predictions" class="img-fluid"></p>
<p>You can see here that it’s possible to selectively turn off and on the various overlaid annotations. If you want to compare how redactions are detected (and not see the content box), then this is an easy way to toggle between.</p>
<section id="viewing-only-high-confidence-predictions" class="level2">
<h2 class="anchored" data-anchor-id="viewing-only-high-confidence-predictions">Viewing only high-confidence predictions</h2>
<p>Not all predictions are created equal, too, so it would be useful to view only those predictions where the confidence was higher than 75%. FiftyOne makes this kind of conditional view easy. You can do it in code, as in the following snippet, or you can do it via the GUI inside the app.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fiftyone <span class="im">import</span> ViewField <span class="im">as</span> F</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Only contains detections with confidence &gt;= 0.75</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># `dataset` is the FiftyOne core object that was created before</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>high_conf_view <span class="op">=</span> dataset.filter_labels(<span class="st">"prediction"</span>, F(<span class="st">"confidence"</span>) <span class="op">&gt;</span> <span class="fl">0.75</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="fiftyone-computervision/highconfidence.png" title="Viewing high-confidence predictions" class="img-fluid"></p>
</section>
<section id="patches-detailed-views-for-detected-objects" class="level2">
<h2 class="anchored" data-anchor-id="patches-detailed-views-for-detected-objects">‘Patches’: detailed views for detected objects</h2>
<p>For a more fine-grained understanding on the ways our model is predicting redactions, we can create what are called ‘patches’ to view and scroll through prediction-by-prediction.</p>
<p><img src="fiftyone-computervision/redaction-patch.png" title="Patch view of some predicted redactions" class="img-fluid"></p>
<p>This is an excellent way to view things through the eyes of your model. These are all the objects it considers to be redactions. We’ll get to finding the ones where it doesn’t do as well in a bit, but this view allows us to immerse ourselves in the reality of how our model is predicting redaction boxes. We can see that certain types of boxes are well-represented in our dataset: coloured or shaded rectangles in particular.</p>
</section>
</section>
<section id="understanding-how-our-model-performs-for-separate-classes" class="level1">
<h1>Understanding how our model performs for separate classes</h1>
<p>We only have two classes in our training data: redaction and content, so doing a class analysis doesn’t help us too much for this problem, but using the mean average precision (MAP) calculation we can see the difference between how well our model does on redactions vs content:</p>
<p><img src="fiftyone-computervision/class-comparison.png" title="A classification report for our dataset" class="img-fluid"></p>
<p>We can also easily plot an interactive chart that quite clearly displays these differences:</p>
<p><img src="fiftyone-computervision/plotting-curves.png" title="Plotting the precision vs recall curves for our two classes" class="img-fluid"></p>
</section>
<section id="viewing-the-false-positives-and-false-negatives" class="level1">
<h1>Viewing the false positives and false negatives</h1>
<p>The previous calculations also added some metadata to each image, denoting whether it was considered a true positive, false positive or false negative. It’s really useful to be able to easily switch between these views, and identifying the images with the largest numbers of false positives and false negatives will help appreciate what our model struggles with.</p>
<p>This view is sorted by total number of false positives in an image. False positives are where the model confidently has predicted something to be a redaction box, for example, that is not actually a redaction box.</p>
<p><img src="fiftyone-computervision/false-positives.png" title="Predicted false positives" class="img-fluid"></p>
<p>In this image you can see that the model predicts a redaction box with 82% confidence that is clearly not a redaction. Note, too, how the smaller redactions to the right and the large partial redaction to the left were not detected.</p>
<p>False negatives are where there were some redactions to be predicted, but our model never made those predictions (or was very unconfident in doing so).</p>
<p><img src="fiftyone-computervision/false-negatives.png" title="False negatives &amp; missing predictions" class="img-fluid"></p>
<p>In this image excerpt, you can see that some predictions were made, but many were also missed. This image shows the ground truth reality of what should have been predicted:</p>
<p><img src="fiftyone-computervision/false-negatives-overlaid.png" title="Overlaying the ground truth, showing many false negatives" class="img-fluid"></p>
<p>Scrolling through the examples with high numbers of false positives and false negatives gives me a really useful indication of which kinds of redactions with which I need to annotate and supplement my training data. I already had a sense of this from my own intuition, but it’s excellent to see this confirmed in the data.</p>
</section>
<section id="finding-detection-mistakes-with-fiftyone-brains-mistakenness-calculation" class="level1">
<h1>Finding detection mistakes with FiftyOne Brain’s <code>mistakenness</code> calculation</h1>
<p>FiftyOne is not only the visual interface, but it also has something called the FiftyOne <a href="https://voxel51.com/docs/fiftyone/user_guide/brain.html"><code>brain</code></a>.</p>
<p>{% include info.html text=“It’s worth being aware of the distinction between the two: FiftyOne itself is open-source and free to use. The brain is closed-source and free for non-commercial uses.” %}</p>
<p>The brain allows you to perform various calculations on your dataset to determine (among other things):</p>
<ul>
<li>visual similarity</li>
<li>uniqueness</li>
<li>mistakenness</li>
<li>hardness</li>
</ul>
<p>(You can also <a href="https://voxel51.com/docs/fiftyone/user_guide/brain.html#brain-embeddings-visualization">visualise embeddings</a> to cluster image or annotation types, but I haven’t used that feature yet so can’t comment as its effectiveness.)</p>
<p>For my dataset, visualising similarity and uniqueness revealed what I already knew: that lots of the images were similar. Knowing the context of the documents well means I’m familiar with how a lot of the documents look the same. Not much of a revelation there.</p>
<p>The mistakenness calculation is useful, however. It compares between the ground truth and the predictions to get a sense of which images it believes contains annotations that might be wrong. I can filter these such that we only show images where it is more than 80% confident mistakes have been made. Instantly it reveals a few examples where there have been annotation mistakes. To take one example, here you can see the ground truth annotations:</p>
<p><img src="fiftyone-computervision/groundtruth-mistake.png" title="Ground truth mistakes" class="img-fluid"></p>
<p>And here you can see what was predicted:</p>
<p><img src="fiftyone-computervision/mistake-what-was-predicted.png" title="What was predicted, revealing mistakes in the ground truth annotations" class="img-fluid"></p>
<p>In this example, it was even clear from the beginning that redactions had been missed, and that the single annotation that had been made (a content box) was incorrect.</p>
</section>
<section id="finding-missing-annotations" class="level1">
<h1>Finding missing annotations</h1>
<p>We can also view images that the FiftyOne brain tagged as containing missing annotations:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>session.view <span class="op">=</span> dataset.match(F(<span class="st">"possible_missing"</span>) <span class="op">&gt;</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="fiftyone-computervision/missing-annotations.png" title="Suggestions for images containing annotations that were missing" class="img-fluid"></p>
<p>Unfortunately the <a href="https://voxel51.com/docs/fiftyone/user_guide/brain.html#brain-sample-hardness"><code>compute_hardness</code></a> method only works for classification models currently, but regardless I think we have a lot to work with already.</p>
</section>
<section id="conclusions-and-next-steps" class="level1">
<h1>Conclusions and Next Steps</h1>
<p>I hope this practical introduction to FiftyOne has given you a high-level overview of the ways the tool can be useful in evaluating your computer vision models.</p>
<p>For my redaction project, I’m taking some clear action steps I need to work on as a result of some of this analysis.</p>
<ul>
<li>I need do annotate more of the kinds of images it struggles with. Specifically, this means images containing redactions that are just white boxes, with a bonus for those white redaction boxes being superimposed on top of a page filed with white boxes (i.e.&nbsp;some sort of table or form).</li>
<li>I need to remove some of the bad/false ground truth annotations that the FiftyOne brain helpfully identified.</li>
<li>I will probably want to repeat this process together in a model that was trained together with the synthetic data to see what differences can be observed.</li>
<li>As a general point, I probably want to incorporate visual inspection of the data at various points in the training pipeline, not just after the model has been trained.</li>
</ul>
<p>If you know any other tools that help with this kind of visual analysis of model performance and how to improve in a data-driven approach, please do <a href="https://twitter.com/strickvl">let me know</a>!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>