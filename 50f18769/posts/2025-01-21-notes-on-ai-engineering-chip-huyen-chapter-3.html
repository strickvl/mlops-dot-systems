<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alex Strick van Linschoten">
<meta name="dcterms.date" content="2025-01-21">

<title>Alex Strick van Linschoten - Notes on ‘AI Engineering’ (Chip Huyen) chapter 3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script defer="" data-domain="mlops.systems" src="https://plausible.io/js/script.js"></script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Alex Strick van Linschoten - Notes on ‘AI Engineering’ (Chip Huyen) chapter 3">
<meta property="og:description" content="Really enjoyed this chapter. My tidied notes from my readings follow below. 150 pages in and we’re starting to get to the good stuff :)">
<meta property="og:image" content="https://mlops.systems/posts/images/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3/ch3.png">
<meta property="og:site-name" content="Alex Strick van Linschoten">
<meta property="og:image:height" content="646">
<meta property="og:image:width" content="1408">
<meta name="twitter:title" content="Alex Strick van Linschoten - Notes on ‘AI Engineering’ (Chip Huyen) chapter 3">
<meta name="twitter:description" content="Really enjoyed this chapter. My tidied notes from my readings follow below. 150 pages in and we’re starting to get to the good stuff :)">
<meta name="twitter:image" content="https://mlops.systems/posts/images/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3/ch3.png">
<meta name="twitter:creator" content="@strickvl">
<meta name="twitter:image-height" content="646">
<meta name="twitter:image-width" content="1408">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Alex Strick van Linschoten</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../til.html">
 <span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/strickvl"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://sigmoid.social/web/@alexstrick"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/strickvl"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mlops.systems/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Notes on ‘AI Engineering’ (Chip Huyen) chapter 3</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">books-i-read</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">llms</div>
                <div class="quarto-category">evaluation</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alex Strick van Linschoten </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 21, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview-and-context" id="toc-overview-and-context" class="nav-link active" data-scroll-target="#overview-and-context">Overview and Context</a></li>
  <li><a href="#challenges-in-evaluating-foundation-models" id="toc-challenges-in-evaluating-foundation-models" class="nav-link" data-scroll-target="#challenges-in-evaluating-foundation-models">Challenges in Evaluating Foundation Models</a></li>
  <li><a href="#language-model-metrics" id="toc-language-model-metrics" class="nav-link" data-scroll-target="#language-model-metrics">Language Model Metrics</a></li>
  <li><a href="#downstream-task-performance-measurement" id="toc-downstream-task-performance-measurement" class="nav-link" data-scroll-target="#downstream-task-performance-measurement">Downstream Task Performance Measurement</a>
  <ul class="collapse">
  <li><a href="#exact-evaluation" id="toc-exact-evaluation" class="nav-link" data-scroll-target="#exact-evaluation">Exact Evaluation</a></li>
  </ul></li>
  <li><a href="#ai-as-judge" id="toc-ai-as-judge" class="nav-link" data-scroll-target="#ai-as-judge">AI as Judge</a>
  <ul class="collapse">
  <li><a href="#benefits" id="toc-benefits" class="nav-link" data-scroll-target="#benefits">Benefits</a></li>
  <li><a href="#three-main-approaches" id="toc-three-main-approaches" class="nav-link" data-scroll-target="#three-main-approaches">Three Main Approaches</a></li>
  <li><a href="#implementation-considerations" id="toc-implementation-considerations" class="nav-link" data-scroll-target="#implementation-considerations">Implementation Considerations</a></li>
  <li><a href="#limitations-and-challenges" id="toc-limitations-and-challenges" class="nav-link" data-scroll-target="#limitations-and-challenges">Limitations and Challenges</a></li>
  </ul></li>
  <li><a href="#specialized-judges" id="toc-specialized-judges" class="nav-link" data-scroll-target="#specialized-judges">Specialized Judges</a></li>
  <li><a href="#comparative-evaluation-for-model-ranking" id="toc-comparative-evaluation-for-model-ranking" class="nav-link" data-scroll-target="#comparative-evaluation-for-model-ranking">Comparative Evaluation for Model Ranking</a>
  <ul class="collapse">
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a></li>
  <li><a href="#advantages" id="toc-advantages" class="nav-link" data-scroll-target="#advantages">Advantages</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges">Challenges</a></li>
  </ul></li>
  <li><a href="#key-takeaways-and-future-implications" id="toc-key-takeaways-and-future-implications" class="nav-link" data-scroll-target="#key-takeaways-and-future-implications">Key Takeaways and Future Implications</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Really enjoyed this chapter. My tidied notes from my readings follow below. 150 pages in and we’re starting to get to the good stuff :)</p>
<section id="overview-and-context" class="level2">
<h2 class="anchored" data-anchor-id="overview-and-context">Overview and Context</h2>
<p>This chapter serves as the first of two chapters (Chapters 3 and 4) dealing with evaluation in AI Engineering. While Chapter 4 will delve into evaluation within systems, Chapter 3 addresses the fundamental question of how to evaluate open-ended responses from foundation models and LLMs at a high level. The importance of evaluation cannot be overstated, though the author perhaps takes this somewhat for granted. The chapter provides a comprehensive framework for understanding various evaluation methodologies and their applications.</p>
</section>
<section id="challenges-in-evaluating-foundation-models" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-evaluating-foundation-models">Challenges in Evaluating Foundation Models</h2>
<p>The evaluation of foundation models presents several unique and complex challenges that make systematic assessment difficult:</p>
<ul>
<li>Existing benchmarks become increasingly inadequate as models improve in their capabilities</li>
<li>As models become better at writing and mimicking human-like responses, evaluation becomes more complex and nuanced</li>
<li>Many foundation models are API-driven black boxes, limiting access to internal workings</li>
<li>Models continuously develop new capabilities, requiring constant adaptation of evaluation methods</li>
<li>There has been notably limited investment in evaluation studies and technologies compared to the extensive resources devoted to enhancing model capabilities</li>
<li>The improvement in model performance necessitates the continuous development of new benchmarks</li>
<li>Without a systematic approach to evaluation, progress can be hindered by various headwinds</li>
</ul>
</section>
<section id="language-model-metrics" class="level2">
<h2 class="anchored" data-anchor-id="language-model-metrics">Language Model Metrics</h2>
<p>The chapter includes a technically detailed section on understanding language model metrics, which while math-heavy, provides fundamental insights into model capabilities:</p>
<ul>
<li>Entropy</li>
<li>Cross-entropy</li>
<li>Perplexity</li>
</ul>
<p>These metrics serve as underlying measures to understand what’s happening within the models and assess their power and conversational abilities. While this section spans 4-5 pages of technical content, it provides some useful foundational understanding of how we can measure a language model’s intrinsic capabilities.</p>
</section>
<section id="downstream-task-performance-measurement" class="level2">
<h2 class="anchored" data-anchor-id="downstream-task-performance-measurement">Downstream Task Performance Measurement</h2>
<p>The chapter transitions from intrinsic metrics to evaluating actual capabilities, dividing evaluation into exact and subjective approaches.</p>
<section id="exact-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="exact-evaluation">Exact Evaluation</h3>
<p>There are two principal approaches to exact evaluation:</p>
<ol type="1">
<li><p><strong>Functional Correctness Assessment</strong></p>
<ul>
<li>Evaluates whether the LLM can successfully complete its assigned tasks</li>
<li>Focuses on practical capability rather than theoretical metrics</li>
<li>Example: In coding tasks, checking if generated code passes all unit tests</li>
<li>Provides clear, objective measures of success</li>
</ul></li>
<li><p><strong>Similarity Measurements Against Reference Data</strong> Four distinct methods are identified:</p>
<ol type="a">
<li><strong>Human Evaluator Judgment</strong>
<ul>
<li>Requires manual comparison of texts by human evaluators</li>
<li>Highly accurate but time and resource-intensive</li>
<li>Limited scalability due to human involvement</li>
<li>Often considered the gold standard despite limitations</li>
</ul></li>
<li><strong>Exact Match Checking</strong>
<ul>
<li>Compares generated response against reference responses for exact matches</li>
<li>Most effective with shorter, specific outputs</li>
<li>Less useful for verbose or creative outputs</li>
<li>Provides binary yes/no results</li>
</ul></li>
<li><strong>Lexical Similarity</strong>
<ul>
<li>Employs established metrics like BLEU, ROUGE, and METEOR</li>
<li>Focuses on word overlap and structural similarities</li>
<li>Known to be somewhat crude in their assessment</li>
<li>Widely used despite limitations due to ease of implementation</li>
</ul></li>
<li><strong>Semantic Similarity</strong>
<ul>
<li>Utilizes embeddings for comparing textual meaning</li>
<li>Less sensitive to specific word choices than lexical approaches</li>
<li>Quality depends entirely on the underlying embeddings algorithm</li>
<li>May require significant computational resources</li>
<li>Generally provides more nuanced comparison than lexical methods</li>
</ul></li>
</ol></li>
</ol>
<p>The chapter includes a brief but relevant sidebar on embeddings and their significance in evaluation, though this digression seemed a bit out of place in the overall flow.</p>
</section>
</section>
<section id="ai-as-judge" class="level2">
<h2 class="anchored" data-anchor-id="ai-as-judge">AI as Judge</h2>
<p>This section explores the increasingly popular approach of using AI systems to evaluate other AI systems.</p>
<section id="benefits" class="level3">
<h3 class="anchored" data-anchor-id="benefits">Benefits</h3>
<ul>
<li>Significantly faster than human evaluation processes</li>
<li>Generally more cost-effective than human evaluation at scale</li>
<li>Studies have shown strong correlation with human evaluations in many cases</li>
<li>AI judges can provide detailed explanations for their decisions</li>
<li>Offers greater flexibility in evaluation approaches</li>
<li>Enables systematic and consistent evaluation at scale</li>
</ul>
</section>
<section id="three-main-approaches" class="level3">
<h3 class="anchored" data-anchor-id="three-main-approaches">Three Main Approaches</h3>
<ol type="1">
<li><strong>Individual Response Evaluation</strong>
<ul>
<li>Assesses response quality based solely on the original question</li>
<li>Often implements numerical scoring systems (e.g., 1-5 scale)</li>
<li>Evaluates responses in isolation without comparison</li>
</ul></li>
<li><strong>Reference Response Comparison</strong>
<ul>
<li>Evaluates generated response against established reference responses</li>
<li>Usually produces binary (true/false) outcomes</li>
<li>Helps ensure responses meet specific criteria</li>
</ul></li>
<li><strong>Generated Response Comparison</strong>
<ul>
<li>Compares two generated responses to determine relative quality</li>
<li>Predicts likely user preferences between options</li>
<li>Particularly useful for:
<ul>
<li>Post-training alignment</li>
<li>Test-time compute optimization</li>
<li>Model ranking through comparative evaluation</li>
<li>Generating preference data</li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="implementation-considerations" class="level3">
<h3 class="anchored" data-anchor-id="implementation-considerations">Implementation Considerations</h3>
<p><img src="images/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3/ch3.png" class="img-fluid"></p>
<ul>
<li>Table 3-3 (page 139) provides an overview of different AI judge criteria used by various AI tools</li>
<li>Notable lack of standardization across different platforms and approaches (see above)</li>
<li>Various scoring systems available, each with their own trade-offs</li>
<li>Adding examples to prompts can improve accuracy but increases token count and costs</li>
<li>Careful balance needed between evaluation quality and resource consumption</li>
</ul>
</section>
<section id="limitations-and-challenges" class="level3">
<h3 class="anchored" data-anchor-id="limitations-and-challenges">Limitations and Challenges</h3>
<ul>
<li>AI judges can show inconsistency in their judgments</li>
<li>Costs can escalate quickly, especially when using stronger models or including more context</li>
<li>Evaluation criteria often remain ambiguous and difficult to standardize</li>
<li>Several inherent biases identified:
<ul>
<li>Self-bias: Models tend to favor responses generated by themselves</li>
<li>Verbosity bias: Tendency to favor longer, more detailed answers</li>
<li>Other biases common to AI applications in general</li>
</ul></li>
</ul>
</section>
</section>
<section id="specialized-judges" class="level2">
<h2 class="anchored" data-anchor-id="specialized-judges">Specialized Judges</h2>
<p>This section presents an innovative challenge to the conventional wisdom of using the strongest available model as a judge. The author introduces a compelling alternative approach:</p>
<ul>
<li>Small, specialized judges can be as effective as larger models for specific evaluation tasks</li>
<li>More cost-effective and efficient than using large language models</li>
<li>Can be trained for highly specific evaluation criteria</li>
<li>Demonstrates comparable performance to larger models like GPT-4 in specific domains</li>
</ul>
<p>Three types of specialized judges are identified: 1. Reward models (evaluating prompt-response pairs) 2. Reference-based judges 3. Preference models</p>
<p>This represents a novel approach that could significantly impact evaluation methodology in the field.</p>
</section>
<section id="comparative-evaluation-for-model-ranking" class="level2">
<h2 class="anchored" data-anchor-id="comparative-evaluation-for-model-ranking">Comparative Evaluation for Model Ranking</h2>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology</h3>
<ul>
<li>Focuses on binary choices between two samples</li>
<li>Simpler for both humans and AI to make comparative judgments</li>
<li>Used in major leaderboards like LMSIS</li>
<li>Requires evaluation of multiple combinations to establish rankings</li>
<li>Various algorithms available for efficient comparison</li>
</ul>
</section>
<section id="advantages" class="level3">
<h3 class="anchored" data-anchor-id="advantages">Advantages</h3>
<ul>
<li>More intuitive evaluation process</li>
<li>Often more reliable than absolute scoring</li>
<li>Reduces cognitive load on evaluators</li>
<li>Provides clear preference data</li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li>Highly data-intensive nature affects scalability</li>
<li>Lacks standardization across implementations</li>
<li>Difficulty in converting comparative measures to absolute metrics</li>
<li>Quality control remains a significant concern</li>
<li>Number of required comparisons can grow rapidly with model count</li>
</ul>
</section>
</section>
<section id="key-takeaways-and-future-implications" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways-and-future-implications">Key Takeaways and Future Implications</h2>
<ol type="1">
<li>The emergence of smaller, specialized judge models represents a significant shift from the traditional approach of using the largest available models</li>
<li>Comparative evaluation offers promising approaches but requires careful consideration of scalability and implementation</li>
<li>The field continues to evolve rapidly, requiring flexible and adaptable evaluation strategies</li>
<li>Sets up crucial discussion for system-level evaluation in Chapter 4</li>
<li>Highlights the ongoing tension between evaluation quality and resource efficiency</li>
</ol>
<p>The chapter effectively establishes the foundational understanding necessary for the more practical, system-focused evaluation discussions to follow in Chapter 4.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="strickvl/mlops-dot-systems" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>