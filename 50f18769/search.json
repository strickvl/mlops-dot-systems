[
  {
    "objectID": "posts/2021-11-27-safety-vulnerability-checker.html",
    "href": "posts/2021-11-27-safety-vulnerability-checker.html",
    "title": "Check your security vulnerabilities with safety",
    "section": "",
    "text": "safety is a tiny tool that checks your package’s dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check.\nIt checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can subscribe via pyup.\nWith that caveat, it’s not perfect, but it’s better than nothing. An easy CI win for open-source projects.\n[I first learned of this tool here. Many thanks to calmcode for continuing to make these really useful videos.]"
  },
  {
    "objectID": "posts/2022-10-24-foundations-mnist-basics.html",
    "href": "posts/2022-10-24-foundations-mnist-basics.html",
    "title": "From the foundation up: Fashion-MNIST basics from Lesson 10",
    "section": "",
    "text": "In lesson 10 of the FastAI course, we began by examining some new research that had just been released (the iMagic paper) along with one of the key insights (‘progressive distillation’) of the paper. I’ll maybe return to that in a separate blogpost but I wanted to focus on the second half of the lesson where we start the work of actually building up our library of tools to replicate all the pieces of Stable Diffusion.\nTo make things more complicated, we’re allowed to use only a basic set of building blocks (at least in the first iteration) so as to really understand how things are working. This includes:\n\nPython\nThe Python standard library\nmatplotlib\nJupyter notebooks and nbdev\n\nThis notebook will showcase some of my experimentation and learnings around the things we went through in this lesson. I’ve also compiled a table showing the various pieces we got to in the lesson, pairing the specific task we were trying to do along with the associated skill or part of the Python standard library.\n\n\n\n\nTask\nSkill / Library\nDocs link\n\n\n\n\n1\nDownload Fashion MNIST\nurllib.requests.urlretrieve\nlink\n\n\n2\nUnzip & separate out the parts\ngzip\nlink\n\n\n\n\npickle\nlink\n\n\n\n\ndestructuring\n\n\n\n3\nTurn the list into a matrix\nchunking into a list of lists\n\n\n\n\n\nyield / next (generators)\nlink\n\n\n4\nPlot out the image using matplotlib\nmatplotlib\nlink\n\n\n5\nDefine a matrix class\n__init__\nlink\n\n\n\n\n__getitem__\nlink\n\n\n6\nAccess tensor values\nPytorch Tensor\nlink\n\n\n\n\nmap\nlink\n\n\n\n\ntensor().reshape()\nlink\n\n\n7\nFind the shape, min and max values\ntensor().shape\n\n\n\n\n\ntensor().min()\nlink\n\n\n\n\ntensor().max()\nlink\n\n\n8\nGenerate some random numbers\nrandom.random\nlink\n\n\n9\nProfile your code\n%timeit\nlink1 / link2\n\n\n\nAt the bottom of the post, I’ll also include my collation of the “Things Jeremy says to do” which has a bit of a precedent on the fastai forums so I thought I’d share them as well.\n\nDownloading MNIST\nWe start by downloading our dataset. I’m going to replicate the work we do in class while using Zalando’s alternative version of MNIST, entitled Fashion-MNIST. I previously used it on this blog (here, for example, but also elsewhere) while working through part 1 of the course, albeit at a much higher level using PyTorch to do similar things that we’ll be doing this time round. It’s comparable but also different, so I’m looking forward to working through whatever challenges are raised.\nThe URLs are specified on the Github Repo, so I’ll include those as constants here first:\n\nBASE_URL = \"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/\"\n\nTRAINING_IMAGES = BASE_URL + \"train-images-idx3-ubyte.gz\"\nTRAINING_IMAGES_LABELS = BASE_URL + \"train-labels-idx1-ubyte.gz\"\nTEST_IMAGES = BASE_URL + \"t10k-images-idx3-ubyte.gz\"\nTEST_IMAGES_LABELS = BASE_URL + \"t10k-labels-idx1-ubyte.gz\"\n\nWe now want a place where we can save them, so we’ll use pathlib to create a data directory in our current working directory, but only if it doesn’t already exist. We also set up the final locations of the data for when we download it:\n\n# create local path directory if it doesn't exist\nfrom pathlib import Path\n\nLOCAL_PATH = Path(\"data\")\n# check whether the path exists; create it if it doesn't exist\nif not LOCAL_PATH.exists():\n    LOCAL_PATH.mkdir(exist_ok=True)\n    \ntraining_images_path = LOCAL_PATH / \"train-images-idx3-ubyte.gz\"\ntraining_images_labels_path = LOCAL_PATH / \"train-labels-idx1-ubyte.gz\"\ntest_images_path = LOCAL_PATH / \"t10k-images-idx3-ubyte.gz\"\ntest_images_labels_path = LOCAL_PATH / \"t10k-labels-idx1-ubyte.gz\"\n\nThere’s probably a more elegant way to do this where we loop over a list, but the way I set up the URLs and the filenames didn’t lend itself to that approach so here I just download them one by one, but only if they don’t already exist.\n\n# download the raw data if it doesn't exist\nfrom urllib.request import urlretrieve\n\nif not training_images_path.exists():\n    urlretrieve(TRAINING_IMAGES, training_images_path)\nif not training_images_labels_path.exists():\n    urlretrieve(TRAINING_IMAGES_LABELS, training_images_labels_path)\nif not test_images_path.exists():\n    urlretrieve(TEST_IMAGES, test_images_path)\nif not test_images_labels_path.exists():\n    urlretrieve(TEST_IMAGES_LABELS, test_images_labels_path)\n\nHere you can see that we have downloaded four files as expected:\n\n!ls -l data\n\ntotal 60328\n-rw-r--r--  1 strickvl  staff   4422102 22 Oct 16:12 t10k-images-idx3-ubyte.gz\n-rw-r--r--  1 strickvl  staff      5148 22 Oct 16:12 t10k-labels-idx1-ubyte.gz\n-rw-r--r--  1 strickvl  staff  26421880 22 Oct 16:12 train-images-idx3-ubyte.gz\n-rw-r--r--  1 strickvl  staff     29515 22 Oct 16:12 train-labels-idx1-ubyte.gz\n\n\n\n\nUnzip the files and separate out the data\nIn the lesson, Jeremy downloads a pre-made MNIST file which contains a tuple of tuples. With Fashion-MNIST, we have four separate files representing our training and test data and our labels for each. Now our task is to unzip those files and grab the data as arrays.\nNote that there are convenience functions provided for both MNIST and Fashion-MNIST to do much of what we do below, since most people want the data as numpy arrays, not Python arrays.\n\n# import relevant standard library packages\nimport gzip\n\nWe have one elephant in the room which is this parse_idx function that I’m showing below in a collapsed cell. This was one part that I didn’t want to get too lost in so I simply went to the source code of one of these helper functions, specifically that written by datapythonista for MNIST.\nThis still all is standard library imports so I think technically I’m ok including this code here since the MNIST download Jeremy used in the lesson comes pre-parsed, it seems. After inspecting the data in the files, it seems that we have a series of bytes that need to be parsed somehow if they’re going to be used as an array. The original function returned a numpy array, but I modified it slightly such that it returns a nested array instead. I consider this not a very interesting part of the process so I’m hiding it away.\n\n#collapse-hide\n# taken from https://github.com/datapythonista/mnist/blob/master/mnist/__init__.py\nimport os\nimport functools\nimport operator\nimport struct\nimport array\nimport tempfile\n\ndef parse_idx(fd):\n    \"\"\"Parse an IDX file, and return it as an array of arrays.\n    \n    Parameters\n    ----------\n    fd : file\n        File descriptor of the IDX file to parse\n    endian : str\n        Byte order of the IDX file. See [1] for available options\n    Returns\n    -------\n    data : array\n        Numpy array with the dimensions and the data in the IDX file\n    1. https://docs.python.org/3/library/struct.html\n        #byte-order-size-and-alignment\n    \"\"\"\n    DATA_TYPES = {0x08: 'B',  # unsigned byte\n                  0x09: 'b',  # signed byte\n                  0x0b: 'h',  # short (2 bytes)\n                  0x0c: 'i',  # int (4 bytes)\n                  0x0d: 'f',  # float (4 bytes)\n                  0x0e: 'd'}  # double (8 bytes)\n\n    header = fd.read(4)\n    if len(header) != 4:\n        raise IdxDecodeError('Invalid IDX file, '\n                             'file empty or does not contain a full header.')\n\n    zeros, data_type, num_dimensions = struct.unpack('>HBB', header)\n\n    if zeros != 0:\n        raise IdxDecodeError('Invalid IDX file, '\n                             'file must start with two zero bytes. '\n                             'Found 0x%02x' % zeros)\n\n    try:\n        data_type = DATA_TYPES[data_type]\n    except KeyError:\n        raise IdxDecodeError('Unknown data type '\n                             '0x%02x in IDX file' % data_type)\n\n    dimension_sizes = struct.unpack('>' + 'I' * num_dimensions,\n                                    fd.read(4 * num_dimensions))\n\n    data = array.array(data_type, fd.read())\n    data.byteswap()  # looks like array.array reads data as little endian\n\n    expected_items = functools.reduce(operator.mul, dimension_sizes)\n    if len(data) != expected_items:\n        raise IdxDecodeError('IDX file has wrong number of items. '\n                             'Expected: %d. Found: %d' % (expected_items,\n                                                          len(data)))\n    return data\n\nThere’s one final piece of the process which comes from the fact that the array (i.e. the data local variable inside the parse_idx function) that gets returned is a single array (i.e. all the pixel values for all 60,000 images just listed one after another). We’ll want to split these up into separate chunks:\n\nsplitting each image into a separate array of 784 values (because our images are 28x28 size)\nthen splitting each image into an array of arrays giving us our 28x28 matrix.\n\nI’ll include the code that we use for this in the class, a chunks function, but I’ll return to how it works once everything is loaded.\n\n# chunk things together\ndef chunks(x, size):\n    for i in range(0, len(x), size): \n        yield x[i:i + size]\n\n\n\nLoad the data into memory\nWe use the chunks function here to turn our array of 47,040,000 values into matrices. To start with, something like x_train will have 60,000 separate arrays of 784 pixels, as we can see when we get the lengths of the initial array and then the length of the first item.\n\n# unzip the files and extract the images \nwith gzip.open(training_images_path, 'rb') as f:\n    pixels = list(parse_idx(f))\n    x_train = list(chunks(pixels, 784))\n\nwith gzip.open(training_images_labels_path, 'rb') as f:\n    y_train = list(parse_idx(f))\n    \nwith gzip.open(test_images_path, 'rb') as f:\n    pixels = list(parse_idx(f))\n    x_valid = list(chunks(pixels, 784))\n    \nwith gzip.open(test_images_labels_path, 'rb') as f:\n    y_valid = list(parse_idx(f))\n\n\nlen(x_train), len(x_train[0])\n\n(60000, 784)\n\n\nThe labels are in a slightly different format, containing single integer values indicating which item the image refers to. We can check and show that we have 60,000 of those values, as we’d expect, and we can even see what values those refer to with a simple conversion:\n\nlen(y_train)\n\n60000\n\n\n\ny_train[0:10]\n\n[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]\n\n\n\n# this list is taken from the README of the Fashion-MNIST repository\n# https://github.com/zalandoresearch/fashion-mnist\nindex_to_label = {\n    0: \"T-shirt/top\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle boot\"\n}\n\nlist(map(lambda x: index_to_label[x], y_train[0:10]))\n\n['Ankle boot',\n 'T-shirt/top',\n 'T-shirt/top',\n 'Dress',\n 'T-shirt/top',\n 'Pullover',\n 'Sneaker',\n 'Pullover',\n 'Sandal',\n 'Sandal']\n\n\n\n\nPlot some examples using matplotlib\nWe can plot out one or two of our images to confirm that we’re seeing the right images and that everything’s as expected:\n\nimport matplotlib as mpl, matplotlib.pyplot as plt\n\nThis is the first image, which should be an ankle boot as per the labels above.\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x_train[0], 28)));\n\n\n\n\nThis is the fourth image, which should be a dress as per the labels above:\n\nmpl.rcParams['image.cmap'] = 'gray'\nplt.imshow(list(chunks(x_train[3], 28)));\n\n\n\n\nEverything looks good!\n\n\nChunking with iterators and generators\nI promised I’d return to the chunks function defined above. There’s a way to define it in a simpler fashion without using generators that goes something like this:\n\ndef chunks(x, size):\n    result = []\n    for i in range(0, len(x), size):\n        subarray = x[i:i + size]\n        result.append(subarray)\n    return result\n\nFunctionally this does almost the same thing as what we have above, but this simpler version is less ideal because we’re iteratively building up our array in memory on the fly.\nThe advantage of using a generator (which we create when we use yield) is that it only grabs the number of items we need at once, so it’s far more memory efficient. (In other words, instead of grabbing all 60,000 images at once, we only make one image (i.e. 784 items / pixels) available at any one time.) With our generator, we can see the next set of items by calling next until we reach a StopIterationError.\nIn our implementation, we don’t get so many of those benefits since we’re still going to store everything in memory anyway. We wrap all our chunk generator functions in list() which causes the interpreter to unfurl or unwrap our chunks into an array of arrays.\n\n\nDefine a matrix class\nIn standard Python if we want to index into one of our array image we’d normally have to do something like image[0][15] whereas in machine learning we generally prefer to do something like image[0, 15]. So our next step in manually implementing these things would be to create a Matrix class that allows us to do this.\n\nclass Matrix:\n    def __init__(self, vals):\n        self.vals = vals\n        \n    def __getitem__(self, vals):\n        return self.vals[vals[0]][vals[1]]\n\n\nimg = list(chunks(x_train[3], 28))\nMatrix(img)[0, 10], img[0][10]\n\n(175, 175)\n\n\nNow we have a function that does what we want. You can see we used the built-in __getitem__ function which we over-wrote. This is what Python uses under the hood when you use square brackets to get values from an array, so by overwriting it we can achieve the behaviour we’re looking for.\nIn reality, this implementation probably leaves a bit to be desired, so now that we’ve implemented it from scratch, we can discard our own version and just use a tensor object from PyTorch:\n\nimport torch\nfrom torch import tensor\n\n\nt = tensor(list(chunks(x_train[3], 28)))\nt.shape\n\ntorch.Size([28, 28])\n\n\nNow that we know it works, we can map all our values and turn them into tensor objects.\n\nx_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\nx_train.shape\n\ntorch.Size([60000, 784])\n\n\nAs mentioned above, using tensors comes with a bunch of extra benefits.\n\n\nFind the shape, min and max values\nOur x_train is in the shape [60000, 784] and our y_train is just 60,000 individual values. So we still need to chunk our images into 28x28 matrices.\n\ny_train.shape\n\ntorch.Size([60000])\n\n\nFor the chunking process, instead of our own function we can use PyTorch’s reshape method. Note that we pass in the desired new shape, so in our case that could be (60000,28,28). It is more common, however, to use and see the number -1 for the first value, which implies that PyTorch should figure out what the value for that first dimension should be. In our case, we have 60,000 items so that’s how big it should be and these two options are both equivalent.\n\n# train_imgs = x_train.reshape((60000,28,28))\ntrain_imgs = x_train.reshape((-1,28,28))\ntrain_imgs.shape\n\ntorch.Size([60000, 28, 28])\n\n\nWe can find out the minimum and maximum value found inside one of our images by using Python’s min, but this is an inbuilt method on the tensor object as well so we can use that now:\n\ntrain_imgs[0].min(), train_imgs[0].max()\n\n(tensor(0), tensor(255))\n\n\nAs expected, our pixel values represent the 256 shades of grey available. Our labels represent the 10 possible clothing types listed above, so we’d expect our minimum and maximum to be zero and nine respectively:\n\ny_train.min(), y_train.max()\n\n(tensor(0), tensor(9))\n\n\n\n\nGenerate some random numbers\nWe finished out the lesson with a brief discussion of random number generators, how to implement them and how this might be important for deep learning work. As we learned, there is no way for computers to generate truly random numbers. People go to interesting lengths to create their own random numbers, such as the infamous lava lamp wall at Cloudflare, but we are left with being able to create only pseudo-random numbers.\nIn the class we see a way based on the Wichmann Hill algorithm used before Python 2.3, which goes as follows:\n\nrnd_state = None\ndef seed(a):\n    global rnd_state\n    a, x = divmod(a, 30268)\n    a, y = divmod(a, 30306)\n    a, z = divmod(a, 30322)\n    rnd_state = int(x)+1, int(y)+1, int(z)+1\n\ndef rand():\n    global rnd_state\n    x, y, z = rnd_state\n    x = (171 * x) % 30269\n    y = (172 * y) % 30307\n    z = (170 * z) % 30323\n    rnd_state = x,y,z\n    return (x/30269 + y/30307 + z/30323) % 1.0\n\nWe have two functions. It’s important to note that the seed is an important part of these pseudo-random number generators. The seed gives us a number that our generator starts with. Note also how we use global state to store the rnd_state variable, and how we are continually updating our values in that state as we generate. Also note that the process of generating the values is fairly simple, but also eminently reproducible.\n\nseed(2349873456787298) # some number I came up with by mashing my keyboard numbers\nrnd_state\n\n(24775, 23862, 14675)\n\n\n\nrand(),rand(),rand()\n\n(0.6580068826155674, 0.669616116835976, 0.9249613879702964)\n\n\nWe now have a function which generates pseudo-random numbers, but as we’ll see, it’s not the fastest bit of code around…\n\n\nProfile our code\nPython has a handy timeit module which allows you to calculate how long your code takes to run. What’s more, it’ll run your code hundreds or thousands of times to get average run times. Here we can see how fast our pseudo-random number generator takes to run, on average:\n\n%timeit -n 10 list(chunks([rand() for _ in range(7840)], 10))\n\n4.05 ms ± 500 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nIf we compare it against PyTorch’s random number generator, we can see a distinct difference, however:\n\n%timeit -n 10 torch.randn(784,10)\n\n56.1 µs ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nNow we’re talking about microseconds vs miliseconds, which is a big difference, so we should probably stick to how PyTorch does it.\n\ntorch.randn??\n\n\nDocstring:\nrandn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\nReturns a tensor filled with random numbers from a normal distribution\nwith mean `0` and variance `1` (also called the standard normal\ndistribution).\n.. math::\n    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\nThe shape of the tensor is defined by the variable argument :attr:`size`.\nArgs:\n    size (int...): a sequence of integers defining the shape of the output tensor.\n        Can be a variable number of arguments or a collection like a list or tuple.\nKeyword args:\n    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n    out (Tensor, optional): the output tensor.\n    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n        Default: ``torch.strided``.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if ``None``, uses the current device for the default tensor type\n        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n        for CPU tensor types and the current CUDA device for CUDA tensor types.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\nExample::\n    >>> torch.randn(4)\n    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n    >>> torch.randn(2, 3)\n    tensor([[ 1.5954,  2.8929, -1.0923],\n            [ 1.1719, -0.4709, -0.1996]])\nType:      builtin_function_or_method\n\n\n\n\nPyTorch has an interesting document entitled ‘Reproducibility’ in which they state the things that you can try to do to make your code as non-deterministic (i.e. non-random) as possible. This is important if you want to be able to reproduce your work. At the top, however, and as I learned during the Sunday Delft FastAI Study Group discussion, it’s not as simple as setting seed values; sometimes even the hardware used needs to be identical in order to achieve reproducibility:\n\n“Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.”\n\nBut why do we care about random values in deep learning to start with? We care because we are often doing things like augmentations in parallel. Augmentations are randomly generated, so if all the parallel processes use the same random number sequences then all the processes will generate the same augmented images.\nWe saw the nice example in the lesson of how our function runs into exactly this problem:\n\nif os.fork():\n    print(f'In parent: {rand()}')\nelse:\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.17354408428968426\nIn child: 0.17354408428968426\n\n\nWe create two forked processes, which are identical copies of each other. Then we call our pseudo-random number generator. You would expect that these two values would be different, but because we’ve copied the entire state then we get identical values. What we’d need to do is to set a new seed at the beginning of each process.\n\nif os.fork():\n    seed(0)\n    print(f'In parent: {rand()}')\nelse:\n    seed(1)\n    print(f'In child: {rand()}')\n    os._exit(os.EX_OK)\n\nIn parent: 0.01693090619965683\nIn child: 0.02258025041320865\n\n\n\n\n“Things Jeremy says to do”\nI thought I’d gather some of the core ‘things Jeremy says to do’ comments from the video lecture for this part of the lecture.\n\nshow the function signature with shift-Tab\nuse ? and ?? at the end of a method or function to show the docs and the source code respectively\nread all the docs for every Python function you use\n\nlook at all the arguments it takes\npractice with that function inside a notebook\n\n(sometimes) read the full source code\npause the video when something in Jeremy’s code is unfamiliar and experiment around with it in a notebook\n\nread the docs and example code for those new concepts\n\n(at some point) read through all the docs for the Tensor object / concept\n\nNote that there’s a bit more when it comes to the imagic demonstration which I’ll try to cover in a separate blogpost.\nIn any case, there was a lot in this lecture. This week is all about matrix multiplication which I’m looking forward to getting to implement myself!"
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "",
    "text": "I am working my way through the fastai course as part of an online meetup group I host.1\nThis week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’).\nJeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course emphasise repeatedly.)\nI decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#the-problem-domain-image-redaction",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#the-problem-domain-image-redaction",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "The Problem Domain: Image Redaction",
    "text": "The Problem Domain: Image Redaction\nUnder the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here.\nQuite often, however, these images are censored or redacted.\n\nKnowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case.\nIt could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless.\nIn the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#getting-the-data",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#getting-the-data",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Getting the Data",
    "text": "Getting the Data\nThe first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training.\nNote that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end.\nAt the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#labelling-the-images-with-prodigy",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#labelling-the-images-with-prodigy",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Labelling the images with Prodigy",
    "text": "Labelling the images with Prodigy\nI had used Explosion.ai’s Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly.\n\nIt took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not:\n\nFrom that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted:\nimport json_lines\nfrom PIL import Image\nfrom pathlib import Path\n\ndef save_resized_image_file(location_path):\n    basewidth = 800\n    img = Image.open(record['image'])\n    wpercent = (basewidth / float(img.size[0]))\n    hsize = int((float(img.size[1]) * float(wpercent)))\n    img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n    img.save(location_path)\n\npath = '/my_projects_directory/redaction-model'\n\nredacted_path = path + \"/redaction_training_data/\" + \"redacted\"\nunredacted_path = path + \"/redaction_training_data/\" + \"unredacted\"\n\nwith open(path + \"/\" + \"annotations.jsonl\", \"rb\") as f:\n    for record in json_lines.reader(f):\n        if record[\"answer\"] == \"accept\":\n            save_resized_image_file(Path(redacted_path + \"/\" + record['meta']['file']))\n        else:\n            save_resized_image_file(Path(unredacted_path + \"/\" + record['meta']['file']))"
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#transferring-the-data-to-paperspace-with-magic-wormhole",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#transferring-the-data-to-paperspace-with-magic-wormhole",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Transferring the data to Paperspace with magic-wormhole",
    "text": "Transferring the data to Paperspace with magic-wormhole\nOnce I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer.\nI used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data.\nAgain, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#using-the-labelled-data-in-our-training",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#using-the-labelled-data-in-our-training",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Using the labelled data in our training",
    "text": "Using the labelled data in our training\nThe process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the DataBlock class instead of ImageDataLoaders for extra flexibility.\npath = Path('redaction_training_data')\n\nfoia_documents = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(224))\n\ndls = foia_documents.dataloaders(path)\n\nfoia_documents = foia_documents.new(\n    item_tfms=Resize(224, method='pad', pad_mode='reflection'),\n    batch_tfms=aug_transforms(max_zoom=1))\ndls = foia_documents.dataloaders(path)\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(10)\nThe images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate.\nI train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-augmentations",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-augmentations",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Experimenting with augmentations",
    "text": "Experimenting with augmentations\nInitially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found.\nIn the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of the documentation here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in.\nI chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-different-architectures",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#experimenting-with-different-architectures",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Experimenting with different architectures",
    "text": "Experimenting with different architectures\nThe course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison.\nThe danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#hosting-the-model-with-mybinder",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#hosting-the-model-with-mybinder",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Hosting the model with MyBinder",
    "text": "Hosting the model with MyBinder\nChapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online.\nUsing MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you’ll see an interface where you should first upload an image — i.e. a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#next-steps",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#next-steps",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Next steps",
    "text": "Next steps\nI’m at the point in the course where I know enough to be dangerous (i.e. train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy:\n\nbetter augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied.\nmore labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me.\ndifferent redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong).\n\nOtherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted.\nI will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution."
  },
  {
    "objectID": "posts/2021-09-06-redaction-classification-chapter-2.html#footnotes",
    "href": "posts/2021-09-06-redaction-classification-chapter-2.html#footnotes",
    "title": "Training a classifier to detect redacted documents with fastai",
    "section": "Footnotes",
    "text": "Footnotes"
  },
  {
    "objectID": "posts/2022-03-03-model-improvements.html",
    "href": "posts/2022-03-03-model-improvements.html",
    "title": "Incremental Improvements to my Redaction Detection Model",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nLast time I wrote about my work on this project, I’d just finished creating synthetic image data to supplement my manual annotations. Before integrating those into the model training, I wanted to make changes to a few hyper parameters to ensure that I’m getting as much out of the current configuration as possible.\nI focused on three ways of improving the model’s performance, each of which ended up having an effect albeit in different ways.\n\nIncreasing the image size\nWhen I started the project, I set the size of the images that would be passed into the training as the training dataset to 384x384 pixels. (It is a convention relating to how some of the older pre-trained models (like EfficientDet) were such that the image size must be divisible by 128.) This turned out to be too small.\nThe next steps up were 512 and 640. The GPU / hardware on which I was training my model seemed to have no problem with either of these image sizes and the performance increased as I worked with either 512 or 640 as the base image sizes.\n\n\nIncreasing the batch size\nAnother important lever at my disposal was either to increase the batch size (the number of images that are used in each epoch) or to decrease the learning rate. (A useful Twitter thread by Boris Dayma explains some of the tradeoffs for one versus the other, along with some references to things to read).\nI had started off with a batch size of 8, but increasing to 16 and then 32 had a big effect on my model’s performance:\n\nBatch sizes of both 16 and 32 eventually converged on more or less the same COCOMetric score of around 74%. The validation loss rate showed pretty clearly that the highest (32) batch size overfit far faster than for 16. It seems that 16 is the best choice for now.\n\n\nBackbone model size\nThe way I’ve set things up to train this object detection model requires two main choices in terms of architecture: a particular pre-trained model and a backbone. VFNet (as mentioned previously) outperformed basically everything else I’ve tried and I think it seems to be a clear best choice for the model. In terms of the backbone, I’d been using resnet50 until now, but following some of the above experiments, it made sense to try increasing the backbone size as well. (An obvious disadvantage to this approach was slower training times and a larger final model size.)\n\nIn this image you can see the stages of improvements we made throughout this whole process. vfnet-pre-synthetic-base was the lowest performer at the beginning, then doubling the batch size gave another boost of almost 8% to our model performance. Then the final increase to the backbone size added another 4% increase bringing us to a score of around 78% for the COCOMetric.\nIt remains to be seen how much of these changes will make sense when I introduce the synthetic data, or if there are more effective boosters to the model performance in the form of adding annotations to areas where the model struggles the most."
  },
  {
    "objectID": "posts/2022-03-25-paperspace-docker-icevision.html",
    "href": "posts/2022-03-25-paperspace-docker-icevision.html",
    "title": "Building my own image to use IceVision with Paperspace",
    "section": "",
    "text": "I’ve been using Paperspace right to fuel my ML/Deep Learning experimentation since more or less the beginning. It was one of the recommended platforms that offered GPUs for the fastai course and when I started working on my redaction project I chose to keep going since I had little reason to change.\nFast-forward a few months, and I’ve had a few issues along the way. Paperspace works by provisioning a Docker image, connecting it to a fixed filesystem / storage backend and then serving this up to you in a web interface as a Jupyter notebook. I found that sometimes there were issues with dependencies breaking, or special pip install magic I had to include in my notebook so that things would work again.\nIncluded in this is the reality that a full install of IceVision — an amazing library for computer vision that handles a lot of the pain around integrating various libraries and use cases — simply takes a while as it has to download and setup some pretty hefty dependencies. I had found that going from zero to working on the day’s specific issue took around 20 minutes when you factored in all the setup, updates from the Github repo, syncing data and so on.\nInspired by my reading and study of Docker — and with a tip from a Paperspace engineer about how I could get started — I set out to build a custom image that handled most of the setup upfront and automatically updated with the latest changes and data.\nAmazingly, it worked more or less immediately! I created a new Dockerfile based of the original suggestion and the core additions were the following:\nRUN wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh && bash icevision_install.sh cuda11 && rm icevision_install.sh\n\nRUN pip install torchtext==0.11.0 --upgrade\nRUN pip install opencv-python ipywidgets icevision-dashboards\nRUN apt update && apt install -y libsm6 libxext6\nRUN apt-get install -y libxrender-dev\n\nCMD make lfs && git lfs pull\nIn order to set this up with Paperspace, you first have to go to your notebooks inside a project and click to create a new Paperspace notebook.\n\nOnce there, you can ignore the suggestion to “select a runtime”, but rather select your machine from the available GPUs. I usually choose the RTX5000 and set it up for an auto-shutdown after 6 hours.\n\nThen you want to click the ‘Advanced Options’ toggle so you can add in all the details of the image being used.\n\nThis is what worked for me. In order to use JupyterLab, the container command should be:\njupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin='*' --ServerApp.allow_credentials=True\nI enter my private GitHub repo (along with my username and a custom token generated to allow Paperspace to download the repo) in the ‘Workspace’ section.\nThen when I click ‘Start Notebook’, it works! No more hanging around for IceVision to install. My Docker image already has this!\nI realise that I’m probably a little late to the party in terms of using Docker and seeing how it can bring some real improvements in terms of reproducibility of environments as well as these little quality-of-life perks like not hanging around to install everything each time you want to use it. This was a really useful experience for me to learn from, and I’ll certainly be using this going forward in other projects I work on."
  },
  {
    "objectID": "posts/2023-03-18-stable-eights-part-two.html",
    "href": "posts/2023-03-18-stable-eights-part-two.html",
    "title": "Building Blocks For Better Stable Eights",
    "section": "",
    "text": "My last blogpost was about my attempt to try to generate images of handwritten ‘eight’ digits from random noise. It was an exploration of some of the process at work in diffusion models and the whole diffusion paradigm in general.\nIt’s come up since then during our Sunday morning ‘Delft FastAI Study Group’ sessions and we’ve been throwing around a few different ideas on how to improve the process to actually output eights. Two in particular seemed like things we’d want to try out:\nI thought I’d try to implement a starter version of both of these in order to learn what they are, and in order to present for our group discussion. Before we get started, we can get some boilerplate setup out of the way (i.e. the status quo by the end of the last post)."
  },
  {
    "objectID": "posts/2023-03-18-stable-eights-part-two.html#training-an-8-digit-classifier-initial-naive-approach",
    "href": "posts/2023-03-18-stable-eights-part-two.html#training-an-8-digit-classifier-initial-naive-approach",
    "title": "Building Blocks For Better Stable Eights",
    "section": "1: Training an ‘8’ digit classifier (initial / naive approach)",
    "text": "1: Training an ‘8’ digit classifier (initial / naive approach)\n\n\nCode\n!pip install -Uqq pip\n!pip install fastai torch datasets rich -Uqq\n# !pip install -Uqq timm\n\nimport IPython\n\n# automatically restart kernel\nIPython.Application.instance().kernel.do_shutdown(restart=True)\n\n\n^C\nERROR: Operation cancelled by user\n\n\n{'status': 'ok', 'restart': True}\n\n\n: \n\n\n\n\nCode\nfrom typing import Union, Callable\n\nfrom fastai.vision.all import *\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.models import vgg19\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# make sure the digits are human-readable\ntorch.set_printoptions(precision=6, sci_mode=False)\n\n# dataset patched together from the original MNIST dataset\npath = Path(\"./mnist_8_or_not/training\")\nfnames = get_image_files(path)\n\n\ndef label_func(x):\n    return x.parent.name\n\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func)\n# set environment based on hostname\nimport os\n\nenvironment_type = \"unknown\"\n\nif \"HOSTNAME\" in os.environ:\n    hostname = os.environ[\"HOSTNAME\"]\n    environment_type = \"local\" if hostname == \"localhost\" else \"cloud\"\n\nmodel_filename = \"eight_classifier.pkl\"\nmodel_base_path = Path(\"/home/\") if environment_type == \"cloud\" else Path(\"./\")\n\n# only train the model if we have no model already\nmodel_path = Path(f\"{model_base_path}/{model_filename}\")\n\nif not model_path.exists():\n    learn = vision_learner(dls, resnet34, metrics=error_rate)\n    learn.fine_tune(6)\n    # export our model so we don't have to retrain it every time from now on\n    learn.export(f\"{model_base_path}{model_filename}\")\nelse:\n    learn = load_learner(f\"{model_base_path}/{model_filename}\")\nan_eight = Path(path / \"8\").ls()[0]\nnot_an_eight = Path(path / \"not_8\").ls()[0]\n\n\ndef get_eight_probability(\n    image_pth: Union[Path, torch.Tensor], learner: Learner\n) -> torch.Tensor:\n    _, _, probs = learner.predict(image_pth)\n    return probs[0]\n\n\n/Users/strickvl/.pyenv/versions/3.10.4/envs/mlops-blog/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n# we generate a 3x28x28 tensor with random values assigned\n# we ensure that we can use PyTorch's autograd on the values\ndef get_noisy_starter_tensor() -> torch.Tensor:\n    return torch.randn(3, 28, 28, requires_grad=True)\n\n\n# this will allow us to display the tensor as an image\ndef display_tensor(tns: torch.Tensor):\n    # Convert the tensor to a NumPy array\n    image_array = tns.detach().numpy()\n\n    # Clip the pixel values between 0 and 1\n    image_array = np.clip(image_array, 0, 1)\n\n    # Transpose the array to (28, 28, 3) shape\n    image_array = image_array.transpose((1, 2, 0))\n\n    # Display the image using Matplotlib\n    plt.imshow(image_array)\n    plt.show()"
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html",
    "href": "posts/2022-09-07-serialisation.html",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nSerialisation and deserialisation. I ran headfirst into these two words on my first day in my new job. From the way my colleagues discussed them, it seemed like this was something I should have learned from a computer science degree; foundational concepts with practical applications throughout most places that computers touched.\nA few months in, I’ve come to appreciate a little more about what the underlying concept is about as well as some of the reasons why it remains both relevant and something that pops up regularly. I’ll begin by setting out some of this context before showing an example of where I encountered it recently in my own project. By the end, you’ll understand why this is such an important (and practical) concept and why you’ll encounter it a lot while doing machine learning."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#the-basics",
    "href": "posts/2022-09-07-serialisation.html#the-basics",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🔢 The Basics",
    "text": "🔢 The Basics\nIn the common definition, serialisation is the process by which you convert something into a sequence of bytes, and deserialisation is when you convert the other way (i.e. from bytes). In some domains it is also known as marshalling or pickling.\nThis commonly is encountered when you need to store some data on disk (i.e. not or no longer in memory). Perhaps you need some kind of permanent storage of that data, or you need to make the data available to another process. The process through which you transform the data (from something that is comprehensible to whatever environment or language you’re working on) is serialisation.\nTo give another example, in a language like Python we often think in and deal through a series of ‘objects’: think dictionaries or even classes in an OOP context. In order to save this to disk, we have to convert it to some other format that firstly is in some format that is stable when saved as a file. We might want to send that data across the network, or have it opened by a different process or a programme running in a different language. Serialisation is the process by which something context and perhaps language-specific gets transformed into this universal substrate (i.e. a sequence of bytes)."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#common-ways-to-serialise-data-in-python",
    "href": "posts/2022-09-07-serialisation.html#common-ways-to-serialise-data-in-python",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🍏 Common ways to serialise data in Python",
    "text": "🍏 Common ways to serialise data in Python\nIn the past, pickle was a commonly-used way of making this conversion. It has a lot of shortcomings, two of which sit at the top of the list:\n\nthere isn’t (as far as I’m aware) much interoperability for objects that are serialised with pickle. If you want to load an object that has been ‘pickled’, the entity doing the ‘unpickling’ will have to be running the exact same version of Python as the one that did the pickling. (If I’m not mistaken, there might even be some cross platform interoperability issues as well.)\nsecurity concerns are serious when it comes to pickle: when you load(...) some pickled object, this will run whatever code is inside with the assumption that it is ‘trusted’. As such, it is unsuitable for use with untrusted data and generally people tend to turn their nose at pickle. (If you do have to interact with some pickled data, pickletools is a handy tool that allows you to inspect and interact with the file without running the arbitrary code packaged inside. While we’re at the library recommendations, it’s also worth checking out fickling which overlaps in functionality somewhat.)\n\nJSON has become a commonly-used format for serialising data (or its cousin JSONL, for too-much-to-load-into-memory-at-once data). This is a common format with many uses, but it does come with a serious shortcoming which is that it only supports certain data types. If you’re saving some custom object of your own creation, you’ll first need to convert that into a format that can be transformed into a JSON object/file. If you don’t, then your object will not be able to be rehydrated from the on-disk representation.\nNote that the Python pickle module serialises data into a binary format, whereas the json module converts it into a text format (i.e. readable and comprehensible to someone browsing files or displaying their contents with something like cat). Moreover, pickle does handle many (most?) objects and types that you can throw at it, though with all the caveats mentioned above.\nI haven’t explored it at all, but while reading a bit about this area I was consistently pointed to Google’s Protobuf format / library which is another way to serialise structured data. I am unable to properly evaluate the extent to which this is an improvement on existing protocols."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#serialisation-and-deserialisation-in-machine-learning",
    "href": "posts/2022-09-07-serialisation.html#serialisation-and-deserialisation-in-machine-learning",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🔐 Serialisation and deserialisation in Machine Learning",
    "text": "🔐 Serialisation and deserialisation in Machine Learning\nI mentioned earlier that this concept and operation was something that I confronted more or less on my first day working in my new job. (We build an open-source framework that supports someone working to build and deploy machine learning models.) In order to understand why this is so important, a small detour showing a basic example of a ZenML pipeline is necessary. What follows is an extremely simple example showcasing how pipelines are composed of steps, and how those are in turn run:\nfrom zenml.steps import step\nfrom zenml.pipelines import pipeline\n\n@step\ndef read_integer() -> int:\n    return 3\n\n@pipeline\ndef basic_pipeline(read_integer) -> None:\n    read_integer()\n\nbasic_pipeline(read_integer=read_integer()).run()\nPipelines are constructed out of a series of steps. The steps are defined with an @step decorator, and pipeline definitions are composed in a similar way. Finally, at the end we specify which steps correspond to which parts of the pipeline definition and then call the run() method to execute our pipeline.\nYou’ll also note the presence of some type annotations as part of how we define our step and pipeline. These are required, and while they may seem simplistic and unnecessary at the moment, later on they will make things much clearer.\nOur pipeline isn’t doing much at the moment, you might think. Behind the scenes, however, ZenML is doing a lot of legwork:\n\nstoring the outputs (and inputs, though there aren’t any in this basic example) of all steps\ncaching those output values or objects, such that if the code doesn’t change then we should just retrieve the cached value.\nvalidating and checking the types of values that get returned so that we can be sure our code is returning what we hope / think it should be returning.\n\nMoreover, it does all this in a way that all this intermediary state is stored on disk and versioned. If you update your pipeline steps then rerun it, ZenML will save the new outputs such that you can go back and inspect where data came from and so on.\nIn order to save all these objects on disk, however, and to bring this story full-circle, ZenML serialises the data when saving the artifacts from pipeline runs, and deserialises that data when those artifacts are needed (by the cache, for example, or when you want to access a step output once your pipeline has completed its run). We call this part of the process ‘materialisation’. (There’s more in our docs on materialisation here, and if you’re searching, be sure to search with a ‘z’ and not an ‘s’, coz America.)"
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#a-basic-custom-materializer",
    "href": "posts/2022-09-07-serialisation.html#a-basic-custom-materializer",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🛠 A basic custom materializer",
    "text": "🛠 A basic custom materializer\nFor most kinds of ‘normal’ Python objects, this is no problem at all. But as we saw above, if we’re going to be able to reconstruct and rehydrate an object from a static sequence of bytes, we’re going to need to do a bit more to make this happen. Within ZenML this means that if you have some special kind of object or type, you’ll need to define a ‘custom materialiser’; this is code that defines how ZenML should serialise and deserialise the objects that you want to be stored as state on disk.\nTo give you a sense of what this will look like, here’s our code from above but updated a little to fit this new scenario:\nimport os\nfrom typing import Type\n\nfrom zenml.artifacts import DataArtifact\nfrom zenml.io import fileio\nfrom zenml.materializers.base_materializer import BaseMaterializer\nfrom zenml.pipelines import pipeline\nfrom zenml.steps import step\n\nclass MyCustomObject:\n    def __init__(self, name):\n        self.name = name\n\nclass MyCustomMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (MyCustomObject,)\n    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n\n    def handle_input(self, data_type: Type[MyCustomObject]) -> MyCustomObject:\n        \"\"\"Read from artifact store\"\"\"\n        super().handle_input(data_type)\n        with fileio.open(os.path.join(self.artifact.uri, \"data.txt\"), \"r\") as f:\n            name = f.read()\n        return MyCustomObject(name=name)\n\n    def handle_return(self, my_obj: MyCustomObject) -> None:\n        \"\"\"Write to artifact store\"\"\"\n        super().handle_return(my_obj)\n        with fileio.open(os.path.join(self.artifact.uri, \"data.txt\"), \"w\") as f:\n            f.write(my_obj.name)\n\n@step\ndef read_custom_object() -> MyCustomObject:\n    return MyCustomObject(\"aria\")\n\n@pipeline\ndef basic_pipeline(read_custom_object) -> None:\n    read_custom_object()\n\nbasic_pipeline(\n    read_custom_object=read_custom_object().with_return_materializers(\n        MyCustomMaterializer\n    )\n).run()\nYou’ll notice a new piece of code which defines the MyCustomMaterializer class. This is subclassed off our BaseMaterializer class and we just have to define two methods, one that handles how to serialise or save the data to disk, and the other that handles how to deserialise or rehydrate the objects/data from disk. We add a special .with_return_materializers call when we run the pipeline; this lets ZenML that when we encounter a weird type of object, it can go ahead and use our custom defined materialiser to handle it.\nI hope you’ll agree that this stuff isn’t too hard to grok, and while the precise steps of how you implement all this might take a bit of getting used to, it’s conceptually not too hard once you understand the foundations of what you’re doing. It took me longer than I’m proud to admit to really understand the elegance of this way of doing things, but all these little pieces add up and you can then go off and use them in your real-life projects."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#materialisation-in-practice-icevision-and-custom-objects",
    "href": "posts/2022-09-07-serialisation.html#materialisation-in-practice-icevision-and-custom-objects",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🕵️ Materialisation in practice: IceVision and Custom Objects",
    "text": "🕵️ Materialisation in practice: IceVision and Custom Objects\nCase in point: my object detection pipeline. I took a bit of a break over the summer, but now I’m back and working to get my pipeline production-ready. Defining the basic steps of my pipeline were fairly easy; I’ve already described that in my last blog post.\nThe moment I started defining my pipeline in code, I immediately hit a whole array of non-standard objects. My data loading steps returned IceVision-specific parsers custom to COCO BBoxes and my training step returned a collection of various custom objects combining code with the trained model parameters. (Note: for some common use cases like training with raw PyTorch or Tensorflow etc, ZenML has defined many standard materialisers already to get you going quickly.) I realised that I’d have to define custom materialisers to handle these different inputs and outputs.\nSome of this wasn’t trivial to implement. Sometimes you might get lucky and the library you work with has implemented some handy features to help with serialisation and deserialisation. From what I can tell, this seems to be the case when saving models with PyTorch, for example. But for the rest it’s often less clear what need to happen and why code works in the way it does. To save the IceVision RecordCollection object, for example, I had to jump through some hoops, converting several sub levels of custom objects along the way, to make sure that my objects were serialisable.\nHere’s the custom materialiser code responsible for handling those conversions and serialisation for the RecordCollection. (Think of RecordCollection just as a type of stored data, parsed and ready to use for model training.)\nimport os\nimport pathlib\nfrom typing import Any, Dict, List, Type\n\nfrom icevision.all import *\nimport srsly\nfrom zenml.artifacts import DataArtifact\nfrom zenml.io import fileio\nfrom zenml.materializers.base_materializer import BaseMaterializer\n\nclass COCOMaterializerParser(Parser):\n    def __init__(self, template_record, records: List[Dict[str, Any]]):\n        super().__init__(template_record=self.template_record())\n\n        self.records = records\n        self.class_map = ClassMap(records[0][\"common\"][\"classes\"])\n        print(self.class_map)\n\n    def __iter__(self) -> Any:\n        yield from self.records\n\n    def __len__(self) -> int:\n        return len(self.records)\n\n    def record_id(self, o: Any) -> Hashable:\n        return o[\"common\"][\"filepath\"]\n\n    def template_record(self) -> BaseRecord:\n        return BaseRecord(\n            (\n                FilepathRecordComponent(),\n                InstancesLabelsRecordComponent(),\n                AreasRecordComponent(),\n                IsCrowdsRecordComponent(),\n                BBoxesRecordComponent(),\n            )\n        )\n\n    def filepath(self, o) -> Path:\n        return pathlib.Path(o[\"common\"][\"filepath\"])\n\n    def img_size(self, o) -> ImgSize:\n        return ImgSize(width=o[\"common\"][\"width\"], height=o[\"common\"][\"height\"])\n\n    def labels_ids(self, o) -> List[Hashable]:\n        return o[\"detection\"][\"label_ids\"]\n\n    def areas(self, o) -> List[float]:\n        return o[\"detection\"][\"areas\"]\n\n    def iscrowds(self, o) -> List[bool]:\n        return o[\"detection\"][\"iscrowds\"]\n\n    def bboxes(self, o) -> List[BBox]:\n        boxes = []\n        for bbox in o[\"detection\"][\"bboxes\"]:\n            a, b, c, d = bbox\n            new_bbox = BBox.from_xyxy(a, b, c, d)\n            boxes.append(new_bbox)\n        return boxes\n\n    def parse_fields(self, o: Any, record: BaseRecord, is_new: bool):\n        if is_new:\n            record.set_filepath(self.filepath(o))\n            record.set_img_size(self.img_size(o))\n\n        record.detection.set_class_map(self.class_map)\n        record.detection.add_areas(self.areas(o))\n        record.detection.add_iscrowds(self.iscrowds(o))\n        record.detection.add_bboxes(self.bboxes(o))\n        record.detection.add_labels(o[\"detection\"][\"labels\"])\n\n\ndef detection_record_collection_to_json(rcoll: RecordCollection) -> str:\n    indexes = list(rcoll._records)\n    records = [rcoll._records[index] for index in indexes]\n    classes = rcoll[0].detection.class_map.get_classes()\n    dict_records = [record.as_dict() for record in records]\n    for record in dict_records:\n        record[\"common\"][\"filepath\"] = str(record[\"common\"][\"filepath\"])\n        bboxes = record[\"detection\"][\"bboxes\"]\n        new_bboxes = []\n        for bbox in bboxes:\n            a, b, c, d = bbox.xyxy\n            new_bbox = [a, b, c, d]\n            new_bboxes.append(new_bbox)\n        record[\"detection\"][\"bboxes\"] = new_bboxes\n        record[\"common\"][\"classes\"] = classes\n    return srsly.json_dumps(dict_records)\n\n\ndef detection_json_str_to_record_collection(records: str) -> RecordCollection:\n    r = srsly.json_loads(records)\n    template_record = ObjectDetectionRecord()\n    parser = COCOMaterializerParser(template_record, r)\n    parsed_records, *_ = parser.parse(data_splitter=SingleSplitSplitter())\n    return parsed_records\n\n\nclass COCOBBoxRecordCollectionMaterializer(BaseMaterializer):\n    ASSOCIATED_TYPES = (RecordCollection,)\n    ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,)\n\n    def handle_input(self, data_type: Type[RecordCollection]) -> RecordCollection:\n        \"\"\"Read from artifact store\"\"\"\n        super().handle_input(data_type)\n        with fileio.open(\n            os.path.join(self.artifact.uri, DEFAULT_RECORD_COLLECTION), \"r\"\n        ) as f:\n            return detection_json_str_to_record_collection(f.read())\n\n    def handle_return(self, record_collection_obj: RecordCollection) -> None:\n        \"\"\"Write to artifact store\"\"\"\n        super().handle_return(record_collection_obj)\n\n        json_string = detection_record_collection_to_json(record_collection_obj)\n        with fileio.open(\n            os.path.join(self.artifact.uri, DEFAULT_RECORD_COLLECTION), \"w\"\n        ) as f:\n            f.write(json_string)\nAs you can see, there’s a decent amount going on here. In my custom materialiser, I have a detection_record_collection_to_json method that constructs the JSON representation of my custom RecordCollection object. I use Explosion’s handy srsly package for their forks + bundling together of various Python serialisation libraries. For the rest, that requires a bit more knowledge of how IceVision handles things like BBox objects and COCO Records under the hood, but you can get the idea that it’s not completely trivial."
  },
  {
    "objectID": "posts/2022-09-07-serialisation.html#serialisation-is-for-everyone",
    "href": "posts/2022-09-07-serialisation.html#serialisation-is-for-everyone",
    "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
    "section": "🥳 Serialisation is for Everyone!",
    "text": "🥳 Serialisation is for Everyone!\nIt’s also not completely impossible to implement either, though, lest you feel like I’m leaving you without hope. My aim with this article was to guide you to the point where you feel you can understand why serialisation is important and to know why you might well encounter it during your data science journey. The moment you need to do something just slightly longer-lasting than an ephemeral training run that is tracked nowhere and just lives in a Colab notebook, that’s when you’ll hit serialisation.\nMoreover, I showed how you can incrementally build up your pipelines with a tool like ZenML to handle lots of parts of the complexity that come with your modelling work.\n[Image credit: Photo by fabio on Unsplash]"
  },
  {
    "objectID": "posts/2021-11-27-pipeline-conversations.html",
    "href": "posts/2021-11-27-pipeline-conversations.html",
    "title": "Launching a podcast about MLOps",
    "section": "",
    "text": "I’ll be co-hosting a new podcast about MLOps, with new episodes out every fortnight. Pipeline Conversations: A Machine Learning Podcast by ZenML is the new podcast from the company where I work. (We build an open-source tool for data scientists to empower them to take control of how their models live in production.)\nOur first episode gets into some of the background for why ZenML exists in the first place. Upcoming episodes will be discussions with guests from the data science and MLOps space.\nI’m excited to get the opportunity to talk with so many interesting and smart people."
  },
  {
    "objectID": "posts/2022-01-03-robust-python-3.html",
    "href": "posts/2022-01-03-robust-python-3.html",
    "title": "Getting practical with type annotations and mypy",
    "section": "",
    "text": "The third chapter of ‘Robust Python’ offers a quick introduction to the practicalities of type annotations in Python. We also see tools like mypy being used to catch places where the reality of your code doesn’t necessarily match the type annotations that you’ve stated.\nFor the first, a quick example can suffice:\nname: str = \"alex\"\n\ndef some_function(some_number: int, some_text: str = \"some text\") -> str:\n    # your code goes here\n    return \"\" # returns a string\nYou can see the different places that type annotations might appear. You can annotate variables in your code. I’ve seen this one less often, but it’s possible. Then you can have type annotations for the parameters when defining functions (some even with default values assigned). You can also have type annotations for the return value of those functions.\n\nNote that type hints are not used at runtime, so in that sense they are completely optional and don’t affect how your code runs when it’s passed through the Python interpreter. (Type hints were introduced in Python 3.5, though there is a way to achieve the same effect using comments and a standard way of listing type annotations that way if you are stuck with a 2.7 codebase, for example.)\nWith some type annotations added to our code, we can use a typechecker like mypy to see whether things are really as we imagine. In Viafore’s own words:\n\n“type checkers are what allow the type annotations to transcend from communication method to a safety net. It is a form of static analysis.”\n\nIf your codebase uses type annotations to communicate intent, and you’re using mypy to catch any of those type errors, remember that typecheckers only catch this certain type of errors. You still need to be doing testing and all the other best practices to help catch the rest.\nOne forward-looking benefit covered by this chapter was how having code covered with type annotations and type checking could give you the confidence to change things in the codebase that otherwise you would have hesitated to even approach. There are, of course, also some tradeoffs and disadvantages to adding this in: particularly around speed of iteration and possibly flexibility, but the book makes a strong case for why most large Python codebases could probably use type checking as part of their arsenal."
  },
  {
    "objectID": "posts/2023-05-29-balochi-language-dataset.html",
    "href": "posts/2023-05-29-balochi-language-dataset.html",
    "title": "Building a Balochi Language Dataset for NLP Applications",
    "section": "",
    "text": "I’m working on building out some language models and utilities for the Balochi language. (Read previous posts in this series for the full context.) Even though there are some 8-10 million estimated speakers, it certainly falls into the category of being a ‘low-resource’ language. Many (most?) things that you’d take for granted when working with English-language models are either non-existent or bare bones for Balochi.\nThe experimentation phase of a project like this rewards a fast iteration speed, so I’m looking for ways to keep moving forward. I don’t need to spend days running a single experiment to validate my ideas; I’m sufficiently green that small datasets and these frequent tweaks to what I’m doing will hopefully reward me.\nI did an initial survey of materials and resources that already exist, collecting a mix of more general language materials alongside some prior work that exists in the NLP space for Balochi. In particular, there are some small datasets on GitHub as well as some more targeted projects for Named Entity Recognition (NER). Since it’s my repository, I also threw in some blog posts that inspired me to get started in the first place (from Lj Miranda and Kurian Benoy, among others).\n\nThe awesome-balochi-nlp repository is my first effort at gathering a list of resources. I’ll be keeping it up to date as I continue.\nFor my work gathering the dataset together, I had my eyes on three potential sources of authentic Balochi texts:\n\nSina Ahmadi’s PersoArabicLID project (language classification for a series of low-resource languages that share a common script) includes (labelled) datasets as part of the repository\nBaask.com — a website that’s been posting Balochi content for around a decade and that I had come across in the past\nKissah.org — a project by Junaid Qadir that collates Balochi stories\n\nThe mechanics of gathering the texts from these sources was straightforward (a few scripts using beautifulsoup and the requests module), but I’ll admit that the experience felt a little uncomfortable. The content from these sources may technically be ‘fair game’ but I’ll admit to a certain queasiness about how easy it was to put together my promo-dataset of Balochi language in an evening. (For that reason, I’m probably not going to open up the dataset until I’ve figured out a way to do that properly; the ideal end-goal is to have datasets like this available publicly on the Huggingface Hub and so on.)\nSo now I have a dataset containing some 2.6 million words of Balochi text. This feels like it’s enough to do some experiments at least, and we’ll see how far I get with it. The first order of business is to look into tokenisation or the process of splitting those texts up into pieces that can be used and processed by the machine learning machinery. Surprise surprise: there aren’t any pre-existing tokenisers for Balochi and while there are language-agnostic tokenisation processes I want to understand the tradeoffs around the different algorithms and approaches they take."
  },
  {
    "objectID": "posts/2023-04-28-removing-git-commits.html",
    "href": "posts/2023-04-28-removing-git-commits.html",
    "title": "How to remove a commit (or two) from your git branch",
    "section": "",
    "text": "I ran into a problem where a tool I was using auto-updated my git branch, forcing dozens of changes. The changes were relatively innocuous, but they were irrelevant to the work I was doing which would make reviewing the Pull Request pretty hard going.\nThis is what I did to remove the traces of that commit from my git branch logs:\nFirst, make sure you are on the branch where the commit you want to remove exists. If not, switch to that branch using:\ngit checkout <branch_name>\nUse git log to find the commit hash (the unique identifier) of the commit you want to remove. The commit hash will be a long string of characters and numbers, e.g., ab12cd34ef56gh78ij90kl12mn34op56qr78st90.\nUse git rebase -i (interactive rebase) to remove the commit. This command will open an editor where you can manipulate the commit history. Use the commit hash that’s one before the one you want to remove:\ngit rebase -i <parent_commit_hash>\nIn the editor that opens, you will see a list of commits starting from the parent commit hash you provided. Find the line with the commit you want to remove. Change the word pick at the beginning of that line to drop or simply delete the entire line. Save and close the editor. (Alternatively, if you’re using VS Code, a sort of UI interface will open which will allow you to select from drop-down pickers which options you want for each of the downstream commits. There is a button at the bottom to switch to the pure text interface if you prefer.)\nGit will perform the rebase, removing the specified commit from the commit tree.\nIf you’re satisfied with the changes, push your branch to the remote repository:\ngit push -f origin <branch_name>\nImportant Note: Using git push -f (force push) can be dangerous, as it overwrites the remote branch with your local one. Make sure you are confident about your changes before using this command. If you’re working on a shared branch with others it’s important to communicate with your team to ensure that no one else is working on the same branch to avoid losing any work.\nThe commits I’d made after the one I wanted to remove didn’t overlap or relate to each other, so I didn’t face any conflicts during the rebase process. However, if you do encounter conflicts, you’ll need to resolve them manually and continue with the rebase using git rebase --continue.\nRemember that rebasing and force pushing can rewrite the commit history, so always be cautious when performing these operations."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "",
    "text": "In the previous post we looked at a single example where a prompt could help us extract structured data from some text. Here were were relying on the inbuilt capabilities of LLMs to ‘reason’ over the text and the task. In this post we’ll try to get a sense of the overall / aggregate performance of gpt-4-turbo on the ISAF Press Releases data extraction task.\nThe first thing we need to do is to load and process the data. If you remember from last time, there were some fields that needed conversion so that we’re comparing apples to apples when we get back our result from the LLM."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html#loading-and-processing-the-data",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html#loading-and-processing-the-data",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "Loading and processing the data",
    "text": "Loading and processing the data\n\n# get data from datasets\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\nimport tqdm as notebook_tqdm\n\n# Load the dataset\ndataset = load_dataset(\"strickvl/isafpressreleases\", split=\"train\")\n\n# Convert the dataset to a pandas DataFrame\ndf = pd.DataFrame(dataset)\n\n\n\n\nI’ll output the column names as a reference since we’ll be using almost all of these in our evaluation.\n\ndf.columns\n\nIndex(['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province',\n       'citydistrict', 'village', 'targetgroup', 'commander', 'position',\n       'minkilled', 'mincaptured', 'capturedcharacterisation',\n       'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid',\n       'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta',\n       'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured',\n       'minfacilitatorscaptured', 'leaderq'],\n      dtype='object')\n\n\nWe set up the same objects to support the Pydantic IsafEvent model. This is the same as last time so I’ll hide the code but you can see it by clicking the popout arrow.\n\n\nCode\nfrom pydantic import BaseModel, Field\nfrom datetime import date\nfrom enum import Enum\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\n\n\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jawzjan = \"jawzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktia = \"paktia\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pol = \"sar_e_pol\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\n\n\nclass TargetGroup(str, Enum):\n    taliban = \"taliban\"\n    haqqani = \"haqqani\"\n    criminals = \"criminals\"\n    aq = \"aq\"\n    hig = \"hig\"\n    let = \"let\"\n    imu = \"imu\"\n    judq = \"judq\"\n    iju = \"iju\"\n    hik = \"hik\"\n    ttp = \"ttp\"\n    other = \"other\"\n\n\nOur IsafEvent is what we’re trying to get to. We want to turn an unstructured piece of text (a press release) into the structured format you see below.\n\nfrom typing import Set\n\n\nclass IsafEvent(BaseModel):\n    name: str = Field(\n        description=\"A title or name for the event which summarises the event as a headline\"\n    )\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    event_type: Set[EventType] = Field(\n        description=\"The event type. Can be multiple types.\"\n    )\n    province: Set[Province] = Field(\n        description=\"The province in which the event occurred. Can be multiple provinces.\"\n    )\n    target_group: Set[TargetGroup] = Field(\n        description=\"The group that was targetted during the event. Can be multiple groups.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\nAgain we can take a look at a single example to understand the task and to construct our first evaluation of this single example. The article looks like this:\n\narticle_id = 15\narticle_text = df[\"text\"][article_id]\nprint(article_text)\n\nDec. 11: Haqqani Facilitator Detained in Khowst; Security Discussed in Farah\nNEWS RELEASE ISAF Joint Command - Afghanistan   2009-12-CA-065 For Immediate Release  KABUL, Afghanistan (Dec. 11) - An\nAfghan-international security force detained a couple of militants in Khowst province today, one of whom was a \nsought-after Haqqani facilitator.  The facilitator is responsible for the shipment and distribution of weapons to \nother militant elements in the area.\n The joint security force searched a compound near the village of Badal Kalay in the Nader Shakhot district where \nintelligence sources indicated the facilitator was located.  The facilitator identified himself and surrendered \nwithout incident.  No shots were fired and no one was injured.\n\n\n\nAnd our prompt looks like this:\n\nquery = f\"\"\"\nThe following is a press release issued by ISAF (formerly operating in Afghanistan):\n{article_text}\n\nPlease extract the following information from the press release:\n- The name of the event (summarising the event / text as a headline)\n- The start date of the event\n- The event type(s)\n- The province(s) in which the event occurred\n- The target group(s) of the event\n- The minimum number of people killed during the event\n- The minimum number of people captured during the event\n- Whether someone was killed or not during the event\n- Whether someone was captured or not during the event\n- Whether the event was a so-called 'kill-capture raid'\n- Whether an airstrike was used during the event\n- Whether no shots were fired during the event\n- The minimum number of leaders killed during the event\n- The minimum number of leaders captured during the event\n\nAnnotation notes:\n- A 'faciliator' is not a leader.\n- If a press release states that 'insurgents' were detained without further details, assign a minimum number of two detained. Interpret 'a couple' as two. Interpret 'several' as at least three, even though it may sometimes refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a small group', and 'multiple' as denoting at least three, even if they sometimes refer to larger numbers. Choose the smaller number if no other information is available in the press release to come up with a minimally acceptable figure. Interpret 'numerous' and 'a handful' as at least four, and 'a large number' as at least five.\n\"\"\"\n\nSince we’re going to be doing a lot of LLM calls I took a moment to refactor the code to make it less verbose. Click through to see the full code for the query_llm but basically it’s just a wrapper around OpenAI, Anthropic and Ollama models and we route to one or the other of those depending on what model you say you want to use.\n\n\nCode\nimport instructor\nfrom anthropic import Anthropic\nfrom openai import OpenAI\n\n\ndef query_llm(message: str, model: str, response_model: BaseModel) -> str:\n    if model in [\"gpt-4\", \"gpt-4-turbo-preview\", \"gpt-4-turbo\"]:\n        client = instructor.patch(OpenAI(), mode=instructor.Mode.JSON)\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": message,\n                },\n            ],\n            response_model=response_model,\n            max_retries=3,\n        )\n    elif model in [\"claude-3-opus-20240229\", \"claude-3-opus-20240229-preview\"]:\n        client = instructor.from_anthropic(Anthropic())\n\n        # note that client.chat.completions.create will also work\n        response = client.messages.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": message,\n                },\n            ],\n            max_tokens=4096,\n            response_model=response_model,\n            max_retries=3,\n        )\n    elif model in [\"mistral\", \"mixtral\", \"llama-3\", \"gemma\", \"gemma:2b\"]:\n        client = instructor.from_openai(\n            OpenAI(\n                base_url=\"http://localhost:11434/v1\",\n                api_key=\"ollama\",  # required, but unused\n            ),\n            mode=instructor.Mode.JSON,\n        )\n\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": message,\n                }\n            ],\n            response_model=response_model,\n        )\n    return response\n\n\nAnd here are the responses for gpt-4-turbo and claude-3-opus-20240229.\n\nopenai_response = query_llm(query, \"gpt-4-turbo\", IsafEvent)\nprint(openai_response)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    event_type={<EventType.detention: 'detention'>},\n    province={<Province.khost: 'khost'>},\n    target_group={<TargetGroup.haqqani: 'haqqani'>},\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\n\nclaude_response = query_llm(query, \"claude-3-opus-20240229\", IsafEvent)\nprint(claude_response)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    event_type={<EventType.detention: 'detention'>},\n    province={<Province.khost: 'khost'>},\n    target_group={<TargetGroup.haqqani: 'haqqani'>},\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\nUnfortunately I was unable to evaluate a local model for this task as I couldn’t get a response back. I just got a Pydantic validation error, suggesting to me that maybe we’re hitting the context length limits of those local models with all the extra metadata in the JSON schema that we’re passing in via instructor. At any rate for now it’s the first immediate validation of the idea to finetune the model if we’re already hitting the limits of raw prompting for those models."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-the-results",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-the-results",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "Evaluating the results",
    "text": "Evaluating the results\nWe’ll want some kind of object to store our evaluation results and you can see all the fields I selected below. For fields where we can say ‘true’ or ‘false’ then I just did that, but for fields with multiple options then I thought a float value from 0 to 1 representing how ‘correct’ the options were would be the best option.\nFor fields with numeric values (i.e. number of killed individuals etc) then I generally created two fields: one was just a boolean field for whether it was correct and then another which was the distance between the annotated value and the value we predicted.\n\nclass EvalResult(BaseModel):\n    start_date_correct: bool = Field(\n        description=\"Whether the start date of the event is correct\"\n    )\n    event_type_score: float = Field(\n        description=\"The score between 0 and 1 for the event type of the event\"\n    )\n    province_score: float = Field(\n        description=\"The score between 0 and 1 for the province of the event\"\n    )\n    target_group_score: float = Field(\n        description=\"The score between 0 and 1 for the target group of the event\"\n    )\n    min_killed_correct: bool = Field(\n        description=\"Whether the minimum number of people killed during the event is correct\"\n    )\n    min_killed_distance: int = Field(\n        description=\"The distance between the minimum number of people killed during the event and the annotated number of people killed during the event\"\n    )\n    min_captured_correct: bool = Field(\n        description=\"Whether the minimum number of people captured during the event is correct\"\n    )\n    min_captured_distance: int = Field(\n        description=\"The distance between the minimum number of people captured during the event and the annotated number of people captured during the event\"\n    )\n    killq_correct: bool = Field(description=\"Whether the 'killq' field is correct\")\n    captureq_correct: bool = Field(\n        description=\"Whether the 'captureq' field is correct\"\n    )\n    killcaptureraid_correct: bool = Field(\n        description=\"Whether the 'killcaptureraid' field is correct\"\n    )\n    airstrike_correct: bool = Field(\n        description=\"Whether the 'airstrike' field is correct\"\n    )\n    noshotsfired_correct: bool = Field(\n        description=\"Whether the 'noshotsfired' field is correct\"\n    )\n    min_leaders_killed_correct: bool = Field(\n        description=\"Whether the minimum number of leaders killed during the event is correct\"\n    )\n    min_leaders_killed_distance: int = Field(\n        description=\"The distance between the minimum number of leaders killed during the event and the annotated number of leaders killed during the event\"\n    )\n    min_leaders_captured_correct: bool = Field(\n        description=\"Whether the minimum number of leaders captured during the event is correct\"\n    )\n    min_leaders_captured_distance: int = Field(\n        description=\"The distance between the minimum number of leaders captured during the event and the annotated number of leaders captured during the event\"\n    )\n\nI then coded up some methods that would calculate the score for each of the fields. Once again, there’s a decent amount of code here so I’ll fold it away and you can unfold the code if you want to take a look.\n\n\nCode\ndef start_date_correct(start_date: date, correct_date: date) -> bool:\n    return start_date == correct_date.date()\n\n\ndef event_type_score(event_type: Set[EventType], correct_event_types: str) -> float:\n    # Convert the correct event types string to a set of EventType enum values\n    correct_event_types_set = set(EventType(t) for t in correct_event_types.split(\";\"))\n\n    # Calculate the number of correct predictions\n    correct_predictions = len(event_type.intersection(correct_event_types_set))\n\n    # Calculate the total number of correct event types\n    total_correct_types = len(correct_event_types_set)\n\n    # Calculate the accuracy percentage\n    if total_correct_types > 0:\n        accuracy = correct_predictions / total_correct_types\n    else:\n        accuracy = 0.0\n\n    return accuracy\n\n\ndef province_score(province: Set[Province], correct_province: str) -> float:\n    # Convert the correct province string to a set of Province enum values\n    correct_province_set = set()\n    invalid_provinces = set()\n    for p in correct_province.split(\";\"):\n        try:\n            correct_province_set.add(Province(p.lower()))\n        except ValueError:\n            invalid_provinces.add(p.lower())\n\n    # Calculate the number of correct predictions\n    correct_predictions = len(province.intersection(correct_province_set))\n\n    # Calculate the total number of provinces\n    total_provinces = len(correct_province_set) + len(invalid_provinces)\n\n    # Calculate the accuracy percentage\n    if total_provinces > 0:\n        accuracy = correct_predictions / total_provinces\n    else:\n        accuracy = 0.0\n\n    return accuracy\n\n\ndef target_group_score(\n    target_group: Set[TargetGroup], correct_target_group: str\n) -> float:\n    # Handle the case where correct_target_group is an empty string\n    if correct_target_group.strip() == \"\":\n        correct_target_group_set = set()\n    else:\n        # Convert the correct target group string to a set of TargetGroup enum values\n        correct_target_group_set = set(\n            TargetGroup(t.lower()) for t in correct_target_group.split(\";\")\n        )\n\n    # Calculate the number of correct predictions\n    correct_predictions = len(target_group.intersection(correct_target_group_set))\n\n    # Calculate the total number of correct target groups\n    total_correct_target_groups = len(correct_target_group_set)\n\n    # Calculate the accuracy percentage\n    if total_correct_target_groups > 0:\n        accuracy = correct_predictions / total_correct_target_groups\n    else:\n        accuracy = 0.0\n\n    return accuracy\n\n\ndef min_killed_correct(min_killed: int, correct_min_killed: str) -> bool:\n    return min_killed == int(correct_min_killed)\n\n\ndef min_killed_distance(min_killed: int, correct_min_killed: str) -> int:\n    return abs(min_killed - int(correct_min_killed))\n\n\ndef min_captured_correct(min_captured: int, correct_min_captured: str) -> bool:\n    return min_captured == int(correct_min_captured)\n\n\ndef min_captured_distance(min_captured: int, correct_min_captured: str) -> int:\n    return abs(min_captured - int(correct_min_captured))\n\n\ndef killq_correct(killq: bool, correct_killq: str) -> bool:\n    return killq == (correct_killq == \"true\")\n\n\ndef captureq_correct(captureq: bool, correct_captureq: str) -> bool:\n    return captureq == (correct_captureq == \"true\")\n\n\ndef killcaptureraid_correct(\n    killcaptureraid: bool, correct_killcaptureraid: str\n) -> bool:\n    return killcaptureraid == (correct_killcaptureraid == \"true\")\n\n\ndef airstrike_correct(airstrike: bool, correct_airstrike: str) -> bool:\n    return airstrike == (correct_airstrike == \"true\")\n\n\ndef noshotsfired_correct(noshotsfired: bool, correct_noshotsfired: str) -> bool:\n    return noshotsfired == (correct_noshotsfired == \"true\")\n\n\ndef min_leaders_killed_correct(\n    min_leaders_killed: int, correct_min_leaders_killed: str\n) -> bool:\n    return min_leaders_killed == int(correct_min_leaders_killed)\n\n\ndef min_leaders_killed_distance(\n    min_leaders_killed: int, correct_min_leaders_killed: str\n) -> int:\n    return abs(min_leaders_killed - int(correct_min_leaders_killed))\n\n\ndef min_leaders_captured_correct(\n    min_leaders_captured: int, correct_min_leaders_captured: str\n) -> bool:\n    return min_leaders_captured == int(correct_min_leaders_captured)\n\n\ndef min_leaders_captured_distance(\n    min_leaders_captured: int, correct_min_leaders_captured: str\n) -> int:\n    return abs(min_leaders_captured - int(correct_min_leaders_captured))\n\n\nI then wrote a function which could generate one of these EvalResult objects given a response from an LLM, the original dataframe (with the ground truth annotations) and the article id of the row.\nYou can see how Claude 3 Opus and GPT-4 Turbo performed on our single article below:\n\ndef evaluate_llm_response(\n    response: IsafEvent, df: pd.DataFrame, article_id: int\n) -> EvalResult:\n    return EvalResult(\n        start_date_correct=start_date_correct(\n            response.start_date, df[\"StartDate\"][article_id]\n        ),\n        event_type_score=event_type_score(\n            response.event_type, df[\"eventtype\"][article_id]\n        ),\n        province_score=province_score(response.province, df[\"province\"][article_id]),\n        target_group_score=target_group_score(\n            response.target_group, df[\"targetgroup\"][article_id]\n        ),\n        min_killed_correct=min_killed_correct(\n            response.min_killed, df[\"minkilled\"][article_id]\n        ),\n        min_killed_distance=min_killed_distance(\n            response.min_killed, df[\"minkilled\"][article_id]\n        ),\n        min_captured_correct=min_captured_correct(\n            response.min_captured, df[\"mincaptured\"][article_id]\n        ),\n        min_captured_distance=min_captured_distance(\n            response.min_captured, df[\"mincaptured\"][article_id]\n        ),\n        killq_correct=killq_correct(response.killq, df[\"killq\"][article_id]),\n        captureq_correct=captureq_correct(\n            response.captureq, df[\"captureq\"][article_id]\n        ),\n        killcaptureraid_correct=killcaptureraid_correct(\n            response.killcaptureraid, df[\"killcaptureraid\"][article_id]\n        ),\n        airstrike_correct=airstrike_correct(\n            response.airstrike, df[\"airstrike\"][article_id]\n        ),\n        noshotsfired_correct=noshotsfired_correct(\n            response.noshotsfired, df[\"noshotsfired\"][article_id]\n        ),\n        min_leaders_killed_correct=min_leaders_killed_correct(\n            response.min_leaders_killed, df[\"minleaderskilled\"][article_id]\n        ),\n        min_leaders_killed_distance=min_leaders_killed_distance(\n            response.min_leaders_killed, df[\"minleaderskilled\"][article_id]\n        ),\n        min_leaders_captured_correct=min_leaders_captured_correct(\n            response.min_leaders_captured, df[\"minleaderscaptured\"][article_id]\n        ),\n        min_leaders_captured_distance=min_leaders_captured_distance(\n            response.min_leaders_captured, df[\"minleaderscaptured\"][article_id]\n        ),\n    )\n\n\nprint(\"Claude 3 Opus Scores\")\nprint(evaluate_llm_response(claude_response, df, article_id))\nprint(\"GPT-4 Turbo Scores\")\nprint(evaluate_llm_response(openai_response, df, article_id))\n\nClaude 3 Opus Scores\n\n\n\nEvalResult(\n    start_date_correct=True,\n    event_type_score=1.0,\n    province_score=1.0,\n    target_group_score=1.0,\n    min_killed_correct=True,\n    min_killed_distance=0,\n    min_captured_correct=True,\n    min_captured_distance=0,\n    killq_correct=True,\n    captureq_correct=True,\n    killcaptureraid_correct=False,\n    airstrike_correct=True,\n    noshotsfired_correct=True,\n    min_leaders_killed_correct=True,\n    min_leaders_killed_distance=0,\n    min_leaders_captured_correct=True,\n    min_leaders_captured_distance=0\n)\n\n\n\nGPT-4 Turbo Scores\n\n\n\nEvalResult(\n    start_date_correct=True,\n    event_type_score=1.0,\n    province_score=1.0,\n    target_group_score=1.0,\n    min_killed_correct=True,\n    min_killed_distance=0,\n    min_captured_correct=True,\n    min_captured_distance=0,\n    killq_correct=True,\n    captureq_correct=True,\n    killcaptureraid_correct=False,\n    airstrike_correct=True,\n    noshotsfired_correct=True,\n    min_leaders_killed_correct=True,\n    min_leaders_killed_distance=0,\n    min_leaders_captured_correct=True,\n    min_leaders_captured_distance=0\n)\n\n\n\nAt first glance it looks like our models performed pretty well. Both got the killcaptureraid status of the article wrong, but otherwise almost perfect across the board.\nDoing this one by one is tedious, of course, and really what we want is an evaluation across a random slice of our articles. So let’s do that next."
  },
  {
    "objectID": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-in-aggregate",
    "href": "posts/2024-06-03-isafpr-evaluating-baseline.html#evaluating-in-aggregate",
    "title": "Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction",
    "section": "Evaluating in aggregate",
    "text": "Evaluating in aggregate\nHere we’ll shuffle our dataframe to take a random sample of our articles, then we’ll iterate over 100 of our rows to get the generated responses for GPT-4. We’ll also time how long it takes to execute the cell so as to have some kind of benchmark for how long the inference takes. We can use that later on to try to beat this baseline.\n\n%%time\n\n# Shuffle the DataFrame\nshuffled_df = df.sample(frac=1, random_state=42)\n\nopenai_responses = {}\n\n# Iterate over the first 5 rows of the shuffled DataFrame\nfor index, row in shuffled_df.head(100).iterrows():\n    openai_responses[index] = query_llm(row[\"text\"], \"gpt-4-turbo\", IsafEvent)\n\nCPU times: user 2.53 s, sys: 34.4 ms, total: 2.57 s\nWall time: 10min 4s\n\n\nAs you can see, this took a total of 10 minutes to run so on average this was around 6 seconds per article. Some articles will be longer than others, of course, so perhaps the aggregate ‘10 minutes per 100 articles’ is a better metric.\nWe now have our results in memory in the openai_responses dictionary. We can now calculate the scores for the results and plot them.\n\nfrom typing import List\nimport matplotlib.pyplot as plt\n\n\ndef calculate_scores(eval_results: List[EvalResult]) -> dict:\n    scores = {}\n    total_count = len(eval_results)\n\n    for attr in EvalResult.__fields__:\n        if attr.endswith(\"_score\"):\n            scores[attr] = (\n                sum(getattr(result, attr) for result in eval_results) / total_count\n            )\n        elif attr.endswith(\"_correct\"):\n            scores[attr] = (\n                sum(getattr(result, attr) for result in eval_results) / total_count\n            )\n        elif attr.endswith(\"_distance\"):\n            if attr == \"min_captured_distance\":\n                capped_values = [\n                    min(max(getattr(result, attr), 0), 1) for result in eval_results\n                ]\n                scores[attr] = 1 - (sum(capped_values) / total_count)\n            else:\n                scores[attr] = 1 - (\n                    sum(getattr(result, attr) for result in eval_results) / total_count\n                )\n\n    return scores\n\n\ndef plot_scores(scores: dict):\n    attributes = list(scores.keys())\n    values = [score * 100 for score in scores.values()]  # Convert scores to percentages\n\n    fig, ax = plt.subplots(figsize=(10, 8))  # Adjust the figure size\n\n    ax.barh(attributes, values)\n    ax.set_xlim(0, 100)  # Set x-axis range from 0 to 100\n    ax.set_xlabel(\"Score (%)\")\n    ax.set_ylabel(\"Attribute\")\n    ax.set_title(\"GPT-4 Turbo Evaluation Scores\")\n\n    for i, v in enumerate(values):\n        ax.text(v + 1, i, f\"{v:.2f}%\", va=\"center\")  # Add percentage symbol to labels\n\n    # Adjust the left margin to remove extra white space\n    plt.subplots_adjust(left=0.2)\n\n    plt.show()\n\n\neval_results = []\nfor article_id, response in openai_responses.items():\n    eval_results.append(evaluate_llm_response(response, df, article_id))\nscores = calculate_scores(eval_results)\nplot_scores(scores)\n\n\n\n\nThis gives us a nice overview of how a mostly-unoptimised approach that just pairs prompting with instructor (i.e. passing in the JSON schema for our expected results) performs on our data. Some scores are pretty good, particularly where it’s just a boolean we’re getting back. For others involving numbers (i.e. numbers of killed or captured) we have much lower scores.\nThe lowest performing score is the check as to whether the text describes a killcaptureraid. Our prompt doesn’t really give much explanation or context as to how we determined this in the original dataset, so maybe an example or two in the prompt would help there.\nSo overall, gpt-4-turbo performs moderately well, but this wouldn’t be acceptable for the purposes of the report that I wrote. The assumption there was that the data was labelled more or less perfectly since I used that data to make accusations and claims about the international military presence in Afghanistan. It simply wouldn’t be good enough to say that we ‘mostly’ got the right numbers when extracting the data from the text. We needed a really high accuracy across the board.\nIf I was doing this properly the obvious next step would be to try to split up the task into discrete / focused parts as I mentioned in the previous post. That way we could let the model take multiple shots at the problem and have less to handle overall in any one reading. I might return to that at a later stage because that might be the way forward with our finetuning work as well (to get best in class results).\nFor now, though, the next step is to prepare our data for finetuning and then to actually run the finetuning. My preference is to select a base model which I can train on my local machine, but since we do have lots of credits with a variety of cloud providers I’ll try to run my finetuning on more powerful machines in the cloud as well."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "",
    "text": "I enjoyed chapter 7 on finetuning. It jams a lot of detail into the 50 pages she takes to explain things. Some areas had more detail than you’d expect, and others less, but overall this was a solid summary / review.\nThe chapter’s essential message can be distilled into three key points:\nSo fine-tuning can be incredibly powerful when applied judiciously, but it requires careful consideration of both technical capabilities and organisational readiness."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#chapter-overview-and-context",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#chapter-overview-and-context",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Chapter Overview and Context",
    "text": "Chapter Overview and Context\nThis long chapter (approximately 50 pages, much like the others) was notably one of the most challenging for Chip to write. It presents fine-tuning as an advanced approach that moves beyond basic prompt engineering, covering everything from fundamental concepts to practical implementation strategies.\nThe depth and breadth of the chapter reflect the complexity of fine-tuning as both a technical and organisational challenge, though the things she writes about doesn’t really cover the reality of what it’s like to work on these kinds of initiatives within a team."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#core-decision-when-to-fine-tune",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#core-decision-when-to-fine-tune",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Core Decision: When to Fine-tune",
    "text": "Core Decision: When to Fine-tune\nThe decision to fine-tune should never be taken lightly. While the potential benefits are significant, including improved model quality and task-specific capabilities, the chapter emphasises that fine-tuning should be considered a last resort rather than a default approach.\n\nNotable Case Study: Grammarly achieved remarkable results with their fine-tuned T5 models, which outperformed GPT-3 variants despite being 60 times smaller. This example illustrates how targeted fine-tuning can sometimes achieve better results than using larger, more general models.\n\n\nReasons to Avoid Fine-tuning\nThe chapter presents several compelling reasons why organisations might want to exhaust other options before pursuing fine-tuning:\n\nPerformance Degradation: Fine-tuning can actually degrade model performance on tasks outside the specific target domain\nEngineering Complexity: The process introduces significant technical overhead\nSpecialised Knowledge Requirements: Teams need expertise in model training\nInfrastructure Demands: Self-serving infrastructure becomes necessary\nOngoing Maintenance: Requires dedicated policies and budgets for monitoring and updates\n\n\n\nFine-tuning vs. RAG: A Critical Distinction\nOne of the most important conceptual frameworks presented is the distinction between fine-tuning and RAG:\n\nFine-tuning focuses on form - how the model expresses information\nRAG specialises in facts - what information the model can access and use\n\nThis separation provides a clear decision framework, though the chapter acknowledges there are exceptions to this general rule."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#progressive-implementation-workflow",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#progressive-implementation-workflow",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Progressive Implementation Workflow",
    "text": "Progressive Implementation Workflow\n\nThe chapter outlines a thoughtful progression of implementation strategies, suggesting organisations should:\n\nBegin with prompt engineering optimisation\nExpand to include more examples (up to approximately 50)\nImplement dynamic data source connections through RAG\nConsider advanced RAG methodologies\nExplore fine-tuning only after exhausting other options\nConsider task decomposition if still unsuccessful"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#memory-bottlenecks-and-technical-considerations",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#memory-bottlenecks-and-technical-considerations",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Memory Bottlenecks and Technical Considerations",
    "text": "Memory Bottlenecks and Technical Considerations\n\nCritical Memory Factors\nThe chapter emphasises three key contributors to a model’s memory footprint during fine-tuning:\n\nParameter count\nTrainable parameter count\nNumeric representations\n\n\nTechnical Note: The relationship between trainable parameters and memory requirements becomes a key motivator for PEFT (Parameter Efficient Fine Tuning) approaches.\n\n\n\nQuantisation Strategies\nThe chapter provides a detailed examination of quantisation approaches, particularly noting the distinction between:\n\nPost-Training Quantisation (PTQ)\n\nMost common approach\nParticularly relevant for AI application developers\nSupported by major frameworks with minimal code requirements\n\nTraining Quantisation\n\nEmerging approach gaining traction\nAims to optimise both inference performance and training costs"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#advanced-fine-tuning-techniques",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#advanced-fine-tuning-techniques",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Advanced Fine-tuning Techniques",
    "text": "Advanced Fine-tuning Techniques\n\nPEFT Methodologies\nThe chapter identifies two primary PEFT approaches:\n\nAdapter-based methods (Additive):\n\nLoRA emerges as the most popular implementation\nIncludes variants like Dora and qDora from Anthropic\nInvolves adding new modules to existing model weights\n\nSoft prompt-based methods:\n\nLess common but growing in popularity\nIntroduces trainable tokens for input processing modification\nOffers a middle ground between full fine-tuning and basic prompting, so maybe interesting for teams who don’t really want to go too deep into finetuning (?)\n\n\n\n\nModel Merging and Multitask Considerations\nThe chapter presents model merging as an evolving science, requiring significant expertise. Three primary approaches are discussed:\n\nSumming\nLayer stacking\nConcatenation (generally not recommended due to memory implications)\n\n\nThere’s a lot of detail in this section (much more than I’d expected) but it was interesting to read about something that I haven’t much practical expertise with."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#core-approaches-to-model-merging",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#core-approaches-to-model-merging",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Core Approaches to Model Merging",
    "text": "Core Approaches to Model Merging\nThe chapter outlines three fundamental approaches to model merging, each with its own technical considerations and trade-offs:\n\nTechnical Architecture: The three primary merging strategies\n\nSumming: Direct weight combination\nLayer stacking: Vertical integration of model components\nConcatenation: Horizontal expansion (though notably discouraged due to memory implications)\n\n\nThe relative simplicity of these approaches belies their potential impact on model architecture and performance. Particularly interesting is how these techniques interface with the broader challenge of multitask learning."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#multitask-learning-a-new-paradigm",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#multitask-learning-a-new-paradigm",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Multitask Learning: A New Paradigm",
    "text": "Multitask Learning: A New Paradigm\nTraditional approaches to multitask learning have typically forced practitioners into one of two suboptimal paths:\n\nSimultaneous Training\n\nRequires creation of a comprehensive dataset containing examples for all tasks\nNecessitates careful balancing of task representation\nOften leads to compromise in per-task performance\n\nSequential Training\n\nFine-tunes the model on each task in sequence\nRisks catastrophic forgetting as new tasks overwrite previous learning\nRequires careful orchestration of task order and learning rates\n\n\n\nKey Innovation: Model merging introduces a third path - parallel fine-tuning followed by strategic combination. This approach fundamentally alters the landscape of multitask learning optimisation."
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#the-parallel-processing-advantage",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#the-parallel-processing-advantage",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "The Parallel Processing Advantage",
    "text": "The Parallel Processing Advantage\nModel merging enables a particularly elegant solution to the multitask learning challenge through parallel processing:\n\nIndividual models can be fine-tuned for specific tasks independently\nTraining can occur in parallel, optimising computational resource usage\nModels can be merged post-training, preserving task-specific optimisations\n\nThis approach brings several compelling advantages:\n\nStrategic Benefits: - Parallel training efficiency - Independent task optimisation - Flexible deployment options - Reduced risk of inter-task interference"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#practical-implications",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#practical-implications",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Practical Implications",
    "text": "Practical Implications\nWhile the implementation details remain somewhat experimental, the potential applications are significant. Organisations can:\n\nDevelop specialised models in parallel\nOptimise individual task performance without compromise\nMaintain flexibility in deployment architecture\nScale their multitask capabilities more efficiently"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#implementation-pathways",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#implementation-pathways",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Implementation Pathways",
    "text": "Implementation Pathways\nThe chapter concludes with two distinct development approaches:\n\nProgression Path\n\nBegin with the most economical and fastest model\nValidate with a mid-tier model\nPush boundaries with the optimal model\nMap the price-performance frontier\nSelect the most appropriate model based on requirements\n\n\n\nDistillation Path\n\nStart with a small dataset and the strongest affordable model\nGenerate additional training data using the fine-tuned model\nTrain a more cost-effective model using the expanded dataset"
  },
  {
    "objectID": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#final-observations",
    "href": "posts/2025-01-26-notes-on-ai-engineering-chip-huyen-chapter-7:-finetuning.html#final-observations",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning",
    "section": "Final Observations",
    "text": "Final Observations\nThe chapter emphasises that while the technical process of fine-tuning isn’t necessarily complex, the surrounding context and implications are highly nuanced. Success requires careful consideration of business priorities, resource availability, and long-term maintenance capabilities. This holistic perspective is crucial for organisations considering fine-tuning as part of their AI strategy."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html",
    "href": "posts/2022-02-08-robust-python-10.html",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "",
    "text": "We’ve read about enumerations and we’ve read about data classes. Now it’s the turn of classes. Chapter 10 of Patrick Viafore’s excellent book, ‘Robust Python’, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I’ve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps.\nFirst off, for someone who has just learned about data classes, how would you explain what is new or distinct when it comes to classes? They’re slightly different syntactically, with classes requiring you to write a bit more boilerplate. Compare the following:\nYou can note how it seems like the data class version is much more readable and involves less boilerplate to achieve the same effect, and for a simple example like this you’re probably right. The difference, and where classes make sense and shine, is when you have a conceptual grouping or type that includes some notion of invariants."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#what-is-an-invariant",
    "href": "posts/2022-02-08-robust-python-10.html#what-is-an-invariant",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "What is an invariant?",
    "text": "What is an invariant?\nMost of this chapter is about invariants and how they relate to classes, and I’ll admit I had never heard of the concept before reading in this book. An invariant is defined as “a property of an entity that remains unchanged throughout the lifetime of that entity.” You can think of it as some kind of context or a property about that particular type that you need to encode and that won’t change.\nThe book gives a pizza example (where a Pizza object could encode that in its list of toppings, the cheese could only be the final topping (i.e. on top) of the pizza). An alternative might be some kind of rule relating to an ID number, where either it must be unique to some kind of specification, or where the ID must conform to some kind of specification.\nEven with this rudimentary definition, you can see how there might be some advantages to being able to account for these rules and properties of the object type. (With data classes, you don’t have as much flexibility to specify all these nuances.) So what happens when you’re instantiating a class and you hit one of those scenarios where your contextual rules dictate that something can’t happen? (i.e. someone tries to create a Pizza object that has cheese as the bottom-layer topping) The book offers up two options:\n\nThrow an exception — this will break you out of the code flow and prevent the object from being constructed\nDo something to make the data fit — you can perform some kind of transformation which sees the cheese ingredient as being forced onto the top layer of the pizza toppings (or whatever is the equivalent for your specific scenario)\n\nNote that the kinds of restrictions posed by these invariants are things that can’t fully be captured by the typing system. We’ve covered type hints and how they can help make your code more robust, but types don’t help much when it comes to the order of a list, for example."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#why-code-around-invariants",
    "href": "posts/2022-02-08-robust-python-10.html#why-code-around-invariants",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Why code around invariants?",
    "text": "Why code around invariants?\nSo why go to all of this trouble in the first place? How does it benefit to code with the invariants in mind? To start with, it’ll probably help you think through edge cases and exceptions that you could do well to be wary of. The invariants alert you to the fact that arguments passed into functions and methods will not always be in the form that you would ideally like. (As a side note, this might also encourage you to add unit tests.)\nIt will help you keep the code that handles the invariants together instead of mixing it in with the code that instantiates the objects. In general, it will enhance your ability to reason about the code and the concepts that your code reflects. This is important not only for the implementation in code, but for how you think about any particular part and how it relates to the rest of your code base.\nThe goal for all of this: fewer bugs and a more robust system. Yes, it takes a bit more effort to think whether there are implicit or explicit invariants, but doing so makes your code and your system more reliable. In Viafore’s words:\n\n“You’re making an easier API for people to think about, and you reduce the risk of people using your objects incorrectly. […] You never want someone to be surprised when using your code.” (p. 141)"
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#invariants-and-class-consumers",
    "href": "posts/2022-02-08-robust-python-10.html#invariants-and-class-consumers",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Invariants and class consumers",
    "text": "Invariants and class consumers\nThe rest of the chapter is about the implementation consequences of thinking about classes in this invariants-first way. For consumers of the class, how should you ensure that the invariants handled are clear? Aside from the implementation itself (in the constructor), docstrings and code comments are suggested as a means to this end. Of course, README files and documentation in general can serve the same purpose, but it’s best if the context and information about invariants is as close to the code as possible."
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#invariants-and-class-maintainers",
    "href": "posts/2022-02-08-robust-python-10.html#invariants-and-class-maintainers",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Invariants and class maintainers",
    "text": "Invariants and class maintainers\nFor (future) maintainers of the class, unit tests are the way to go. Make sure that the relevant scenarios and invariants are covered by testing code and you will have extra confidence that your object instantiation really does do what you intend. Your code should already be doing the checking for invariants on the instantiation side, but unit tests are a way of ensuring that this is actually the case (and also that these invariants remain covered as the code base continues to evolve.\n(The book offers one way of doing such tests for invariants with contextlib.contextmanager on page 145.)"
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#encapsulation-and-classes",
    "href": "posts/2022-02-08-robust-python-10.html#encapsulation-and-classes",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "Encapsulation and classes",
    "text": "Encapsulation and classes\nAs the final chunk of the chapter, we learn about private, protected and public access to the properties and methods of a class, and how they relate to the maintenance of invariants.\nThis is an important part of the story. As users interface with your class and API, encapsulation is a way to ensure that they update and interact with the these properties in a way that is under your control. For example, even if at instantiation you enforce the Pizza object having cheese as the top-layer topping, what do we have in place to ensure that the user doesn’t just amend the toppings property such that the cheese is the bottom-layer topping (i.e. AFTER instantiation)? Encapsulation — having an entity hide or restrict access to certain properties and actions — is how you handle that.\nThe book goes into a fair amount of detail on the uses of these different levels of access, and introduces the idea of ‘accessors’ and ‘mutators’ as an alternative to the more commonly-used ‘getters’ and ‘setters’.\nRemember, “you use invariants to allow users to reason about your objects and reduce cognitive load.” (p. 151)"
  },
  {
    "objectID": "posts/2022-02-08-robust-python-10.html#so-what-am-i-supposed-to-use",
    "href": "posts/2022-02-08-robust-python-10.html#so-what-am-i-supposed-to-use",
    "title": "What are invariants and how can they help make your Python classes more robust?",
    "section": "So what am I supposed to use?",
    "text": "So what am I supposed to use?\n\nThe end of the chapter offers this really helpful flowchart diagram which summarises the choices that we’ve covered during the previous three chapters. I really want to highlight that this chapter helped me think about classes in a way I hadn’t, despite having been through courses, having read numerous articles and of course coded in this class-oriented fashion for several years.\nThe next few chapters continue onwards by thinking about how to design your interfaces such that they make sense for your users and allow your code base to grow with as few headaches as possible."
  },
  {
    "objectID": "posts/2021-09-08-baseline-mlops-understanding.html",
    "href": "posts/2021-09-08-baseline-mlops-understanding.html",
    "title": "A Baseline Understanding of MLOps",
    "section": "",
    "text": "Next week I’m due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It’s a new field to me. I’ve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there.\nA top-down explanation is probably the best way to think of what we’re doing when we talk about ‘doing MLOps’: we’re doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‘in production’. It isn’t just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve.\nThe kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‘full stack machine learning engineer’, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "",
    "text": "In this blog I want to walk through how I trained my first tokenizer(s) on a small Balochi language corpus. I used the Huggingface Tokenizers library and FastAI / Spacy to get a sense of the interfaces involved. There’s also some naive pre-processing I did to get the corpus into a format that the tokenizer could handle. I’m not sure if this is the best way to do it, but it worked for this first iteration.\nWe can get straight into the implementation details, but the general process was:\nI’ll go through each of these steps in turn."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#load-our-text-corpus",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#load-our-text-corpus",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Load our text corpus",
    "text": "Load our text corpus\nHere I walk through my .txt files and load the paths into a list. You can see we have 4294 files to work with.\n\nimport os\n\n\ndef get_txt_file_paths(directory):\n    txt_file_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".txt\"):\n                file_path = os.path.join(root, file)\n                txt_file_paths.append(file_path)\n    return txt_file_paths\n\n\n# Replace \"directory_path\" with the actual path of the directory you want to search\ndirectory_path = \"../data/raw_text\"\ntxt_paths = get_txt_file_paths(directory_path)\n\nlen(txt_paths)\n\n4294"
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#pre-process-the-texts",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#pre-process-the-texts",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Pre-process the texts",
    "text": "Pre-process the texts\nI still don’t fully have a good sense of the best ways to do this, not least of all because I’m not sure of the tradeoffs for decisions I take. For example, I frequently hear that people remove punctuation during pre-processing, but I’m not sure how that’s helpful. It feels like you’d be removing context more than anything else.\nI had similar thoughts on the removal of numbers, but in the end I removed them along with any a-z or A-Z English-language characters. I also removed excess whitespace.\n\nimport re\n\ndef clean_text(file_path):\n    # Open the file and read it into memory\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n\n    # Remove English-language characters and numbers\n    text = re.sub(r\"[a-zA-Z0-9]\", \"\", text)\n\n    # Remove any excess whitespace\n    text = re.sub(r\"[^\\S\\n]+\", \" \", text)\n\n    return text\n\n\nfor path in txt_paths:\n    cleaned_text = clean_text(path)\n\n    # write the cleaned text to a new file with an incremented filename\n    # write the files all into the '../data/processed_text' directory\n    with open(\n        f'../data/processed_text/{path.split(\"/\")[-1]}', \"w\", encoding=\"utf-8\"\n    ) as file:\n        file.write(cleaned_text)"
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#lessons-learned",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#lessons-learned",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Lessons learned",
    "text": "Lessons learned\nThis first attempt at tokenisation was instructive in a number of respects.\nI didn’t show what was going on under the hood with the FastAI wrapper, but if you look at the source code you’ll see that the line spacy = WordTokenizer() assumes that the base language we’re dealing with is English. You can of course pass in a language code to the WordTokenizer initialization, but since it uses Spacy under the hood here and since Balochi isn’t represented as an official language supported by Spacy, when you’re basically out of luck. You hit an error and you can either continue using simplistic algorithms like the ones demonstrated above (essentially splitting on word delimiters) or you can abandon FastAI and dive into Spacy.\nAt that point, you’ll have to start implementing a whole bunch of things yourself in order to get going quickly. For example, you’ll ideally want to come up with all the list of punctuation marks, stop words, stemming rules and so on that I mentioned last time. (It might well be that it’s possible to get up and running faster for a non-standard language with Spacy, but it wasn’t clear to me how to do that.)\nI do actually now intend to make a contribution to the Spacy repo to have Balochi represented there, and to open the window for others to contribute to the language metadata directly, but that didn’t help me in the moment. You’ll notice that I didn’t show how you can save a serialized version of the Spacy/FastAI tokeniser because I wasn’t able to figure out how to get access to the underlying Spacy object. I’m sure it’s possible since I can read the Spacy API documentation showing which method to use but FastAI didn’t itself expose this functionality directly.\nMy initial impression from working with both libraries and spending some time with their documentation is that Spacy might end up being more useful for low-resource languages given the extent to which they support a more complete range of old-school NLP methods and techniques. That said, the 🤗 Tokenizers library was much easier to get up and running with and I think it’s a great option for anyone who wants to get started quickly with tokenization. They support most of the major algorithms you’d ever need to use and if they don’t you can always implement something yourself to extend it."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#balochi-tokenizers-on-huggingface-hub",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#balochi-tokenizers-on-huggingface-hub",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Balochi Tokenizers on Huggingface Hub",
    "text": "Balochi Tokenizers on Huggingface Hub\nI’m still working through a way to open up the core dataset (along with constructing as I work), but this first iteration of the tokenizer is now available over on the Huggingface Hub. You can load it for use with the single line:\ntokenizer = Tokenizer.from_file(\"../models/30k-balochi-tokenizer.json\")\nThe organisation is something I created together with some Balochi colleagues who expressed an interest in working together on this effort. I’m really happy to have made their acquaintance and I hope I’ll be able to make steady progress on this project with their help. (If you’re interested in contributing, please request access to the organization and/or contact me for more information.)\nWhile creating the tokenizer repository, I also noted how Balochi (as with Spacy) is not represented as a language recognised by the metadata tracking languages used on the Hub. Frustratingly, you’re asked to input an ISO-639-1 two-letter code to represent the language of the model, but of course Balochi doesn’t have one of those. Balochi only has an ISO-693-2 and ISO-693-3 code. I’ll have to see how we can get Balochi represented on the Hub given all this. It can’t be the first time that this has happened."
  },
  {
    "objectID": "posts/2023-06-03-training-custom-balochi-tokenizer.html#next-steps",
    "href": "posts/2023-06-03-training-custom-balochi-tokenizer.html#next-steps",
    "title": "Tokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy",
    "section": "Next steps",
    "text": "Next steps\nNow that I have this first iteration complete, I want to reflect a bit on how to know when the tokenizer is ‘good enough’. In particular, how do you evaluate tokenisers? Are there ways of benchmarking this? There must have been work done on this and I want to understand both what the start of the art is as well as how to know when I’ve reached it.\nI also watched an extremely rewarding talk on low-resource languages (blog notes to follow!) and there was a section in that which stressed the foundational nature of tokenisation as part of language models. It also highlighted a failure mode where bad tokenisation made a model perform very badly on a certain kind of task. So based on this context I would like to understand how to evaluate tokenisers and how to know when I’ve reached a good enough point.\nI also have a grab-bag of odds and ends relating to tokenization (GPU-based tokenization! tiktoken! etc.) that I mean to write up alongside the above."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html",
    "href": "posts/2021-10-25-debugging.html",
    "title": "Some things I learned about debugging",
    "section": "",
    "text": "I’ve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it’d be useful to put down some thoughts about things that I’ve learned along the way."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#logging-printing",
    "href": "posts/2021-10-25-debugging.html#logging-printing",
    "title": "Some things I learned about debugging",
    "section": "Logging & Printing",
    "text": "Logging & Printing\nThese are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them.\nThere are some scenarios where simple print calls aren’t enough. If you’re running code through a series of tests, then the test harness will often consume all output to stdout so you won’t see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers.\nOnce you can see what’s happening at a particular moment, you can see if what you expected to happen at that moment is actually happening."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#breakpoint-your-way-to-infinity",
    "href": "posts/2021-10-25-debugging.html#breakpoint-your-way-to-infinity",
    "title": "Some things I learned about debugging",
    "section": "Breakpoint your way to infinity!",
    "text": "Breakpoint your way to infinity!\nThe breakpoint() function comes built-in with Python. It’s a convenience wrapper around some pdb magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment.\nI wish I had known about this earlier on. It’s extremely useful for understanding exactly how a function or piece of code is being executed."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#come-with-hypotheses",
    "href": "posts/2021-10-25-debugging.html#come-with-hypotheses",
    "title": "Some things I learned about debugging",
    "section": "Come with hypotheses",
    "text": "Come with hypotheses\nIf you don’t have a sense of what you expect to happen, it’s going to be hard to determine if what you’re doing is having any effect or not.\nI’ve been lucky to do some pairing sessions with people as they work through bugs and problems, and I’ve had this ‘come with a hypothesis’ behaviour modelled really well for me.\nIt’s not a panacea; there’s still a lot of work to be done around this, but it’s sort of the foundation, particularly for non-trivial bugs."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#leave-your-assumptions-at-the-door",
    "href": "posts/2021-10-25-debugging.html#leave-your-assumptions-at-the-door",
    "title": "Some things I learned about debugging",
    "section": "Leave your assumptions at the door",
    "text": "Leave your assumptions at the door\nDon’t assume what’s written is what’s actually working. This applies to the code you’re working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place.\nThe rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you’re importing as well. Of course, it’s probably more likely that you’re misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#follow-the-thread-wherever-it-leads",
    "href": "posts/2021-10-25-debugging.html#follow-the-thread-wherever-it-leads",
    "title": "Some things I learned about debugging",
    "section": "Follow the thread wherever it leads",
    "text": "Follow the thread wherever it leads\nThis is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#be-systematic",
    "href": "posts/2021-10-25-debugging.html#be-systematic",
    "title": "Some things I learned about debugging",
    "section": "Be systematic",
    "text": "Be systematic\nI’ve found a few times now, that there are certain moments where I notice I’m far far down the road. I’ll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I’ve made in order to reach this point.\nI’ll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I’m travelling down. I’ll specifically write down all the conditions that need to be present for this bug to present (as far as I know them).\nQuite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn’t, it’s extremely useful in re-grounding myself and reminding me of why I’m going down rabbit hole x or y."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#know-when-to-stop",
    "href": "posts/2021-10-25-debugging.html#know-when-to-stop",
    "title": "Some things I learned about debugging",
    "section": "Know when to stop",
    "text": "Know when to stop\nIn an ideal world you’d get to follow every windy road and to figure out everything that doesn’t make sense. But — and this is again especially true for fast-moving startups — you might not always have time to do that.\nThis is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you’d planned on spending on a particular bug. If you’re finding that it’s taking far longer than expected, and you have other things you’re committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#remember-this-is-the-work",
    "href": "posts/2021-10-25-debugging.html#remember-this-is-the-work",
    "title": "Some things I learned about debugging",
    "section": "Remember: this is the work",
    "text": "Remember: this is the work\nSometimes when I’m fixing bugs I have the feeling that I’m wasting my time somehow, or that I should be doing something more productive. It’s often the case, though, that this is the work. I’m low on experience, but proxy experience that I’ve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers."
  },
  {
    "objectID": "posts/2021-10-25-debugging.html#know-when-to-ask-for-help",
    "href": "posts/2021-10-25-debugging.html#know-when-to-ask-for-help",
    "title": "Some things I learned about debugging",
    "section": "Know when to ask for help",
    "text": "Know when to ask for help\nSometimes there are bugs which turn out to be bigger than you’re able to handle. It’s certainly worth pushing back against that feeling the first few times you feel it. Early on it’s often going to feel like the bug is unsolvable.\nBut some times there are pieces of context you don’t have, which a quick overview of what you’ve done and tried might alert someone more season to the fact that you’re going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice."
  },
  {
    "objectID": "posts/2021-09-18-writing-code.html",
    "href": "posts/2021-09-18-writing-code.html",
    "title": "Writing Code",
    "section": "",
    "text": "I read Daniel Roy Greenfeld’s post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I’ve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time.\nJust like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.)\nFor me, this looks like the following:\n\ncoding at work during the week\nsmaller focused exercises from PythonMorsels, Exercism, LeetCode and AlgoExpert\ncode written while working my way through the fastai course; this will probably manifest as blog posts here as well, outlining some small project I completed along the way.\na bigger project, perhaps a package, that I’ll start building at some point. I have some ideas for things I want to implement. I’ll pick one soon. It’ll probably be related in some way to the fastai coding. I’m thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words.\nsmaller scripts to solve daily problems in my digital life. I’ll store those on my GitHub somewhere and write up the design decisions around the more interesting ones here.\n\nOne thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html",
    "title": "All the things I learned while trending on Hacker News",
    "section": "",
    "text": "My previous two blog posts — here and here — were trending / on the front page of Hacker News, driving over 20,000 new visitors to this blog. Welcome! I learned a few new tricks (and some mistakes I’d made) during the ensuing discussion so I thought I’d share some of these here. Some of them might trigger some mini side-investigations into certain hypotheses, too, which is even more exciting. Let’s dive in."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#temperature-0",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#temperature-0",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Temperature = 0",
    "text": "Temperature = 0\nSome commenters rightly pointed out that setting the temperature to 1 for some of the OpenAI inference meant that I was more likely to have less stable and less factually consistent responses. I also heard back from the OpenPipe team that there was maybe no hard and fast rule on this but that I should experiment around for my specific use case.\nThere was enough strongly-voiced opinions on this that I might see if I can rerun the evals using 0 as the temperature to see how much of a difference it makes."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#function-calling-vs-json-mode-vs-prompt-vs-some-schema-forcing-library",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#function-calling-vs-json-mode-vs-prompt-vs-some-schema-forcing-library",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Function calling vs JSON mode vs prompt vs some schema-forcing library",
    "text": "Function calling vs JSON mode vs prompt vs some schema-forcing library\nIn a previous baseline eval for OpenAI’s models I used instructor to coerce the output into Pydantic objects. This time round I just used a strongly-worded request in the prompt to request a JSON response and turned on JSON mode (with the response_format={\"type\": \"json_object\"} passed into the create method). That was enough to ensure that every response I got back was valid JSON.\nI’ve since been reading about the performance differences between these different responses, and how certain models (like the OpenAI GPT class) do much better with function-calling than with just a prompt and/or JSON mode.\nI’ll be blogging about the differences between these options (and how exactly they work) but I think there’s also enough potential here for me to try this as well in a separate round of reruns of the evals.\nSpecifically: what’s the difference in performance between the prompt that I used (which effectively stuffed the schema into the prompt) and using a more formalised function-calling approach? Given what I’ve read, I suspect function-calling will prove superior, but by how much?"
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#llama3-eos-and-pad-tokens",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#llama3-eos-and-pad-tokens",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Llama3 EOS and PAD tokens",
    "text": "Llama3 EOS and PAD tokens\nI had some helpful comments suggesting there was maybe something untoward going on with these tokens during my Llama3 local finetune. I did set them in my axolotl config, but it’s well possible that something went wrong there. I’m planning to return to some local finetunes (since my credits on the one-click providers is not infinite and I want to make the local setup work) so I will dive into this soon. Llama3 performed really well so it seems there’s just some small bug here."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#the-one-click-finetuned-models-can-be-run-locally",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#the-one-click-finetuned-models-can-be-run-locally",
    "title": "All the things I learned while trending on Hacker News",
    "section": "The one-click finetuned models can be run locally",
    "text": "The one-click finetuned models can be run locally\nI found out that it is possible to download the adapters from places like Predibase and OpenPipe and just set things up to run them locally, but it’s just buried a bit in the docs (or not documented at all.)\nPart of the difficulty with documenting how users can do this is that (thanks to CUDA setup intricacies) there isn’t really an easy one-approach-fits-all option. Docker is maybe the closest to this, but at the moment you have to do some of the legwork yourself."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#others-have-had-similar-success-with-finetuned-models",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#others-have-had-similar-success-with-finetuned-models",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Others have had similar success with finetuned models",
    "text": "Others have had similar success with finetuned models\nThere were a few other links to successes that others had with finetuning models for structured data extraction posted in the HN thread. See this doctoral dissertation. Also this article in Nature. And of course the OG LoRA Land paper."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#controversial-content-means-openai-tries-less-hard",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#controversial-content-means-openai-tries-less-hard",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Controversial content means OpenAI tries less hard?",
    "text": "Controversial content means OpenAI tries less hard?\nOne commenter suggested that the nature of the content might be the reason why OpenAI’s GPT performance wasn’t as good as the finetuned models. My experience of this is that it’s binary — either you get a real response or you get a canned ‘this is too sensitive a topic’ reply — but (s)he suggested that instead of getting rejected I might just get a degraded-in-quality response.\nThis would need some further testing to confirm or deny. A nice little experiment for someone."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#what-about-anthropics-claude-models",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#what-about-anthropics-claude-models",
    "title": "All the things I learned while trending on Hacker News",
    "section": "What about Anthropic’s Claude models?",
    "text": "What about Anthropic’s Claude models?\nI should probably have done this as well, but I just hit up against the timebox I allocated for the evaluation work. I’ll try to do some experiments with Haiku and the new Sonnet 3.5 to see if a mixture of tool use (aka function calling) and stuffing the prompt with more examples might be able to get us to feature parity with the finetuned models. Watch this space."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#data-labelling-issues",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#data-labelling-issues",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Data labelling issues",
    "text": "Data labelling issues\nOne commenter found some inconsistencies in the data labelling around dates. I’ll admit to not really having a good answer around this, but also I didn’t dive into the issues raised too deeply. They showed some examples of where the ‘ground truth’ date assigned to a press release was wrong. There’s of course the possibility I may have made mistakes while doing the labelling, and there might be some cases where press releases were emailed out earlier than when they were published on the website, but that’s much harder to show. I’ll dig into this a bit at some point, though this is lower priority on the ‘next steps’ list."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#you-cant-predict-what-people-want-to-read",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#you-cant-predict-what-people-want-to-read",
    "title": "All the things I learned while trending on Hacker News",
    "section": "You can’t predict what people want to read!",
    "text": "You can’t predict what people want to read!\nThe title I chose was clearly designed to be a bit provocative and/or draw readers in, but it wasn’t hyperbolic. My evals did actually show my finetuned models ‘beating’ GPT-4o. That said, the blog before it, setting up the evals I did, was written in a fairly general way and I was really surprised that people enjoyed reading that one so much. It just goes to show that you just need to keep showing up, writing and publishing and you don’t know what people will like. Most of the time I’m writing just for me anyway, so anything on top is just a bonus.\nAlongside this is the understanding that the things that a Hacker News audience enjoys are not necessarily the same things that the wider world and readership enjoys. That’s worth bearing in mind."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#github-pages-hosting-holds-up",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#github-pages-hosting-holds-up",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Github Pages hosting holds up!",
    "text": "Github Pages hosting holds up!\nMy blog is a Quarto blog hosted on Github Pages. As such I don’t pay anything for this hosting. I was pleasantly surprised that Github Pages did well in scaling up in response to the traffic. There was no slowness or downtime on the site. Good to know that just because you’re using open-source software and free tools that you’re not penalised."
  },
  {
    "objectID": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#next-steps",
    "href": "posts/2024-07-07-all-the-things-i-learned-while-trending-on-hacker-news.html#next-steps",
    "title": "All the things I learned while trending on Hacker News",
    "section": "Next Steps",
    "text": "Next Steps\nMy next effort will be to dive into the deployment side of these finetuned LLMs along with some of the low-hanging fruit mentioned above."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "",
    "text": "TL;DR:\nLast week I received the results for the MU123 module of my Open University Maths degree. I was really happy to have achieved a distinction. The course starts off slowly — one of the reasons I went with the OU — but there was still a lot of work that went into that grade. I used a combination of study techniques to revise and cement my understanding along the way, but a core tenet of how I study anything is spaced repetition. (I actually care so much about spaced repetition that I’m one of the co-founders of the Spaced Repetition Foundation and have written about it a fair bit over on my other blog.) For my maths work thus far this has meant creating a not insignificant quantity of Anki cards. (Just checked and I have 673 cards that I created for that first module.)\nWhen I posted about my degree on LinkedIn, Matt Squire from the always-worth-following FuzzyLabs (seriously, check out their blog!) replied:\nHe was referring to CoachBot, another web tool I built to scratch a personal itch around getting study prompts when learning a language outside a classroom/group environment.\nAlongside my use of Anki to cement the more fact-y parts of the things that I’m studying, I’ve been a consistent user of ChatGPT to supplement the practice questions that my course gives me. As part of the Open University course, you get access to some practice questions that (I think) are generated from templates. The advantage of using those is that the system will grade your answer, but it’s pretty rigid / inflexible to use and you can only practice in the sandbox of one particular unit or topic so you miss out on all the very real benefits of interleaved practice. (Interleaved practice is making sure that you mix up the topics and things that you’re studying so that you switch e.g. from your Chinese vocabulary to your maths to whatever else it is that you’re studying. You’ll learn the materials far better if you do your retrieval practice using interleaving than if you study just one topic in isolation, then another topic in isolation and so on.)\nThere was also no way in the OU system to really get a sense of how strong you are in the different areas beyond just the static once-done-instantly-forgotten end of unit exams and tests. Thus the tool available to me didn’t handle spaced repetition or interleaving, two core parts of what have been shown to be good study techniques / foundations.\nWhat I really wanted was:"
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#introducing-mathsprompt",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#introducing-mathsprompt",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "🧮 Introducing MathsPrompt",
    "text": "🧮 Introducing MathsPrompt\nSo that’s what I built :) MathsPrompt has three core modes. Firstly we have the core, a place you go to review the problems you’re studying:\n\nThis pretty closely replicates the interface you see when using Anki. You get a question to solve and then when you’ve solved it you grade how easy you found the exercise. (Note that all we have here are prompts for things to solve. There aren’t any generated solutions to check against; most of the time all of this can be looked up fairly easily if need be.) Then depending on how easy or hard you found it, you’ll see that prompt or exercise later or sooner. More on that later.\nTo populate the database of questions / prompts, we have the input screen which was actually the first thing I built.\n\nYou type or copy-paste in some maths exercise, give it some tags (per topic name or unit) and it’ll add that question to the database. Then it’ll send that question (embedded in a lovingly-crafted prompt) to ChatGPT to get 5 more similar questions (but with different values / variables) and it’ll then add those (with an autogenerated flag) to the database as well. This adds a little bit of variation into the mix and ensures that I’m actually getting to practice things I haven’t seen before.\nFinally, we have a ‘Focus’ mode where you can pick one of the tags that you want to specifically focus on (in a non-interleaved way) and you’ll just get infinite random questions relating to that particular topic area:\n\nThere are ways I’d like to improve this going forward (see below for those) but for now I’m happy with having this functional prototype to use in my studies. The data (in the form of my database) is completely separated out from the web interface so I can update and improve this without worrying that the questions I’ve created and the answers I’ve inputted will need to be restarted from scratch going forward."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#why-rust",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#why-rust",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "Why Rust?",
    "text": "Why Rust?\nI chose to build the project using Rust as I’ve been reading a couple of books over the past few months and I wanted a way to get my hands dirty. (Those two books, Rust in Action by Tim McNamara and Command-Line Rust by Ken Youens-Clark, were really great in getting me going / started.) Rust comes with a considerable reputation for its steep learning curve but I found that the passive understanding of syntax / workflows I’d developed by reading in those books coupled with ChatGPT was enough to get this project off the ground at a fast pace.\nIt certainly helped that I knew what I wanted to build and had thought through what the database schema as well as the core functionality on a few long train journeys recently with paper and pen in hand. It also probably helped that I’d had some previous exposure to and experience with Golang and that our Python development at work happens in the context of mypy strict mode which is as close as Python gets to being forced to think through your code in terms of types."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#chatgpt-to-generate-question-variations",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#chatgpt-to-generate-question-variations",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "🤖 ChatGPT to generate question variations",
    "text": "🤖 ChatGPT to generate question variations\nI considered going down the path of generating question templates for every question, and then having some random numbers get inserted into those templates, but for a lot of questions that wouldn’t have worked. For instance, within trigonometry there are lots of ways that the variables are somehow interrelated or some values wouldn’t make sense at all. The angles inside a triangle still need to sum up to 180 degrees, for example.\nDespite a few misgivings and hesitations around GPT-4’s mathematical abilities (which have been widely noted) I figured it’s a quicker way to get going with this project than any other. Moreover, if I find I’m having issues with the quality of the autogenerated questions I can also just remove them from the set that get shown to me. It also added an extra piece of complexity to the project (and thus something new to learn) since I had to make those calls to the OpenAI API as part of the app."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#frontend-web-ui",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#frontend-web-ui",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "Frontend / Web UI",
    "text": "Frontend / Web UI\nI studied web development / engineering for 3 years at Launch School yet have very little interest in importing some framework behemoth into this project. Where possible I’m pragmatically and philosophically aligned towards simplicity so I chose just to make my site in raw HTML, CSS and JavaScript. Really nothing special there. Probably lots of things I could do to make the site look unique, but that can come later if at all. I was pleased that I got to use many of the things I’d learnt in the past for the JavaScript part, however!"
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#rust-server-backend",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#rust-server-backend",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "🖥️ Rust Server / Backend",
    "text": "🖥️ Rust Server / Backend\nAll the server logic is captured in a single file and this was a strong GPT-4 collaboration piece. I knew enough to tweak things once I had some basic code, but writing it from scratch is far beyond my personal abilities right now so I just iterated many times to get to this.\nIt’s all pretty simple. Most of the logic is around sending the text of the input question to OpenAI and then parsing whatever gets returned back. The server logic is all standard. I have no idea if actix is the standard for web servers in the Rust world but it was what ChatGPT recommended so I went with it and it was easy to get my heard round setting up the endpoints and plugging in the logic for what happens when requests came in."
  },
  {
    "objectID": "posts/2023-07-23-maths-prompt-rust-project.html#postgresql-database",
    "href": "posts/2023-07-23-maths-prompt-rust-project.html#postgresql-database",
    "title": "Building MathsPrompt: a tool to help me review and practice problems for my degree",
    "section": "💾 PostgreSQL Database",
    "text": "💾 PostgreSQL Database\nI started out with SQLite but then I wanted a few extras that I knew weren’t built in with SQLite (particularly some types) so I just created a local Postgres database, created the tables and populated it all with some dummy data to get started.\nAgain, nothing fancy here. Four tables to hold the questions, the tags for the questions as well as metadata around how well I did when answering each question.\nI considered hosting this online somewhere but that wasn’t necessary either and keeping everything local kept my iteration speed fast. Towards the end I refactored the Rust code to allow you to use any database you want (local or managed/hosted) so this gives me a way to put it all online should I want."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html",
    "href": "posts/2023-06-01-why-tokenisation.html",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "",
    "text": "For the types of machine learning that involve neural networks, the training process generally involves passing data and some weights into a function which we use to continually and iteratively optimise the weights. We hope that by showing lots of examples of the right way to do things (as per our data and annotations) we’ll emerge with a model (i.e. the updated weights) that performs the way we’d expect.\nThis whole process has various kinds of mathematics at its core, some basic calculations and some higher-order ideas to help figure out how to improve the weights. For all this, we need our data to be in a form that can pass through these calculations. We’re in the domain of natural / human languages at the moment, so we need somehow to turn our words into some kind of numerical form. Tokenisation is a big part of that process.\nMost of what goes on with tokenisation is — to some extent — around finding a way to optimise the amount of data we have to feed into our model either during training or inference. We want to do both of these in the most efficient manner possible. Smaller amounts of data needed to train (or faster ways of processing the data) means you can do more with less."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#simple-tokenization",
    "href": "posts/2023-06-01-why-tokenisation.html#simple-tokenization",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "🔡 Simple tokenization",
    "text": "🔡 Simple tokenization\nIf you think about a text string, a naive approach might be to just split it up by character.\n\nsentence = \"Some 10 million people speak the Balochi language, and most of them are located in Iran and Pakistan.\"\nprint(list(sentence))\n\n['S', 'o', 'm', 'e', ' ', '1', '0', ' ', 'm', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 's', 'p', 'e', 'a', 'k', ' ', 't', 'h', 'e', ' ', 'B', 'a', 'l', 'o', 'c', 'h', 'i', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'm', 'o', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', 'm', ' ', 'a', 'r', 'e', ' ', 'l', 'o', 'c', 'a', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'I', 'r', 'a', 'n', ' ', 'a', 'n', 'd', ' ', 'P', 'a', 'k', 'i', 's', 't', 'a', 'n', '.']\n\n\nWe can get the unique characters from our sentence to save a bit of space:\n\nprint(set(list(sentence)))\n\n{'f', 'a', '1', 'm', 'd', 'e', 'k', '.', 'g', 'B', 'c', 's', 'i', 'r', 'u', 't', 'p', 'l', ',', 'I', '0', 'o', 'S', 'h', 'n', ' ', 'P'}\n\n\nWe can save even more space by transforming our sentence into lowercase text:\n\nprint(set(list(sentence.lower())))\n\n{'f', 'a', '1', 'm', 'd', 'e', 'k', '.', 'g', 'c', 's', 'i', 'r', 'u', 't', 'p', 'l', ',', '0', 'o', 'h', 'n', 'b', ' '}\n\n\nFor Balochi this might look something like this:\n\nbalochi_sentence = \" اِدا کسے است کہ انگریزی ءَ گپ جت بہ کنت\"\n# translates to \"Is there someone here who speaks English?\"\nbalochi_chars = set(list(balochi_sentence.lower()))\nprint(balochi_chars)\n\n{'ء', 'ک', 'پ', 'ج', 'َ', 'ب', 'ے', 'ن', 'ر', 'ا', 'س', 'د', 'ِ', 'ی', 'ت', 'ہ', 'ز', ' ', 'گ'}\n\n\nAnd we can get a mapping of characters to integers quite easily from here:\n\nbalochi_char_mapping = {char: index for index, char in enumerate(sorted(balochi_chars))}\nprint(balochi_char_mapping)\n\n{' ': 0, 'ء': 1, 'ا': 2, 'ب': 3, 'ت': 4, 'ج': 5, 'د': 6, 'ر': 7, 'ز': 8, 'س': 9, 'ن': 10, 'َ': 11, 'ِ': 12, 'پ': 13, 'ک': 14, 'گ': 15, 'ہ': 16, 'ی': 17, 'ے': 18}\n\n\nYou can already see some wonkiness in how the sorted mapping is displayed. This derives from the fact that the Balochi script is written from right-to-left and this pattern is not well supported in a world dominated by English.\nThe mapping is what we want, and we can use this to map our original sentence into a sequence of numbers:\n\nbalochi_sentence_ids = [balochi_char_mapping[char] for char in balochi_chars]\nprint(balochi_sentence_ids)\n\n[1, 14, 13, 5, 11, 3, 18, 10, 7, 2, 9, 6, 12, 17, 4, 16, 8, 0, 15]\n\n\nWhen it comes to language, the things we care at the tail end of all our modelling all relate to sequences of words and not characters. While our vocabulary (i.e. our list of unique characters) would be pretty small with character-level tokenization, we’d have some other issues:\n\nloss of semantic meaning – our model would likely find it harder to ‘learn’ the higher level concepts without first finding a way past the idea of words and how they represent meaning in a way that pure characters don’t)\nincreased sequence length – if we think of a sentence as a sequence of words, a sequence of characters would be much longer in sheer numbers. This adds overhead in terms of the complexity of processing and training on the text.\n\nAt the other end of the spectrum we have word-based tokenisation:\n\nbalochi_words = set(balochi_sentence.split())\nprint(balochi_words)\nword_mapping = {word: index for index, word in enumerate(sorted(balochi_words))}\nprint(word_mapping)\nword_ids = [word_mapping[word] for word in balochi_words]\nprint(word_ids)\n\n{'گپ', 'جت', 'بہ', 'اِدا', 'است', 'کہ', 'انگریزی', 'کسے', 'ءَ', 'کنت'}\n{'ءَ': 0, 'است': 1, 'انگریزی': 2, 'اِدا': 3, 'بہ': 4, 'جت': 5, 'کسے': 6, 'کنت': 7, 'کہ': 8, 'گپ': 9}\n[9, 5, 4, 3, 1, 8, 2, 6, 0, 7]\n\n\nThis has the advantage of keeping our data at what feesl like an appropriate level of semantic abstraction, but you can probably imagine that our vocabulary size could well get out of control. If we have enough data, eventually our vocabulary size could grow to hundreds of thousands of items and then we’re going to have the same problem we had with long sequences in character-level tokenisation.\nThere are various ways of dealing with this. The blunt-force aproach would be to discard the words with a low frequency. We could pick some number (100,000 perhaps) and say that we’ll only include the 100,000 most common words from our corpus. Anything else will get replaced with something like “UNK” or “xxunk” that we’ll know isn’t a real word but just signifies that there was a low-frequency word there. This keeps our vocabulary (relatively) limited, but as you can imagine we might lose important information by discarding all those ‘uncommon’ words."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#linguistically-enhanced-tokenization",
    "href": "posts/2023-06-01-why-tokenisation.html#linguistically-enhanced-tokenization",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "📚 Linguistically-enhanced tokenization",
    "text": "📚 Linguistically-enhanced tokenization\nBefore we get to the current best-in-class solution to this problem, it’s worth mentioning that there are some approaches that use some hand-crafted features derived from a linguistic understanding of a particular language.\nFor example, in deciding which words to leave out of tokenization, we might want to ignore ones which tend not to give so much useful information. For English, these are works like “the”, “or” or “a”. (You can get a sense of these words here.\nWe also might want to use ‘stemming’ and/or ‘lemmatisation’ as a way of reducing the total number of words in our vocabulary:\n\nStemming reduces the word to a more basic form, i.e. ‘the stem’. So the words “connection”, “connected” and “connects” might all reduce down to “connect”. Note that this stemmed word might not actually exist in English.\nLemmatisation is similar, but it uses a bit of extra knowledge of the language to reduce the words. For example, “good”, “better” and “best” might all reduce down to “good” even though they are spelled in quite different ways.\n\nBoth stemming and lemmatisation (as far as I know) and some other related techniques require a pre-existing knowledge base to exist and to have been hand-coded or hard-coded into the software you use to process your text. For some languages that’s not a problem, but for Balochi these resources haven’t yet been created. A few years back it might even have been the next step for me in my Balochi project to go ahead and work on preparing these kinds of linguistic features and work using these techniques. They require a considerable amount of expertise in the specific language you’re working on, and I’m assuming they take a long time to put together as well.\nLuckily, there is a technique which allows us the best of many worlds: small(ish) vocabulary and no need for years constructing language-specific lists of words and their roots. Let the CPU handle all that!"
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#subword-tokenisation",
    "href": "posts/2023-06-01-why-tokenisation.html#subword-tokenisation",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "👶 Subword tokenisation",
    "text": "👶 Subword tokenisation\nSubword tokenisation is when you let the computer decide how to figure out the right balance between characters and words when it comes to splitting the text corpus up. The technique seems to have gained popularity for tokenisation in only the last decade, though the original algorithm on which some of it was based dates back to 1994.\nThe basic rule of thumb is this: split the words into the optimum number of pieces given a specific text corpus. So if we had two words, “car” and “cat”, in our corpus, the tokens we might generate from these would be: “ca##” “##r” and “##t”. The ‘##’ means that something can join to the letter from that side. Obviously in this small example, we didn’t really save any space, but for large volumes of data we’re going to generate down to just the right balance between characters and letters.\nThis technique was actually first proposed by Philip Gage as a compression algorithm in 1994, but then presumably rediscovered or reimplemented for tokenisation in a series of updates building on the original idea. There have thus been several implementations of this algorithmic family:\n\nWordPiece (Schuster & Nakajima in 2012) – used in BERT and DistinBERT\nByte Pair Encoding (BPE) (Sennrich in 2015)\nSentence Piece / Unigram (Kudo & Richardson in 2018) – used in XLM-RoBERTa\nBPE Dropout (Provilkov et al. in 2020)\nDynamic programming encoding (DPE) (He et al. in 2020)\n\n(Thanks to Masato Hagiwara for a useful summary of the history and key developments on his blog here.)\nThis is my summary of some of the key differences to bear in mind:\n\nThe key difference between the tokenisation process we’ve seen and subword tokenisation is that now we need a text dataset and a ‘training’ process to ‘learn’ how to split words down into smaller chunks. I’ll get into the specific details of how this works along with some examples for Balochi in the next blog."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#extra-meta-tokens",
    "href": "posts/2023-06-01-why-tokenisation.html#extra-meta-tokens",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "😜 Extra Meta-Tokens",
    "text": "😜 Extra Meta-Tokens\nThere are a few extra tokens that get generated during some of the above tokenisation methods that it’s probably worth talking about now. These tokens are added to the vocabulary of tokens and they represent various contextual information. For example:\n\n\n\n\n\n\n\nToken\nPurpose / Meaning\n\n\n\n\nCLS\n‘classification’. Token prepended to the start of each text chunk.\n\n\nSEP\n‘separate’. Token used to separate sentences inside a text chunk.\n\n\n##\n(mentioned above). Used to denote other tokens can be attached here.\n\n\nBOS\n‘beginning of stream’. Also used to denote the beginning of a sentence.\n\n\nPAD\n‘pad’. A way to make arrays of tokens the same length / size.\n\n\nMASK\n‘mask’. Used to mask a word in a sentence and used in training.\n\n\nxxmaj\nindicates that the next word begins with a capital letter.\n\n\nUNK\n‘unknown’. Used when you need to limit your vocabulary size.\n\n\n\nNote that these aren’t universally used. The xx prefix is something that FastAI uses in its tokenisation to avoid the chance that something like ‘PAD’ is being used as an actual word in the text."
  },
  {
    "objectID": "posts/2023-06-01-why-tokenisation.html#numericalising-the-tokens",
    "href": "posts/2023-06-01-why-tokenisation.html#numericalising-the-tokens",
    "title": "The What, Why, and How of Tokenisation in Machine Learning",
    "section": "🔢 Numericalising the tokens",
    "text": "🔢 Numericalising the tokens\nOnce we have our list of tokens and their ids (see above), it isn’t enough for us just to pass that in for training our models. Neural networks will attach to anything that gives a bit of signal when they are learning from data. If we have a list of tokens and ‘dog’ is assigned the number 3 and ‘cat’ is assigned the number 10, our model might assign some kind of ranking or importance to those numbers. So we have to pass our values in a way that doesn’t lead to this kind of unanticipated signal. The way we do this for language is to ‘one-hot encode’ the values.\nSo instead of:\n\nbalochi_sentence_ids = [word_mapping[word] for word in balochi_sentence.split()]\nprint(balochi_sentence_ids)\n\n[3, 6, 1, 8, 2, 0, 9, 5, 4, 7]\n\n\n…we can generate an array of arrays. For each word in the sentence, we have a subarray that has a length of our vocabulary and then we turn the value in the word’s index to 1 if that’s the word at this point in our sentence. It’ll be easier to see in an example :)\n\nimport torch\nimport torch.nn.functional as F\n\n\ninput_ids = torch.tensor(balochi_sentence_ids)\none_hot_encodings = F.one_hot(input_ids, num_classes=len(balochi_words))\nprint(one_hot_encodings)\n\ntensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]])\n\n\nSo you can see for the first word (i.e. the first subarray) we have a 1 at index 3 and this corresponds exactly to our sentence and the mapping of words. (I hope it’s clear now also why we might want to have some kind of limitation of just how large our vocabulary gets.)\nIn my next post I’ll walk through all of the details showing how you train your own subword tokenizer, compare how it works in two popular Python libraries (Spacy and 🤗 Tokenisers and in general show how all of this fits together in the bigger picture."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "",
    "text": "Had the first of a series of meet-ups I’m organising in which we discuss Chip Huyen’s new book. My notes from reading the chapter follow this, and then I’ll try to summarise what we discussed in the group.\nAt a high-level, I really enjoyed the final part of the chapter where she got into how she was thinking about the practice of ‘AI Engineering’ and how it differs from ML engineering. Also the use of the term ‘model adaptation’ was an interesting way of encompassing all the different things that engineers are doing to get the LLM to better follow their instructions."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#chapter-1-notes",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#chapter-1-notes",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Chapter 1 Notes",
    "text": "Chapter 1 Notes\nThe chapter begins by establishing AI Engineering as the preferred term over alternatives like GenAI Ops or LLM Ops. This preference stems from a fundamental shift in the field, where application development has become increasingly central to working with AI models. The “ops” suffix inadequately captures the breadth and nature of work involved in modern AI applications.\n\nFoundation Models and Language Models\nThe text provides important technical context about different types of language models. A notable comparison shows that while Mistral 7B has a vocabulary of 32,000 tokens, GPT-4 possesses a much larger vocabulary of 100,256 tokens, highlighting the significant variation in model capabilities and design choices.\nTwo primary categories of language models are discussed:\n\nMasked Language Models (like BERT and modern BERT variants)\nAutoregressive Language Models (like those used in ChatGPT)\n\nThe term “foundation model” carries dual significance, referring both to these models’ fundamental importance and their adaptability for various applications. This terminology also marks an important transition from task-specific models to general-purpose ones, especially relevant in the era of multimodal capabilities."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#ai-engineering-vs-traditional-approaches",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#ai-engineering-vs-traditional-approaches",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "AI Engineering vs Traditional Approaches",
    "text": "AI Engineering vs Traditional Approaches\nAI Engineering differs substantially from ML Engineering, warranting its distinct terminology. The key distinction lies in its focus on adapting and evaluating models rather than building them from scratch. Model adaptation techniques fall into two main categories:\n\nPrompt-based techniques (prompt engineering) - These methods adapt models without updating weights\nFine-tuning techniques - These approaches require weight updates\n\nThe shift from ML Engineering to AI Engineering brings new challenges, particularly in handling open-ended outputs. While this flexibility enables a broader range of applications, it also introduces significant complexity in evaluation and implementation of guardrails."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#the-ai-engineering-stack",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#the-ai-engineering-stack",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "The AI Engineering Stack",
    "text": "The AI Engineering Stack\nThe framework consists of three distinct layers:\n\n1. Application Development Layer\n\nFocuses on prompt crafting and context provision\nRequires rigorous evaluation methods\nEmphasizes interface design and user experience\nPrimary responsibilities include evaluation, prompt engineering, and AI interface development\n\n\n\n2. Model Development Layer\n\nProvides tooling for model development\nIncludes frameworks for training, functioning, and inference optimisation\nRequires systematic evaluation approaches\n\n\n\n3. Infrastructure Layer\n\nHandles model serving\nManages underlying technical requirements"
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#planning-ai-applications",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#planning-ai-applications",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Planning AI Applications",
    "text": "Planning AI Applications\nThe chapter outlines a modern approach to AI application development that differs significantly from traditional ML projects. Rather than beginning with data collection and model training, AI engineering often starts with product development, leveraging existing models. This approach allows teams to validate product concepts before investing heavily in data and model development.\nKey planning considerations include:\n\nSetting appropriate expectations\nDetermining user exposure levels\nDeciding between internal and external deployment\nUnderstanding maintenance requirements\n\nA notable insight is the “80/20” development pattern: while reaching 80% functionality can be relatively quick, achieving the final 20% often requires equal or greater effort than the initial development phase."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#evaluation-and-implementation-challenges",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#evaluation-and-implementation-challenges",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Evaluation and Implementation Challenges",
    "text": "Evaluation and Implementation Challenges\nThe chapter emphasises that working with AI models presents unique evaluation challenges compared to traditional ML systems. This complexity stems from:\n\nThe open-ended nature of outputs\nDifficulty in implementing strict guardrails\nChallenges in type enforcement\nThe need for comprehensive evaluation strategies"
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#data-and-model-adaptation",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#data-and-model-adaptation",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Data and Model Adaptation",
    "text": "Data and Model Adaptation\nThe text discusses how data set engineering and inference optimisation, while still relevant, take on different forms in AI engineering compared to traditional ML engineering. The focus shifts from raw data collection and processing to effective model adaptation and deployment strategies."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#modern-development-paradigm",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#modern-development-paradigm",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Modern Development Paradigm",
    "text": "Modern Development Paradigm\nA significant paradigm shift is highlighted in the development approach: unlike traditional ML engineering, which typically begins with data collection and model training, AI engineering enables a product-first approach. This allows teams to validate concepts using existing models before committing to extensive data collection or model development efforts."
  },
  {
    "objectID": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#discussion-summary",
    "href": "posts/2025-01-19-notes-on-ai-engineering-chapter-1.html#discussion-summary",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 1",
    "section": "Discussion summary",
    "text": "Discussion summary\nThe conversation started with a bit on how AI Engineering represents an interesting shift in the software engineering landscape, potentially opening new career paths for traditional software engineers. While developers may not need deep mathematical knowledge of derivatives and linear algebra upfront, there’s a growing recognition that understanding how AI systems behave - their constraints and opportunities - is becoming increasingly valuable.\nA key tension emerged in the discussion around enterprise adoption. While there’s significant enthusiasm around AI applications, particularly on social media where developers showcase apps with substantial user bases, enterprise companies often maintain their traditional team structures. This creates an interesting dynamic where companies might maintain their existing ML engineering teams while simultaneously forming new “tiger teams” focused on generative AI initiatives, leading to organisational friction.\nThe group discussed how while it’s now possible for software engineers to quickly build AI applications by calling APIs, they often hit limitations that require deeper understanding. This raises questions about whether the “shallow” approach of purely application-level development is sustainable, or whether engineers will inevitably need to develop deeper technical knowledge around model behaviour, evaluation, and fine-tuning.\nA particularly notable challenge discussed was handling the non-deterministic nature of AI systems. Traditional software engineering practices, like unit testing, don’t translate cleanly to systems where outputs can vary even with temperature set to zero. This highlights how AI Engineering requires new patterns and practices beyond traditional software engineering approaches.\nThe discussion also touched on evaluation techniques, including the use of log probabilities to understand model confidence and improve prompts. This represents an emerging area where traditional ML evaluation meets new challenges in assessing large language model outputs."
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\nmatplotlib.rc('image', cmap='Greys')"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#get-the-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#get-the-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Get the data",
    "text": "Get the data\nFirst thing to do: we’ll download the MNIST dataset for us to use as our example.\n\npath = untar_data(URLs.MNIST_SAMPLE)\n\n\n# what does this do?\nPath.BASE_PATH = path\n\n\npath.ls()\n\n(#3) [Path('valid'),Path('labels.csv'),Path('train')]\n\n\n\n# see what's inside the training folder\n(path / 'train').ls()\n\n(#2) [Path('train/7'),Path('train/3')]\n\n\n3 and 7 are the labels or targets of our dataset. We are trying to predict whether, given a particular image, we are looking at a 3 or a 7.\n\nthrees = (path / 'train'/'3').ls().sorted()\nsevens = (path / 'train'/'7').ls().sorted()\nthrees\n\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]\n\n\n\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nim3\n\n\n\n\nNow we’re getting a slice of the image to see how it is represented underneath as numbers.\nThe first index value is the rows we want, and then we get the columns.\n\narray(im3)[4:10,4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\narray(im3)[4:10,4:15]\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254],\n       [  0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253],\n       [  0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10],\n       [  0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0],\n       [  0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43]], dtype=uint8)\n\n\nWe can also have the same as a tensor. This is a PyTorch object which will allow us to get the benefits of everything PyTorch has to offer (plus speed optimisation if we use a GPU).\n\ntensor(im3)[4:10,4:15]\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254],\n        [  0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253],\n        [  0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10],\n        [  0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0],\n        [  0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43]], dtype=torch.uint8)\n\n\nIt basically looks the same except for the name of the object at this moment.\nIf we plot out the values of a different slice we can visualise how the numbers are used to output the actual image.\n\nim3_tensor = tensor(im3)\ndf = pd.DataFrame(im3_tensor[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n  \n    \n       \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n      12\n      13\n      14\n      15\n      16\n      17\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      29\n      150\n      195\n      254\n      255\n      254\n      176\n      193\n      150\n      96\n      0\n      0\n      0\n    \n    \n      2\n      0\n      0\n      0\n      48\n      166\n      224\n      253\n      253\n      234\n      196\n      253\n      253\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      3\n      0\n      93\n      244\n      249\n      253\n      187\n      46\n      10\n      8\n      4\n      10\n      194\n      253\n      253\n      233\n      0\n      0\n      0\n    \n    \n      4\n      0\n      107\n      253\n      253\n      230\n      48\n      0\n      0\n      0\n      0\n      0\n      192\n      253\n      253\n      156\n      0\n      0\n      0\n    \n    \n      5\n      0\n      3\n      20\n      20\n      15\n      0\n      0\n      0\n      0\n      0\n      43\n      224\n      253\n      245\n      74\n      0\n      0\n      0\n    \n    \n      6\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      249\n      253\n      245\n      126\n      0\n      0\n      0\n      0\n    \n    \n      7\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      14\n      101\n      223\n      253\n      248\n      124\n      0\n      0\n      0\n      0\n      0\n    \n    \n      8\n      0\n      0\n      0\n      0\n      0\n      11\n      166\n      239\n      253\n      253\n      253\n      187\n      30\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9\n      0\n      0\n      0\n      0\n      0\n      16\n      248\n      250\n      253\n      253\n      253\n      253\n      232\n      213\n      111\n      2\n      0\n      0\n    \n    \n      10\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      43\n      98\n      98\n      208\n      253\n      253\n      253\n      253\n      187\n      22\n      0\n    \n  \n\n\n\nNote that the rest of this is 28x28 pixels for the whole image, and each pixel can contain values from 0 to 255 to represent all the shades from white to black."
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#approach-1-pixel-similarity",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#approach-1-pixel-similarity",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "APPROACH 1: PIXEL SIMILARITY",
    "text": "APPROACH 1: PIXEL SIMILARITY\n\nFind the average pixel value for each pixel of the 3s and 7s\nUse this ‘ideal’ 3 and 7 to compare a single image that we want to know whether it’s a 3 or a 7\n\n\nthree_tensors = [tensor(Image.open(img)) for img in threes]\nseven_tensors = [tensor(Image.open(img)) for img in sevens]\nlen(three_tensors), len(seven_tensors)\n\n(6131, 6265)\n\n\nWe can view these images with the fastai show_image function. This takes a tensor and makes it viewable in a Jupyter Notebook:\n\nshow_image(three_tensors[8]);\n\n\n\n\n\ntorch.stack(three_tensors)[0][4:15, 4:15]\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,  42, 118, 219, 166],\n        [  0,   0,   0,   0,   0,   0, 103, 242, 254, 254, 254],\n        [  0,   0,   0,   0,   0,   0,  18, 232, 254, 254, 254],\n        [  0,   0,   0,   0,   0,   0,   0, 104, 244, 254, 224],\n        [  0,   0,   0,   0,   0,   0,   0,   0, 207, 254, 210],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  84, 206, 254],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 209],\n        [  0,   0,   0,   0,   0,   0,   0,   0,  91, 137, 253],\n        [  0,   0,   0,   0,   0,   0,  40, 214, 250, 254, 254],\n        [  0,   0,   0,   0,   0,   0,  81, 247, 254, 254, 254],\n        [  0,   0,   0,   0,   0,   0,   0, 110, 246, 254, 254]], dtype=torch.uint8)\n\n\nIt’s fairly common to have to convert a collection (i.e. in our case, a list) of tensors into a single tensor object with an extra dimension, so that’s why we have the torch.stack method which takes a collection and returns a rank-3 tensor.\n\n\n\n\n\n\nTip\n\n\n\nYou’ll see here that we also convert our values into floats and normalise them (i.e. divide by 255) since this will help us at a later stage.\n\n\n\nstacked_threes = torch.stack(three_tensors).float()/255\nstacked_sevens = torch.stack(seven_tensors).float()/255\n\nWe can see that the first axis of this tensor is our 6131 images, and each image has 28x28 pixels. The shape tells you the length of each axis.\n\nstacked_threes.shape\n\ntorch.Size([6131, 28, 28])\n\n\nNote: the length of a tensor’s shape is its rank. We have a rank-3 tensor:\n\nlen(stacked_threes.shape)\n\n3\n\n\n\n# you can get the same value by using `ndim`\nstacked_threes.ndim\n\n3\n\n\n\nRank = the number of axes in a tensor.\nShape = the size of each axis of a tensor.\n\n\nGetting the ideal 3\nWe take our stack of threes (i.e. a rank-3 tensor) and we calculate the mean across the first dimension (i.e. the 6131 individual threes).\nBy the end, you can see that mean3 is actually a 28x28 tensor (i.e. a single image) that represents the ideal version of our threes data.\n\nmean3 = stacked_threes.mean(0)\n\n\nmean3.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean3);\n\n\n\n\n\n# repeat calculations for 7s\nmean7 = stacked_sevens.mean(0)\nshow_image(mean7);\n\n\n\n\n\n\nComparing our ideal numbers with an individual sample\n\nsample_3 = stacked_threes[35]\nsample_7 = stacked_sevens[35]\nshow_image(sample_3);\n\n\n\n\nThere are two main ways we can measure the distance (i.e. similarity) in this context:\n\nthe Mean Absolute Difference aka L1 Norm – the mean of the absolute value of differences (our absolute value replaces negative values with positive values)\nthe Root Mean Squared Error (RMSE) aka L2 Norm – we square all the differences (making everything positive), get the mean of those values, and then square root everything (which undoes all the squaring).\n\n\n# in code it looks like this\n\ndef get_l1_norm(tensor1, tensor2):\n    return (tensor1 - tensor2).abs().mean()\n\ndef get_l2_norm(tensor1, tensor2):\n    return ((tensor1 - tensor2)**2).mean().sqrt()\n\n\nl1_norm_distance_3 = get_l1_norm(sample_3, mean3)\nl2_norm_distance_3 = get_l2_norm(sample_3, mean3)\n\nl1_norm_distance_3, l2_norm_distance_3\n\n(tensor(0.1401), tensor(0.2545))\n\n\n\nl1_norm_distance_7 = get_l1_norm(sample_3, mean7)\nl2_norm_distance_7 = get_l2_norm(sample_3, mean7)\n\nl1_norm_distance_7, l2_norm_distance_7\n\n(tensor(0.1670), tensor(0.3121))\n\n\nThe differences from our sample_3 to the mean3 is less than the differences between our sample_3 and the mean7. This totally makes sense and is what we were expecting!\n\nassert l1_norm_distance_3 < l1_norm_distance_7\nassert l2_norm_distance_3 < l2_norm_distance_7\n\n\n\nThe PyTorch built-in ways of calculating loss\nPyTorch exposes a variety of loss functions at torch.nn.functional which it recommends importing as F. These are available by default under that name in fastai.\n\n# get a sense of what options are available by inspecting the functions that are available to us with `rich`\n\n# !pip install -Uqq rich\n# from rich import inspect as rinspect\n# rinspect(F, methods=True)\n\n\npytorch_l1_norm_distance_3 = F.l1_loss(sample_3.float(), mean3)\npytorch_l1_norm_distance_7 = F.l1_loss(sample_3.float(), mean7)\n\nassert pytorch_l1_norm_distance_3 < pytorch_l1_norm_distance_7\npytorch_l1_norm_distance_3, pytorch_l1_norm_distance_7\n\n(tensor(0.1401), tensor(0.1670))\n\n\nFor the L2 norm, the PyTorch function only calculates the mean squared error loss, so we have to add in the square root at the end ourselves:\n\npytorch_l2_norm_distance_3 = F.l1_loss(sample_3.float(), mean3).sqrt()\npytorch_l2_norm_distance_7 = F.l1_loss(sample_3.float(), mean7).sqrt()\n\nassert pytorch_l2_norm_distance_3 < pytorch_l2_norm_distance_7\npytorch_l2_norm_distance_3, pytorch_l2_norm_distance_7\n\n(tensor(0.3743), tensor(0.4087))\n\n\nWhen to choose one vs the other?\n\nMSE penalises bigger mistakes more than the L1 norm\nMSE is more lenient with small mistakes than the L1 norm"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#load-the-fashion-mnist-dataset-with-pytorch",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#load-the-fashion-mnist-dataset-with-pytorch",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Load the Fashion MNIST dataset with PyTorch",
    "text": "Load the Fashion MNIST dataset with PyTorch\nThe Fashion MNIST dataset was created as a more advanced / difficult (and perhaps interesting) alternative to MNIST for computer vision tasks. It was first released in 2017 and I thought it might be interesting to try the same technique as we tried there on this new data set.\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\n\nlabels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n\n\nI found it a bit hard to extract the data for the images, stored as it was together with the labels. Here you can see me getting the image and the label.\n\nshow_image(training_data[1][0][0]);\n\n\n\n\n\ntraining_data[1][1]\n\n0\n\n\nThere are 60,000 items in this training dataset, and 10 categories, so it makes sense that the set of images for the two categories I extracted would be 6000 each.\n\ntraining_dresses = [item[0][0] for item in training_data if item[1] == 3]\ntraining_pullovers = [item[0][0] for item in training_data if item[1] == 2]\n\nlen(training_dresses), len(training_pullovers)\n\n(6000, 6000)\n\n\n\ntraining_dresses_tensor = torch.stack(training_dresses)\ntraining_pullovers_tensor = torch.stack(training_pullovers)\n\n\ntraining_dresses_tensor.shape\n\ntorch.Size([6000, 28, 28])"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-mean-image-for-a-dress-and-a-pullover",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-mean-image-for-a-dress-and-a-pullover",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Calculate the mean image for a dress and a pullover",
    "text": "Calculate the mean image for a dress and a pullover\n\nmean_training_dress = training_dresses_tensor.mean(0)\nmean_training_pullover = training_pullovers_tensor.mean(0)\n\n\nmean_training_dress.shape\n\ntorch.Size([28, 28])\n\n\n\nshow_image(mean_training_dress);\n\n\n\n\n\nshow_image(mean_training_pullover);\n\n\n\n\nI chose the two because I wondered whether there might be a decent amount of crossover between the two items. You can see even in the ‘mean’ / average versions of the items that they look fairly similar."
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#assemble-the-validation-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#assemble-the-validation-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Assemble the validation data",
    "text": "Assemble the validation data\n\ntest_dresses = [item[0][0] for item in test_data if item[1] == 3]\ntest_pullovers = [item[0][0] for item in test_data if item[1] == 2]\n\nlen(test_dresses), len(test_pullovers)\n\n(1000, 1000)\n\n\n\ntest_dresses_tensor = torch.stack(test_dresses)\ntest_pullovers_tensor = torch.stack(test_pullovers)\n\n\ntest_dresses_tensor.shape\n\ntorch.Size([1000, 28, 28])\n\n\nI also extract a single dress and a single pullover to check the loss in the next section.\n\nsample_test_dress = test_dresses_tensor[0]\nsample_test_pullover = test_pullovers_tensor[10]\n\n\nshow_image(sample_test_dress);\n\n\n\n\n\nshow_image(sample_test_pullover);"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-loss-comparing-a-random-pullover-with-validation-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#calculate-the-loss-comparing-a-random-pullover-with-validation-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Calculate the loss comparing a random pullover with validation data",
    "text": "Calculate the loss comparing a random pullover with validation data\n\ndef get_l1_norm(tensor1, tensor2):\n    return (tensor1 - tensor2).abs().mean()\n\ndef get_l2_norm(tensor1, tensor2):\n    return ((tensor1 - tensor2)**2).mean().sqrt()\n\n\nl1_norm_distance_dress = get_l1_norm(sample_test_dress, mean_training_dress)\nl2_norm_distance_dress = get_l2_norm(sample_test_dress, mean_training_dress)\n\nl1_norm_distance_dress, l2_norm_distance_dress\n\n(tensor(0.1134), tensor(0.1766))\n\n\n\nl1_norm_distance_pullover = get_l1_norm(sample_test_pullover, mean_training_pullover)\nl2_norm_distance_pullover = get_l2_norm(sample_test_pullover, mean_training_pullover)\n\nl1_norm_distance_pullover, l2_norm_distance_pullover\n\n(tensor(0.1713), tensor(0.2220))\n\n\nThe differences from our sample_test_dress to the mean_training_dress is less than the differences between our sample_test_dress and the mean_training_pullover. This totally makes sense and is what we were expecting!\n\nassert l1_norm_distance_dress < l1_norm_distance_pullover\nassert l2_norm_distance_dress < l2_norm_distance_pullover"
  },
  {
    "objectID": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#using-broadcasting-to-check-our-predictions-on-our-validation-data",
    "href": "posts/2022-05-11-fashion-mnist-pixel-similarity.html#using-broadcasting-to-check-our-predictions-on-our-validation-data",
    "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
    "section": "Using broadcasting to check our predictions on our validation data",
    "text": "Using broadcasting to check our predictions on our validation data\nThis function returns the L1 norm loss when calculated between two tensors. Because of the final tuple passed into .mean() (i.e. (-1, -2)), we can actually use this function to calculate the distance between a single image as compared to a full rank-3 tensor.\n\ndef fashion_mnist_distance(tensor1, tensor2):\n    return (tensor1 - tensor2).abs().mean((-1, -2))\n\n\nfashion_mnist_distance(sample_test_dress, mean_training_dress)\n\ntensor(0.1134)\n\n\n\nfashion_mnist_distance(sample_test_dress, mean_training_pullover)\n\ntensor(0.2864)\n\n\nAgain, our dress is ‘closer’ to the mean_training_dress than it is to the mean_training_pullover.\nWe can now create a function that predicts (by way of calculating and comparing these two losses) whether an item is a dress or not.\n\ndef is_dress(x):\n    return fashion_mnist_distance(x, mean_training_dress) < fashion_mnist_distance(x, mean_training_pullover)\n\n\nis_dress(sample_test_dress)\n\ntensor(True)\n\n\n\nis_dress(sample_test_pullover)\n\ntensor(False)\n\n\n…as expected…\nFinally, we can get an overall sense of how well our prediction function is on average. We get it to calculate predictions for the entire test dataset and average them out.\n\ndress_accuracy = is_dress(test_dresses_tensor).float().mean()\n\n\npullover_accuracy = 1 - is_dress(test_pullovers_tensor).float().mean()\n\n\ncombined_accuracy = (dress_accuracy + pullover_accuracy) / 2\ncombined_accuracy, dress_accuracy, pullover_accuracy\n\n(tensor(0.9175), tensor(0.9730), tensor(0.8620))\n\n\nOverall, I think 91% accuracy using this fairly simple mechanism isn’t too bad at all! That said, in the fastai course we’re here to learn about deep learning, so in the next post I will dive more into the beginnings of a more advanced approach to making the same calculation."
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html",
    "href": "posts/2021-09-14-python-versioning-package-managers.html",
    "title": "A Baseline Python Development Setup",
    "section": "",
    "text": "The world of Python versioning (and the downstream package versioning) is wild. This StackOverflow thread gives you a sense of some of the core issues at play. (As an indication of the importance of the issue, even BDFL Guido van Rossum himself has the current second most upvoted answer.)\nFor a really vanilla and close-to-core-python setup, a combination of venv and pip seem to be the way to go. venv is part of the standard library and as such is pretty close to a default option.\nFor something a bit more involved, that handles dependencies and package installation in a slightly more deft manner, the combination of pyenv, pyenv-virtualwrapper and poetry works really well. I’ll detail some of the setup gotchas and usage patterns below."
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-for-versioning-python-itself",
    "href": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-for-versioning-python-itself",
    "title": "A Baseline Python Development Setup",
    "section": "pyenv for versioning Python itself",
    "text": "pyenv for versioning Python itself\npyenv lets you install multiple versions of Python on the same machine. The interface to switch between local versions and whatever you’ve decided will be your global option is pretty intuitive.\nVisit the pyenv github page for more on installation. (If you’re on a Mac you can simply do a brew install pyenv.)\nTo see which versions of Python you have installed locally:\npyenv versions\nTo see versions of Python which are available for installation:\npyenv install —list\nNote that, as I understand it, these versions are not dynamically updated. You get an updated list of new Python versions by updating pyenv, in other words.\nTo install a specific version of Python, and to make it available for use:\npyenv install 3.9.1\nTo set that version of Python as the global version (i.e. running python will use this version by default):\npyenv global 3.9.1\nIf you are in a project directory and wish to only use a particular version of Python in that directory (and its subdirectories):\npyenv local 3.8.2\nThis creates a .python-version file in that directory with the desired local version."
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-virtualenv-for-managing-virtual-environments",
    "href": "posts/2021-09-14-python-versioning-package-managers.html#pyenv-virtualenv-for-managing-virtual-environments",
    "title": "A Baseline Python Development Setup",
    "section": "pyenv-virtualenv for managing virtual environments",
    "text": "pyenv-virtualenv for managing virtual environments\npyenv-virtualenv is a plugin that connects the work of selecting which version of Python to use (through pyenv, which we’ve previously installed) to the work of creating and running virtual environments to keep code contained in quasi-sandbox environments. When you install packages in virtual environments they don’t conflict with other locations where you might have conflicting versions of those same packages installed.\nRead installation instructions and the docs here. (If you installed pyenv with homebrew, be sure to do the same with pyenv-virtualenv).\nTo create a virtual environment for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtual environment directory:\npyenv virtualenv 3.8.2 my-virtual-env-3.8.2\nThis will create a virtual environment based on Python 3.8.2 under $(pyenv root)/versions in a folder called my-virtual-env-3.8.2.\nTo list what virtual environments have been created and are available to use:\npyenv virtualenvs\nAs a common workflow pattern, you’d create your directory and cd into it, and then you can set the virtual environment you just created as the one to use for that directory:\nmkdir test-project && cd test-project\npyenv local my-virtual-env-3.8.2\nThis should change the prompt in your terminal window and you’ll thus know that you’re now working out of that virtual environment. Any time you return to that folder you’ll automatically switch to that environment.\nThe manual way of turning on and off virtual environments is:\npyenv activate env-name\npyenv deactivate env-name\nTo remove a virtual environment from your system:\npyenv uninstall my-virtual-env\n(This is the functional equivalent of removing the directories in $(pyenv root)/versions and $(pyenv root)/versions/{version}/envs.)"
  },
  {
    "objectID": "posts/2021-09-14-python-versioning-package-managers.html#poetry-for-handling-package-installation-and-dependencies",
    "href": "posts/2021-09-14-python-versioning-package-managers.html#poetry-for-handling-package-installation-and-dependencies",
    "title": "A Baseline Python Development Setup",
    "section": "poetry for handling package installation and dependencies",
    "text": "poetry for handling package installation and dependencies\npython-poetry is the latest standard tool for handling package installations and dependency management.\nYou can use poetry without the previous two tools, but really they work best all together. Follow the installation instructions documented on their page to get it going.\nThen update poetry:\npoetry self update\npoetry is one of those tools that’s able to update itself.\nFor basic usage for a new project, you can follow the following workflow. There are two ways to start a new project using poetry: using new or init. For example:\npoetry new some-project-name\nThis will kickstart your new project by creating a bunch of files and a directory structure suitable for most projects, like so:\nsome-project-name\n├── pyproject.toml\n├── README.rst\n├── some-project-name\n│   └── __init__.py\n└── tests\n    ├── __init__.py\n    └── test_some-project-name.py\nYou might want to use a src folder (above the some-project-name in our example) which is fairly commonly used, in which case amend the command as follows:\npoetry new --src some-project-name\npoetry init doesn’t do all the extra work of creating a directory and file structure. It merely creates a pyproject.toml file interactively, using some smart defaults. For a minimal use of poetry, this is definitely the way to go.\nThe add command adds required packages to your pyproject.toml and installs them (along with all their dependencies). It does a lot under the hood to make sure that dependencies are correctly resolving before installing. For example:\npoetry add zenml\nTo add packages only to be used in the development environment:\npoetry add --dev zenml\nTo list all installed packages in your current environment / project:\npoetry show\nTo uninstall a package and remove it (and its dependencies) from the project:\npoetry remove zenml\nTo install all relevant packages and dependencies of a project that you’ve newly cloned into:\npoetry install\nNote that it is possibly worth creating some custom scripts to handle some of the overhead of using these tools, depending on your common development workflows."
  },
  {
    "objectID": "posts/2021-12-29-robust-python-1.html",
    "href": "posts/2021-12-29-robust-python-1.html",
    "title": "What makes code robust?",
    "section": "",
    "text": "We use a lot of modern Python idioms, libraries and patterns at work, so I’ve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, Robust Python: Write Clean and Maintainable Code by Patrick Viafore, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I’ll be working my way through it in the coming months and reflecting on things as I encounter them.\nThe first chapter is mainly about setting the scene for all the technical pieces that follow. Patrick asks the core questions: what is robust code and why do we even care? What problems does it solve to think about code in this way.\nWhat I took away was that a robust codebase emphasises good communication as well as avoiding accidental complexity. A lot has been written about ‘clean code’ and how to achieve this, but it seems that ‘Robust Python’ is arguing for looking a bit further into the future, when you have to come back to refactor your code three months after you wrote it, or when your colleague needs to do the same.\n\n“Writing robust code means deliberately thinking about the future.” (p. 3)\n\nYou write robust code, in other words, because you know that the codebase is going to be changing and shifting and that whatever you write today may need to be modified at a later date:\n\n“A robust codebase is resilient and error-free in spite of constant change.” (p. 4)\n\nWe’re trying to solve for the way that code is often hard to reason about or understand when you’re outside the original moment when it was written. Accordingly, it pays dividends to take a bit of extra time upfront to write code such that it does communicate intent well, and that you haven’t made things more complicated than they need to be.\nMoreover, the communication of intent needs to be done in a way that is asynchronous. The book goes into a bit more detail about why communication practices that require minimal cost and minimal proximity are to be preferred. These include: the code itself, in-code comments, tests, version control history, wikis, and in-project documentation.\nThe first part of the book is all about type annotation, using mypy, and how working with types helps makes your code more robust. We use a lot of this at work so I’m excited to take a deep dive into this."
  },
  {
    "objectID": "posts/2025-01-12-prompt-content-notes-on-chapter-6-prompt-engineering-for-llms.html",
    "href": "posts/2025-01-12-prompt-content-notes-on-chapter-6-prompt-engineering-for-llms.html",
    "title": "Prompt Content: Notes on ‘Prompt Engineering for LLMs’ ch 5",
    "section": "",
    "text": "Chapter 5 of ‘Prompt Engineering for LLMs’ tackles the kinds of things you might want to include in your prompt. (Chapter 6 thinks through the order, structuring and weighting of these different pieces of content, so this is purely about the ‘what’ and not the ‘how’).\nWe split the kinds of content up into static and dynamic content. For static you can think of fixed instructions (‘always respond politely, and in the first person’) whereas dynamic content is assembled on the fly and is (potentially) custom to each user or query. The classic example of dynamic content insertion is your bog standard RAG app.\nThis was a very tactical chapter and I think the rest of the book will keep this up. Most of what gets discussed usually has a little bitesize example to illustrate which I found helpful. This illustration was used to illustrate the differences between static and dynamic context inclusion.\n\nFor static content, the book explores two types: lists of instructions and few-shot prompting. For instructions, we get some useful rules of thumb:\n\nask for positives vs negatives and does instead of don’ts.\ngive reasons for the things you’re asking\navoid absolutes\n\n(and use the system message for these kinds of instruction as most LLMs have been trained to follow them)\nFor few-shot prompting, we dive into all the tradeoffs around how many does ‘few’ mean, what order they should be included, how to get a representative sample and so on. We also consider the tradeoffs and biases that can be subtly introduced with few-shot prompts: issues around scaling the # of examples and accidental picking up of spurious patterns.\nOne thing I’ve often done is to have a ‘best examples first, then the edge cases’ pattern for how I include these examples but we learn how this can bias the LLM to be unduly pessimistic and cautious. (All this stuff is really super tactical / in the weeds, but it’s what I was hoping for…)\nTL;DR: use static prompting where it’s appropriate. use few-shot prompts as well, but be SUPER careful about how these get used and make sure to run evals to see if there are not better orders and amounts of those few-shot examples.\nFor dynamic context, the chapter first thinks through how we might think through which dynamic examples or context to include. This will be influenced by latency requirements, cost considerations + how much can be prepared ahead of time as well as thinking about how to decide which parts of a theoretically infinite amount of extra context you could provide.\nThe bulk of the dynamic context discussion focuses around RAG, and we even get a tiny POC RAG implementation using FAISS as vector storage. We read about the tradeoffs of choosing lexical retrieval (e.g. ElasticSearch + some naive algorithm like Jaccard similarity) vs neural search (e.g. embeddings-driven retrieval with cosine similarity).\nFinally, the chapter closes with a discussion of summarization. What if, e.g., you have so much context that you want to include but you hit the limit? Then you might want to compress it somehow. We read about hierarchical summarisation (which sometimes has to be recursive if there’s too much context).\nWe also get a nice warning about the ‘rumour problem’ which I’ve personally experienced when you summarise a summary (and maybe you summarise a summary of a summary) and things get lost or misrepresented along the way. But for just one level of summarisation that shouldn’t be too big an issue with modern LLMs.\nWe also get into general vs specific summaries. In other words, when you ask the LLM to summarise some text, do you do it with a certain task in mind or do you get a general summary? A general summary is more flexible and can be used in many places, but while specific summaries might give better results for a specific task, you might end up having to rerun your summarization for different tasks.\nNext up: chapter six which tackles exactly how you put all these pieces of context and content together in your prompt."
  },
  {
    "objectID": "posts/2022-10-16-notational-precedence.html",
    "href": "posts/2022-10-16-notational-precedence.html",
    "title": "Avoiding BIDMAS, or how J does notation",
    "section": "",
    "text": "One of the topics that comes up early on in Open University’s MU123 mathematics course is precedence. Those who grew up in English-speaking countries will probably know this as BODMAS or BIDMAS. The order of precedence for execution of a mathematical expression gives us an idea for how to resolve expressions that don’t make sense. For example, 3 + 1 x 4 can either amount to 7 or 16, depending on how when you do the multiplication step.\nBrackets are one way to make things more precise, and that’s probably why they’re the B in BIDMAS and that they go first. We could write 3 + (1 x 4) to make it really clear that we wanted the 1 x 4 sub-expression to be evaluated first.\nWith the rules of precedence, we technically wouldn’t need to add in any brackets because we could (likely) assume that people would follow the standard rules and they would know that we have to evaluate multiplications before we evaluate the additions. So we have a way, but it maybe feels a bit unsatisfactory.\nSome languages or domains, however, have notational rules which don’t rely on a meta-schema of precedence rules like BIDMAS to tell you which expressions should be evaluated first. Instead, the order is determined in other ways, with the option of brackets when needed.\nSeveral of the languages in the APL family, like J, simply evaluate from right to left in the order that expressions are encountered. See this example in J:\n   3 + 1 * 4\n7\n   4 * 3 + 1\n16\nThe order in which the expressions are evaluated determines the answer.\nThinking and reading a bit about these orders of precedence brought me to learn a bit about other traditions of mathematical notation. The one most used and that you’ll be most familiar with is called infix notation i.e. 3 + 4.\nPrefix notation (AKA Polish notation) is when we write + 3 4 (to the same end) and postfix notation (AKA reverse Polish notation) is when we write 3 4 +. (The Polish part relates back to Jan Łukasiewicz, who invented it in 1924.) These kinds of notation are used in Lisp and Clojure, for example.\nWhy would you want to use a notation style like this? Some possible reasons:\n\nthe operands in the expressions can handle arbitrary numbers of arguments, making them more efficient to write\nthey are consistent with the syntax used for functions in computer programming (which can be easier to get your mind round)\nthey’re clearer to read and (mostly) unambiguous, unlike infix notation which (see above) requires a whole order of precedence if you’re not using brackets\nthere’s no confusion or need for precedence rules\nit’s faster for a machine to evaluate, since the way expressions are formulated is much easier to translate into computer code.\n\nSo there you go. I’m unclear whether there are more fundamental benefits to living in the world of post-/prefix notation, and perhaps it’s a little like the people who argue that we’d all be better off if we lived in a base-12 world instead of base-10, but that’s beside the point for now.\nI’ll try to share some more diversions from my mathematics study along the way, hopefully powered by J which I’m trying to get back into."
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "",
    "text": "Large Language Models have transformed how we interact with text, offering capabilities that seemed like science fiction just a few years ago. They can write poetry, generate code, and engage in sophisticated reasoning. Yet surprisingly, one seemingly straightforward task – document translation – remains a significant challenge. This is a challenge I understand intimately, both as a developer and as a historian who has spent years working with multilingual primary sources.\nBefore the era of LLMs, I spent years conducting historical research in Afghanistan, working extensively with documents in Dari, Pashto, and Arabic. This wasn’t just casual reading – it was deep archival work that resulted in publications like “Poetry of the Taliban” and “The Taliban Reader”, projects that required painstaking translation work with teams of skilled translators. The process was time-consuming and resource-intensive, but it was the only way to make these primary sources accessible to a broader audience.\nAs someone who has dedicated significant time to making historical sources more accessible, I’ve watched the rise of LLMs with great interest. These models promise to democratise access to multilingual content, potentially transforming how historians and researchers work with primary sources. However, the reality has proven more complex. Current models, while powerful, often struggle with or outright refuse to translate certain content. This is particularly problematic when working with historical documents about Afghanistan – for instance, a 1984 document discussing the Soviet-Afghan conflict might be flagged or refused translation simply because it contains the word “jihad”, even in a purely historical context. The models’ aggressive content filtering, while well-intentioned, can make them unreliable for serious academic work.\nAfter repeatedly bumping into these limitations in my own work, I built tinbox (shortened from ‘translation in a box’), a tool that approaches document translation through a different lens. What if we had a tool that could handle these sensitive historical texts without balking at their content? What if researchers could quickly get working translations of primary sources, even if they’re not perfect, to accelerate their research process? As a historian, having access to even rough translations of primary source materials would have dramatically accelerated my research process. As a developer, I knew we could build something better than the current solutions.\nThe name “tinbox” is a nod to the simple yet effective nature of the tool – it’s about taking the powerful capabilities of LLMs and packaging them in a way that actually works for real-world document translation needs. Whether you’re a researcher working with historical documents, an academic handling multilingual sources, or anyone needing to translate documents at scale, tinbox aims to provide a more reliable and practical solution."
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#the-hidden-complexity-of-document-translation",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#the-hidden-complexity-of-document-translation",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "The Hidden Complexity of Document Translation",
    "text": "The Hidden Complexity of Document Translation\nThe problem of document translation sits at an interesting intersection of challenges. On the surface, it might seem straightforward – after all, if an LLM can engage in complex dialogue, surely it can translate a document? It can, but there are some edge cases and limitations.\nWhen working with real-world documents, particularly PDFs, we encounter a cascade of complications. First, there’s the issue of model refusal. LLMs frequently decline to translate documents, citing copyright concerns or content sensitivity. This isn’t just an occasional hiccup – it’s a systematic limitation occurring regularly that makes these models unreliable for production use out of the box.\nThen there’s the scale problem. Most documents aren’t just a few paragraphs; they’re often dozens or hundreds of pages long. This runs headlong into the context window limitations of current models. Breaking documents into smaller chunks might seem like an obvious solution, but this introduces its own set of challenges. How do you maintain coherence across chunks? What happens when a sentence spans two pages? How do you handle formatting and structure?\nThe PDF format adds another layer of complexity. Most existing tools rely on Optical Character Recognition (OCR), which introduces its own set of problems. OCR can mangle formatting, struggle with complex layouts, and introduce errors that propagate through to the translation. Even when OCR works perfectly, you’re still left with the challenge of maintaining the document’s original structure and presentation."
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#a-word-about-translations-fidelity-and-accuracy",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#a-word-about-translations-fidelity-and-accuracy",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "A Word About Translations, Fidelity and Accuracy",
    "text": "A Word About Translations, Fidelity and Accuracy\nHaving worked professionally as a translator and worked as an editor for teams of translators, I’m acutely aware of the challenges and limitations of LLM-provided translations. While these models have made remarkable strides, they face several significant hurdles that are worth examining in detail.\nOne of the most prominent issues is consistency. LLMs often struggle to maintain consistent terminology across multiple API calls, which becomes particularly evident in longer documents. Technical terms, product names, and industry-specific jargon might be translated differently each time they appear, creating confusion and reducing the professional quality of the output. This problem extends beyond mere terminology – the writing style and tone can drift significantly between chunks of text, especially when using the chunking approach necessary for longer documents. You might find yourself with a document that switches unexpectedly between formal and informal registers, or that handles technical depth inconsistently across sections.\nEven formatting poses challenges. The way LLMs handle structural elements like bullet points, numbered lists, or text emphasis can vary dramatically across sections. What starts as a consistently formatted document can end up with a patchwork of different styling approaches, requiring additional cleanup work.\nPerhaps more fundamentally, LLMs struggle to find the right balance between literal and fluent translation. Sometimes they produce awkwardly literal translations that technically convey the meaning but lose the natural flow of the target language. Other times, they swing too far in the opposite direction, producing fluid but unfaithful translations that lose important nuances from the source text. This challenge becomes particularly acute when dealing with idioms and cultural references, where literal translation would be meaningless but too free a translation risks losing the author’s intent.\nCultural nuances present another significant challenge. LLMs often miss or mishandle culture-specific references, humour, and wordplay. They struggle with regional variations in language and historical context, potentially stripping away layers of meaning that a human translator would carefully preserve. This limitation becomes even more apparent in specialised fields – medical texts, legal documents, technical manuals, and academic writing all require domain expertise that LLMs don’t consistently demonstrate.\nThe technical limitations of these models add another layer of complexity. The necessity of breaking longer texts into chunks means that broader document context can be lost, making it difficult to maintain coherence across section boundaries. While tools like tinbox attempt to address this through seam repair and sliding window approaches, it remains a significant challenge. Cross-references between different parts of the document might be missed, and maintaining a consistent voice across a long text can prove difficult.\nFormat-specific problems abound as well. Tables and figures might be misinterpreted, special characters can be mangled, and the connections between footnotes or endnotes and their references might be lost. Page layout elements can be corrupted in the translation process, requiring additional post-processing work.\nReliability and trust present another set of concerns. LLMs are prone to hallucination, sometimes adding content that wasn’t present in the original text or filling in perceived gaps with invented information. They might create plausible but incorrect translations or embellish technical details. Moreover, they provide no indication of their confidence in different parts of the translation, no flags for potentially problematic passages, and no highlighting of ambiguous terms or phrases that might benefit from human review.\nWhen it comes to handling source texts, LLMs show particular weakness with poor quality inputs. They struggle with grammatically incorrect text, informal or colloquial language, and dialectal variations. Their handling of abbreviations and acronyms can be inconsistent, potentially introducing errors into technical or specialised documents.\nThe ethical and professional implications of these limitations are significant. There’s often a lack of transparency about the translation process, no clear audit trail for translation decisions, and limited ability to explain why particular choices were made. This raises concerns about professional displacement – not just in terms of jobs, but in terms of the valuable human judgment that professional translators bring to sensitive translations, the opportunity for cultural consultation, and the role of specialist translators in maintaining high standards in their fields.\nThese various limitations underscore an important point: while LLMs are powerful tools for translation, they should be seen as aids to human translators rather than replacements, especially in contexts requiring high accuracy, cultural sensitivity, technical precision, legal compliance, or creative fidelity. The future of translation likely lies in finding ways to combine the efficiency and broad capabilities of LLMs with the nuanced understanding and expertise of human translators.\nSo why build a tool like this given all these problems? I think there’s still a use for something like this in fields where there are few translators and a huge backlog of materials where there’s a benefit to reading them in your own mother tongue, even in a ‘bad’ translation. (That said, having done a decent amount of comparison of outputs for languages like Arabic, Dari and Pashto, I actually don’t find the translations to be terrible, especially for domains like the news or political commentary.) For myself, I am working on a separate tool or system which takes in primary sources and incrementally populates a knowledge database. Having ways to ingest materials written in foreign languages is incredibly important for this, and having a way to do it that doesn’t break the bang (i.e. by using local models) is similarly important."
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#engineering-a-solution",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#engineering-a-solution",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "Engineering a Solution",
    "text": "Engineering a Solution\ntinbox takes a simple approach to solving these issues through two core algorithmic features. The first is what I call “page-by-page with seam repair.” Instead of treating a document as one continuous piece of text, we acknowledge its natural segmentation into pages. Each page is translated independently, but – and this is crucial – we then apply a repair process to the seams between pages.\nThis seam repair is where things get interesting. When a sentence spans a page boundary, we identify the overlap and re-translate that specific section with full context from both pages. This ensures that the translation flows naturally, even across page boundaries. It’s a bit like being a careful tailor, making sure the stitches between pieces of fabric are invisible in the final garment.\nFor continuous text documents (read: a .txt file containing multiple tens of thousands of words), we take a different approach using a sliding window algorithm. Think of it like moving a magnifying glass across the text, where the edges of the glass overlap with the previous and next positions. This overlap is crucial – it provides the context necessary for coherent translation across chunk boundaries.\nThe implementation details matter here. We need to carefully manage memory, handle errors gracefully, and provide progress tracking for long-running translations. The codebase is structured around clear separation of concerns, making it easy to add support for new document types or translation models.\nMoreover, we need to ensure that in the case of failure we’re able to resume without wasting what we spent translating earlier parts of the document."
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#the-engineering-details",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#the-engineering-details",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "The Engineering Details",
    "text": "The Engineering Details\nThe architecture reflects these needs. At its core, tinbox uses a modular design that separates document processing from translation logic. This allows us to handle different document types (PDFs, Word documents, plain text) with specialised processors while maintaining a consistent interface for translation.\nError handling is particularly crucial. Translation is inherently error-prone, and when you’re dealing with large documents, you need robust recovery mechanisms. We implement comprehensive retry logic with exponential backoff, ensuring that temporary failures (like rate limits) don’t derail entire translation jobs.\nFor large documents, we provide checkpointing and progress tracking. This means you can resume interrupted translations and get detailed insights into the translation process. The progress tracking isn’t just about displaying a percentage – it provides granular information about token usage, costs, and potential issues.\n\nPage-by-Page with Seam Repair\nThe page-by-page algorithm handles PDFs by treating each page as a separate unit while ensuring smooth transitions between pages. Pseudocode that can help you understand how this works goes something like this:\ndef translate_with_seam_repair(document, overlap_size=200):\n    translated_pages = []\n    \n    for page_num, page in enumerate(document.pages):\n        # Translate current page\n        current_translation = translate_page(page)\n        \n        if page_num > 0:\n            # Extract and repair the seam between pages\n            previous_end = translated_pages[-1][-overlap_size:]\n            current_start = current_translation[:overlap_size]\n            \n            # Re-translate the overlapping section with full context\n            repaired_seam = translate_with_context(\n                text=current_start,\n                previous_context=previous_end\n            )\n            \n            # Update translations with repaired seam\n            translated_pages[-1] = translated_pages[-1][:-overlap_size] + repaired_seam\n            current_translation = repaired_seam + current_translation[overlap_size:]\n        \n        translated_pages.append(current_translation)\n    \n    return \"\\n\\n\".join(translated_pages)\n\n\nSliding Window for Text Documents\nFor continuous text documents, we use a sliding window approach. Again, pseudocode to help understand how this works goes something like this, though the actual implementation is different:\ndef translate_with_sliding_window(text, window_size=2000, overlap=200):\n    chunks = []\n    position = 0\n    \n    while position < len(text):\n        # Create window with overlap\n        end = min(len(text), position + window_size)\n        window = text[position:end]\n        \n        # Translate window\n        translation = translate_window(window)\n        chunks.append(translation)\n        \n        # Slide window forward, accounting for overlap\n        position = end - overlap\n    \n    return merge_chunks(chunks, overlap)\n\n\nCLI Usage Examples\nThe tool provides a simple command-line interface:\n# Basic translation of a PDF to Spanish\ntinbox --to es document.pdf\n\n# Specify source language and model\ntinbox --from zh --to en --model anthropic:claude-3-5-sonnet-latest chinese_doc.pdf\n\n# Use local model via Ollama for sensitive content\ntinbox --model ollama:mistral-small --to en sensitive_doc.pdf\n\n# Advanced options for large documents\ntinbox --to fr --algorithm sliding-window \\\n       --window-size 3000 --overlap 300 \\\n       large_document.txt"
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#other-notable-features",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#other-notable-features",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "Other notable features",
    "text": "Other notable features\nThe CLI interface for tinbox currently is built on top of litellm so it technically supports most models you might want to use with it, though I’ve only enabled OpenAI, Anthropic, Google/Gemini and Ollama as base providers for now.\nThe Ollama support was one I was keen to offer since translation is such a token-heavy task. I also really worry about the level of sensitivity / monitoring on the cloud APIs and have run into that in the past (particularly with regard to my previous work as a historian working on issues relating to Afghanistan). Ollama-provided local models should solve that issue, perhaps at the expense of access to the very latest and greatest models."
  },
  {
    "objectID": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#things-still-to-be-done",
    "href": "posts/2025-02-16-tinbox:-an-llm-based-document-translation-tool.html#things-still-to-be-done",
    "title": "Tinbox: an LLM-based document translation tool",
    "section": "Things still to be done",
    "text": "Things still to be done\nThere’s lots of improvements still to be made. I’m particularly interested in exploring semantic section detection, which could make the chunking process more intelligent. There’s also work to be done on preserving more complex document formatting and supporting additional output formats.\nCurrently the tool is driven by whatever you tell it to do. Most decisions are in your hands. You have to choose the model to use for translation, notably. I am most interested in using this tool for some other side-projects and for low-resource languages so one of the important things I’ll be doing is to pick sensible defaults depending on the language and input document type you choose.\nFor example, some vision language models like GPT-4o are able to handle translating directly from an image in Urdu to English, the open-source versions (like llama3.2-vision) struggle much more with these kinds of tasks so it’s possible I might even need to insert an intermediary step of transcribe, then translate the transcribed text into English etc. In fact, for highest-fidelity of translation I almost certainly might want to enable that option.\nThe code is available at GitHub, and I welcome contributions and feedback."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "",
    "text": "Finally back on track and reading the next chapter of Chip Huyen’s book, ‘AI Engineering’. Here are my notes on the chapter."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#overview-and-core-philosophy",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#overview-and-core-philosophy",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Overview and Core Philosophy",
    "text": "Overview and Core Philosophy\n\n“Data will be mostly just toil, tears and sweat.”\n\nThis is how we start the chapter :) This candid assessment frames dataset engineering as a discipline that requires both technical sophistication and pragmatic persistence. While the chapter’s placement might have been suitable earlier in the book, its position allows it to build effectively on previously established concepts."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-curation-the-foundation",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-curation-the-foundation",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Data Curation: The Foundation",
    "text": "Data Curation: The Foundation\nData curation addresses various use cases including fine-tuning, pre-training, and training from scratch, with specific considerations for chain of thought reasoning and tool use. The process addresses three fundamental aspects:\n\nData Quality: The equivalent of ingredient quality in cooking\nData Coverage: Analogous to having the right mix of ingredients\nData Quantity: Determining the optimal volume of ingredients\n\n\nQuality Criteria\nData quality encompasses multiple dimensions:\n\nRelevance to task requirements\nConsistency in format and structure\nSufficient uniqueness\nRegulatory compliance (especially critical in regulated industries)\n\n\n\nCoverage Considerations\nCoverage involves strategic decisions about data proportions:\n\nLarge language models often utilize significant code data (up to 50%) in training, which appears to enhance logical reasoning capabilities beyond just coding\nLanguage distribution can be surprisingly efficient (even 1% representation of a language can enable meaningful capabilities)\nTraining proportions may vary across different stages of the training process\n\n\n\nQuantity and Optimization\nA key phenomenon discussed is ossification, where extensive pre-training can effectively freeze model weights, potentially hampering fine-tuning adaptability. This effect is particularly pronounced in smaller models.\nKey quantity considerations include:\n\nTask complexity correlation with data requirements\nBase model performance implications\nModel size considerations (OpenAI notes that with ~100 examples, more advanced models show superior fine-tuning performance)\nPotential for using lower quality or less relevant data for initial fine-tuning to reduce high-quality data requirements\nRecognition of performance plateaus where additional data yields diminishing returns\n\n\n\nData Acquisition Process\nThe chapter provides a detailed example workflow for creating an instruction-response dataset:\n\nInitial dataset identification (~10,000 examples)\nLow-quality instruction removal (reducing to ~9,000)\nLow-quality response filtering (removing 3,000)\nManual response writing for remaining high-quality instructions\nTopic gap identification and template creation (100 templates)\nAI synthesis of 2,000 new instructions\nManual annotation of synthetic instructions\n\nFinal result: 11,000 high-quality examples"
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-augmentation-and-synthesis",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-augmentation-and-synthesis",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Data Augmentation and Synthesis",
    "text": "Data Augmentation and Synthesis\n\nSynthesis Objectives\n\nIncreasing data quantity\nExpanding coverage\nEnhancing quality\nAddressing privacy concerns\nEnabling model distillation\n\n\nNotable Research: An Anthropic paper (2022) found that language model-generated datasets can match or exceed human-written ones in quality for certain tasks.\n\nNote that some teams actually prefer AI-generated preference data due to human fatigue and inconsistency factors.\n\n\nSynthesis Applications\nThe chapter distinguishes between pre-training and post-training synthesis:\n\nSynthetic data appears more frequently in post-training\nPre-training limitation: AI can reshape existing knowledge but struggles to synthesize new knowledge\n\n\n\nLLaMA 3 Synthesis Pipeline\nA comprehensive workflow example:\n\nAI generation of problem descriptions\nSolution generation in multiple programming languages\nUnit test generation\nError correction\nCross-language translation with test verification\nConversation and documentation generation with back-translation verification\n\nThis pipeline generated 2.7 million synthetic coding examples for LLaMA 3.1’s supervised fine-tuning.\n\n\nModel Collapse Considerations\nThe chapter addresses the risk of model collapse in synthetic data usage:\n\nPotential loss of training signal through repeated synthetic data use\nCurrent research suggests proper implementation can avoid collapse\nImportance of quality control in synthetic data generation\n\n\n\nModel Distillation\nNotable example: BuzzFeed’s fine-tuning of Flan T5 using LoRa and OpenAI’s text-davinci-003 generated examples, achieving 80% inference cost reduction."
  },
  {
    "objectID": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-processing-best-practices",
    "href": "posts/2025-02-05-notes-on-ai-engineering-chip-huyen-chapter-8-dataset-engineering.html#data-processing-best-practices",
    "title": "Dataset Engineering: The Art and Science of Data Preparation",
    "section": "Data Processing Best Practices",
    "text": "Data Processing Best Practices\n\nExpert Tip: “Manual inspection of data has probably the highest value to prestige ratio of any activity in machine learning.” - Greg Brockman, OpenAI co-founder\n\n\nProcessing Guidelines\nThe chapter emphasizes efficiency optimization:\n\nOrder optimization (e.g., deduplication before cleaning if computationally advantageous)\nTrial run validation before full dataset processing\nData preservation (avoid in-place modifications)\nOriginal data retention for:\n\nAlternative processing needs\nTeam requirements\nError recovery\n\n\n\n\nTechnical Processing Approaches\nDeduplication strategies include:\n\nPairwise comparison\nHashing methods\nDimensionality reduction techniques\n\nMultiple libraries are referenced (page 400) for implementation.\n\n\nData Cleaning and Formatting\n\nHTML tag removal for signal enhancement\nCareful prompt template formatting, crucial for:\n\nFine-tuning operations\nInstruction tuning\nModel performance optimization\n\n\n\n\nData Inspection\nThe chapter emphasizes the importance of manual data inspection:\n\nUtilize various data exploration tools\nDedicate time to direct data examination (recommended: 15 minutes of direct observation)\nConsider this step non-optional in the process"
  },
  {
    "objectID": "posts/2024-06-02-isafpr-prompting-baseline.html",
    "href": "posts/2024-06-02-isafpr-prompting-baseline.html",
    "title": "Structured Data Extraction for ISAF Press Releases with Instructor",
    "section": "",
    "text": "I’m currently participating in the Maven LLM course / conference. Originally focused on finetuning LLMs, it’s since expanded to encompass a wide range of LLM-related topics. I thought I’d try to work through a small project alongside the course to get some practical experience with fine-tuning LLMs.\nI previously published a dataset from my time working in Afghanistan: the ISAF Press Releases dataset. (See here for a blogpost I wrote describing the dataset in more detail.) Even though it was not really intended as a dataset to be used for any kind of model training, I thought it might serve well to finetune an LLM on top of it. The dataset is made up of press releases from the International Security Assistance Force (ISAF) and I had previously annotated them, extracting out metadata of interest.\nHere’s an example:\nFrom this I extracted the following metadata:\nThere were a few other pieces of metadata captured but you probably get the idea. Check out the dataset’s card which gives full details.\nThere are 4822 such events in the dataset and as you might imagine it took quite a long time to manually annotate all this data. An example like the one above is fairly straightforward but it’s sometimes unclear exactly how many people were involved. Take this press release:\nThe number of people killed is specified as “multiple” and the number of those captured is specified as “dozens”. So that means a minimum of 3 killed and a minimum of 24 captured. But you have to be reading fairly closely to pick all of this up, and it gets even more complicated when they refer to multiple events in the same press release (and so on).\nIt occurred to me recently that it might make for an interesting test of an LLM’s ability to extract this data out of the raw text in a structured format. So ideally I’d input the press release and I’d get out a JSON object (or Pydantic or whatever) which populates all the various fields.\nI’m lucky in that I’ve already lovingly labeled such a large dataset so I can be really confident in the quality which will allow me to focus on the task of finetuning an LLM to do this task.\nSo my learning goals from this project are to:\nFor the project itself, I keep reading (and watching) that you can get GPT-4 level performance on specific focused tasks by finetuning LLMs and I wanted to see how much can be done with limited resources (or just how cherry-picked those public examples actually are.)\nI have a ‘complete’ dataset which includes press releases published after I finished working on my report, so ideally I’ll be able to use the model to label the remaining data (if it’s good enough in terms of accuracy). I’d also like to see how the speed of a finetuned model compares to using something like GPT-4.\nLet’s try this out with a simple prompt and a single example to see how it performs out of the box! We load the dataset first:\nWe can take a peek at the columns in the dataset:\nIf we look at the options for the ‘eventtype’ column, you’ll see that we have some single-word event types at the top, but also some semi-colon-separated event types. We’ll need to handle these a bit differently when we get to finetuning but perhaps let’s ignore that for now.\nWe have everything we need to set up the task and the data structures that will be filled by our LLM. Let’s start with the event type where we just create an enum to store the options:\nWe can create a similar enum to store the provinces in Afghanistan:\nFinally we can create an IsafEvent which is a Pydantic model where we’ll store all the various pieces of data we’re interested in. We include descriptions of the different fields which will help our LLM to understand the data it’s working with.\nNow let’s get a sample article to work with:\nNow we can construct a simple prompt to help guide the LLM to extract the right data:"
  },
  {
    "objectID": "posts/2024-06-02-isafpr-prompting-baseline.html#structured-data-extraction-with-instructor",
    "href": "posts/2024-06-02-isafpr-prompting-baseline.html#structured-data-extraction-with-instructor",
    "title": "Structured Data Extraction for ISAF Press Releases with Instructor",
    "section": "Structured data extraction with Instructor",
    "text": "Structured data extraction with Instructor\nNow we can use Instructor to help ensure that our data is extracted out according to our Pydantic model and just use GPT-3.5 to see how it performs:\n\nimport instructor\nfrom openai import OpenAI\n\n# patch the client to add `response_model` to the `create` method\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nopenai_resp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    response_model=IsafEvent,\n)\n\nprint(openai_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=<EventType.detention: 'detention'>,\n    province=<Province.khost: 'khost'>,\n    target_group='Haqqani facilitator',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\nAs you can see, GPT-3.5 did pretty well! It was able to extract all the data more or less as I’d have hoped. It was able to determine that “Khowst” province in the article was “Khost” province in the schema, and it correctly determined that two individuals were detained. The only thing where I would have done things differently was to state that a minimum of one leader was captured. In this project I didn’t consider a ‘faciliator’ to be a leader so in the original dataset this would have been a 0:\n\ndf[\"minleaderscaptured\"][article_id]\n\n'0'\n\n\nIt’s hard for the LLM to have known that we’re taking that approach without having specified it in the prompt, but we can try again, updating the prompt with that rule:\n\nupdated_query = f\"\"\"\nThe following is a press release issued by ISAF (formerly operating in Afghanistan):\n{article_text}\n\nPlease extract the following information from the press release:\n- The name of the event\n- The start date of the event\n- The end date of the event\n- The event type\n- The province in which the event occurred\n- The target group of the event\n- The minimum number of people killed during the event\n- The minimum number of people captured during the event\n- Whether someone was killed or not during the event\n- Whether someone was captured or not during the event\n- Whether the event was a so-called 'kill-capture raid'\n- Whether an airstrike was used during the event\n- Whether no shots were fired during the event\n- The minimum number of leaders killed during the event\n- The minimum number of leaders captured during the event\n\nSo-called 'facilitators' aren't considered leaders in this task.\n\"\"\"\n\n\nimport instructor\nfrom openai import OpenAI\n\n# patch the client to add `response_model` to the `create` method\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nopenai_resp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    response_model=IsafEvent,\n)\n\nprint(openai_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=<EventType.detention: 'detention'>,\n    province=<Province.khost: 'khost'>,\n    target_group='Haqqani facilitator',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=True,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=0\n)\n\n\n\nUnfortunately it’s still getting it wrong. Let’s try with GPT-4 this time and hope that it’s better at following instructions:\n\nopenai_resp = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    response_model=IsafEvent,\n)\n\nprint(openai_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=<EventType.detention: 'detention'>,\n    province=<Province.khost: 'khost'>,\n    target_group='Haqqani',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=1\n)\n\n\n\nUnfortunately we’re still getting the same response. I could fiddle around with the prompt a bit more to get the result I wanted, but you can see that this approach of encoding all these edge cases into the prompt isn’t going to scale very well. It’s also going to overfit a little to the training data and really what we want is a model that can do well at any new articles we pass into it.\nEven when we try Claude’s Opus model we get the same result, which tells us that for sure we’d have to amend this in the prompt to fix the problem.\n\nfrom anthropic import Anthropic\nimport instructor\n\nclient = instructor.from_anthropic(Anthropic())\n\n# note that client.chat.completions.create will also work\nclaude_opus_resp = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        },\n    ],\n    max_tokens=4096,\n    response_model=IsafEvent,\n)\n\nprint(claude_opus_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detained in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=<EventType.detention: 'detention'>,\n    province=<Province.khost: 'khost'>,\n    target_group='Haqqani',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=1\n)\n\n\n\nAnd again, using Ollama locally with their mixtral model we get the same result:\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nmixtral_resp = client.chat.completions.create(\n    model=\"mixtral\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": query,\n        }\n    ],\n    response_model=IsafEvent,\n)\nprint(mixtral_resp)\n\nIsafEvent(\n    name='Haqqani Facilitator Detention in Khowst',\n    start_date=datetime.date(2009, 12, 11),\n    end_date=datetime.date(2009, 12, 11),\n    event_type=<EventType.detention: 'detention'>,\n    province=<Province.khost: 'khost'>,\n    target_group='Haqqani militants',\n    min_killed=0,\n    min_captured=2,\n    killq=False,\n    captureq=True,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=True,\n    min_leaders_killed=0,\n    min_leaders_captured=1\n)"
  },
  {
    "objectID": "posts/2024-06-02-isafpr-prompting-baseline.html#next-steps",
    "href": "posts/2024-06-02-isafpr-prompting-baseline.html#next-steps",
    "title": "Structured Data Extraction for ISAF Press Releases with Instructor",
    "section": "Next steps",
    "text": "Next steps\nIf there’s one thing the Maven conference has done well it’s to emphasise the importance of getting a solid sense of an initial baseline from which you can (measureably) improve. So what I’d like to do next is to make a simple evaluation of the performance of the different models on this task.\nComing up with a score will be interesting as there are multiple pieces of information to compare. Numbers to numbers is an easy comparison, but what happens when it gets the wrong category? Do I subtract a mark from the score, or do I keep scores for all the different attributes? It isn’t clear to me how best to construct this score.\nIt also occurred to me while writing the above code that when I was doing the annotation I did so in ‘passes’. So I’d read the article and I’d be reading it looking only for the numbers of killed and captured individuals and nothing else. Then I’d reread the article and extract the data and location, and so on with multiple passes. This is to keep me focused on small details as if I tried to make note of everything at a single pass then I’d certainly miss things or get things wrong. So extrapolating out to LLMs (which, to be clear, don’t work like humans when they read text) I’m wondering whether it might make sense to finetune multiple specialised LLMs to be really best in class at extracting one or two data points instead of expecting it to do what I was unable (i.e. extracting everything at a single pass). Obviously it’d be preferable to have a single model, but I’m wondering whether we might push the limits of what an LLM can do at some of the longer or more complex articles. We’ll have to keep that in mind going forward.\nThe one thing I’ll have to make sure to do before I run the evaluation is to make sure that my data structures are set up to match the original dataset. You’ll remember that above we ignored the fact the eventtype field could have multiple types separated by a semicolon. So I’ll have to make sure that my data structures are set up to handle that. I’ll also improve the descriptions of the fields where possible and try to make the prompt a bit more performant.\nIn the next blog I’ll show the results of evaluating some of our baseline LLMs (proprietary and open-source) to see how they perform for this task. Once we have that baseline then we can continue to the task of actually finetuning an LLM."
  },
  {
    "objectID": "posts/2022-01-30-robust-python-8.html",
    "href": "posts/2022-01-30-robust-python-8.html",
    "title": "How and where to use enums in Python",
    "section": "",
    "text": "The second part of Viafore’s ‘Robust Python’ is all about user-created types. We start simple in chapter eight and consider the Enum type as a way of defining a particular restricted set of values. An example might help get us started:\nfrom enum import Enum\n\nclass TrafficLightsState(Enum):\n  RED = \"red\"\n  YELLOW = \"yellow\"\n  GREEN = \"green\"\n  OFF = \"off\"\n\ncurrent_state = TrafficLightsState.GREEN\nprint(current_state.value) # prints 'green'\nWe subclass off Enum and define the pairings of values that belong together. I hope you can see already that this is a readable way to define these values and show that they are part of the same semantic grouping.\nIf we’re using these definitions not because we care about the values themselves but because we want to be able to evaluate whether the state of one particular traffic light is the same as a different traffic light, we can use auto to automatically assign values (ascending integers, by default) in the following way:\nfrom enum import Enum, auto\n\nclass TrafficLightsState(Enum):\n  RED = auto()\n  YELLOW = auto()\n  GREEN = auto()\n  OFF = auto()\n\ncurrent_state = TrafficLightsState.GREEN\nprint(current_state.value) # prints 3\nYou can iterate through your enums or get their length just as if it was a list, too.\nWhile writing the above text, I realised that I was getting confused about the difference between types and classes in Python. It turns out that whatever differences once existed, they aren’t much of a thing any more and to all intents and purposes they’re practically the same thing.\nA lot of the enum-related definitions at work are defined in this file. You can see that we tend not to use auto, though I’m not really sure why. (We don’t ever seem to compare against actual values.)\nIf you want to make sure that the actual values assigned to these grouped constants are unique, you can add the @unique decorator which will enforce that you aren’t duplicating values.\nBetter still for the readability of your code, you can use this collective type in your type annotations. For sure the difference between these two options should be clear:\ndef get_status(some_input: str) -> str:\n    # code goes here\n\ndef get_status(some_input: str) -> TrafficLightsState:\n    # code goes here\nIn the first case, it is far less clear what’s going on.\nNote that if you’re purely looking for a way to restrict the assignation to a particular variable, you can also use the Literal type, introduced in Python 3.8, though remember that it doesn’t help with iteration, runtime checking or map values from name to value. For all that, you’ll want to be using Enum.”\nIf you want a way to combine Enums together, you can subclass from enum.Flag. Consider the case of when you have a list of enums for days of the week, but you want to represent the weekend as a pairing of Saturday and Sunday (if you were in Europe, e.g.). You could do the following:\nfrom enum import Flag, auto\nclass Weekday(Flag):\n    MONDAY = auto()\n    TUESDAY = auto()\n    WEDNESDAY = auto()\n    THURSDAY = auto()\n    FRIDAY = auto()\n    SATURDAY = auto()\n    SUNDAY = auto()\n    \nweekend = Weekday.SATURDAY | Weekday.SUNDAY\nYou can perform bitwise operations on these combined groupings, but note that the values must support bitwise operations. (Strings don’t support them, while integers do.)\nFinally, the chapter covers the special case of IntEnum and IntFlag which allows for the conversion of integer values. This can be confusing and lead to non-robust behaviours, so the book discourages this particular usage.\nNext up is Data Classes, something I’m extremely interested in getting to grips with as it comes up in our codebase at work a decent amount."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nIn the first part of this series, I made the case for why you might want to include some kind of data validation if you’re working on training a model in general, and if your working on object detection in specific. There are many things that can go wrong with your data inputs and you ought to have some kind of safeguards in place to prevent some tricky failures and bugs."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#tldr-for-data-validation-with-great-expectations",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#tldr-for-data-validation-with-great-expectations",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "TL;DR for data validation with Great Expectations",
    "text": "TL;DR for data validation with Great Expectations\n\n👀 Data validation helps give you confidence in the raw ingredients that feed into your models, especially in scenarios where you retrain or fine-tune regularly.\n✅ For object detection problems, there are many ways your data can fail in some silent way. You should want to be aware of when your training data isn’t meeting your assumptions of what it should look like.\n🛠 Great Expectations is a general purpose data validation tool that goes a long way to restoring trust in your data, and their automatic profiling feature is really useful when getting started.\n💪 In this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of Great Expectations to get you started with increasing your confidence in your object detection annotations. I will show you a concrete example where I created some validation rules for my manually-annotated dataset. I then applied those rules to my synthetic dataset in order to validate it."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#initial-notebook-based-setup",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#initial-notebook-based-setup",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "Initial notebook-based setup",
    "text": "Initial notebook-based setup\nIn the last post I showed how you can easily use the Great Expectations library directly on a Pandas DataFrame, manually specifying values you expect to be the case for your data. For example, perhaps your data should always have certain columns, or the values of a certain column should always be a certain type or mostly range between certain values. You can define all these fairly easily, leveraging your domain knowledge of the data.\nIf you know you’re going to want to use Great Expectations as a more fully-fledged part of your pipeline or workflow, you’ll probably want to go through the more extensive setup stages and create a dedicated ‘context’ which can be longer-lasting than just length of your script runtime. Think of the ‘context’ as somewhere all your expectations and configuration of how to access your data is stored.\nFull instructions on how to set all this up can be found in the docs, but for the most part it’s a matter of pip installing Great Expectations, running great_expectations init , and then great_expectations datasource new.\nThat final command will take you through an interactive setup that has you fill in and amend Jupyter notebooks. (I’m not fully sold on the prominence of this workflow that has you spinning up a Jupyter runtime, dynamically editing notebooks and so on, but I found doing it for my project wasn’t as inconvenient as I’d expected. Plus, there are non-interactive and pure Pythonic ways to get everything configured if you need or prefer that.)\nOnce you have your context created and your data sources connected, you can move on to the main course: using the Profiler."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#using-the-great-expectations-profiler",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#using-the-great-expectations-profiler",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "Using the Great Expectations Profiler",
    "text": "Using the Great Expectations Profiler\nSetting up your validations (i.e. your ‘expectations’) for your data can be done in a number of different ways. We saw last time how you can define these manually, but in this post I want to show how you can follow another recommended workflow by allowing the profiler to review your data and to make an initial set of assumptions about the boundaries and patterns embedded in those values.\nNote, as the docs mention, the expectations that are automatically generated from your dataset are “deliberately over-fitted on your data”. This means that if your DataFrame has 10,321 rows, one of the expectations generated will be that datasets due for validation with this suite of expectations will also have exactly 10,321 rows:\n\n“The intention is for this Expectation Suite to be edited and updated to better suit your specific use case - it is not specifically intended to be used as is.” (source)\n\nYou’ll want and have to do a decent amount of manual checking through, amending and updating any expectations that get created during this process. That said, I am finding that it makes a lot of sense to start with some kind of initial baseline of assumptions that can be corrected versus starting from complete zero and building things up purely based on your domain knowledge of the data.\nNeedless to say, this whole process assumes you have a decent grasp on the domain context and have explored your data already. You probably wouldn’t go to the trouble of setting up Great Expectations if you were doing something that required only a quick solution, but it bears repeating that the expectations you define are only as good as your understanding of the limits and underlying realities of your data. This is probably why something like Great Expectations lends itself quite well to a data-centric approach.\nGetting the profiler to work requires a few interlocking abstractions to be created or instantiated:\nexpectation_suite_name = \"redaction_annotations_suite\"\n\nmain_batch_request = RuntimeBatchRequest(\n    datasource_name=\"redaction_data\",\n    data_connector_name=\"default_runtime_data_connector_name\",\n    data_asset_name=\"main_annotations_df\",  # This can be anything that identifies this data_asset for you\n    runtime_parameters={\"batch_data\": main_annotations_df},  # df is your dataframe\n    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n)\n\ncontext.create_expectation_suite(\n    expectation_suite_name=expectation_suite_name, overwrite_existing=True # toggle this as appropriate\n)\nvalidator = context.get_validator(\n    batch_request=main_batch_request, expectation_suite_name=expectation_suite_name\n)\n\nprofiler = UserConfigurableProfiler(profile_dataset=validator)\nsuite = profiler.build_suite()\ncontext.save_expectation_suite(suite) # use this to save your suite in the context for reuse\nThe above code perhaps seems like a lot, but really all you’re doing is getting your data, making the relevant connections between Great Expectations and your context, and then running the profiler so it can work its magic.\n\nYou can’t yet see the specific values that were imputed from your data, but even this high-level output shows you some of the expectations that it’s thinking would be useful to create.\nAt this stage, you’ll want to take some time to review the specific expectations. You’ll want to:\n\nensure that they make sense for your dataset\nremove any of the really rigid expectations (e.g. that any dataset must have exactly the same number of rows)\nuse the inputed expectations as a springboard for any other ideas that might come to mind\n\nNote that this is an essential step to complete before moving forward. You could use the unedited auto-generated expectations suite as your data validation, but it would almost certainly have little use or value for you. The auto-generated suite is a starting place that you need to amend and tailor to your specific situation.\nIn my case, I was able to amend some of the min / max values to more suitable defaults. (You amend these expectations in the .json file that was created inside the expectations subfolder within your context.) I also included some other domain-driven expectations that the profiler couldn’t have known to include. For example, I know from having immersed myself in this data for several months now that most annotations should have a ‘horizontal’ or ‘square’ orientation. Great Expectations doesn’t create this expectation automatically, so I add it to the list of basic assumptions already generated."
  },
  {
    "objectID": "posts/2022-04-26-data-validation-great-expectations-part-2.html#viewing-data-docs-reports-on-validated-data",
    "href": "posts/2022-04-26-data-validation-great-expectations-part-2.html#viewing-data-docs-reports-on-validated-data",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
    "section": "Viewing Data Docs reports on validated data",
    "text": "Viewing Data Docs reports on validated data\nOnce you have a suite of expectations set up to your liking, you can run a checkpoint against your original data just to make sure you haven’t introduced or amended something that doesn’t match up with the original data. You should get no errors at this point.\ncheckpoint_config = {\n    \"name\": \"my_checkpoint\",\n    \"config_version\": 1,\n    \"class_name\": \"SimpleCheckpoint\",\n    \"validations\": [\n        {\n            \"batch_request\": {\n                \"datasource_name\": \"redaction_data\",\n                \"data_connector_name\": \"default_runtime_data_connector_name\",\n                \"data_asset_name\": \"main_annotations_df\",\n            },\n            \"expectation_suite_name\": expectation_suite_name,\n        }\n    ],\n}\ncontext.add_checkpoint(**checkpoint_config)\n\nresults = context.run_checkpoint(\n    checkpoint_name=\"my_checkpoint\",\n    batch_request={\n        \"runtime_parameters\": {\"batch_data\": main_annotations_df},\n        \"batch_identifiers\": {\n            \"default_identifier_name\": \"default_identifier\"\n        },\n    },\n)\n\ncontext.build_data_docs() # builds data docs to inspect the results\nWhat you really want, however, is to run your expectations suite against new data. That’s the real value of what Great Expectations brings, i.e. to check that incoming data due to be added to your larger base dataset conforms to the broad realities of that base dataset.\nIn my case, the first thing I was interested to check was whether the synthetic images I created would match the expectations suite I’d created based off my core hand-annotated data. (Quick context if you haven’t been following the project so far: I have a core dataset which is manually annotated for the objects inside images. I also created two sets of synthetic data to supplement the manual annotations, which boosted my model performance considerably.)\n\nThe web UI is where you can go to get a visual overview of where your data is passing and failing to meet your (great) expectations. You will want (and I will need) to configure your expectations suite to meet the core assumptions you make about your data derived from your particular domain.\nFor my case, some expectations I will add that are specific to my use case:\n\nredaction annotations should mostly be of horizontal orientation\ncontent annotations should mostly be of portrait orientation\nmost images should have only one content annotation\nannotations shouldn’t be larger than the associated image, or positioned outside the boundaries of that image. (Because of how you define them, in reality this is several expectations, but conceptually it’s just one or two).\nthe area taken up by most annotations should be less than half that taken up by the total image\n\n…and so on. I hope it’s clear now how Great Expectations can be a tremendous asset that can give you confidence in your data.\nIn the next and final post of the series, I will explore some other tools that you can consider when performing these kinds of validation. I will also offer my take on when each tool would be appropriate, as well as where they would be appropriate to use within the machine learning workflow and lifecycle."
  },
  {
    "objectID": "posts/saba.html",
    "href": "posts/saba.html",
    "title": "Alex Strick van Linschoten",
    "section": "",
    "text": "from typing import Sequence\n\nfrom google.cloud import vision\n\n\ndef analyze_image_from_uri(\n    image_uri: str,\n    feature_types: Sequence,\n) -> vision.AnnotateImageResponse:\n    client = vision.ImageAnnotatorClient()\n\n    image = vision.Image()\n    image.source.image_uri = image_uri\n    features = [vision.Feature(type_=feature_type) for feature_type in feature_types]\n    request = vision.AnnotateImageRequest(image=image, features=features)\n\n    response = client.annotate_image(request=request)\n\n    return response\n\n\ndef print_text(response: vision.AnnotateImageResponse):\n    print(\"=\" * 80)\n    for annotation in response.text_annotations:\n        vertices = [f\"({v.x},{v.y})\" for v in annotation.bounding_poly.vertices]\n        print(\n            f\"{repr(annotation.description):42}\",\n            \",\".join(vertices),\n            sep=\" | \",\n        )"
  },
  {
    "objectID": "posts/2021-11-21-on-failure.html",
    "href": "posts/2021-11-21-on-failure.html",
    "title": "On failure",
    "section": "",
    "text": "I’ve been working as a machine learning engineer now for a few months now. If there’s one thing that I have found characterises my experience so far, it’s failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand.\nThere hasn’t been a week that’s gone by since I started where I didn’t encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. My last post was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process.\nFailure isn’t fun. My initial reaction to hitting something I don’t understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it’d be a different kind of work.\nTwo posts by Julia Evans are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‘Life in Code’ and ‘Close to the Machine’.\nThe point is this: we are paid to confront this failure. This is the work. Thinking that it’s a distraction from the work — some kind of imaginary world where there are no blockers or failing tests — is the real illusion."
  },
  {
    "objectID": "posts/2023-05-02-logarithms-exponents-mu123.html",
    "href": "posts/2023-05-02-logarithms-exponents-mu123.html",
    "title": "Exponents and Logarithms: a MU123 review",
    "section": "",
    "text": "I just finished a unit of my Open University Maths degree that’s all about exponents and logarithms (unit 13 of MU123). This is the penultimate unit and these final units are where the difficulty has started to stack up for me.\nThe 2020 COVID pandemic was when all of us had lots of exposure to the idea and reality of exponential curves. In fact, it seemed early on like the people who really understood what exponential growth can do were the most worried. Cancer’s another thing where something that keeps on doubling or increasing can cause real havoc when it compounds. The book talks a bit about Moore’s law, though I recall watching a talk at JuliaCon in 2018 where Sophie Wilson spoke about all the ways where we may be edging up at various physical realities these days that might slow things down.\nLogarithms were the other big part of the module, and these were fascinating to work with. I had familiarity with the idea of them, mainly just as a button on a calculator, but it was really interesting to start to play around with all the ways they could be useful. Prior to the widespread availability of calculators they were the de facto way of doing big calculations, since using something like logarithm tables or a slide rule would give you a practically useful approximation of your answer in a way that didn’t require you to do complicated calculations in your head.\nI got hold of both an old copy of a book of logarithm tables as well as a slide rule — old tech ftw — and look forward to becoming more familiar with them once the unit has come to an end in a few weeks.\nThe previous unit (trigonometry) touched on working with radians and (doing some exercises) it was immediately clear why someone would want to use radians instead of degrees as the unit when working with a certain kind of problem. Similarly, for this exponents & logarithms unit, we came across e, Euler’s number, and had some exposure to how there were quite a few places where it made sense to work with e as the base of our logarithms instead of base 10. I really enjoy these parts of mathematics, where abstract concepts or things that people come up with allow us to manipulate ideas and symbols and objects that in turn allow us to solve problems, or think about things in new ways. In many ways these are the things I enjoy the most while studying the course materials.\nWe came up on a few places where we were asked to prove something using various identities that we’d previously explored. For example, we prove that for any base b and any positive numbers x and y, that:\n\\[\\log_b x - \\log_b y = \\log_b(\\frac{x}{y})\\]\nWe’re not given a great deal of guidance on this task aside from an analogous example. I know the idea of ‘proof’ is a big thing in higher-level mathematics so I’m looking forward to getting a bit more experience with this.\nAs with most of the previous units, the bigger picture is somewhat elusive. I know a lot more about exponents and logarithms than I knew a few weeks ago, but I’m still unsure of how it connects to everything else around it. I’m hoping that comes with time, but in the meanwhile I’m trying to write these blogposts to at least take a step back and think through those bigger connections.\nIn the end, logarithms and exponents are another trick, another tool in the box, and another example of how mental models or abstractions can allow for other new thoughts to be had. We end up using some of the index laws that we learned a few months back to allow us to manipulate these new symbols. Manipulating symbols allows us to either simplify things (so we can think at a different level) or move further into something more complex (so we can see what’s going on, through whatever lens we’ve brought to look through).\nAs a project and as a path to be explored, it’s exciting to take these steps even if sometimes you just have to trust that everything will come together in the end. Next up: “Mathematics Everywhere”, the final unit which is all about practical applications using the things we’ve learned earlier in the module, with a serving of abstract mathematics thrown in for good measure!"
  },
  {
    "objectID": "posts/2023-01-01-on-mathematical-literacy.html",
    "href": "posts/2023-01-01-on-mathematical-literacy.html",
    "title": "On mathematical literacy",
    "section": "",
    "text": "I’m at the half-way point in MU123, the first module in the mathematics degree I’m currently working towards. So far we’ve covered models, properties of numbers, some basic statistics, basic algebra and a starter kit on how to think about graphs.\nAs the first thing you do in the BSc Mathematics (Q31) The module is intended as a way for people who haven’t thought about or used mathematics for many years to re-engage. It gives a common vocabulary and shared understanding of several topics from different areas, such that eventually those with more (recent) experience can join classes with the late-starters (like myself).\nAll of the above preamble is a way of saying that I understand why MU123 exists and have some sense of why it’s structured and programmed the way it is, but as a student, it can feel a little scattered. Specifically, it feels like we are being armed with a large bag of tricks. If you show me a graph, I can find the gradient and the y-intercept. If you give me a surd to simplify, I know what to do. Asked to interpret the standard deviation summary for a data set, I have an intuition for what those numbers mean.\nAll that is good and well, and there’s something satisfying to mastering the individual techniques, much like the early days of learning a new programming language. Some context to the use of the technique is given, but for the most part you are using the techniques to solve specific questions and problems.\nThe missing piece is the sense of how all the various parts, all the tricks, connect together into the whole game of mathematics. For now, I’m writing this post to acknowledge the sense of there being a ‘lack’ in the programme but with the hope and anticipation that this will improve the further down the road we go.\nI should also probably take some responsibility for my own education. In particular, there are probably things I can be doing to make the connections and links myself based on how I’m understanding things as we go. The Open University programme for Maths very much seems to take an ‘a la carte’ approach which means that the onus is more on the student to fill in the gaps. This blog should probably serve as a record of my efforts to make those connections and fill in those gaps."
  },
  {
    "objectID": "posts/2023-07-24-database-backups-tarsnap.html",
    "href": "posts/2023-07-24-database-backups-tarsnap.html",
    "title": "Automating database backups with Tarsnap",
    "section": "",
    "text": "Yesterday I wrote about my MathsPrompt tool which serves up questions to help me practice new skills I’m learning as part of my mathematics degree. Today I realised that all the data (both autogenerated and copy-pasted) is being stored in my database and that I hadn’t really given much thought yet to ensuring a long life for that data.\nI put together a shell script that backs up my data to Tarsnap, my preferred cloud backup service for things like this. It has the tagline “online backups for the truly paranoid” and while it perhaps isn’t the best choice for rich media like photos and videos, it’s really great for compressible text data. I use it on all my personal cloud machines to painlessly back up critical files.\nSo my backup script goes as follows:\n#!/bin/sh\npg_dump -U my_username -F t mathsprompt > /tmp/mathsprompt_backup.tar || exit 1\n/usr/local/bin/tarsnap -c \\\n    -f \"$(uname -n)-$(date +%Y-%m-%d_%H-%M-%S)-mathsprompt-db\" \\\n    /tmp/mathsprompt_backup.tar \nrm /tmp/mathsprompt_backup.tar\nThen I set up a cronjob to handle running this script every day at 5pm. Adding that was as simple as adding the following line (via crontab -e) to my previously configured jobs:\n0 17 * * * path/to/tarsnap-db-backup.sh\nThere are fancier things I could do to backup this database, for instance configuring a streaming backup (a useful suggestion I learned about here), but that’d be overkill for my particular scenario, I think.\nTarsnap is also sufficiently cheap that I don’t need to worry about costs or using up too much space, but in the future I might consider doing some regular cleanup of really old database backups."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html",
    "href": "posts/2022-05-13-sgd-whole-game.html",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev\n\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#initialise-the-parameters",
    "href": "posts/2022-05-13-sgd-whole-game.html#initialise-the-parameters",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "1. Initialise the parameters",
    "text": "1. Initialise the parameters\nWe begin with random values. We also make sure to set up our Tensor so that we’re able to calculate the gradients.\n\nparams = torch.randn(3).requires_grad_()"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#calculate-the-predictions",
    "href": "posts/2022-05-13-sgd-whole-game.html#calculate-the-predictions",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "2. Calculate the predictions",
    "text": "2. Calculate the predictions\nWe make the calculations by passing our parameter values into our function f. We can visualise what our predictions would look like with those parameters.\n\npreds = f(time, params)\n\n\ndef show_preds(preds, ax=None):\n    if ax is None:\n        ax = plt.subplots()[1]\n        ax.scatter(time, speed)\n        ax.scatter(time, to_np(preds), color = 'red')\n\nshow_preds(preds)\n\n\n\n\nYou can see that there’s a decent amount of difference between the curve denoting our predictions for the params (in red) and the actual function (in blue)."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#calculate-the-loss",
    "href": "posts/2022-05-13-sgd-whole-game.html#calculate-the-loss",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "3. Calculate the loss",
    "text": "3. Calculate the loss\nWe use the mean squared error as a way of calculating our loss.\n\nloss = mse(preds, speed)\nloss\n\ntensor(200.6502, grad_fn=<SqrtBackward0>)\n\n\nThis number is a measure of how far off our predictions are from the actual values. We want to improve this loss and drive it down even further, and for that we’ll need the gradients."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#calculate-the-gradient",
    "href": "posts/2022-05-13-sgd-whole-game.html#calculate-the-gradient",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "4. Calculate the gradient",
    "text": "4. Calculate the gradient\nAs described in the previous post, we use PyTorch’s inbuilt ability to calculate gradients.\n\nloss.backward()\nparams.grad\n\ntensor([166.3746,  10.6914,   0.6876])\n\n\nWe can update our parameters based on the learning rate. For our purposes here we can choose a really small learning rate: 0.00001 or 1e-5. This is what the values of our parameters would look like after that operation:\n\nparams * 0.00001\n\ntensor([ 1.2895e-05,  5.8427e-06, -2.3174e-06], grad_fn=<MulBackward0>)\n\n\n\nparams\n\ntensor([ 1.2895,  0.5843, -0.2317], requires_grad=True)"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#step-our-weights",
    "href": "posts/2022-05-13-sgd-whole-game.html#step-our-weights",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "5. Step our weights",
    "text": "5. Step our weights\nWe can step our parameters using the formula previously described: multiply the learning rate by the gradients.\n\nlearning_rate = 0.00001\nparams.data -= learning_rate * params.grad.data\nparams.grad\n\ntensor([166.3746,  10.6914,   0.6876])\n\n\nWe can visualise whether this has improved our function’s curve or not:\n\npreds = f(time, params)\nmse(preds, speed)\n\ntensor(200.3723, grad_fn=<SqrtBackward0>)\n\n\n\nshow_preds(preds)\n\n\n\n\nOur loss has gone from 268.4112 to 268.1312. An improvement, but it feels like a small improvement!"
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#repeat-and-iterate",
    "href": "posts/2022-05-13-sgd-whole-game.html#repeat-and-iterate",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "6. Repeat and iterate",
    "text": "6. Repeat and iterate\nTo save ourselves some time, we can create a function that will help us in iterating through and repeating the above steps:\n\ndef repeat_sgd(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= learning_rate * params.grad.data\n    params.grad = None\n    if prn:\n        print(loss.item())\n    return preds\n\n\n# iterate a few times\nfor i in range(10):\n    repeat_sgd(params)\n\n200.3722686767578\n199.81639099121094\n199.53848266601562\n199.2605743408203\n198.98269653320312\n198.70480346679688\n198.4269561767578\n198.14910888671875\n197.87124633789062\n197.59344482421875\n\n\nWe see that our loss is going down, so everything is improving as we’d hope. The progress seems slow, but it’s progress nonetheless. I imagine we could increasing the learning rate to make the loss go down faster."
  },
  {
    "objectID": "posts/2022-05-13-sgd-whole-game.html#stop-when-weve-iterated-enough",
    "href": "posts/2022-05-13-sgd-whole-game.html#stop-when-weve-iterated-enough",
    "title": "Stochastic Gradient Descent: a mini-example of the whole game",
    "section": "7. Stop when we’ve iterated enough",
    "text": "7. Stop when we’ve iterated enough\nWe’ve only iterated a few times here, but really what we’d want to do is keep going until we reached our stopping point (either we’ve taken too long or our model is ‘good enough’ for our needs)."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html",
    "href": "posts/2022-02-10-synthetic-image-data.html",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "",
    "text": "This blog outlines my process (and a few false starts) for generating a series of synthetic images (and corresponding annotations) to supplement training data used in a machine learning project. This problem is one for which there aren’t many (any?) pre-existing data sets that I can repurpose so I’ve been trying to find ways to bootstrap and improve the performance of the model I’m training.\nBefore I dive into the details, I wanted to include a little context on the wider project and what I’m seeking to accomplish. It is a relatively common practice for documents released as part of FOIA requests to contain redactions. With so many documents being released — and perhaps in specific cases where legal teams are dealing with huge numbers of those redacted documents — it can be useful to identify which documents are redacted and/or to get a sense of just how much has been redacted. If you have 10 or 20 documents you can fairly easily get that overview, but if you have 10,000 or a million documents? That’s where my project comes in: I want to train a model to make it easy to detect redactions in a document and to generate statistics on what proportion of a document or documents have been redacted.\nYou can read more about the problem domain, about my initial forays into annotating a dataset for this problem, as well as view some examples of these redactions (and perhaps why they’re not as easy to identify as you might think). You can even try out a demo showing some of what the model can identify here. Note that this isn’t the latest version of the model so it’s not the absolute best performance."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#whats-the-deal-with-synthetic-images",
    "href": "posts/2022-02-10-synthetic-image-data.html#whats-the-deal-with-synthetic-images",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "What’s the deal with synthetic images?",
    "text": "What’s the deal with synthetic images?\nIt’s a truism that in computer vision projects you probably need or want a lot of data to get good results. For the Facebooks and Bytedances of the world this perhaps isn’t an issue: they have access to a ton of data, for better or for worse. But for me, I don’t have teams of data annotators or millions of users generating all this data. This is probably the norm for small- to medium-sized computer vision problems being solved out in the world, especially with more non-traditional entrants into the field who are just trying to do things with the skills instead of generating research and so on.\nInstead of using huge amounts of data, we need to be smarter about how we work, levelling ourselves up with whatever tricks of the trade we can muster. The fastai course contains a great number of these best practices, perhaps unsurprisingly since it is in some way targeted at individuals seeking to solve their domain-specific problems. One of the key insights I took away from earlier parts of the fastai book was the benefits of using pre-trained models. With a wealth of these models available and accessible, you don’t need to start your work from scratch. Instead, fine-tune your model and benefit from the expertise and hard work of others.\nYou do need some data to get started with fine-tuning a pre-trained model, however. That’s why I took a bit of time to make some initial annotations. I currently have annotated 2097 images, labelling where I have found redactions on the images as well as a box to show which parts of the image contain text or content. That approach has done pretty well so far, with in the low to mid seventies in terms of a % COCO score. (This is a commonly-used metric to assess the performance for object detection problems.) I want to go further, though, which is where synthetic images come in.\nThe big bottleneck in the annotation process is, of course, me. Depending on how many redactions any particular image contains, it could take me 5-10 minutes for a single image’s worth of annotations. This does not scale. Part of the speedup for this process is to use self-training, but I’ll write about that separately. Another option that has is often used is to generate images which approximate (to a greater or lesser degree) the actual real images. The useful thing about generating the images yourself is that you know where you placed the redactions, so you have the annotations at the same time.\nMy overall goal here was to boost my model’s performance. I didn’t know how how well these synthetic images would contribute, or even if they’d contribute to any boost at all. I was also quite conscious of the fact that you could probably spend a year generating pixel-perfect synthetic redacted documents. I didn’t want to waste too much time doing that, so at various points I had to make decisions as to whether a particular stage was good enough."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-1-get-a-baseline-naive-trial",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-1-get-a-baseline-naive-trial",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 1: Get a Baseline / Naive Trial",
    "text": "Phase 1: Get a Baseline / Naive Trial\nWhen I started this, I didn’t know how hard or easy it was going to be, so I set myself a low bar. I knew it was theoretically possible to create images with Python, but I’d never done it before so didn’t have a sense of the range of possibilities.\nIn situations like this, I find Jupyter notebooks really reveal their strengths. Experimentation is easy and the pace of iteration can be really high. A few minutes of searching around and it seemed like Pillow (aka ‘PIL’) was probably the best option to go with. I noted that you could edit, resize, copy and paste images. For my basic version of a synthetic image generator, that’s most of what I needed to do:\n\nTake an image that we know contains no redactions.\nGet a separate image file that is of a redaction box / squiggle or shape.\nRandomly resize the redaction shape.\nPaste the redaction shape at a random location on top of the base unredacted image.\n\nAnd voila! Finding unredacted images was easy since I had previously used fastai to build a model that could detect to ~95% accuracy whether an image contained a redaction or not. For the redactions, it took me about an hour with Pixelmator Pro and its ‘quick selection’ tool to extract 100 examples of various kinds of redaction that I knew were commonly found in the data set. You can see some of this variety in the illustration that follows, though note that each individual redaction snippet was its own separate image for the purposes of my synthetic generation.\n\nI found that it was pretty trivial to generate images of the kind I proposed above. The placement of the redactions didn’t always make sense, and sometimes the random resize that the redaction underwent meant that it was either far too small or far too large. I also hadn’t included any steps to capture the annotation in this prototype, but I knew it was possible so continued onwards."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#detour-get-stuck-pretty-quickly-experience-bbox-sprawl",
    "href": "posts/2022-02-10-synthetic-image-data.html#detour-get-stuck-pretty-quickly-experience-bbox-sprawl",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl",
    "text": "Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl\nBuoyed by my success in the prototype stage, I immediately added a bunch of improvements and features to what I wanted to achieve. I knew I wanted to make sure that the redaction stayed within the boundaries of the original base image. I also wanted to ensure that it stayed within the boundaries of the content of the base image — i.e. redactions generally tend to be made on top of content which tends not to be right on the outer margins.\nI rushed into things too fast without thinking the problem through and quite quickly got into deep waters as all the various pieces started to overlap. I was somehow still in notebook mode, passing various objects through various other custom libraries, not sure what I was passing where. In short: it was a mess.\nOne thing that tripped me up really fast was bboxes. (A bbox, in case this means nothing to you, is a data structure or type that allows you to represent where a box is positioned if you were to paste it on top of a base image (for example). It seems that there are different conventions about how to represent this concept of the location of a box on top of some other larger space. Some people represented it with pairs of coordinates, such that for each of the four corners of the box you’d have an [x, y] pair to represent each point. Others took this bbox type to contain references to the xmin, ymin, xmax, and ymax values of the box. In this way you could reconstruct the various corners since you had two opposite corners specified. Another option was that used by COCO, which was [xmin, ymin, width, height]. And yet another option was to represent a bounding box by [x_center, y_center, width, height]. (This is a useful article that details some of these representation differences.)\nI’m sure there are people who are really good at keeping multiple types of x and y coordinates, each with slightly different nuances, in their heads. I am not such a person and after an hour or two of struggling in these deep waters I realised I needed to regroup.\nMy notebook experiments had been good for uncovering the range of possibility, but now that I had a better sense of the edges of the problem — and the twists and turns of dealing with bounding boxes — I had to take a more systematic approach. I spent some time with pen and paper thinking through the flow that this synthetic generation process would have to include. I thought through what the various independent parts of this could be, and how data would flow through this set of steps."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-2-generate-my-own-base-images",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-2-generate-my-own-base-images",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 2: Generate My Own Base Images",
    "text": "Phase 2: Generate My Own Base Images\nThe first part of this process was to generate my own base images. In general, the types of base unredacted images in the core data set were relatively unremarkable. These were mostly letters, reports or some kind of form / table. I figured I could approximate this pretty quickly. By chance, that very weekend I happened to listen to an episode of the Real Python podcast which interviewed the creator of borb, a Python package for creating and manipulating PDFs. I knew I wanted images in the end, but I had already created a tool to extract images from PDFs and I figured borb would probably save me time, even if it meant I had to do some converting back and forth between images and PDF files.\nThe great thing about borb is that it offers an easy abstraction with which to reason about creating PDF documents. Have some text and want it to be displayed on a page? Done. Want that text to be displayed in three columns? Done. Want do insert some images and have the text flow round it? Done. Have styling requirements? Done. And on and on. I figured that this was just the level of abstraction I needed — rather than staying in the world of pixel primitives like lines and boxes.\nOnce I got going it was easy to generate base images with multi-column text and some random coloured shapes thrown in here and there. (I used lorem-text to generate random Latin texts.) After I created the PDF I then had to convert it into an image format for use elsewhere in the generator pipeline but I think that speed hit was a price worth paying."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-3-generate-my-own-redactions",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-3-generate-my-own-redactions",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 3: Generate My Own Redactions",
    "text": "Phase 3: Generate My Own Redactions\nThe redactions weren’t quite as easy as the base images. The easiest version of a redaction box was literally that: a black box that sits on top of the base image. That much was easy to create. Pillow had some useful interfaces that I could use to quickly create randomly sized boxes. I could even add text to them in the upper left corner as I’d noticed that many of the real redactions did that.\nIt was less clear to me how I’d go about generating the other kinds of redactions, particularly ones that resembled a handwritten mark in thick black marker over the top of a document. In the end, I decided not to go any further with anything that wasn’t a box, but I did make the redaction boxes more varied. I set it such that the box would be filled with a random colour. If the colour was dark enough, I made sure that the text was in a light (contrasting) colour. And ensure that there wasn’t always a text on the box.\nNot perfect, but still it gave me a way to move forward."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#the-big-picture-bringing-it-all-together",
    "href": "posts/2022-02-10-synthetic-image-data.html#the-big-picture-bringing-it-all-together",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "The Big Picture: Bringing It All Together",
    "text": "The Big Picture: Bringing It All Together\nWith these pieces complete, I had the basics of the next version of my synthetic image generation. You can see the flow and progression of my script in the following diagram:\n\nYou’ll note that there were a number of other steps that supported the image creation. I did again descend into bbox hell when calculating exactly where to paste the redaction image, but with a much more modularised approach to my code I didn’t get lost. Type hints also kept me honest about what variables I was passing in and out of the functions I’d created.\nI ended up using the initial model I’d trained so far in the step that figured out where the content of the image was. You’ll recall that this was one of the annotations I’d already been generating when I annotated my data, and since it’s a fairly simple computer vision task I was already seeing excellent performance from that specific class in object detection. IceVision, a library that I’m using for the computer vision and deep learning parts of this project, allowed me to fairly easily make this inference on the images and extract the bbox coordinates for the content box.\nI made sure to include a lot of random variation in the first two steps where the base and redaction images were created. I didn’t remove the original naive approach completely. Instead, I made it 50% likely that we’d generate an image versus just picking one of the unredacted images from our store. Then I gave the same chance for the redaction as to whether we’d use an actual redaction snippet or one of the computer-generated boxes. There was lots of resizing and colouring and various other randomisation that was also included."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#phase-5-make-the-images-look-old-and-worn",
    "href": "posts/2022-02-10-synthetic-image-data.html#phase-5-make-the-images-look-old-and-worn",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Phase 5: Make The Images Look Old and Worn",
    "text": "Phase 5: Make The Images Look Old and Worn\nOnly one step remained. I realised that when I generated the images completely from scratch, not using any of the real base images or redaction snippets, that they looked very new and unrealistic. A significant proportion of the documents in the collection looked like they’d been photocopied a thousand times and in general had seen better days. Sometimes the quality was such to make them unreadable. I realised if I was going to get good results with the overall goal (i.e. improve my model’s performance) I’d have to make the synthetic creations look old somehow.\nAfter some exploration I settled on augraphy as how I’d process the newly generated images to look old and worn. Luckily for me, this package seems to have been created explicitly to support machine learning workflows for synthetic data creation, and it seemed to be (somewhat) actively maintained. There was a default set of so-called ‘augmentations’ that Augraphy suggested I apply to my image. Unfortunately it was simply too aggressive. I guess for some workflows it would have been great, but the page ended up looking somewhat unintelligible by the end. Compare these two examples:\n\nNot only did the default Augraphy transforms often make the redaction indistinguishable, it shifted parts of the image around on the page for these crinkle and scrunch effects, which would have rendered my annotations inaccurate.\nThat said, as you can see from the left image, it was pretty easy to switch out the default for a set of random transforms to be applied that wasn’t quite so aggressive. I’m thankful that tools like this exist out in the open-source space and that allow me to get on with the work of solving the actual problem I’m interested in working on."
  },
  {
    "objectID": "posts/2022-02-10-synthetic-image-data.html#final-results-2097-synthetic-images",
    "href": "posts/2022-02-10-synthetic-image-data.html#final-results-2097-synthetic-images",
    "title": "It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
    "section": "Final Results: 2097 Synthetic Images",
    "text": "Final Results: 2097 Synthetic Images\n\nThis gif gives you a brief sense of some of the images I generated as a result of the process I’ve detailed above. They’re not perfect, and as I write I currently don’t know how well they will perform when training my model.\nI have 2097 real annotated images, so I’m going to combine them with a maximum of an equal number of synthetic images. I’ll try out different proportions of real to synthetic, but that’s also a topic for another blogpost to follow. Stay tuned!\nIt took about three and a half hours to create these 2000+ images on my laptop. There are LOTS of places where I could have made speed improvements, notably all the conversion between PDF and image objects, the inference for the content box and also the fact that the pipeline wasn’t performed in parallel on all my CPU cores. I spent about 30 minutes exploring Ray as a means to getting this process to be executed in parallel but it ended up being not as simple as I’d initially thought so I’ve left that to one side for now. In any case, I won’t be creating so many synthetic images at once so often, so it wasn’t a real blocking point for my work.\nNote, too, that the annotations get created as part of the same script. I append them to a synthetic annotations file at the same time as the synthetic images is generated, and the file is subject to being combined with the real annotations at a later stage.\nThere are obviously lots of ways this synthetic data creation process could be optimised, but I was recently reminded that it’s also important not to lose momentum and not to let the perfect be the enemy of the good.\nThe next step is to carry out an experiment to see the effect of adding in the synthetic annotations on model performance. There are a bunch of really tricky aspects to this (most notably finding ways to make sure not to allow my training data to leak into the validation data) but I’ll save all that for my next blogpost.\n(If you got all the way to the end, well done!)"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "",
    "text": "My last post outlined the kinds of evaluation I need and want to understand how well my finetuned LLM is performing in the task of structured data extraction from press releases. Let’s start with the core metric I’m interested in, accuracy, and then later we can dive into some of the other evaluation metrics as well."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#tldr",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#tldr",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "TL;DR",
    "text": "TL;DR\nThe headline for this post could well have been: finetuned models beat OpenAI, but evals were a bit painful to implement. There’s a lot of hidden code here in this post and it was slow to run. This step was the first time during the work for the finetuning course where I felt the pain and tradeoffs around the choice to finetune. I can see that without a system of some kind to handle this, the complexity of maintaining it all will start to mount up. But more on that at the end!\nThis is a long post with lots of detail. I’ve tried to minimise the amount of code you see, but if you want to see how the charts or evals were done, expand the ‘code’ sections. If you’re interested in cutting straight to the aggregate results, click here to go to the end of this post. (To see the rest of the blog posts about this project, please click here. Some context: I’m doing some finetuning as part of the Hamel Husain / Dan Becker Finetuning course on Maven using some data I collected and labeled a few years back that makes for a cool little test of how good it works for structured data extraction.)"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#loading-the-datasets",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#loading-the-datasets",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Loading the datasets",
    "text": "Loading the datasets\nThe data is all available on the Hugging Face Hub in a public repository, and for the purposes of these evaluations I want to use the test split of the dataset since none of our models have seen that data yet so it’s good for determining how well our model performs with new data.\n\n\nCode\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\n\ntest_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"test\")\ntest_df = pd.DataFrame(test_dataset)\n\n\n\ntest_dataset\n\nDataset({\n    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],\n    num_rows: 724\n})\n\n\nWe’ll first add an extra column to our DataFrame and then make a prediction for each and every row in the dataset. We’ll store a copy of the prediction to the column so as to make sure we don’t have to do this compute-intensive step repeatedly.\nBut first we’ll assemple the data as Pydantic objects so as to handle validation and other quality of life features.\n\n\nCode\nfrom enum import Enum\nfrom typing import Dict, Set, Annotated, Optional\nfrom pydantic import BaseModel, Field, validator, ValidationInfo\nfrom datetime import date\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\n\n\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jowzjan = \"jowzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktya = \"paktya\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pul = \"sar_e_pul\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\n\n\nclass TargetGroup(str, Enum):\n    taliban = \"taliban\"\n    haqqani = \"haqqani\"\n    criminals = \"criminals\"\n    aq = \"aq\"\n    hig = \"hig\"\n    let = \"let\"\n    imu = \"imu\"\n    judq = \"judq\"\n    iju = \"iju\"\n    hik = \"hik\"\n    ttp = \"ttp\"\n    other = \"other\"\n\n\ndef validate_event_type(value: str):\n    valid_values = [\n        \"airstrike\",\n        \"detention\",\n        \"captureandkill\",\n        \"insurgentskilled\",\n        \"exchangeoffire\",\n        \"civiliancasualty\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_province(value: str):\n    valid_values = [\n        \"badakhshan\",\n        \"badghis\",\n        \"baghlan\",\n        \"balkh\",\n        \"bamyan\",\n        \"day_kundi\",\n        \"farah\",\n        \"faryab\",\n        \"ghazni\",\n        \"ghor\",\n        \"helmand\",\n        \"herat\",\n        \"jowzjan\",\n        \"kabul\",\n        \"kandahar\",\n        \"kapisa\",\n        \"khost\",\n        \"kunar\",\n        \"kunduz\",\n        \"laghman\",\n        \"logar\",\n        \"nangarhar\",\n        \"nimroz\",\n        \"nuristan\",\n        \"paktya\",\n        \"paktika\",\n        \"panjshir\",\n        \"parwan\",\n        \"samangan\",\n        \"sar_e_pul\",\n        \"takhar\",\n        \"uruzgan\",\n        \"wardak\",\n        \"zabul\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_target_group(value: str):\n    valid_values = [\n        \"taliban\",\n        \"haqqani\",\n        \"criminals\",\n        \"aq\",\n        \"hig\",\n        \"let\",\n        \"imu\",\n        \"judq\",\n        \"iju\",\n        \"hik\",\n        \"ttp\",\n        \"other\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\nclass IsafEvent(BaseModel):\n    name: str = Field(\n        description=\"A title or name for the event which summarises the event as a headline\"\n    )\n    text: Optional[str] = Field(description=\"The full text of the press release\")\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    event_type: Set[Annotated[str, Field(validator=validate_event_type)]] = Field(\n        description=\"The event type. Can be multiple types.\"\n    )\n    province: Set[Annotated[str, Field(validator=validate_province)]] = Field(\n        description=\"The province in which the event occurred. Can be multiple provinces.\"\n    )\n    target_group: Set[Annotated[str, Field(validator=validate_target_group)]] = Field(\n        description=\"The group that was targetted during the event. Can be multiple groups.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n    predictions: Dict[str, str] = Field(\n        default={},\n        description=\"The predictions from the model. Keys are the model name and the value is the prediction\",\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\nHere’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:\n\n\nCode\nfrom typing import List\n\nevents: List[IsafEvent] = []\n\nfor i, row in list(test_df.iterrows()):\n    event_types = set(\n        eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n    )\n    provinces = set(province.strip().lower() for province in row[\"province\"].split(\",\"))\n    target_groups = set(\n        target_group.strip().lower() for target_group in row[\"targetgroup\"].split(\",\")\n    )\n\n    events.append(\n        IsafEvent(\n            name=row[\"name\"],\n            text=row[\"text\"],\n            start_date=row[\"StartDate\"].to_pydatetime().date(),\n            event_type=event_types,\n            province=provinces,\n            target_group=target_groups,\n            min_killed=int(row[\"minkilled\"]),\n            min_captured=int(row[\"mincaptured\"]),\n            killq=row[\"killq\"] == \"true\",\n            captureq=row[\"captureq\"] == \"true\",\n            killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n            airstrike=row[\"airstrike\"] == \"true\",\n            noshotsfired=row[\"noshotsfired\"] == \"true\",\n            min_leaders_killed=int(row[\"minleaderskilled\"]),\n            min_leaders_captured=int(row[\"minleaderscaptured\"]),\n        )\n    )\n\nprint(events[:2])\n\n\n[\n    IsafEvent(\n        name='5',\n        text='2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n        start_date=datetime.date(2013, 1, 24),\n        event_type={'insurgentskilled'},\n        province={'ghazni'},\n        target_group={'taliban'},\n        min_killed=1,\n        min_captured=0,\n        killq=True,\n        captureq=False,\n        killcaptureraid=False,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=1,\n        min_leaders_captured=0,\n        predictions={}\n    ),\n    IsafEvent(\n        name='2',\n        text='2011-11-S-034\\nISAF Joint Command - Afghanistan\\nFor Immediate Release\\n\\nKABUL, Afghanistan (Nov. \n20, 2011)\\nA coalition security force detained numerous suspected insurgents during an operation in Marjeh \ndistrict, Helmand province, yesterday.  The force conducted the operation after receiving information that a group \nof insurgents were at a compound in the area.  After calling for the men inside to come out peacefully, the \ninsurgents emerged and were detained without incident.',\n        start_date=datetime.date(2011, 11, 19),\n        event_type={'detention'},\n        province={'helmand'},\n        target_group={''},\n        min_killed=0,\n        min_captured=4,\n        killq=False,\n        captureq=True,\n        killcaptureraid=True,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=0,\n        min_leaders_captured=0,\n        predictions={}\n    )\n]\n\n\n\nSo when we’re making the prediction we’re hoping to get a JSON string like this out from the model:\n\njson_str = events[0].model_dump_json(exclude={\"text\", \"predictions\"})\nprint(json_str)\n\n{\"name\":\"5\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tali\nban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}\n\n\n\nI’m starting with full evaluations using the GPT models and I’ll need a slightly more elaborate prompt in order to get decent results. I can’t pass in the exact same prompt as the one I used for the finetuned model since the GPT models haven’t been trained or finetuned to respond to those specific prompts. This is sort of an interesting problem to have: how much effort do we put into the GPT prompts to try to get the same level of accuracy as the finetuned model? Or in other words, is there even a way to really compare like to like between models that must accept different prompts?\nLet’s try this out for OpenAI GPT-4o and GPT-4 Turbo and see how we get on. You’ll note how long the prompt has to be to give the GPT models a fighting chance against the finetuned models. Ideally I’d stuff in even more examples into the context, but I also don’t want to explode the number of tokens I’m using.\n\nfrom openai import OpenAI\nfrom rich import print\nimport json\nimport os\n\n\ndef query_openai(article_text: str, model: str) -> str:\n    query = (\n        f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):\\n{article_text}\\n\\n\"\n        \"## Extraction request\\n\"\n        \"Please extract the following information from the press release:\\n\"\n        \"- The name of the event (summarising the event / text as a headline)\\n\"\n        \"- The start date of the event\\n\"\n        \"- The event type(s)\\n\"\n        \"- The province(s) in which the event occurred\\n\"\n        \"- The target group(s) of the event\\n\"\n        \"- The minimum number of people killed during the event\\n\"\n        \"- The minimum number of people captured during the event\\n\"\n        \"- Whether someone was killed or not during the event\\n\"\n        \"- Whether someone was captured or not during the event\\n\"\n        \"- Whether the event was a so-called 'kill-capture raid'\\n\"\n        \"- Whether an airstrike was used during the event\\n\"\n        \"- Whether no shots were fired during the event\\n\"\n        \"- The minimum number of leaders killed during the event\\n\"\n        \"- The minimum number of leaders captured during the event\\n\\n\"\n        \"## Annotation notes:\\n\"\n        \"- A 'faciliator' is not a leader.\\n\"\n        \"- If a press release states that 'insurgents' were detained without further \"\n        \"details, assign a minimum number of two detained. Interpret 'a couple' as \"\n        \"two. Interpret 'several' as at least three, even though it may sometimes \"\n        \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \"\n        \"small group', and 'multiple' as denoting at least three, even if they \"\n        \"sometimes refer to larger numbers. Choose the smaller number if no other \"\n        \"information is available in the press release to come up with a minimally \"\n        \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \"\n        \"and 'a large number' as at least five.\\n\\n\"\n        \"## Example:\\n\"\n        \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \"\n        \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \"\n        \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \"\n        \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \"\n        \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \"\n        \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \"\n        \"positions, resulting in several more insurgents being killed.'\\n\\n\"\n        'Output: `{\"name\":\"Several insurgents killed in '\n        'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi'\n        'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"'\n        ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`'\n    )\n\n    # set up the prediction harness\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n    response = client.chat.completions.create(\n        model=model,\n        response_format={\"type\": \"json_object\"},\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise \"\n                \"and always make sure you are correct, drawing inference from the text of the \"\n                \"press release.\\n\\n You always return a JSON string with the following schema: \"\n                \"## JSON Schema details\\n\"\n                \"Here is some of the schema for the JSON output string you \"\n                \"should make use of: event_types = ['airstrike', 'detention', \"\n                \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \"\n                \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \"\n                \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \"\n                \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \"\n                \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \"\n                \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \"\n                \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \"\n                \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        temperature=1,\n    )\n\n    return response.choices[0].message.content\n\nWe can make sure this function works with a quick example:\n\njson_str = query_openai(events[0].text, \"gpt-4o\")\nprint(json.loads(json_str))\n\n{\n    'name': 'Taliban leader Alaudin killed in Ghazni',\n    'start_date': '2013-01-24',\n    'event_type': ['insurgentskilled'],\n    'province': ['ghazni'],\n    'target_group': ['taliban'],\n    'min_killed': 1,\n    'min_captured': 0,\n    'killq': True,\n    'captureq': False,\n    'killcaptureraid': True,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 1,\n    'min_leaders_captured': 0\n}\n\n\n\nOur model is working (as expected) and we’re also getting a JSON string back. Let’s assemble something that will iterate through all of our test data, get predictions, and then store those predictions on our Pydantic object.\nFor the bulk predictions, we’ll make sure to do this async, since there are lots of events and we don’t want to waiting all day. You’ll see I also had to add some retries to the function to account for rate limiting on the GPT-3.5-turbo model.\n\n\nCode\n# make async work within a notebook\nimport nest_asyncio\n\nnest_asyncio.apply()\n\nimport aiohttp\nimport asyncio\nfrom typing import List\nfrom openai import OpenAI\n\n\nasync def async_query_openai(\n    session,\n    article_text: str,\n    model: str,\n    max_retries: int = 3,\n    retry_delay: float = 1.0,\n) -> str:\n    query = (\n        f\"The following is a press release issued by ISAF (formerly operating in Afghanistan):\\n{article_text}\\n\\n\"\n        \"## Extraction request\\n\"\n        \"Please extract the following information from the press release:\\n\"\n        \"- The name of the event (summarising the event / text as a headline)\\n\"\n        \"- The start date of the event\\n\"\n        \"- The event type(s)\\n\"\n        \"- The province(s) in which the event occurred\\n\"\n        \"- The target group(s) of the event\\n\"\n        \"- The minimum number of people killed during the event\\n\"\n        \"- The minimum number of people captured during the event\\n\"\n        \"- Whether someone was killed or not during the event\\n\"\n        \"- Whether someone was captured or not during the event\\n\"\n        \"- Whether the event was a so-called 'kill-capture raid'\\n\"\n        \"- Whether an airstrike was used during the event\\n\"\n        \"- Whether no shots were fired during the event\\n\"\n        \"- The minimum number of leaders killed during the event\\n\"\n        \"- The minimum number of leaders captured during the event\\n\\n\"\n        \"## Annotation notes:\\n\"\n        \"- A 'faciliator' is not a leader.\\n\"\n        \"- If a press release states that 'insurgents' were detained without further \"\n        \"details, assign a minimum number of two detained. Interpret 'a couple' as \"\n        \"two. Interpret 'several' as at least three, even though it may sometimes \"\n        \"refer to seven or eight. Classify the terms 'a few', 'some', 'a group', 'a \"\n        \"small group', and 'multiple' as denoting at least three, even if they \"\n        \"sometimes refer to larger numbers. Choose the smaller number if no other \"\n        \"information is available in the press release to come up with a minimally \"\n        \"acceptable figure. Interpret 'numerous' and 'a handful' as at least four, \"\n        \"and 'a large number' as at least five.\\n\\n\"\n        \"## Example:\\n\"\n        \"Article text: 'ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \"\n        \"Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \"\n        \"service members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \"\n        \"their position talking on radios today. After gaining positive identification of the insurgent positions, the \"\n        \"coalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \"\n        \"in the area with weapons. After positive identification, coalition forces continued firing on the various insurgent \"\n        \"positions, resulting in several more insurgents being killed.'\\n\\n\"\n        'Output: `{\"name\":\"Several insurgents killed in '\n        'Helmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi'\n        'n_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"'\n        ':false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}`'\n    )\n\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n    retries = 0\n    while retries < max_retries:\n        async with session.post(\n            \"https://api.openai.com/v1/chat/completions\",\n            headers={\"Authorization\": f\"Bearer {client.api_key}\"},\n            json={\n                \"model\": model,\n                \"response_format\": {\"type\": \"json_object\"},\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are an expert at identifying events in a press release. You are precise \"\n                        \"and always make sure you are correct, drawing inference from the text of the \"\n                        \"press release.\\n\\n You always return a JSON string with the following schema: \"\n                        \"## JSON Schema details\\n\"\n                        \"Here is some of the schema for the JSON output string you \"\n                        \"should make use of: event_types = ['airstrike', 'detention', \"\n                        \"'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], \"\n                        \"provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', \"\n                        \"'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', \"\n                        \"'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', \"\n                        \"'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', \"\n                        \"'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', \"\n                        \"'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', \"\n                        \"'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n\",\n                    },\n                    {\"role\": \"user\", \"content\": query},\n                ],\n                \"temperature\": 1,\n            },\n        ) as response:\n            result = await response.json()\n            if \"error\" in result:\n                error_message = result[\"error\"][\"message\"]\n                if \"Rate limit reached\" in error_message:\n                    # retry_delay_ms = float(\n                    #     error_message.split(\"Please try again in \")[1].split(\"ms\")[0]\n                    # )\n                    retry_delay_ms = 35000\n                    retry_delay_seconds = retry_delay_ms / 1000\n                    print(\n                        f\"Rate limit exceeded. Retrying in {retry_delay_seconds} seconds...\"\n                    )\n                    await asyncio.sleep(retry_delay_seconds)\n                    retries += 1\n                    continue\n                else:\n                    print(f\"Error during prediction.\\nFull result object: {result}\")\n                    return \"\"\n            try:\n                return result[\"choices\"][0][\"message\"][\"content\"]\n            except KeyError:\n                print(f\"Error during prediction.\\nFull result object: {result}\")\n                return \"\"\n\n    print(f\"Max retries exceeded for event.\\nFull result object: {result}\")\n    return \"\"\n\n\nasync def get_gpt_predictions_async(\n    model: str,\n    events: List[IsafEvent],\n    logging_n: int = 100,\n    max_concurrent_requests: int = 5,\n) -> List[IsafEvent]:\n    async with aiohttp.ClientSession() as session:\n        semaphore = asyncio.Semaphore(max_concurrent_requests)\n        tasks = []\n        for i, event in enumerate(events, start=1):\n            if i % logging_n == 0:\n                print(f\"Predicting event {i} of {len(events)} using {model}\")\n\n            async def make_request(session, event):\n                async with semaphore:\n                    return await async_query_openai(\n                        session, event.text, model, max_retries=5\n                    )\n\n            task = asyncio.ensure_future(make_request(session, event))\n            tasks.append(task)\n\n        predictions = await asyncio.gather(*tasks)\n        for event, prediction in zip(events, predictions):\n            event.predictions[model] = prediction\n\n    return events\n\n\nasync def main():\n    events_4o = await get_gpt_predictions_async(\n        \"gpt-4o\", events, max_concurrent_requests=10\n    )\n    events_4turbo = await get_gpt_predictions_async(\n        \"gpt-4-turbo\", events_4o, max_concurrent_requests=10\n    )\n    full_events = await get_gpt_predictions_async(\n        \"gpt-3.5-turbo\", events_4turbo, max_concurrent_requests=10\n    )\n\n\nawait main()\n\n\nSo as you can now see, we have three predictions attached to each event.\n\nprint(events[0])\n\nIsafEvent(\n    name='5',\n    text='2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n    start_date=datetime.date(2013, 1, 24),\n    event_type={'insurgentskilled'},\n    province={'ghazni'},\n    target_group={'taliban'},\n    min_killed=1,\n    min_captured=0,\n    killq=True,\n    captureq=False,\n    killcaptureraid=False,\n    airstrike=False,\n    noshotsfired=False,\n    min_leaders_killed=1,\n    min_leaders_captured=0,\n    predictions={\n        'gpt-4o': '{\\n  \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n  \"start_date\": \"2013-01-24\",\\n  \n\"event_type\": [\"insurgentskilled\", \"captureandkill\"],\\n  \"province\": [\"ghazni\"],\\n  \"target_group\": [\"taliban\"],\\n \n\"min_killed\": 1,\\n  \"min_captured\": 0,\\n  \"killq\": true,\\n  \"captureq\": false,\\n  \"killcaptureraid\": true,\\n  \n\"airstrike\": false,\\n  \"noshotsfired\": false,\\n  \"min_leaders_killed\": 1,\\n  \"min_leaders_captured\": 0\\n}',\n        'gpt-4-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": true,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-3.5-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni province\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": false,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}'\n    }\n)\n\n\n\nI have all these predictions living in memory right now so it’s probably a good time to commit these to a dataset and push them to the Hugging Face Hub in case the notebook crashes or my local machine shuts down or something else unexpected.\nI’ll create a function to handle this as we’ll be repeating this process for the other models as well. It’s a bit verbose but I thought it preferable so you can see what’s going on.\n\n\nCode\nfrom datasets import Dataset\n\n\ndef convert_to_dataset(data: List[IsafEvent]) -> Dataset:\n    names = []\n    texts = []\n    start_dates = []\n    provinces = []\n    target_groups = []\n    event_types = []\n    predictions = []\n    min_killeds = []\n    min_captureds = []\n    killqs = []\n    captureqs = []\n    killcaptureraids = []\n    airstrikes = []\n    noshotsfireds = []\n    min_leaders_killeds = []\n    min_leaders_captureds = []\n\n    for item in data:\n        names.append(item.name)\n        texts.append(item.text)\n        start_dates.append(item.start_date)\n        provinces.append(item.province)\n        target_groups.append(item.target_group)\n        event_types.append(item.event_type)\n        predictions.append(item.predictions)\n        min_killeds.append(item.min_killed)\n        min_captureds.append(item.min_captured)\n        killqs.append(item.killq)\n        captureqs.append(item.captureq)\n        killcaptureraids.append(item.killcaptureraid)\n        airstrikes.append(item.airstrike)\n        noshotsfireds.append(item.noshotsfired)\n        min_leaders_killeds.append(item.min_leaders_killed)\n        min_leaders_captureds.append(item.min_leaders_captured)\n\n    dataset_dict = {\n        \"name\": names,\n        \"text\": texts,\n        \"predictions\": predictions,\n        \"start_date\": start_dates,\n        \"province\": provinces,\n        \"target_group\": target_groups,\n        \"event_type\": event_types,\n        \"min_killed\": min_killeds,\n        \"min_captured\": min_captureds,\n        \"killq\": killqs,\n        \"captureq\": captureqs,\n        \"killcaptureraid\": killcaptureraids,\n        \"airstrike\": airstrikes,\n        \"noshotsfired\": noshotsfireds,\n        \"min_leaders_killed\": min_leaders_killeds,\n        \"min_leaders_captured\": min_leaders_captureds,\n    }\n    dataset = Dataset.from_dict(dataset_dict)\n\n    return dataset\n\n\ndef convert_and_push_dataset(\n    events: List[IsafEvent], name: str, split_name: str = \"train\"\n):\n    \"\"\"Convert a list of Pydantic objects to a HF Dataset object, then push to\n    the hub.\"\"\"\n    hf_token = os.getenv(\"HUGGINGFACE_API_KEY\")\n\n    dataset = convert_to_dataset(events)\n    dataset.push_to_hub(\n        f\"strickvl/{name}\",\n        token=hf_token,\n        private=True,\n        create_pr=True,\n        split=split_name,\n    )\n\n\nA more concise and abstract version of the convert_to_dataset function could be something like:\ndef convert_to_dataset(data: List[BaseModel]) -> Dataset:\n    dataset_dict = {}\n\n    for field_name, field_value in data[0].__fields__.items():\n        field_type = field_value.outer_type_\n        if field_type in [str, int, float, bool, date]:\n            dataset_dict[field_name] = [getattr(item, field_name) for item in data]\n        elif field_type == set:\n            dataset_dict[field_name] = [list(getattr(item, field_name)) for item in data]\n        elif issubclass(field_type, BaseModel):\n            dataset_dict[field_name] = [getattr(item, field_name).dict() for item in data]\n        else:\n            dataset_dict[field_name] = [getattr(item, field_name) for item in data]\n\n    dataset = Dataset.from_dict(dataset_dict)\n    return dataset\nBut for now let’s just push our data to the Hub.\n\nconvert_and_push_dataset(events, \"isafpressreleases_with_preds\", split_name=\"test\")"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#reloading-the-predictions-dataset",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#reloading-the-predictions-dataset",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Reloading the predictions dataset",
    "text": "Reloading the predictions dataset\nLet’s start by loading our dataset and then we can get into adding some local model predictions:\n\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds\")[\n    \"test\"\n].to_list()\n\nWe trained some local models, so let’s add those predictions to the dataset."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-tinyllama-predictions",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-tinyllama-predictions",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned TinyLlama predictions",
    "text": "Finetuned TinyLlama predictions\n\n\nCode\nfrom typing import Union\nimport torch\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer\n\n\ndef prompt(press_release: str) -> str:\n    return f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\n\n### Instruction:\n\nPRESS RELEASE TEXT: \"{press_release}\"\n\n### Response:\n\"\"\"\n\n\ndef prompt_tok(\n    model: AutoPeftModelForCausalLM,\n    tokenizer: AutoTokenizer,\n    press_release: str,\n    return_ids: bool = False,\n) -> Union[str, torch.Tensor]:\n    _p = prompt(press_release)\n    input_ids = tokenizer(_p, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n    out_ids = model.generate(input_ids=input_ids, max_new_tokens=5000, do_sample=False)\n    ids = out_ids.detach().cpu().numpy()\n    if return_ids:\n        return out_ids\n    return tokenizer.batch_decode(ids, skip_special_tokens=True)[0][len(_p) :]\n\n\ntinyllama_sharegpt_model_id = \"strickvl/isafpr-tiny-llama-lora-templatefree\"\nmodel = AutoPeftModelForCausalLM.from_pretrained(tinyllama_sharegpt_model_id).cuda()\ntokenizer = AutoTokenizer.from_pretrained(tinyllama_sharegpt_model_id)\ntokenizer.pad_token = tokenizer.eos_token\n\nfor row in preds_test_data:\n    out = prompt_tok(model, tokenizer, row[\"text\"])\n    row[\"predictions\"][\"tinyllama-templatefree\"] = out\n\n\nNow if we inspect we’ll see that the new model predictions have been saved into the dataset:\n\nfrom rich import print\n\nprint(preds_test_data[0])\n\n{\n    'name': '5',\n    'text': '2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n    'predictions': {\n        'gpt-3.5-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni province\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": false,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": true,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4o': '{\\n  \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n  \"start_date\": \"2013-01-24\",\\n  \n\"event_type\": [\"insurgentskilled\", \"captureandkill\"],\\n  \"province\": [\"ghazni\"],\\n  \"target_group\": [\"taliban\"],\\n \n\"min_killed\": 1,\\n  \"min_captured\": 0,\\n  \"killq\": true,\\n  \"captureq\": false,\\n  \"killcaptureraid\": true,\\n  \n\"airstrike\": false,\\n  \"noshotsfired\": false,\\n  \"min_leaders_killed\": 1,\\n  \"min_leaders_captured\": 0\\n}',\n        'tinyllama-templatefree': '\\n{\"name\":\"Taliban leader killed in \nGhazni\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"\n],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsf\nired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'tinyllama-sharegpt': \n'{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"airstrike\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"],\n\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":true,\"noshotsfire\nd\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}'\n    },\n    'start_date': datetime.date(2013, 1, 24),\n    'province': ['ghazni'],\n    'target_group': ['taliban'],\n    'event_type': ['insurgentskilled'],\n    'min_killed': 1,\n    'min_captured': 0,\n    'killq': True,\n    'captureq': False,\n    'killcaptureraid': False,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 1,\n    'min_leaders_captured': 0\n}"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-predictions",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-predictions",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Mistral predictions",
    "text": "Finetuned Mistral predictions\nAs I noted previously, it was impossible to get the finetuned Mistral model working locally so I did the inference over on Modal where I could spin up a juicy A100 to make the predictions. You’ll see below that the model didn’t perform very well, failing almost all of the evaluations. This is the mistral-lora-templatefree model you’ll see in the charts."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-openai-predictions",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-openai-predictions",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned OpenAI predictions",
    "text": "Finetuned OpenAI predictions\nI used OpenAI’s one-click finetuning service to finetune the gpt-3.5-turbo-1106 model. I iterated over my dataset to generate predictions using that finetuned model using the OpenAI SDK.\n\n\nCode\nfrom openai import OpenAI\nimport os\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds_2\")[\n    \"train\"\n].to_list()\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nfor row in preds_test_data:\n    response = client.chat.completions.create(\n        model=\"ft:gpt-3.5-turbo-1106:SOME_MODEL_ID_GOES_HERE\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n            },\n            {\"role\": \"user\", \"content\": row[\"text\"]},\n        ],\n        temperature=0,\n    )\n    row[\"predictions\"][\"finetuned-openai-gpt-3.5-turbo-1106\"] = response.choices[\n        0\n    ].message.content"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-models-via-openpipe",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-mistral-models-via-openpipe",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Mistral models (via OpenPipe)",
    "text": "Finetuned Mistral models (via OpenPipe)\nI finetuned Mistral 7B and Mistral 8x7B models using OpenPipe so as to have something reasonable to compare the other models to. As always, OpenPipe makes it pretty easy to spin up a finetuning job and get predictions.\n\n\nCode\nfrom openpipe import OpenAI\nimport os\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_test_predictions_old\")[\n    \"train\"\n].to_list()\n\nclient = OpenAI(openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")})\n\nfor i, row in enumerate(preds_test_data, 1):\n    completion_7b = client.chat.completions.create(\n        model=\"openpipe:twelve-pumas-invent\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n            },\n            {\"role\": \"user\", \"content\": row[\"text\"]},\n        ],\n        temperature=0,\n        openpipe={\"tags\": {\"prompt_id\": \"counting\", \"any_key\": \"any_value\"}},\n    )\n\n    row[\"predictions\"][\"finetuned-mistral-7b-optimised-openpipe\"] = completion_7b.choices[0].message.content\n    \n    if i % 100 == 0:\n        print(f\"{i}/724 rows complete\")"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-solar-llm-via-predibase",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-solar-llm-via-predibase",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Solar LLM (via Predibase)",
    "text": "Finetuned Solar LLM (via Predibase)\nPredibase announced a new best-in-class model for finetuning, the Solar LLM from Upstage, a week or so ago so I thought I’d try it out. The advantage of this model is that it’s trained to be good at the kinds of tasks people commonly finetune models for, like structured data extraction. As you’ll see below, it did pretty well! The base model is this one, I think, on the Hugging Face Hub so it’s available for you all to use as well.\n\n\nCode\nfrom predibase import Predibase\n\npb = Predibase(api_token=\"MY_API_TOKEN_GOES_HERE\")\n\nlorax_client = pb.deployments.client(\"solar-1-mini-chat-240612\")\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_test_predictions\")[\n    \"train\"\n].to_list()\n\nfor i, row in enumerate(preds_test_data, 1):\n    prompt = f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\\n\\n### Instruction:\\n\\nPRESS RELEASE TEXT: \"{row['text']}\"\\n\\n### Response:\"\"\"\n    response = lorax_client.generate(prompt, adapter_id=\"isafpr/2\", max_new_tokens=300).generated_text\n\n    row[\"predictions\"][\"ft-solar-1-mini-chat-240612-predibase\"] = response\n\n    if i % 100 == 0:\n        print(f\"{i}/724 rows complete\")"
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-llama3-predictions-via-openpipe",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#finetuned-llama3-predictions-via-openpipe",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Finetuned Llama3 predictions (via OpenPipe)",
    "text": "Finetuned Llama3 predictions (via OpenPipe)\nMy locally finetuned Llama3 model hadn’t really worked well, but on OpenPipe the outputs seemed to look ok, so I used these predictions for the final evaluation.\n\n\nCode\nfrom openpipe import OpenAI\nimport os\nfrom datasets import load_dataset\n\npreds_test_data = load_dataset(\"strickvl/isafpressreleases_with_preds_3\")[\n    \"train\"\n].to_list()\n\nclient = OpenAI(openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")})\n\nfor row in preds_test_data:\n    completion = client.chat.completions.create(\n        model=\"openpipe:fine-steaks-taste\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\",\n            },\n            {\"role\": \"user\", \"content\": row[\"text\"]},\n        ],\n        temperature=0,\n        openpipe={\"tags\": {\"prompt_id\": \"counting\", \"any_key\": \"any_value\"}},\n    )\n\n    row[\"predictions\"][\"finetuned-llama3-7b-32k-openpipe\"] = completion.choices[\n        0\n    ].message.content\n\n\nBy the end of this process you can see we have a bunch of predictions attached to each entry in our dataset. You can view all of these in the public dataset I published on the Hugging Face Hub.\n\nfrom rich import print\n\nprint(preds_test_data[0])\n\n{\n    'name': '5',\n    'text': '2013-01-S-025\\n\\nKABUL, Afghanistan (Jan. 25, 2013)\\nDuring a security operation in Andar district, \nGhazni province, yesterday, an Afghan and coalition force killed the Taliban leader, Alaudin. Alaudin oversaw a \ngroup of insurgents responsible for conducting remote-controlled improvised explosive device and small-arms fire \nattacks against Afghan and coalition forces. Prior to his death, Alaudin was planning attacks against Afghan \nNational Police in Ghazni province.',\n    'predictions': {\n        'finetuned-llama3-7b-32k-openpipe': \n'{\"name\":\"1\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal\niban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'finetuned-mistral-7b-optimised-openpipe': \n'{\"name\":\"1\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal\niban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'finetuned-openai-gpt-3.5-turbo-1106': \n'{\"name\":\"4\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"tal\niban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"nosh\notsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'gpt-3.5-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni province\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": false,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4-turbo': '{\\n    \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n    \"start_date\": \n\"2013-01-24\",\\n    \"event_type\": [\"captureandkill\"],\\n    \"province\": [\"ghazni\"],\\n    \"target_group\": \n[\"taliban\"],\\n    \"min_killed\": 1,\\n    \"min_captured\": 0,\\n    \"killq\": true,\\n    \"captureq\": false,\\n    \n\"killcaptureraid\": true,\\n    \"airstrike\": false,\\n    \"noshotsfired\": false,\\n    \"min_leaders_killed\": 1,\\n    \n\"min_leaders_captured\": 0\\n}',\n        'gpt-4o': '{\\n  \"name\": \"Taliban leader Alaudin killed in Ghazni\",\\n  \"start_date\": \"2013-01-24\",\\n  \n\"event_type\": [\"insurgentskilled\", \"captureandkill\"],\\n  \"province\": [\"ghazni\"],\\n  \"target_group\": [\"taliban\"],\\n \n\"min_killed\": 1,\\n  \"min_captured\": 0,\\n  \"killq\": true,\\n  \"captureq\": false,\\n  \"killcaptureraid\": true,\\n  \n\"airstrike\": false,\\n  \"noshotsfired\": false,\\n  \"min_leaders_killed\": 1,\\n  \"min_leaders_captured\": 0\\n}',\n        'mistral-lora-templatefree': '1',\n        'tinyllama-sharegpt': \n'{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"airstrike\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"],\n\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":true,\"noshotsfire\nd\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'tinyllama-templatefree': '\\n{\"name\":\"Taliban leader killed in \nGhazni\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\"taliban\"\n],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsf\nired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}',\n        'ft-solar-1-mini-chat-240612-predibase': \n'\\n\\n{\"name\":\"2\",\"start_date\":\"2013-01-24\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"ghazni\"],\"target_group\":[\n\"taliban\"],\"min_killed\":1,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":true,\"airstrike\":false,\"\nnoshotsfired\":false,\"min_leaders_killed\":1,\"min_leaders_captured\":0}'\n    },\n    'start_date': datetime.date(2013, 1, 24),\n    'province': ['ghazni'],\n    'target_group': ['taliban'],\n    'event_type': ['insurgentskilled'],\n    'min_killed': 1,\n    'min_captured': 0,\n    'killq': True,\n    'captureq': False,\n    'killcaptureraid': False,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 1,\n    'min_leaders_captured': 0\n}\n\n\n\nUnfortunately the Qwen2 inference on Predibase is still not working so I’ll skip that finetuned model for the moment.\nNow that we have predictions from seven finetuned models and three OpenAI models (to compare against), we can run our evaluations. I’ll start with a simple check to see what proportion of the predictions are even valid JSON."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#evals-were-a-pain-this-time-round",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#evals-were-a-pain-this-time-round",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Evals were a pain (this time round)…",
    "text": "Evals were a pain (this time round)…\nMost of the evaluation work is represented here in this notebook, and that was perhaps the seeds of my own misfortune. I had some models that worked locally, and then a bunch of other models deployed in different environments and with different services.\nNot only that, but it was pretty slow to iterate through the 724 row so my test data (which the models hadn’t seen during finetuning, just to be clear) since I implemented it fairly naively.\nIf I were to now make some updates to the models, or get them working locally, I’d really want to make sure that I have a way to run these evals locally as well. Moreover, I’d want a way to run a subset of the evals (i.e. on a slice of the data) and then at some point switch that out so that they could run across all the data.\nAll of this is completely within the realm of possible, but for this round I was more focused on getting the results than I was about making the process repeatable and/or efficient. I know I can’t run all the models concurrently on the same machine, so maybe the way forward is simply to have a reliable cloud GPU provider like Modal where I can farm out these evaluations. I had a really good experience with them when I used them, so that’s probably the way forward there.\nIn general, it was also painful having the models living in different places. I had to remember so many things. In any ideal world, you want a standard interface for inference to all your models, especially if they’re for the same use case or project. It’s convenient that my finetuned GPT3.5 is automatically deployed and served by OpenAI, and the same goes for Llama3 and Solar or Mistral, but I want a single place where I can see them all. Until now I hadn’t really seen this project or problem as being so much about MLOps, but when you have multiple models in play and you’re finetuning and updating them and data is changing all the time, then you’ll need a way of managing all this.\nThis is funny to me since I work at an MLOps company – we build an open-source MLOps framework that helps you set up a platform – but I hadn’t anticipated it’d reach this point where I’d need something like a ZenML so soon. This is, of course, one of the major tradeoffs of finetuning LLMs, in that you have to manage all this stuff in order to make it work reliably and repeatably. Even at this early stage of my project, it’s clear that you need a way to keep everything straight without making mistakes."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#but-evals-give-me-a-way-to-know-if-im-making-progress",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#but-evals-give-me-a-way-to-know-if-im-making-progress",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "…but evals give me a way to know if I’m making progress",
    "text": "…but evals give me a way to know if I’m making progress\nEven though the evaluations were somewhat painful to implement (at least in the form of this Jupyter notebook), they have given me an amazing gift in that I now have a task-specific way to know whether any of the improvements or refinements to either the training data or to the model are helping move me forward. Without this I’d essentially be flying blind."
  },
  {
    "objectID": "posts/2024-07-01-full-finetuned-model-evaluation.html#next-steps",
    "href": "posts/2024-07-01-full-finetuned-model-evaluation.html#next-steps",
    "title": "My finetuned models beat OpenAI’s GPT-4",
    "section": "Next Steps",
    "text": "Next Steps\nI had originally thought and suggested that I’d want to train multiple models to be super-specialists in their field, so for example to have one model that was really good at estimating how many people were captured in a particular event. Seeing the performance of my models, I’m not sure that’s the obvious next step for this project, or if I’d really be able to boost the accuracy by a significant amount by taking that approach.\nThis project is all about accuracy, so it’s possible that I might want to try that out, but for now I’m still exploring all the different phases of the LLM finetuning process so I’ll put the submodels idea on the backburner.\nThe first obvious next step is to run some evaluations for the non-accuracy-related tests mentioned in my last blog. For example, I’d like to see how it performs with out of domain data (i.e. completely made up data about something completely different).\nThe other next step is to get into some of the details around model serving. I’d like to take my top three performers and dive into how LLM model serving is done. I’m familiar with non-LLM model serving and some of the ways people do that through my work, but LLM serving has it’s own tricks, tradeoffs and tools and I’m eager to learn more about those.\nIf this was a problem that I was deeply invested in solving beyond these already excellent results, I’d probably also want to dive into the areas where my LLMs struggled. So I’d take all the places where my LLMs failed to get the answer correct, load them up into some kind of web interface like Lilac or Argilla and really inspect my data further. Understanding the failure scenarios will probably do more for the accuracy than any tweaking of the finetuning parameters or the like.\nFor now, I’m just happy the finetuned models beat GPT-4!"
  },
  {
    "objectID": "posts/2022-05-02-pet-cat-image-classifier-fastai.html",
    "href": "posts/2022-05-02-pet-cat-image-classifier-fastai.html",
    "title": "How my pet cat taught me a lesson about validation data for image classification",
    "section": "",
    "text": "I’m participating in the latest iteration of the fastai course as taught by Jeremy Howard. This past week we got a very high-level overview of some of the ways deep learning is proving very powerful in solving problems as well as how we can use its techniques to fairly quickly get great results on image classification problems.\nI’ve done the earlier parts of the course before, so some of these demonstrations were less mind-blowing than the first time I saw them. For this iteration of the course, Jeremy showcased a Kaggle notebook which trains a model to distinguish whether an image is of a bird or not.\nLast time I did the course, I trained an image classifier model to distinguish whether an image was redacted or not to around 95% accuracy. (This actually was the genesis of my larger redaction object detection project that I’ve been blogging about for the past few months.)\n\nThe key ingredients: what goes into a model?\nThe course teaches things top-down, so we start off with both the practical experience of training state-of-the-art models as well as the overall context to what goes into these high-level functions. These pieces include:\n\nyour input data — this style of programming differs from traditional software engineering where your functions take data in order to ‘learn’ how to make their predictions\nthe ‘weights’ — when we’re using pre-trained models, you can think of these as an initial set of variables that are already pretty useful in that configuration and can do a lot of things.\nyour model — this is what you’re training and, once trained, you can think of it as a function in and of itself that takes in inputs and outputs predictions.\nthe predictions — these are the guesses that your model makes, based on whatever you pass in as inputs. So if you pass in an image of a cat to a model (see below), the prediction could be whether that cat is one particular kind or another.\nyour ‘loss’ — this is a measure of checking how well your model is doing as it trains.\na means of updating your weights — depending on how well (or badly) the training goes, you’ll want a way of updating the weights so that each time it gets a bit better at optimising for whatever you’ve set up your model to do. In lesson one we learn about stochastic gradient descent, a way of optimising and updating these weights automatically.\nyour labels — these are the ground truth assertions that get used to determine how well the model is doing as it trains.\ntransformations & augmentations — more on this will come in lesson two, but these allow you to squeeze more value out of your data. This is especially valuable when you’re fine-tuning a model and don’t have massive amounts of data to use for training.\n\nRepresented in code, the classic fastai example where you train a model to distinguish between cats and dogs is as follows:\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\nThis small code snippet contains all the various parts just mentioned. The high-level API and abstractions that fastai provides allows you to work with these concepts in a way that is fast and flexible, though if you need to dive into the details you can do so as well.\n\n\nImage classification isn’t just about images\nOne of the parts of the first chapter I enjoy the most is the examples of projects where image classification was applied to problems or scenarios where it doesn’t first appear that the problem has anything to do with computer vision.\nWe see malware converted into images and distinguished using classification. We see sounds in an urban environment converted into images and classified with fastai. In the study group I host for some student on the course, one of our members presented an initial proof of concept of using images of music to distinguish genre:\n{% twitter https://twitter.com/kurianbenoy2/status/1520470393760272384?cxt=HHwWgMCi-Y3x5ZkqAAAA %}\nI like the creativity needed to think of how to turn problems and data into a form such that they can become computer vision problems.\n\n\nMy own efforts: classifying my cat\nTrue story: a few years ago my cat escaped from the vet and a reward was mentioned for anyone who found our cute ginger cat. Throughout the course of the day, the vets were perplexed to see people coming in with random ginger cats that they’d found in the neighborhood, but none of them were ours! With this iteration of the course, therefore, I was curious to try out this simple but slightly silly example and see how well a deep learning model could do at recognising distinguishing Mr Blupus — don’t ask! — from other random photos of ginger cats.\nTraining the model was pretty easy. Like any cat owner, I have thousands of photos of our cat so an initial dataset to use was quick to assemble. I downloaded a few hundred random ginger cat photos via DuckDuckGo using some code Jeremy had used in his bird vs forest Kaggle notebook. A few minutes and ten epochs later, I had achieved 96.5% accuracy on my validation data after fine-tuning resnet50!\n{% twitter https://twitter.com/strickvl/status/1520405802091175936 %}\nAfter the initial excitement died down, I realised that the result was probably an illusion. Our cat is an indoor cat and we have a relatively small house. Couple that with the fact that the backdrops to the photos of Mr Blupus are relatively distinctive (particular kinds of sheets or carpets) and it seems pretty clear that the model wasn’t learning how to identify our cat, but rather it was learning how to distinguish photos of our house or our carpets.\n☹️\nLuckily, chapter one gets into exactly this problem, showing an example of how exactly this validation issue can give you a false sense of confidence in your model. When I evaluated my model on the validation data it wasn’t a fair test, since in all likeliness may model had already seen a similar backdrop to whatever was found inside the validation set.\nI discussed this when I presented this to those at the study group / meetup yesterday and we agreed that it’d be best if I held out some settings or locations from the training entirely. I took 30 minutes to do that in the evening and had a third ‘test’ dataset which consisted of 118 images of our cat in certain locations that the model wasn’t trained on and thus couldn’t use to cheat. I added a few more photos to the training data so that there were enough examples from which to learn.\n\nI was supposedly getting 98% accuracy now, but I knew that number to be false. I then needed to figure out how to get the accuracy for my held-out test set. With a lot of help from Francesco and a really useful blogpost on doing batch inference with fastai, I first got the predictions for my test data:\ntest_files = [fn for fn in sorted((Path(\"/path/to/test_set_blupus_photos\")).glob('**/*')) if fn.is_file()]\ntest_dl = learn.dls.test_dl(test_files)\npreds, _ = learn.get_preds(dl=test_dl)\nI then created a tensor with the ground truth predictions for my test set and compared them with what my model had predicted:\ngts = torch.tensor([0 for _ in range(118)])\naccuracy = (gts == preds.argmax(dim=1))\nAt this point, getting the final accuracy was as simple as getting the proportion of correct guesses:\nsum([1 for item in accuracy if item]) / len(preds)\nThis gave me an accuracy on my held-out test set of 93.2% which was surprisingly good.\nI half wonder whether there is still some cheating going on somehow, some quality of the photos or the iPhone camera I used to take them that is being used to distinguish the photos of my cat vs other ginger cats.\nNevertheless, this was a useful lesson for me to learn. I realised while working with the tensors in the final step above that I’m not at all comfortable manipulating data with PyTorch so luckily that’ll get covered in future lessons.\nUPDATE:\nFollowing some discussion in the fastai forums, it was suggested that I take a look at Grad-CAM in chapter 18. This is a technique to visualise the activations which allows you to see which parts of the image it is paying the most attention to (sort of). I ran the code using a sample Blupus image and this was the result. I don’t understand how most (any?) of this works, but it was really cool to have a working result of sorts nonetheless!"
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "",
    "text": "This chapter was all about RAG and agents. It’s only 50 pages, so clearly there’s only so much of the details she can get into, but it was pretty good nonetheless and there were a few things in here I’d never really read. Also Chip does a good job bringing the RAG story into the story about agents, particularly in terms of how she defines agents. (Note that the second half of this chapter, on agents, is available on Chip’s blog as a free excerpt!)\nAs always, what follows is just my notes on the things that seemed interesting to me (and a high-level overview of the main points of the chapter just for future reference). YMMV!"
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#chapter-structure-and-framing",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#chapter-structure-and-framing",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Chapter Structure and Framing",
    "text": "Chapter Structure and Framing\nThis chapter undertakes the ambitious task of unifying two major paradigms in AI engineering: Retrieval-Augmented Generation (RAG) and Agents. At first glance, combining these topics might seem surprising given their scope and complexity. However, Chip creates a compelling framework that positions both as sophisticated approaches to context construction.\nThe unifying thesis presents RAG as a specialised case of the agent pattern, where the retriever functions as a tool at the model’s disposal. Both patterns serve to transcend context limitations and maintain current information, though agents ultimately offer broader capabilities. This framing provides an elegant theoretical bridge between these technologies while acknowledging their distinct characteristics."
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#retrieval-augmented-generation-rag",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#retrieval-augmented-generation-rag",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Retrieval-Augmented Generation (RAG)",
    "text": "Retrieval-Augmented Generation (RAG)\n\nCore Concepts and Context Windows\nThe discussion begins with a fundamental examination of RAG’s purpose: enhancing model outputs with query-specific context to produce more grounded and useful results. Chip introduces a fascinating variation on Parkinson’s Law:\n\nContext Expansion Law: Application context tends to expand to fill the context limits supported by the model.\n\nThis observation challenges the common assumption that RAG might become obsolete with infinite context models. Chip argues that larger context windows don’t necessarily solve the fundamental challenges RAG addresses, particularly noting that models often struggle with information buried in the middle of large context windows.\n\n\nRetrieval Architecture and Algorithms\nThe retrieval architecture discussion introduces two primary paradigms:\n\nSparse Retrieval: Term-based approaches that rely on explicit matching of terms between queries and documents. The primary example is the TFIDF (Term Frequency-Inverse Document Frequency) algorithm, which evaluates term importance based on frequency patterns.\n\n\nDense Retrieval: Embedding-based approaches that transform text into vector representations, requiring specialised vector databases for storage and sophisticated nearest-neighbour search algorithms for retrieval.\n\n\n\nCost Considerations and Trade-offs\nA striking revelation emerges regarding the cost structure of RAG systems: vector database expenses often consume between one-fifth to half of a company’s total model API spending. This cost burden becomes particularly acute for systems requiring frequent embedding updates due to changing data. Chip notes that both vector storage and vector search queries can be surprisingly expensive operations.\n\n\nRetrieval Optimisation Techniques\n\nThe chapter presents several sophisticated approaches to optimisation:\nChunking Strategies: While the section is brief, it addresses the critical trade-offs in how documents are segmented for retrieval.\nQuery Rewriting: A powerful but potentially complex technique that enhances initial queries with contextual information. For example, transforming a query like “how about her?” into “how about Aunt Mabel from the previous question?” Chip notes this can introduce latency issues and suggests careful consideration before implementation.\nContextual Retrieval: Introduces the innovative “chunks-for-chunks” approach, where each retrieved chunk triggers additional retrievals for supplementary context. This might include retrieving related tags or associated metadata to enrich the initial results.\nHybrid Search: Combines term-based and embedding-based retrieval, typically implementing a re-ranking process. A common pattern involves using term-based retrieval (like Elasticsearch) to obtain an initial set of ~50 (or however many!) documents, followed by embedding-based re-ranking to identify the most relevant subset.\n\n\nEvaluation Framework\nThe evaluation framework centres on two primary metrics:\n\nContext Precision: The percentage of retrieved documents that are relevant to the query. Generally easier to measure and optimise.\n\n\nContext Recall: The percentage of all relevant documents that are successfully retrieved. More challenging to measure as it requires comprehensive dataset annotation."
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#agents",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#agents",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Agents",
    "text": "Agents\n\nFoundational Definition\nChip provides a clear definition of an agent:\n\nAgent Definition: An entity capable of perceiving its environment and acting upon it, characterised by: - The environment it operates in (defined by use case) - The set of actions it can perform (augmented by tools)\n\n\n\nTool Types and Capabilities\nThe chapter delineates three primary categories of tools:\nKnowledge Augmentation Tools: - RAG systems - Web search capabilities - API calls for information retrieval\nCapability Extension Tools: - Code interpreters - Terminal access - Function execution capabilities These have been shown to significantly boost model performance compared to prompting or fine-tuning alone.\nWrite Actions: - Data manipulation capabilities - Storage and deletion operations\n\n\nPlanning Architecture\nThe planning process emerges as a four-stage cycle:\n\nPlan Generation: Task decomposition and strategy development\nInitial Reflection: Plan evaluation and potential revision\nExecution: Implementation of planned actions, often involving specific function calls\nFinal Reflection: Outcome evaluation and error correction\n\nChip includes an interesting debate about foundation models as planners, noting Yan LeCun’s assertion that autoregressive models cannot truly plan, though this remains a point of discussion in the field.\n\n\nPlan Execution Patterns\n\nThe execution of agent plans reveals a fascinating interplay between computational patterns and practical implementation. Chip identifies several fundamental execution patterns that form the backbone of agent behaviour, each offering distinct advantages and trade-offs in different scenarios.\n\nExecution Paradigms: The core patterns through which agents transform plans into actions, ranging from simple sequential execution to complex conditional logic.\n\nThe primary execution patterns include:\nSequential Execution: The most straightforward pattern, where actions are performed one after another in a predetermined order. This approach offers predictability and simplicity but may not maximise efficiency when actions could be performed concurrently.\nParallel Execution: Enables multiple actions to be performed simultaneously when dependencies permit. While this pattern can significantly improve performance, it introduces complexity in managing concurrent operations and handling potential conflicts.\nConditional Execution: Implements decision points through if statements, allowing agents to adapt their execution path based on intermediate results or environmental conditions. This pattern introduces crucial flexibility but requires careful handling of branch logic and state management.\nIterative Execution: Utilises for loops to handle repetitive tasks or process collections of items. This pattern is particularly powerful when dealing with datasets or when similar actions need to be performed multiple times with variations.\n\nPattern Selection: The choice of execution pattern often emerges from the intersection of task requirements, system constraints, and performance goals.\n\nThe effectiveness of these patterns depends heavily on the underlying system architecture and the specific requirements of the task at hand. For instance, parallel execution might offer theoretical performance benefits but could introduce unnecessary complexity for simple, linear tasks. Similarly, conditional execution provides valuable flexibility but requires robust error handling and state management to maintain system reliability.\nChip emphasises that these patterns aren’t mutually exclusive - sophisticated agent systems often combine multiple patterns to create more complex and capable execution strategies. This hybrid approach allows for the development of highly adaptable agents that can handle a wide range of tasks while maintaining system stability and performance.\n\n\nPlanning Optimisation\nThe chapter provides several practical tips for improving agent planning:\n\nEnhance system prompts with more examples\nProvide better tool descriptions and parameter documentation\nSimplify complex functions through refactoring\nConsider using stronger models or fine-tuning for plan generation\n\n\n\nFunction Calling Implementation\nThe function calling architecture requires:\n\nTool inventory creation, including:\n\nFunction names and entry points\nParameter specifications\nComprehensive documentation\n\nTool usage specification (required vs. optional)\nVersion control for function names, parameters, and documentation\n\n\n\nPlanning Granularity\nChip introduces an important discussion of planning levels, analogous to temporal planning horizons (yearly plans vs. daily tasks). This presents a fundamental trade-off:\n\nPlanning Trade-off: Higher-level plans are easier to generate but harder to execute, while detailed plans are harder to generate but easier to execute.\n\n\n\nTool Selection and Evaluation\nThe chapter provides a systematic approach to tool selection:\n\nConduct ablation studies to measure performance impact\nMonitor tool usage patterns and error rates\nAnalyze tool call distribution\nConsider model-specific tool preferences (noting that GPT-4 tends to use a wider tool set than ChatGPT)\n\n\n\nMemory Systems\nThe memory architecture comprises two core functions:\n\nMemory Functions: - Memory management - Memory retrieval\n\nThe system supports three types of memory:\n\nInternal knowledge\nShort-term memory\nLong-term memory\n\nThese systems prove crucial for:\n\nManaging information overflow\nMaintaining session persistence\nEnsuring model consistency\nPreserving data structural integrity\n\n\n\nEvaluation and Failure Modes\nThe comprehensive evaluation framework considers:\n\nPlanning effectiveness\nTool execution accuracy\nSystem latency\nOverall efficiency\nMemory system performance"
  },
  {
    "objectID": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#conclusion",
    "href": "posts/2025-01-24-notes-on-ai-engineering-chip-huyen-chapter-6.html#conclusion",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 6",
    "section": "Conclusion",
    "text": "Conclusion\nThe unifying thread of context construction provides a compelling framework for understanding these technologies not as separate entities, but as complementary approaches to extending model capabilities."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "",
    "text": "(This is part of a series of blog posts relating to and responding to the live FastAI course (part 2) being taught October-December 2022. To read others, see the ones listed for the ‘parttwo’ tag.)\nMuch awaited and anticipated, the second part of the FastAI course is being taught live again. If part one is about getting solid foundations and learning how to get going in a practical/useful way, part two is about approaching things from the foundations but with research or ‘impractical’ questions kept in mind. The backdrop of the current iteration is the breakthroughs happening in the world of generative computer vision models like Stable Diffusion, which we’ll explore and deconstruct (and reconstruct?!) over the coming weeks.\nDiving into the details of how things work means that along the way we’re much more likely to encounter (legitimate) specialist vocabulary and techniques as well as a decent dose of jargon. Whereas bringing up the intricacies of particular algorithms, architectures or mathematical methods was unnecessary during part one, it seems like part two is a little bit more of a venue for that kind of material. I will use these blogs as a way of reviewing materials and concepts introduced during the lectures as well as keeping track of the big questions I have.\nIn this blog, in particular, I’ll keep a glossary at the bottom for some new terms which were introduced. I may repeat this for subsequent blog reviews, depending on what’s covered in those lessons. I’ll also keep a section containing new mathematical symbols that are introduced. (This blog mainly relates to the core lecture given during week 1. I’ll update it later with some small extras that came up from the 9A and 9B videos, or expand those into separate posts on their own.)\nStable Diffusion isn’t, in itself, a model that I’m especially interested in, except insofar as it teaches me fundamental principles about the craft of deep learning or about doing research in this field. As such, my plan and current intention is to stick to documenting core mental models or bigger-picture lessons that I’m taking away from the lessons rather than each individual step that Jeremy made along the way. (This seems to be the motivation behind including it in the course at all. Stable Diffusion touches so many topics (big and small) and getting to grips with this one thing will help understand many other things about machine learning and the world of research.)"
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#fundamentals-still-count",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#fundamentals-still-count",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "Fundamentals still count",
    "text": "Fundamentals still count\nEven though there are a hundred and one small innovations and technologies which make something like Stable Diffusion possible, in the end we’re still dealing with Deep Learning and we’re still dealing with finding ways of converting things into numbers which can be used by machines to update weights by way of evaluating loss functions. So many of the individual pieces that make up how you build something like Stable Diffusion amount to:\n\nfigure out how to get this non-number-like thing into a numeric representation (ideally a vector of some kind)\ndo all the usual deep learning things that we’ve done a thousand times and that we know work\nat the end, maybe find a way to convert the numeric representation that our model learned into some kind of form that is useful to us\n\nObviously the details are important and nobody is creating magical generative art with this very high-level hand-wavy explanation, but for someone at the earlier end of their journey into deep learning it is reassuring that the fundamentals continue to have relevance and that those mental models remain useful as a way of thinking about new developments."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#the-tricks-are-the-way",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#the-tricks-are-the-way",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "The tricks are the way",
    "text": "The tricks are the way\nThe other pleasant surprise was the enduring relevance of ‘tricks’. In chapter one of the FastAI book, Jeremy & Sylvain showcase a number of examples where clever approaches are taken to solve problems with Deep Learning:\n\na malware classification program is made by converting malware code into an image which is used to train a model\na fraud detection algorithm is trained by converting images of computer mouse movements\n…and so on\n\nEven amongst the Delft FastAI study group, Kurian trained a classifier to detect genre in music samples using a similar method (i.e. using images as an intermediary form for the samples which were used in training). The book emphasises:\n\n“In general, you’ll find that a small number of general approaches in deep learning can go a long way, if you’re a bit creative in how you represent your data! You shouldn’t think of approaches like the ones described here as”hacky workarounds,” because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.”\n\nMost of these ‘tricks’ seem to relate to either performance improvements (i.e. how can we get this training to happen faster, or with fewer compute needs) or ways of getting your problem domain into a form that we can use deep learning techniques on them. In the case of Stable Diffusion, one of the problems we have to address is how to work in this multi-modal manner, where text is used to represent a particular idea (which in turn needs a vector/numeric representation) but where we also want to represent that same idea in image form.\nAt the same time, we have the whole autoencoder part of the story — whereby we use an encoder to turn a large image into a (smaller-sized) latent representation which can be used in training, and then we use a decoder to turn a noisy latent into a full-sized image — which seems to mainly be about making the training process more efficient.\nEach of these techniques come with their own complexities and histories, but it’s just notable to me how the story of the development of machine learning techniques seems somehow to be a succession of these small incremental innovations that progressively accrue. That’s not to say that there aren’t big breakthroughs in either understanding why things work the way they do, or in the more tactical method space, but it just seemed very apparent in the unpacking of Stable Diffusion that a great deal of creative stitching together of ideas had taken place.\nThe historian in me is fascinated by the different pathways that the field has explored, or the reasons why certain techniques emerged when they did, or how hardware improvements gave tried-and-rejected techniques a new lease of life, but I’m guessing that probably doesn’t help much with the work of research."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-train-the-diffusion-model",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-train-the-diffusion-model",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "💪 What happens when we train the diffusion model",
    "text": "💪 What happens when we train the diffusion model\nA diffusion model is a neural network that we train. The way it works is that it removes noise from an image (passed in as input along with a text prompt) such that the output more closely resembles the prompt. When we are training our network, we pass in the vectorised words along with the latent forms of the images (since those are much smaller file sizes and thus faster / more efficient to train). We use the encoder to get a latent representation of the image that we use for training.\nFor the text caption, we want a way to represent the association of images with text captions in vector space. In other words, if there are various phrases that all represent more or less the same image if you were to translate those phrases into an image, then those should be similar when represented as a vector. The technique or trick for this is to use ‘contrastive loss’, a particular kind of loss function which allows us to calculate the relative similarity of two vectors. This contrastive loss is what gives us the first two letters of ‘CLIP’, a neural network developed by OpenAI.\nThe CLIP model takes some text and outputs an embedding, i.e. some features in vector form that our unet can use for training along with the images in their latent representation form."
  },
  {
    "objectID": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-generate-an-image",
    "href": "posts/2022-10-17-fastai-part-2-lesson-9-stable-diffusion.html#what-happens-when-we-generate-an-image",
    "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
    "section": "🎨 What happens when we generate an image",
    "text": "🎨 What happens when we generate an image\nWhen generating our image we can use the neural network we trained to progressively remove noise from our candidate image. We start off with a more or less completely noisy image, then apply the unet to it and it returns the noise that it calculates is sitting on top of a latent representation that approximates the vectorised version of our prompt. We take a fraction of that, remove it, and repeat a few times. (Currently that can take as many as 50 iterations before we reach a really impressive image, but new techniques are in review which would dramatically reduce the need for so many iterations.)\nNote that it is during the inference stage where we need the decoder part of our (VAE) encoder to turn a latent tensor representation of an image into a fully-fledged large picture."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "",
    "text": "Really enjoyed this chapter. My tidied notes from my readings follow below. 150 pages in and we’re starting to get to the good stuff :)"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#overview-and-context",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#overview-and-context",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Overview and Context",
    "text": "Overview and Context\nThis chapter serves as the first of two chapters (Chapters 3 and 4) dealing with evaluation in AI Engineering. While Chapter 4 will delve into evaluation within systems, Chapter 3 addresses the fundamental question of how to evaluate open-ended responses from foundation models and LLMs at a high level. The importance of evaluation cannot be overstated, though the author perhaps takes this somewhat for granted. The chapter provides a comprehensive framework for understanding various evaluation methodologies and their applications."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#challenges-in-evaluating-foundation-models",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#challenges-in-evaluating-foundation-models",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Challenges in Evaluating Foundation Models",
    "text": "Challenges in Evaluating Foundation Models\nThe evaluation of foundation models presents several unique and complex challenges that make systematic assessment difficult:\n\nExisting benchmarks become increasingly inadequate as models improve in their capabilities\nAs models become better at writing and mimicking human-like responses, evaluation becomes more complex and nuanced\nMany foundation models are API-driven black boxes, limiting access to internal workings\nModels continuously develop new capabilities, requiring constant adaptation of evaluation methods\nThere has been notably limited investment in evaluation studies and technologies compared to the extensive resources devoted to enhancing model capabilities\nThe improvement in model performance necessitates the continuous development of new benchmarks\nWithout a systematic approach to evaluation, progress can be hindered by various headwinds"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#language-model-metrics",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#language-model-metrics",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Language Model Metrics",
    "text": "Language Model Metrics\nThe chapter includes a technically detailed section on understanding language model metrics, which while math-heavy, provides fundamental insights into model capabilities:\n\nEntropy\nCross-entropy\nPerplexity\n\nThese metrics serve as underlying measures to understand what’s happening within the models and assess their power and conversational abilities. While this section spans 4-5 pages of technical content, it provides some useful foundational understanding of how we can measure a language model’s intrinsic capabilities."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#downstream-task-performance-measurement",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#downstream-task-performance-measurement",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Downstream Task Performance Measurement",
    "text": "Downstream Task Performance Measurement\nThe chapter transitions from intrinsic metrics to evaluating actual capabilities, dividing evaluation into exact and subjective approaches.\n\nExact Evaluation\nThere are two principal approaches to exact evaluation:\n\nFunctional Correctness Assessment\n\nEvaluates whether the LLM can successfully complete its assigned tasks\nFocuses on practical capability rather than theoretical metrics\nExample: In coding tasks, checking if generated code passes all unit tests\nProvides clear, objective measures of success\n\nSimilarity Measurements Against Reference Data Four distinct methods are identified:\n\nHuman Evaluator Judgment\n\nRequires manual comparison of texts by human evaluators\nHighly accurate but time and resource-intensive\nLimited scalability due to human involvement\nOften considered the gold standard despite limitations\n\nExact Match Checking\n\nCompares generated response against reference responses for exact matches\nMost effective with shorter, specific outputs\nLess useful for verbose or creative outputs\nProvides binary yes/no results\n\nLexical Similarity\n\nEmploys established metrics like BLEU, ROUGE, and METEOR\nFocuses on word overlap and structural similarities\nKnown to be somewhat crude in their assessment\nWidely used despite limitations due to ease of implementation\n\nSemantic Similarity\n\nUtilizes embeddings for comparing textual meaning\nLess sensitive to specific word choices than lexical approaches\nQuality depends entirely on the underlying embeddings algorithm\nMay require significant computational resources\nGenerally provides more nuanced comparison than lexical methods\n\n\n\nThe chapter includes a brief but relevant sidebar on embeddings and their significance in evaluation, though this digression seemed a bit out of place in the overall flow."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#ai-as-judge",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#ai-as-judge",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "AI as Judge",
    "text": "AI as Judge\nThis section explores the increasingly popular approach of using AI systems to evaluate other AI systems.\n\nBenefits\n\nSignificantly faster than human evaluation processes\nGenerally more cost-effective than human evaluation at scale\nStudies have shown strong correlation with human evaluations in many cases\nAI judges can provide detailed explanations for their decisions\nOffers greater flexibility in evaluation approaches\nEnables systematic and consistent evaluation at scale\n\n\n\nThree Main Approaches\n\nIndividual Response Evaluation\n\nAssesses response quality based solely on the original question\nOften implements numerical scoring systems (e.g., 1-5 scale)\nEvaluates responses in isolation without comparison\n\nReference Response Comparison\n\nEvaluates generated response against established reference responses\nUsually produces binary (true/false) outcomes\nHelps ensure responses meet specific criteria\n\nGenerated Response Comparison\n\nCompares two generated responses to determine relative quality\nPredicts likely user preferences between options\nParticularly useful for:\n\nPost-training alignment\nTest-time compute optimization\nModel ranking through comparative evaluation\nGenerating preference data\n\n\n\n\n\nImplementation Considerations\n\n\nTable 3-3 (page 139) provides an overview of different AI judge criteria used by various AI tools\nNotable lack of standardization across different platforms and approaches (see above)\nVarious scoring systems available, each with their own trade-offs\nAdding examples to prompts can improve accuracy but increases token count and costs\nCareful balance needed between evaluation quality and resource consumption\n\n\n\nLimitations and Challenges\n\nAI judges can show inconsistency in their judgments\nCosts can escalate quickly, especially when using stronger models or including more context\nEvaluation criteria often remain ambiguous and difficult to standardize\nSeveral inherent biases identified:\n\nSelf-bias: Models tend to favor responses generated by themselves\nVerbosity bias: Tendency to favor longer, more detailed answers\nOther biases common to AI applications in general"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#specialized-judges",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#specialized-judges",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Specialized Judges",
    "text": "Specialized Judges\nThis section presents an innovative challenge to the conventional wisdom of using the strongest available model as a judge. The author introduces a compelling alternative approach:\n\nSmall, specialized judges can be as effective as larger models for specific evaluation tasks\nMore cost-effective and efficient than using large language models\nCan be trained for highly specific evaluation criteria\nDemonstrates comparable performance to larger models like GPT-4 in specific domains\n\nThree types of specialized judges are identified: 1. Reward models (evaluating prompt-response pairs) 2. Reference-based judges 3. Preference models\nThis represents a novel approach that could significantly impact evaluation methodology in the field."
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#comparative-evaluation-for-model-ranking",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#comparative-evaluation-for-model-ranking",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Comparative Evaluation for Model Ranking",
    "text": "Comparative Evaluation for Model Ranking\n\nMethodology\n\nFocuses on binary choices between two samples\nSimpler for both humans and AI to make comparative judgments\nUsed in major leaderboards like LMSIS\nRequires evaluation of multiple combinations to establish rankings\nVarious algorithms available for efficient comparison\n\n\n\nAdvantages\n\nMore intuitive evaluation process\nOften more reliable than absolute scoring\nReduces cognitive load on evaluators\nProvides clear preference data\n\n\n\nChallenges\n\nHighly data-intensive nature affects scalability\nLacks standardization across implementations\nDifficulty in converting comparative measures to absolute metrics\nQuality control remains a significant concern\nNumber of required comparisons can grow rapidly with model count"
  },
  {
    "objectID": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#key-takeaways-and-future-implications",
    "href": "posts/2025-01-21-notes-on-ai-engineering-chip-huyen-chapter-3.html#key-takeaways-and-future-implications",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 3",
    "section": "Key Takeaways and Future Implications",
    "text": "Key Takeaways and Future Implications\n\nThe emergence of smaller, specialized judge models represents a significant shift from the traditional approach of using the largest available models\nComparative evaluation offers promising approaches but requires careful consideration of scalability and implementation\nThe field continues to evolve rapidly, requiring flexible and adaptable evaluation strategies\nSets up crucial discussion for system-level evaluation in Chapter 4\nHighlights the ongoing tension between evaluation quality and resource efficiency\n\nThe chapter effectively establishes the foundational understanding necessary for the more practical, system-focused evaluation discussions to follow in Chapter 4."
  },
  {
    "objectID": "posts/2021-12-30-robust-python-2.html",
    "href": "posts/2021-12-30-robust-python-2.html",
    "title": "What’s special about types in Python?",
    "section": "",
    "text": "The first section of Robust Python dives into types. We begin by taking a step back to think about what exactly types are being used for, and what they might bring us. Python was not (until v3.5) a language with which you could easily use typing. I remember going to the Pylondinium conference in London in 2018 and going to a talk by Bernat Gabor about type hints in Python. Back then I didn’t have much of a sense of how new they were to many people, but even now I don’t get the feeling that they’ve been universally adopted. Hence Patrick’s book, I suppose…\nA type is defined in the book as being “a communication method”, both to / for computers (“mechanical representation”) as well as for humans (“semantic representation”). For the computer, when a variable is of a certain type this determines what methods can be called on that particular object. As such, though I’m straying into territory I don’t fully understand, I believe it also helps with compilation efficiency. (Python is a dynamically-typed language so any errors or type mismatches will only become apparent at runtime, however).\nFor humans, types can help signal intent. This connects with my previous chapter summary from this book where I stated that code should communicate intent well to be considered ‘robust’. Take the following simple code snippet:\ndates = [...]\n\ndef process_date(input):\n  date = extract_date(input)\n  dates.append(date)\n  return date\nWe have an extract_date function (defined elsewhere in the code), but we have no real sense of what this input parameter would be. Are we taking in strings as input? Are we taking in datetime.datetime objects? Does the extract_date function accept both, or do we need to ensure that we are only taking a specific type? All these questions could be cleared up with a simple type hint as part of the function definition, like so:\ndates = [...]\n\ndef process_date(input: datetime.datetime):\n    date = extract_date(input)\n  dates.append(date)\n  return date\nNow we know what the input should be, and we can also add a type hint to the extract_date function as well which will help communicate our intent.\nWe also learn how Python is more towards the ‘strongly-typed’ side of things on the language spectrum. If you try to concatenate a list with a dict in Python using the + operator, Python will throw a TypeError and fail. If you try to do the same in Javascript you get two different answers depending on the order of the two operands:\n>>> [] + {}\n\"[object Object]\"\n\n>>> {} + []\n0\nFor our purposes, using Python, we can use the strong typing to our advantage.\nPython is dynamically typed, though, which takes a bit more caution to handle in a robust manner. Any type mismatches will only be found at runtime — at least using just the vanilla install of the language without any extra imports or modules.\nThe chapter ends with a brief discussion of duck typing, defined as “the ability to use objects and entities in a programming language as long as they adhere to some interface”. We gain a lot in terms of increased composability, but if you rely on this feature of the language too much then it can become a hindrance in terms of communicating intent.\nThis chapter didn’t add too many new concepts or skills to my current understanding of the benefits of types, but it was useful to have this concept of ‘communicating intent’ to be reiterated. When I think back to how I’ve heard types mentioned in the past, they often get cast in a technical sense, whereas thinking about communication between developers I think is a more motivating framing."
  },
  {
    "objectID": "posts/2021-09-10-chapter1and2recall.html",
    "href": "posts/2021-09-10-chapter1and2recall.html",
    "title": "Retrieval Practice with fastai chapters 1 and 2",
    "section": "",
    "text": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover.\nWe start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I’ve seen much in Python imports.\n\n# !pip install fastai\nfrom fastai.vision.all import *\n\nThen we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not.\nThe simple assert testing was a little trick that I saw mentioned somewhere this past week. It’s not a full-fledged test suite, but it’s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else.\n\n# define the function that'll classify the images\ndef is_cat(string):\n    return string[0].isupper()\n\nassert is_cat(\"abs\") == False\nassert is_cat(\"Abs\") == True\n\nNow we have to import the data for the files and apply whatever custom transforms we want applied to them.\nI had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be.\nIt’s interesting that we actually don’t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that’s because the task is so close to that of the original resnet architecture.\n\n# import the data\npath = untar_data(URLs.PETS)/'images'\ndls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224))\n\nThen it’s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we’ll see displayed in the output.\n\n# instantiate a learner\nlearner = cnn_learner(dls, resnet34, metrics=error_rate)\n\n# fine-tune the model\nlearner.fine_tune(5)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.140326\n      0.019799\n      0.008119\n      00:19\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.046906\n      0.021923\n      0.006089\n      00:24\n    \n    \n      1\n      0.041144\n      0.009382\n      0.004060\n      00:25\n    \n    \n      2\n      0.028892\n      0.004109\n      0.002030\n      00:25\n    \n    \n      3\n      0.008950\n      0.002290\n      0.001353\n      00:25\n    \n    \n      4\n      0.004486\n      0.002822\n      0.001353\n      00:25\n    \n  \n\n\n\nAnd here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad!\n\nlearner.show_results()"
  },
  {
    "objectID": "posts/2021-12-11-redaction-progress-week-one.html",
    "href": "posts/2021-12-11-redaction-progress-week-one.html",
    "title": "73% accuracy for redaction object detection",
    "section": "",
    "text": "Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all.\nOnce I had my redactions, I needed to convert the files from a Prodigy format into a .coco annotation format. I am using IceVision, a really useful computer vision library, for which it is easier if I pass in the annotations in the .coco format.\nFrom that point, it was fairly easy to follow the steps of the object detection tutorial outlined in the IceVision documentation. I ran into some problems with Paperspace Gradient not easily installing and importing IceVision. For some reason files don’t get unzipped on Paperspace, but it’s possible to just do this manually:\n\nDo the basic install, including the import of icevision.all. Wait for the error to get raised, then open up a terminal and enter:\n\ncd /root/.icevision/mmdetection_configs/\nrm v2.16.0.zip\nwget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip\nunzip v2.16.0.zip\nThen run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal):\njupyter nbextension enable --py widgetsnbextension\nThis enables ipywidgets in the notebook, I think.\nOnce through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected VFNet as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%:\n\nIf we look at some of the results (using model_type.show_results()) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect:\n\nI was surprised that something like this worked as well as it did:\n\nIt wasn’t perfect, but I don’t remember having annotated too many of this specific redaction type, so I’m fairly happy with how it worked out. You can see it still makes a number of mistakes and isn’t always precise about where the boxes should go. I hope that’ll improve as I add more examples of this type of redaction.\nMy next steps for this project include the following:\n\ncreate synthetic data. The redactions are probably easy enough to mimic where we’ll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It’ll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy.\npotentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates).\nthink through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training.\nadd in experiment tracking. I’ve never used something like Weights & Biases, so I’m excited to try that out and have a real process for tracking my progress throughout this project.\ncleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It’s starting to get a bit unwieldy and I’m worried I’ll start to forget the order things were done and some of those small details."
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html",
    "href": "posts/2021-09-16-ch4-tensors.html",
    "title": "Tensors all the way down",
    "section": "",
    "text": "Code\n#!pip install -Uqq fastbook\n#!pip install fastai\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\n\nmatplotlib.rc('image', cmap='Greys')\nIn chapter 4 of the book, we start to really get into what’s going on under the hood with deep learning. Turns out, tensors are a pretty important piece. We are still in the realm of computer vision, and we are going to work on distinguishing between handwritten digits.\nFirst we use the untar_data function to grab a sample of data from the famous MNIST data set. This function returns the path where that data was stored locally.\nNow we want to briefly inspect the contents of one of our training data folders. This is for the number 7. You can see that it’s just a series of .png image files.\nIn order to look at a single image, we can just open it using Image.open which comes from the Python Image Library (PIL).\nJupyter knows how to display various files, so we can see that image above. But what exactly is an image made up of? If we turn that image into an array, or to a tensor (the next two cells), slicing them so you aren’t just seeing zeros on the edges, then you can see that these images are made up of a matrix of values from 0 to 255.\nWe can use the show_image function to turn those 0-255 values back into an image, like so:\nA really nice way of visualising exactly what is going on is to turn this image into a pandas dataframe and then for every individual pixel value, use that value as the background gradient for that cell. Here’s an example of part of an image of a handwritten number 3.\nSo now we have a toolkit of ways to view the pixel values that make up an image. We also have a mental model for how we can think about images and how computers represent those images stored on our machine.\nBut how might we then best go about knowing whether a particular image is a 3, let’s say, or a 7?\nOne naive approach might be just to get the average value for each individual pixel for all of the threes in our training data, and then just compare the difference between our sample image and this average representation.\nLet’s try that now."
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html#getting-the-average-values-for-our-images",
    "href": "posts/2021-09-16-ch4-tensors.html#getting-the-average-values-for-our-images",
    "title": "Tensors all the way down",
    "section": "Getting the average values for our images",
    "text": "Getting the average values for our images\nWe’ll set up two lists with images of the digits converted to tensors. You can see that we have 6131 images in our ‘threes’ list.\n\nthrees_tensors = [tensor(Image.open(i)) for i in threes_dir]\nsevens_tensors = [tensor(Image.open(i)) for i in sevens_dir]\nlen(threes_tensors)\n\n6131\n\n\nWe can view an individual image, as before, with the show_image function:\n\nshow_image(threes_tensors[3])\n\n<AxesSubplot:>\n\n\n\n\n\nNow in order to get the average values for each pixels, we can use the stack method to handle the first part of this.\nThink of it as basically adding an extra dimension to your data structure, such that you have a ‘stack’ (it’s a useful mental image) of those images.\n\nthrees_stack = torch.stack(threes_tensors)\n\nIf we look at the shape of our Pytorch stack now, we can see we have our 28x28 image, but we have a stack of 6131 of them.\n\nthrees_stack.shape\n\ntorch.Size([6131, 28, 28])\n\n\nEach individual image is still a tensor:\n\na_three = threes_stack[3][4:16, 4:16]\na_three\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0, 104, 253, 253, 253, 255, 253],\n        [  0,   0,   0,   0,   0, 178, 248, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 186, 252, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 186, 252, 243, 172, 172,  39,  39],\n        [  0,   0,   0,   0,   0,  39,  53,  47,   0,   0,   0,  29],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  54, 208],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   3,  41, 253, 252],\n        [  0,   0,   0,   0,   0,   0,   5,  41, 165, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 109, 163, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 186, 252, 252, 252, 252, 253, 252],\n        [  0,   0,   0,   0,   0, 187, 253, 253, 253, 253, 134,  77]], dtype=torch.uint8)\n\n\nGenerally speaking, for some operations (like getting the mean average) we’re going to want to convert the values to floats, and it also makes sense to normalise the values at the same time. Instead of having a range of 0-255, we want a range of 0-1.\n\nthrees_stack[3][4:16, 4:16].float()/255\n\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9725, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9529, 0.6745, 0.6745, 0.1529, 0.1529],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.2078, 0.1843, 0.0000, 0.0000, 0.0000, 0.1137],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8157],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.1608, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.1608, 0.6471, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7333, 0.9922, 0.9922, 0.9922, 0.9922, 0.5255, 0.3020]])\n\n\nNow that we’ve done it for a single image, we can perform the same operations on our whole Pytorch stack.\n\nthrees_stack = torch.stack(threes_tensors).float()/255\nsevens_stack = torch.stack(sevens_tensors).float()/255\nthrees_stack.shape # it's good to keep in touch with the shape of our stack\n\ntorch.Size([6131, 28, 28])\n\n\nNow we’re getting closer to our desired result. We can squash the stack down into just two dimensions with a simple call to .mean(0), where 0 is the index value of the dimension through which we want to calculate the mean. You’ll see now that the shape property of our threes_means variable is simply a 28x28 image.\n\nthrees_means = threes_stack.mean(0)\nthrees_means.shape\n\ntorch.Size([28, 28])\n\n\nWhen we show that image, you’ll see that it’s a sort of blurry ‘ideal’ version of a three\n\nshow_image(threes_means)\n\n<AxesSubplot:>\n\n\n\n\n\nWe can do the same for the sevens:\n\nsevens_means = sevens_stack.mean(0)\nshow_image(sevens_means)\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html#validation-comparing-our-average-three-with-a-specific-three",
    "href": "posts/2021-09-16-ch4-tensors.html#validation-comparing-our-average-three-with-a-specific-three",
    "title": "Tensors all the way down",
    "section": "Validation: Comparing our average three with a specific three",
    "text": "Validation: Comparing our average three with a specific three\nNow we have our average values, we want to compare these with a single specific digit image. We’ll get the difference between those values and whichever difference is the smallest will most likely be the best answer.\nOur averaged three is still threes_means and we can get a single three from our validation set like this:\n\nthrees_dir_validation = (path/'valid/3').ls().sorted()\nsevens_dir_validation = (path/'valid/7').ls().sorted()\n\nim3_validation_path = threes_dir_validation[5]\nim3_validation = tensor(Image.open(im3_validation_path)).float()/255\n\nim7_validation_path = sevens_dir_validation[3]\nim7_validation = tensor(Image.open(im7_validation_path)).float()/255\n\nshow_image(im3_validation)\n\n<AxesSubplot:>\n\n\n\n\n\n\nshow_image(im7_validation)\n\n<AxesSubplot:>\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCalculating the difference between two objects\n\n\nWe can use two different measurements of the difference between our mean value and the individual image:\n\nmean absolute difference (calculated by taking the mean of the absolute difference between the two tensor values). Also known as the L1 Norm.\nroot mean squared error (calculated by first squaring the difference between the two tensor values, taking the mean and then square rooting those values). Also known as the L2 Norm.\n\nThe second option, the RMSE, gives a stronger signal, you might say, for the differences because you are taking the averages from the squared values. Squaring the difference also takes care of any negative values you might have.\n\nmean_absolute_difference_3 = (im3_validation - threes_means).abs().mean()\nroot_mean_squared_error_3 = ((im3_validation - threes_means)**2).mean().sqrt()\nmean_absolute_difference_3, root_mean_squared_error_3\n\n(tensor(0.1188), tensor(0.2160))\n\n\n\nmean_absolute_difference_7 = (im7_validation - threes_means).abs().mean()\nroot_mean_squared_error_7 = ((im7_validation - threes_means)**2).mean().sqrt()\nmean_absolute_difference_7, root_mean_squared_error_7\n\n(tensor(0.1702), tensor(0.3053))\n\n\nWe can now see that our individual three image is indeed closer to the threes_means composite image than to the sevens_means composite image. A smaller value at this point is what we’re looking for, and the threes have it.\nIt turns out that there is another way to calculate the difference that’s built in to Pytorch as loss functions:\n\nF.l1_loss(im3_validation, threes_means), F.mse_loss(im3_validation, threes_means).sqrt()\n\n(tensor(0.1188), tensor(0.2160))\n\n\nIt’s a bit more concise, though it does obscure what’s going on under the hood in terms of calculations."
  },
  {
    "objectID": "posts/2021-09-16-ch4-tensors.html#results-of-the-naive-approach",
    "href": "posts/2021-09-16-ch4-tensors.html#results-of-the-naive-approach",
    "title": "Tensors all the way down",
    "section": "Results of the naive approach",
    "text": "Results of the naive approach\nSo this tells us that our single three is closer to an ideal 3 than an ideal 7, which is great since it reflects the ground truth of our problem. But can we get a metric to know how well we perform on average against a large number of threes and sevens from our validation set?\nYes, since we have that dataset ready for use!\n\nvalid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()]).float()/255\nvalid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()]).float()/255\nvalid_3_tens.shape, valid_7_tens.shape\n\n(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))\n\n\nNow we can write a helper function that will allow us to calculate the distance between two images. We use the RMSE or L1 Norm for this difference calculation:\n\ndef mnist_distance(a, b):\n    return (a - b).abs().mean((-1, -2))\n\nWe can use this function on our previous example:\n\nmnist_distance(im3_validation, threes_means)\n\ntensor(0.1188)\n\n\nWe can continue onwards by comparing the rank 3 tensor with the rank 2 tensor. This brings a concept called ‘broadcasting’ into play.\nWe are comparing a tensor with 2 dimensions with a tensor with 3 dimensions, so Pytorch behaves as if both tensors have three dimensions, and (without taking extra memory) pretends as if there are multiple copies of the image in 2 dimensions. This effectively makes it as if we’re comparing two 3-dimensional tensors.\nFrom this next calculation, we see returned back a collection of the distances between all of the validation images.\n\nmnist_distance(valid_3_tens, threes_means)\n\ntensor([0.1328, 0.1523, 0.1245,  ..., 0.1383, 0.1280, 0.1138])\n\n\nIn order to check whether an image is a 3, we basically need to know whether the difference for the number 3 is larger than the difference for the number 7.\nWe can write a helper function for that:\n\ndef is_3(img):\n    return mnist_distance(img, threes_means) < mnist_distance(img, sevens_means)\n\nWe can now check our ground truth examples:\n\nis_3(im3_validation), is_3(im7_validation)\n\n(tensor(True), tensor(False))\n\n\nThat’s what we expected to happen. Our 3 image is a 3, and our 7 image is not a 3.\nIf we want to check the distance in general for our validation set, we have to convert them into floats and then get the mean, but it’s really easy. Again, this uses broadcasting:\n\nvalidation_accuracy_3 = is_3(valid_3_tens).float().mean()\nvalidation_accuracy_7 = 1 - is_3(valid_7_tens).float().mean()\nvalidation_accuracy_3, validation_accuracy_7\n\n(tensor(0.9168), tensor(0.9854))\n\n\nOverall, then, we can calculate how good our toy or baseline model is for the entire problem:\n\n(validation_accuracy_3 + validation_accuracy_7) / 2\n\ntensor(0.9511)\n\n\nPretty good!\nThis was of course just a naive way to solve the problem. There are more advanced techniques which we’ll tackle next."
  },
  {
    "objectID": "posts/2022-04-04-ml-portfolio-best-practices.html",
    "href": "posts/2022-04-04-ml-portfolio-best-practices.html",
    "title": "Some characteristics of best-in-class ML portfolio projects",
    "section": "",
    "text": "Ekko was the last time I worked on a big project that would be presented publicly. An open-source framework that provided realtime infrastructure and in-transit message processing for web applications was a group project that I worked on together with three other colleagues, and we took the time to really make the how and the why really explicit. We made animations, diagrams, charts, and I learned a lot about what’s hard when explaining technical projects, even when the audience is expected to be (mostly) technically literate.\nI’ve been working on my redaction project since December and slowly but surely I’m tying the ends together and getting ready for it to come to a close. As part of the final touches, I want to offer something equivalent to how we presented Ekko. From reading around and exposure to various projects over the years, it seems to me that machine learning projects sometimes have different emphases and conventions. This blog post is my attempt to list some of the characteristics of really great ML portfolio projects, with an emphasis on how the project is presented."
  },
  {
    "objectID": "posts/2022-04-04-ml-portfolio-best-practices.html#some-top-projects",
    "href": "posts/2022-04-04-ml-portfolio-best-practices.html#some-top-projects",
    "title": "Some characteristics of best-in-class ML portfolio projects",
    "section": "Some top projects",
    "text": "Some top projects\n\nHealthsea by Edward Schmuhl (@aestheticedwar1) is my current favourite project writeup, blending amazing visuals, full explanation and a clear overview\nThis project (by @ahmed_besbes_) was recommended to me and although it’s more of a step-through of how the project works and was created, it also is clearly presented and very visual.\nFor computer vision projects, Hugging Face Spaces is a great place to find interesting Gradio demos, though after a while they blend into each other a little. HF Spaces also doesn’t seem like it gets used for full project explanation that often."
  },
  {
    "objectID": "posts/2022-04-04-ml-portfolio-best-practices.html#characteristics-of-top-projects",
    "href": "posts/2022-04-04-ml-portfolio-best-practices.html#characteristics-of-top-projects",
    "title": "Some characteristics of best-in-class ML portfolio projects",
    "section": "Characteristics of top projects",
    "text": "Characteristics of top projects\nSome things I think make a great portfolio project stand out:\n\nvisual design — looks count for a lot, for better or for worse.\ninteractivity — if there is some kind of a demo or application that I can play around with in order to relate to concepts being written about, that’d be great.\nvisual explanations alongside pure text — a diagram or animation can really help bring explanations to life.\na clear overview — the structure of the writeup should be clear and readers should be able to get a high-level overview first without necessarily needing to read through every last detail.\nexplain what problem you’re solving — spend (probably) more time than you think is necessary to explain what problem you’re solving and set up the context for the work you did.\ncode snippets are ok, but don’t just dump your source code.\npresent your dead ends — don’t just present the happy path; feel free to present things that didn’t work out as well. Readers will want to know that you encountered difficulties and there are benefits from seeing how you made decisions along the way.\npresent further work and next steps — offer hints at what other work could be done on the project, even if you’re done with it for now.\ndon’t lose track of the use case — show that you thought about the specific problem you were solving and not just as a technical problem in a void. (Real-world use cases have constraints, and your solution should live within a universe where those constraints directed you).\nFeel free to link out — you can easily link to other places where you’ve gone into the details about a particular problem you encountered. No need to cram every single last detail into the project portfolio.\nDon’t forget the purpose of the portfolio — it doesn’t need to be an exhaustive catalogue of every last detail; it just needs to offer a compelling overview that is understandable as an independent entity.\n\nThere are other aspects which are more table stakes for anything you write online — no typos, clear writing and so on.\nI took the time to step back from the project to write this down as I move into a phase where I’ll increasingly focus on the full writeup and I wanted to have a list to remind me of the things I valued in these kinds of projects.\nIf you have good examples of ML portfolio projects (or really great blog write-ups with interactivity and so on), please let me know in the comments!"
  },
  {
    "objectID": "posts/2021-11-25-entr-reruns-tests.html",
    "href": "posts/2021-11-25-entr-reruns-tests.html",
    "title": "entr: a tool to run commands when files change",
    "section": "",
    "text": "It’s a fairly common pattern that you have some code that you’re repeatedly running. Perhaps you’re fixing a failing test, and you just have to keep running it every time you make a fix.\nEnter entr. This handy little tool reruns a particular command whenever changes are detected in a particular set of files.\nLet’s take the example I mentioned above: you have a failing test that you’re debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in src and you’re using pytest, then you could use something like the following:\nls src/*.py | entr -c pytest test.py::test_some_feature\nSo now, any time you change any Python file inside the src folder, it’ll rerun your test. The -c flag will clear the terminal every time the test runs.\n[Many thanks to calmcode for continuing to make these really useful videos.]"
  },
  {
    "objectID": "posts/2023-05-14-mu123-complete-maths.html",
    "href": "posts/2023-05-14-mu123-complete-maths.html",
    "title": "Finishing MU123",
    "section": "",
    "text": "I completed and submitted the final exam for the MU123 module that I’ve been studying since October last year. This module is the first step on my journey towards a BSc Mathematics degree from the Open University, something I’m really happy I have the time to do on the side of my full-time job.\nMathematics was always something I enjoyed studying at school, but for various reasons I stopped taking it as a subject by the time I was 15 or so. This means that I skipped many (most) important skills and subjects like linear algebra, calculus and more. (As an aside, looking back now I find it amazing that it was even possible for me to emerge from a supposedly fancy school with such a shoddy grasp — if at all — of basic techniques and concepts.)\nNow I get the chance to fill in the missing pieces, pieces that feel increasingly fundamental the further I progress down the rabbit hole(s) of both software engineering and machine learning. The Q31 degree programme is very slow to start, mainly because the early modules take a good amount of time to address people like me who maybe never studied things previously at school. The loose mental model you hear thrown about is that the first set of modules should bring you to a solid A-Level standard, and from then on things get a bit more serious in terms of the topics covered.\nThis matches my experience so far with MU123. For those for whom mathematics doesn’t feel as distant a memory, there is a fast-track approach where you start a bit further down the tracks, but eventually the idea is that everyone meets up together later on.\nI really enjoyed the topics covered in MU123. There was a good mix of variety as well as some depth, especially as we started to reach the later units. This module is often taken by students working towards other degrees (in the sciences, perhaps) so it is purposefully open and welcoming. It was clear that a lot of effort had been taken to make everything approachable and engaging, even if some of the ‘multimedia’ felt a bit old.\nI wonder whether the more advanced Maths modules will dispense with the hand-holding and friendly tones of the excellent written materials issued for MU123. For now, though, I don’t have any new classes starting until October, when I’ll be taking a double module together. (Results from MU123 will be issued at some point, though I’m not sure exactly when.) I plan on doing some personal exploration in a couple of areas of mathematics as well as working on some machine learning projects. More on that here in the coming weeks!"
  },
  {
    "objectID": "posts/2022-05-15-fashion-mnist-neural-network.html",
    "href": "posts/2022-05-15-fashion-mnist-neural-network.html",
    "title": "A neural network for Fashion MNIST data",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntraining_dresses = [item[0][0] for item in training_data if item[1] == 3]\ntraining_pullovers = [item[0][0] for item in training_data if item[1] == 2]\ntest_dresses = [item[0][0] for item in test_data if item[1] == 3]\ntest_pullovers = [item[0][0] for item in test_data if item[1] == 2]\n\ntraining_dresses_tensor = torch.stack(training_dresses)\ntraining_pullovers_tensor = torch.stack(training_pullovers)\ntest_dresses_tensor = torch.stack(test_dresses)\ntest_pullovers_tensor = torch.stack(test_pullovers)\n\ntrain_x = torch.cat([training_dresses_tensor, training_pullovers_tensor]).view(-1, 28*28)\ntrain_y = torch.cat([torch.ones(len(training_dresses)), torch.zeros(len(training_pullovers))]).unsqueeze(1)\n\nvalid_x = torch.cat([test_dresses_tensor, test_pullovers_tensor]).view(-1, 28*28)\nvalid_y = torch.cat([torch.ones(len(test_dresses)), torch.zeros(len(test_pullovers))]).unsqueeze(1)\n\ntrain_dset = list(zip(train_x, train_y))\nvalid_dset = list(zip(valid_x, valid_y))\n\ntrain_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\n\ndls = DataLoaders(train_dl, valid_dl)\n\ndef initialise_params(size, std=1.0):\n    return (torch.randn(size) * std).requires_grad_()\n\ndef fashion_mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1 - predictions, predictions).mean()\n\ndef batch_accuracy(x_batch, y_batch):\n    preds = x_batch.sigmoid()\n    correct = (preds > 0.5) == y_batch\n    return correct.float().mean()\n\n\nIn the previous post we used stochastic gradient descent to train a model to fit a linear function to our Fashion MNIST data, specifically the difference between a pullover and a dress.\nIn this final stage, we will take the next step to creating a neural network in code that will be used to detect that same difference between a pullover and a dress. The key difference here is that we will need to ‘add non-linearity’ to our function. I have no mathematics background so I have very little intuitive (or learned!) understanding of specifically what that means, but my current mental model as learned during the course is that linear functions just aren’t flexible enough to learn more complex patterns. In the end, what we want is a function that will fit to the patterns in our training data (as mapped to a multidimensional space). Simple linear functions aren’t going to cut it.\nWhat this looks like in code is this:\n\nweights1 = initialise_params((28*28, 30))\nbias1 = initialise_params(30)\nweights2 = initialise_params((30, 1))\nbias2 = initialise_params(1)\n\ndef simple_network(x_batch):\n    result = x_batch@weights1 + bias1\n    result = result.max(tensor(0.0))\n    result = result@weights2 + bias2\n    return result\n\nYou can see the three layers of our simple network pretty clearly in the code above. The middle layer is what otherwise is known as a ReLU or rectified linear unit. It basically means that negative values passing through that function become zero and all positive values are unchanged. When you plot the function it looks like this:\n\nplot_function(F.relu)\n\n\n\n\nWhen we put a non-linear function in between two linear functions, then this network is able to encode and express more complicated patterns. This is basically all we’re doing with deep learning: we stack these layers on to make the functions more and more capable of modelling and representing complex things.\nWe can express the above simple network in PyTorch-specific code (functionally it’s the same):\n\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\nAt this point, training a model is similar to what we did last time round:\n\nlearn = Learner(dls, simple_net, opt_func=SGD, loss_func=fashion_mnist_loss, metrics=batch_accuracy)\nlearn.fit(30, 0.1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.166980\n      0.079354\n      0.959000\n      00:00\n    \n    \n      1\n      0.087369\n      0.057471\n      0.961000\n      00:00\n    \n    \n      2\n      0.059999\n      0.050108\n      0.963500\n      00:00\n    \n    \n      3\n      0.048050\n      0.046509\n      0.963500\n      00:00\n    \n    \n      4\n      0.041847\n      0.044165\n      0.964000\n      00:00\n    \n    \n      5\n      0.038310\n      0.042841\n      0.963500\n      00:00\n    \n    \n      6\n      0.036514\n      0.041464\n      0.964500\n      00:00\n    \n    \n      7\n      0.034653\n      0.040640\n      0.964500\n      00:00\n    \n    \n      8\n      0.033204\n      0.039827\n      0.965000\n      00:00\n    \n    \n      9\n      0.032370\n      0.039344\n      0.965000\n      00:00\n    \n    \n      10\n      0.032060\n      0.038778\n      0.965000\n      00:00\n    \n    \n      11\n      0.031814\n      0.038854\n      0.965500\n      00:00\n    \n    \n      12\n      0.031212\n      0.037872\n      0.965000\n      00:00\n    \n    \n      13\n      0.030837\n      0.037836\n      0.964500\n      00:00\n    \n    \n      14\n      0.030193\n      0.037372\n      0.965000\n      00:00\n    \n    \n      15\n      0.030213\n      0.037250\n      0.965000\n      00:00\n    \n    \n      16\n      0.030058\n      0.036885\n      0.965000\n      00:00\n    \n    \n      17\n      0.029849\n      0.036862\n      0.965500\n      00:00\n    \n    \n      18\n      0.029551\n      0.036518\n      0.965500\n      00:00\n    \n    \n      19\n      0.029381\n      0.036236\n      0.966000\n      00:00\n    \n    \n      20\n      0.029358\n      0.035996\n      0.966500\n      00:00\n    \n    \n      21\n      0.028987\n      0.036024\n      0.966000\n      00:00\n    \n    \n      22\n      0.028436\n      0.035731\n      0.966500\n      00:00\n    \n    \n      23\n      0.028494\n      0.035813\n      0.967000\n      00:00\n    \n    \n      24\n      0.028174\n      0.035535\n      0.966000\n      00:00\n    \n    \n      25\n      0.028089\n      0.035361\n      0.966500\n      00:00\n    \n    \n      26\n      0.027961\n      0.035035\n      0.966500\n      00:00\n    \n    \n      27\n      0.027761\n      0.035248\n      0.966000\n      00:00\n    \n    \n      28\n      0.028016\n      0.035006\n      0.966500\n      00:00\n    \n    \n      29\n      0.027430\n      0.034841\n      0.967500\n      00:00\n    \n  \n\n\n\n\nplt.plot(L(learn.recorder.values).itemgot(2))\n\n\n\n\n\nlearn.recorder.values[-1][2]\n\n0.9674999713897705\n\n\nWe don’t actually emerge at the end of this with a vastly superior score to what we had at the end of the last notebook, but this basis (the simple neural network) has far more open vistas within which we can work and build upon.\nFinally, to round out my understanding, I put together a little diagram showing the various pieces that go into the Learner class when we instantiate it, adding some of the other concepts etc below it as I felt was appropriate. This isn’t a complete picture by any means, but I find it helpful to visualise how things are layered and pieced together:"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "",
    "text": "If you’re reading this blog, you’ve probably visited the Huggingface website and you’ve almost certainly tried out one of their ‘Spaces’. These are deployed mini-applications hosted on Huggingface infrastructure. I’ve created spaces of my own, and at work I added a way for people to quickly deploy a ZenML server as a ‘Space’. I love browsing all the spaces that exist and they’re really a testament to the creativity, smartness and elbow-grease contributed by the thriving open-source community that Huggingface facilitates through their platform.\nI’ve been working on my Terraform skills for a while and recently I thought up a little project that I hope will help deepen my skill-building as well as be useful for others. My goals were to build something that would:\nThis blogpost will describe my process and some of the things I learned along the way. A special thanks to Sean Kane at SuperOrbital for writing an extremely useful blogpost that guided me on this journey (alongside the official Hashicorp documentation). (TL;DR: check out the finished provider here!)"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#the-terraform-golden-path",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#the-terraform-golden-path",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "The Terraform Golden Path",
    "text": "The Terraform Golden Path\nWhat I wanted to be able to do was define my resource in something like the following manner:\nterraform {\n  required_providers {\n    huggingface-spaces = {\n      source = \"strickvl/huggingface-spaces\"\n    }\n  }\n}\n\nprovider \"huggingface-spaces\" {\n  apikey = var.huggingface_apikey\n}\n\nresource \"huggingface-spaces_space\" \"zenml_server\" {\n    name     = \"test-zenml-space\"\n    private  = false\n    template = zenml/zenml\n}\nThis really simple interface would allow people to spin up Huggingface spaces without needing to click through buttons on the Huggingface website. You can get a sense of how it works if you were to do it via that web interface here:\n\n\n\nWeb interface to deploy Huggingface Spaces as of November 2023"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-http-requests-to-deploy-spaces",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-http-requests-to-deploy-spaces",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Using HTTP Request(s) to Deploy Spaces",
    "text": "Using HTTP Request(s) to Deploy Spaces\nInternally, this web interface will just be using API calls (through some layer of abstraction) to make the deployment, so we can take a look at the Huggingface documentation and experiment a bit to find out how we can make such deployments using the raw underlying HTTP.\nIt’s important to have a sense of the HTTP API because this is what we’re going to ultimately be using when we write the Terraform provider. For the provider, we’ll have to use Go to define how that works so in the interests of clarity I’ll just show the HTTP requests on their own first.\nThere’s a useful Space by Enzo that allows you to play around with the Hub API, and the Huggingface docs cover the endpoints available for the Hub API.\nFor our purposes, we’re interested in the POST /api/repos/create endpoint. Note that we’re not making our query to the api/spaces endpoint, but rather to the repos endpoint. When you create a space, what you’re actually doing is creating a repository with certain custom features that Huggingface then knows to instantiate and deploy as a ‘Space’.\nSo as a simple HTTP request we can run the following in the terminal:\nhttp POST \"https://huggingface.co/api/repos/create\" \\\n  Authorization:\"Bearer YOUR_TOKEN_GOES_HERE\" \\\n  type=space \\\n  name=test-hf-api \\\n  private=false sdk=docker template=zenml/zenml\nYou’ll need to add in your authorisation token that you create on the Huggingface site and you can change the specific template you’re creating and the name as you see fit. Running this command will do the same as clicking through the various buttons on the web interface. (I’m using the httpie CLI tool to make the request, but you could just as well use something like curl if you prefer.)\nOnce we’ve confirmed that the basic creation of a Space based on the template works, we can try some other commands. If we wanted to rename the space, we can ‘move’ it. (Think how renaming folders in the UNIX terminal is also accomplished with the mv command.)\nhttp POST \"https://huggingface.co/api/repos/move\" \\\n  Authorization:\"Bearer YOUR_TOKEN_GOES_HERE\" \\\n  fromRepo=\"strickvl/test-hf-api\" toRepo=\"strickvl/my-renamed-space\" type=\"space\"\nAnd then to delete the space we can do the following:\nhttp DELETE \"https://huggingface.co/api/repos/delete\" \\\n  Authorization:\"Bearer YOUR_TOKEN_GOES_HERE\" \\\n  type=space \\\n  name=my-renamed-space \\\n  type=space\nSo the API allows us to interact with the Space without using the website as expected. So far so good!"
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#terraform-provider-basics",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#terraform-provider-basics",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Terraform Provider Basics",
    "text": "Terraform Provider Basics\nAt the end I want this provider to be available on the Terraform Registry for others to use, so in that case the repository needs to be named in the format terraform-provider-THE_NAME_GOES_HERE. I’ve chosen terraform-provider-huggingface-spaces.\nTerraform released a framework for creating these Terraform providers which is a new (v2) way of doing this. It’s worth noting that it’s fairly new and many (most?) of the community providers you see on the Terraform registry are using the old way. There’s lots of support and even a repository you can use as a template scaffold for your own efforts. That’s what we’ll be using as well."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#implementing-the-provider",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#implementing-the-provider",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Implementing the Provider",
    "text": "Implementing the Provider\nIt’s all implemented in Go, and while the template / framework got me started, there was a fair amount of boilerplate code to be written. Between Claude, GPT-4 (when Claude cut me off for making too many queries) and a bit of elbow grease, I got it all implemented and working. You can view the finished provider up on the Terraform Registry here."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-the-provider",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#using-the-provider",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "Using the provider",
    "text": "Using the provider\nSo now you can use code like this:\nterraform {\n  required_providers {\n    huggingface-spaces = {\n      source = \"strickvl/huggingface-spaces\"\n    }\n  }\n}\n\nprovider \"huggingface-spaces\" {\n  token = var.huggingface_token\n}\n\nvariable \"huggingface_token\" {\n  type        = string\n  description = \"The Hugging Face API token.\"\n  sensitive   = true\n}\n\nresource \"huggingface-spaces_space\" \"test_space\" {\n  name     = \"test-hf-api-${formatdate(\"YYYYMMDD\", timestamp())}\"\n  private  = true\n  sdk      = \"docker\"\n  template = \"zenml/zenml\"\n}\n\ndata \"huggingface-spaces_space\" \"test_space_data\" {\n  id = huggingface-spaces_space.test_space.id\n}\n\noutput \"test_space_id\" {\n  value = huggingface-spaces_space.test_space.id\n}\n\noutput \"test_space_name\" {\n  value = data.huggingface-spaces_space.test_space_data.name\n}\n\noutput \"test_space_author\" {\n  value = data.huggingface-spaces_space.test_space_data.author\n}\n\noutput \"test_space_last_modified\" {\n  value = data.huggingface-spaces_space.test_space_data.last_modified\n}\n\noutput \"test_space_likes\" {\n  value = data.huggingface-spaces_space.test_space_data.likes\n}\n\noutput \"test_space_private\" {\n  value = data.huggingface-spaces_space.test_space_data.private\n}\n\noutput \"test_space_sdk\" {\n  value = data.huggingface-spaces_space.test_space_data.sdk\n}\nAnd this will create a Hugging Face Space using the ZenML Docker template. You can even specific specific hardware that you want your space to be provisioned with, or state that you want persistent storage to be added, or add secrets and/or environment variables as part of the deployment. I would encourage you to visit and read the documentation for the provider for more information.\nUnfortunately at some point towards the end of my development I seem to have triggered the Hugging Face API’s rate limiter and now I’m in some purgatory where I can’t do anything on the Hugging Face Hub for the next 23+ hours. Not sure what that means for my ability to test this functionality in our CI, but I’ll be exploring this next."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-this-unlocks",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-this-unlocks",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "What this unlocks",
    "text": "What this unlocks\nI’m pretty excited that I finally got this working. This unlocks a few things for me.\nFirstly, I’m one of the lead engineers responsible for maintaining mlstacks, a Python package and library that allows you to quickly spin up MLOps infrastructure. We started off with Kubernetes-based implementations of all of the possible infrastructure combinations you might need, but it’s clear that for many use cases that’s too heavy a footprint. With this provider, we can easily add in the ability to spin up an Argilla annotation instance, for example, or any of the many other options.\n\nHugging Face technically allow you to create any kind of repository, so potentially this opens up a way to deploy any kind of Docker-backed image to the Hugging Face Hub. Given how simply and easy these are to deploy and configure, I’m personally excited by this idea and how it makes various parts of MLOps more approachable and accessible to others.\nI also think that this provider might be used by others. Technically you could use the Python SDK for the huggingface_hub, or perhaps raw HTTP requests using curl, but I find this interface useful and more maintainable. (I have a bit more work to do on the provider to add more robust testing and CI in that regard.) The newly-established mlinfra tool might also find this provider useful as part of its parallel attempt to allow for the deployment of MLOps infrastructure."
  },
  {
    "objectID": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-i-learned",
    "href": "posts/2024-03-31-writing-a-custom-terraform-provider-to-deploy-huggingface-spaces.html#what-i-learned",
    "title": "Writing a custom Terraform provider to deploy Huggingface Spaces",
    "section": "What I learned",
    "text": "What I learned\nI’ve recently been reminded quite frequently that projects that seem daunting are often just a case of keeping moving forward, putting one step in front of another. I first thought of the idea for this provider a year ago, yet it only took me a weekend to implement the functionality that I’ve now released.\nI found it quite hard to understand the precise endpoints of the Hugging Face API that were available. Some documentation can be found, but it’s split over various places and still is incomplete. The actual source code for the Hub’s API is closed-source so it’s not even possible just to go to the code implementation. Once I’m done with the testing for the provider I’ll maybe see if I can go back and make a PR to improve the Hugging Face documentation.\nWorking with Go code is pretty pleasant. There were some new parts of Go I hadn’t previously encountered, notably modules. Using the new Terraform provider plugin template / system means you have to use modules, but it wasn’t too hard to figure out. Go is certainly easier to grok as a non-expert user than Rust.\nA final ‘as always’ lesson was that my work was really helped by taking some time away from code to think through exactly how the provider would work, what pieces of information would be needed where and so on. I don’t always remember this lesson — it’s really easy to just start typing on a keyboard — but I appreciated having a game plan and some notes to keep me on track when I had choices to make.\nSo give the provider a try and let me know if you found it useful! (Also let me know if you’d like some other functionality adding and I’ll see what I can do!)"
  },
  {
    "objectID": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html",
    "href": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html",
    "title": "Starting to read Prompt Engineering for LLMs",
    "section": "",
    "text": "I’m posting some of my summary notes while reading through John Berryman and Albert Ziegler’s “Prompt Engineering for LLMs”. What follows are my notes from the first two chapters. It was a bit too long for a post to LinkedIn so I’m posting my notes in full here."
  },
  {
    "objectID": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-1-introduction-to-prompt-engineering",
    "href": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-1-introduction-to-prompt-engineering",
    "title": "Starting to read Prompt Engineering for LLMs",
    "section": "Chapter 1: Introduction to Prompt Engineering",
    "text": "Chapter 1: Introduction to Prompt Engineering\nThe opening chapter frames prompt engineering as a comprehensive discipline that extends far beyond just crafting individual prompts. It positions prompt engineering as an integral part of the entire lifecycle of LLM-based applications.\n\nKey Points\n\nThe field of language modeling has seen exponential growth, as evidenced by the GPT series progression from 2018 to 2022:\n\nGPT-1 (2018): 117M parameters\nGPT-2 (2019): 1.5B parameters\nGPT-3 (2020): 175B parameters\nSubsequent models showing continued scaling\n\nPrompt engineering encompasses:\n\nThe structural design of prompts themselves\nStrategic thinking about prompt implementation throughout the application lifecycle\nIntegration of prompts into larger systems and workflows\n\nHistorical Context:\n\nThe chapter provides background on language modeling evolution\nPlaces modern LLMs in the broader context of NLP development\n\n\nThis introductory framework suggests that effective prompt engineering requires both technical skill in prompt construction and strategic understanding of how prompts function within larger systems and applications."
  },
  {
    "objectID": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-2-understanding-llms",
    "href": "posts/2025-01-09-understanding-llms-with-prompt-engineering-for-llms.html#chapter-2-understanding-llms",
    "title": "Starting to read Prompt Engineering for LLMs",
    "section": "Chapter 2: Understanding LLMs",
    "text": "Chapter 2: Understanding LLMs\nChapter two tries to peel back the layers of how LLMs produce their output. If you can understand how they work (at least a bit more than ‘it’s magic’), you can better guide them to produce outputs that are valuable for you.\nA very hard chapter to write, I imagine. It is almost certainly a bit too technical for someone ‘non-technical’, but a more experienced user might find some of the analogies too simplistic. I thought the balance was well handled but I probably wouldn’t recommend this to just anyone..\nSome key insights: expect LLMs to respond in a similar way to the training data that went into creating them. (Unfortunately, many model providers are pretty tight-lipped as to the specific composition of that training data, though you can make some guesses…)\n\n“The better you know the training data, the better the intuition you can form about the likely output of an LLM trained on that training data.”\n\nWe then get into a section on tokenization and what that means for how LLMs ‘see’ the world and why this results in certain weaknesses. Most importantly, just remember that LLMs don’t process and interact with text in the same way that humans do. Easy to forget when you’re interacting through a chat interface, but important nonetheless.\n\nI liked this example about capitalization and how tokenization made it hard for the earlier generations of models to do something as ‘simple’ as turning words into upper-case versions.\nEven though this isn’t a problem with more recent models, it reminds you to be cognisant of how much extra work you’re having your model do. The more you can remove extra work, the better responses you’ll get. If you try to have your model do too many things at the same time, you’ll have poorer results.\nThe section on LLMs as auto-regressive models was excellent, though, again, probably not the easiest read for a non-technical reader. Key: LLMs move forward through their text as they ‘read’ the contents. They cannot backtrack, they cannot take things back that they write. They just have to keep moving forward.\nThis can lead to repetitions, getting lost in certain patterns and behaviours. One solution to this: filtering out after the response is given. Another option: playing with temperature and randomness.\nI loved this section on temperatures and how to think about which to choose. Very practical, even amidst a chapter targeted at helping you understand why LLMs behave the way they do.\n\nAlso a useful insight that errors often compound when it comes to temperatures greater than 1. I hadn’t realised that before.\nAfter tokens and temperature we move on to transformers! I found the explanation worked, though the really technical again are bound to be disappointed and the non-technical might find it a bit too hand-wavy. YMMV. Overall enough information was given to understand the key insight around attention:\n\n“Information flows from left to right. Information flows from bottom to top.”\n\nAfter we understand this, we can also understand how processing (‘reading’) text happens much faster than the output: it’s ~ an order of magnitude slower to to output than it is to read the input, even with caching and parallelism in the computation.\nSo the order of the contents of the prompt matters a lot, as does the formulation and the extent to which you make the LLM work hard on the problem at hand or other extraneous tasks.\nA nice illustrative summation:\n\n“Could a human expert who knows all the relevant general knowledge by heart complete the prompt in a single go without backtracking, editing or note-taking?” (if not, then you might find the LLM will struggle with the task or completion)\n\nSo to sum up:\n\nLLMs are completion engines\nLLMs mimic their training data\nLLMs produce one token at a time and can’t backtrack\nLLMs read through the text a single time, from beginning to end\n\nSimple-seeming insights, but ones with large consequences. Tomorrow we move beyond the static models and on to RLHF, the chat models and the differences that come with using the API."
  },
  {
    "objectID": "posts/2021-11-26-environment-variables.html",
    "href": "posts/2021-11-26-environment-variables.html",
    "title": "How to set and get environment variables using Python",
    "section": "",
    "text": "If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this:\nimport os\n\nos.environ['SOME_ENV_VARIABLE'] = 13.5\nAnd to access an environment variable, there are actually a number of different ways. All these three are essentially the same:\nos.getenv('SOME_ENV_VARIABLE')\nos.environ.get('SOME_ENV_VARIABLE')\nos.environ('SOME_ENV_VARIABLE')\nFor the final one (os.environ('SOME_ENV_VARIABLE')), if the variable doesn’t exist, it’ll return a KeyError, whereas the first two will just return None in that case."
  },
  {
    "objectID": "posts/2022-01-06-nbdev-early-impressions.html",
    "href": "posts/2022-01-06-nbdev-early-impressions.html",
    "title": "Learning about ‘nbdev’ while building a Python package for PDF machine learning datasets",
    "section": "",
    "text": "While working to develop a computer vision model that detects redactions in documents obtained as a result of FOIA requests, I have encountered some tasks that I end up repeating over and over again. Most of the raw data in the problem domain exists in the form of PDFs. These PDF files contain scanned images of various government documents. I use these images as the training data for my model.\nThe things I have to do as part of the data acquisition and transformation process include the following:\n\ndownloading all the PDF files linked to from a particular website, or series of web pages\nconverting and splitting all the downloaded PDF files into appropriately sized individual image files suitable for use in a computer vision model\ngenerating statistics on the data being downloaded and processed, as well as (further down the line) things like detecting data drift for incoming training data\nsplitting up data as appropriate for train / validation / test data sets\nextracting text data from the images via an OCR process\nversioning, syncing and uploading those images to an S3 bucket or some other cloud equivalent for use in the overall workflow\n\nIt’s not hard to see that many of these things likely apply to multiple machine learning data acquisition scenarios. While writing the code to handle these elements in my specific use case, I realised it might be worth gathering this functionality together in an agnostic tool that can handle some of these scenarios.\nI had wanted to try out nbdev ever since it was announced back in 2019. The concept was different to what I was used, but there were lots of benefits to be had. I chose this small project to give it an initial trial run. I didn’t implement all of the above features. The two notable missing parts are text extraction and data versioning and/or synchronisation.\npdfsplitter is the package I created to scratch that itch. It’s still very much a work in progress, but I think I did enough with nbdev to have an initial opinion.\nI think I had postponed trying it out because I was worried about a steep learning curve. It turned out that an hour or two was all it took before I was basically up and running, with an understanding of all the relevant pieces that you generally use during the development lifecycle.\nBuilt in to nbdev in general is the ability to iterate quickly and driven by short, small experiments. This is powered by Jupyter notebooks, which are sort of the core of everything that nbdev is about. If you don’t like notebooks, you won’t like nbdev. It’s a few years since it first saw the light of day as a tool, and as such it felt like a polished way of working, and most of the pieces of a typical development workflow were well accounted for. In fact, a lot of the advantages come from convenience helpers of various kinds. Automatic parallelised testing, easy submission to Anaconda and PyPi package repositories, automatic building of documentation and standardising locations for making configuration changes. All these parts were great.\nPerhaps the most sneakily pleasant part of using nbdev was how it encouraged best practices. There’s no concept of keeping test and documentation code in separate silos away from the source notebooks. Following the best traditions of literate programming, nbdev encourages you to do that as you develop. Write a bit of code here, write some narrative explanation and documentation there, and write some tests over there to confirm that it’s working in the way you expected. When Jeremy speaks of the significant boost in productivity, I believe that a lot of it comes from the fact that so much is happening in one place.\nWhile working on pdfsplitter, I had the feeling that I could just focus on the problem at hand, building something to help speed up the process of importing and generating images from PDF data for machine learning projects.\nNot everything was peaches and roses, however. I ran into a weird mismatch with the documentation pages generated and my GitHub fork of nbdev since I was using main as the default branch but nbdev still uses master. I will be submitting an issue to their repository, and it was an easy fix, but it was confusing to struggle with that early on in my process. I’m also not sure how well nbdev will gel with large teams of developers, especially when they’re working on the same notebooks / modules. I know reviewnb exists now and even is used within fastai for code reviews, but I would imagine an uphill battle trying to persuade a team to take a chance with that.\nI’ve been using VSCode at work, supercharged with GitHub Copilot and various other goodies, so it honestly felt like a bit of a step back to be forced to develop inside the Jupyter notebook interface, absent all of my tools. I also found the pre-made CLI functions a little fiddly to use — fiddly in the sense that I wish I’d set up some aliases for them early on as you end up calling them all the time. In fact, any time I made a change I would find myself making all these calls to build the library and then the documentation, not forgetting to run the tests and so on. That part felt a bit like busy work and I wish some of those steps could be combined together. Maybe I’m using it wrong.\nAll in all, I enjoyed this first few hours of contact with nbdev and I will continue to use it while developing pdfsplitter. The experience was also useful to reflect back into my current development workflow and environment, especially when it comes to keeping that close relationship between the code, documentation and tests.\n[Photo by Laura Ockel on Unsplash]"
  },
  {
    "objectID": "posts/2022-02-05-robust-python-9.html",
    "href": "posts/2022-02-05-robust-python-9.html",
    "title": "Upgrade your Python dicts with data classes",
    "section": "",
    "text": "I’ve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn’t taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore’s book, ‘Robust Python’, specifically chapter nine.\nAn example upfront might help ground the conversation. Here is a data class in action:\nimport datetime\nfrom dataclasses import dataclass\nfrom typing import Literal\n\n@dataclass\nclass CatPassport:\n  name: str\n  breed: CatBreed\n  issue_date: datetime.date\n  expiry_date: datetime.date\n  gender: Literal['male', 'female']\n\naria = CatPassport(\"Aria\", CatBreed('bengal'), datetime.date(2022, 01, 05), datetime.date(2025, 01, 04), 'female')\nprint(aria.name) # prints 'Aria'\nFrom this you can see that it’s an easy way to represent structured data made up of different types. Where it excels over simply using a dict or a class you write yourself is the fact that it auto-generates a number of __ dunder helper methods. You get __str__ and __repr__ to handle what this object looks like when you try to print() it. It also creates an __eq__ method which allows you to check for equality between two objects of the same type with the == comparison operator.\n(If you want to add a way to compare between your data class objects, you can add arguments to the @dataclass decorator like @dataclass(eq=True, order=True) which will handle the creation of the relevant dunder methods.\nThe fact that data classes are just classes at heart mean that you can also add behaviours to these collections of values, something that isn’t possible with a plain dict.\nYou can specify that your data class should be frozen (@dataclass(frozen=True)) which effectively makes it an immutable data store, though taking note that objects stored as values on the data class’ properties might themselves not be immutable (think lists and dicts).\nAfter reading the chapter in ‘Robust Python’, I read around a little to get a sense of this concept. I read the official docs which were fairly helpful, but in fact it was the PEP document (557) that I found most interesting. I haven’t previously taken the time to dive into the specifics of PEP specifications before, but I discovered that they are pretty readable and you get a real sense of what problem a particular feature or addition to the language was trying to solve.\nPEP 557 explains some of the alternatives and why it might be useful to include this new feature. I also learned about the attrs package and how data classes are actually just a subset of what attrs offers. (As a side note, I was surprised that attrs seems to have been mentioned nowhere in ‘Robust Python’, even in the context of the upcoming Pydantic chapter. Perhaps it was just too confusing to have all these things alongside one another.)\nOther options to consider alongside data classes when dealing with heterogenous data inside a single object or structure include TypedDict and namedtuple, but it seems like the default for this kind of scenario should probably just be a data class, though I should add that it is only part of the standard library for Python 3.7 and above."
  },
  {
    "objectID": "posts/2021-09-18-reading-python.html",
    "href": "posts/2021-09-18-reading-python.html",
    "title": "Reading Python Code",
    "section": "",
    "text": "It’s a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you’re going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote.\nOne way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you’re reading.\nI’ll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it’ll be useful for someone else as well."
  },
  {
    "objectID": "posts/2021-09-18-reading-python.html#good-quality-python-code",
    "href": "posts/2021-09-18-reading-python.html#good-quality-python-code",
    "title": "Reading Python Code",
    "section": "Good Quality Python Code",
    "text": "Good Quality Python Code\n\njinja — a templating engine written in Python (and see the recommendations for supplemental reading and watching for jinja here)\nhowdoi — a search tool for coding answers via the command line\nflask — a micro-web framework for Python\nFastAPI — another web framework that’s a bit larger than flask\ndiamond — a Python daemon that collects and publishes system metrics\nwerkzeug — a web server gateway library\nrequests — an HTTP library, now part of the Python standard library\ntablib — library for Pythonic way to work with tabular datasets\nclick — a Python package for creating command line interfaces\npathlib — part of the Python standard library; a module to handle filesystem paths (also the corresponding PEP proposal #428)\ndataclasses — a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding PEP proposal #557)\njoblib — a library to support lightweight pipelining in Python"
  },
  {
    "objectID": "posts/2021-09-18-reading-python.html#other-resources",
    "href": "posts/2021-09-18-reading-python.html#other-resources",
    "title": "Reading Python Code",
    "section": "Other Resources",
    "text": "Other Resources\n\n500 Lines or Less — a book in which specific small open-source projects are profiled to understand how they approached their particular challenge.\nThe Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks — examination of the structure of the software of some open-source software applications.\nThe Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks — the second volume in the series."
  },
  {
    "objectID": "posts/2021-12-15-redaction-taxonomy.html",
    "href": "posts/2021-12-15-redaction-taxonomy.html",
    "title": "A Taxonomy of Redaction",
    "section": "",
    "text": "One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn’t always share redaction practices.\nI took a bit of time to try to understand the different kinds of redactions in my (pretty huge) data set. I didn’t have any special process for selecting these images; I randomly sorted the immediate ~70,000 images I have collected and looked through to try to identify some patterns.\nTaking a close look at the actual parts of images that contain redactions gives me a better sense of the challenges involved in detecting those redactions. As I iterate through my collection of images, I can start to build up an intuitive sense of where class imbalances might exist. Among the images that contain redactions, for example, which ones are most represented and which contain fewer examples? In general, where do I need to focus my efforts when it comes to improving my model?\nThe first easy distinction to draw is that between digital and hand-applied redactions.\n\nIt seems that the trend in this is towards digital redactions. Perhaps it is seen as less reliable, or perhaps it’s more time consuming to attach the reasons for redactions having happened. Perhaps, too, there are some legal reasons why each redaction needed to start having a specific reason applied to it.\nAt first glance, no pun intended, it would appear that digital redactions are much easier to recognise. They’re often uniform in how they are applied and are usually pretty blunt in their appearance. There are some non-redaction uses for totally black or grey boxes laid on top of text, but they aren’t common and it’s a pretty strong feature to have to predict.\nHandwritten redactions are also easy to recognise, but potentially the borders are harder to make out. Sometimes having a thinner pen with which redactions are applied might make it slightly less accurate.\nIt is more practically important to distinguish between redactions that are easy to recognise vs ones that take some time to notice. I can use my own speed at noticing the redaction on a page as a gauge. It’s not a perfect analogy, but Jeremy Howard’s adage that if a human can reliably do some kind of classification or object detection, then probably a computer can as well. I guess the inverse is also true: if a human will find it hard to recognise a particular feature in an image, then a computer will probably also find it hard.\nThere isn’t much point spending too long with the ‘easy’ redactions. These are usually whatever is boxy and blunt. It’s the stereotype of a redacted document, one like what was used as the cover art on the (much-censored) Guantánamo Diary by Mohamedou Ould Slahi.\n\nSometimes you see that the entire page has been redacted with some kind of a coloured box. Other times entire columns of information has been redacted from a table. These definitely feel like they are the more recent types of redactions.\nOne thing that makes detecting redactions hard, on the other hand, is if the number of redactions is small. It stands to reason that lots of small redactions can stand out at first glance, whereas a single small redaction on one corner of the page is maybe harder to notice.\nThe hardest of redactions seems like it is in examples like this:\n\nA white box on top of other white boxes! I often have to look quite closely at these to distinguish what is normal text and what is a redaction box. Some of them have a faint thin grey boundary box around them, which I guess ends up being pretty useful as a way to make that distinction. Surprisingly, the model that I’ve trained so far is not terrible at making these kinds of distinctions.\n\nI have a few hundred annotated images so far, but I now have an intuitive sense of the hard parts of the object detection test. I also have a sense of how represented I feel like those hard parts are — not very.\nAs I wrote in my previous update on my progress in this project, the next step is very much to find ways to increase the volume of good training data that I’m using to train my model. Part of that will involve creating synthetic data, part of that will be using self-training to speed up my annotation, and of course another part will just be doing more manual annotation. I’ve already started work on creating the synthetic data. More on that next time!"
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html",
    "href": "posts/2022-05-31-redaction-production-introduction.html",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nSo we’ve trained our model… now what? Are we done?\nIf we’re just exercising our curiosity or have a tight focus for our work, then we might well be done with our work. Our responsibility within a larger team might only be for this specific step of training the model, for example. For many cases, however, we’re going to want to do something with our model, perhaps making it available to others via some web interface or an online API.\nThe next blog posts in this series will focus on the challenges and practicalities of getting a model ‘in production’. I’ll sidestep the nuances of exactly what we mean by ‘in production’ for the moment, but suffice it to say that the end goal is to have a way to not only make our model available to other consumers but also to deal with re-training and/or re-deploying new models to take the place of older or stale models. There is a whole spectrum of variety in this context that goes by the name “MLOps”. This blog post will try to provide a high level overview of some of the basic elements relevant to getting my redaction model into production."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#tldr-what-will-you-learn",
    "href": "posts/2022-05-31-redaction-production-introduction.html#tldr-what-will-you-learn",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🚦 TL;DR: What will you learn?",
    "text": "🚦 TL;DR: What will you learn?\n\n🤖 Deploying a model and automating everything on the way requires a decent number of steps and tools to do it properly.\n👀 I take you through the core steps you need to think about when working with a continuously deployed object detection model.\n💪 I end by outlining the specific pieces I will need to build as I get my own model out into the world and ‘in production’."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#what-is-in-production-for-the-redaction-model",
    "href": "posts/2022-05-31-redaction-production-introduction.html#what-is-in-production-for-the-redaction-model",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🚀 What is ‘in production’ for the redaction model?",
    "text": "🚀 What is ‘in production’ for the redaction model?\n“Production” relates heavily to the use case. An image classification model used across the United States to identify dents on rental cars is going to have a very different profile to a privately hosted language model being used as part of an internal company chatbot interface. In the case of our redaction model, there are two main scenarios that I’m interested in supporting:\n\nan online API which can power other services building on top of the functionality the model enables, albeit document-by-document. A user could upload a specific document and they’d receive specific predictions and results for just that document.\nan offline-first model which handles large volumes of input data and that does not require internet connectivity. For example, legal teams trying to get a sense of what kinds of documents are redacted as part of a case (and to what extent) could run an extended inference process over an entire set of documents.\n\nThese two scenarios have different requirements. Of the two, the online version is perhaps slightly more complex given the more complex serving needs and potential guarantees around speed of execution and so on. The second use offline / on-prem scenario has its own challenges around making sure that inference is fast enough to be able to work over massive document collections, but I’m not sure I’d consider that an MLOps challenge so I will mostly be focusing on the online deployment in this series."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#handling-failure-and-success",
    "href": "posts/2022-05-31-redaction-production-introduction.html#handling-failure-and-success",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "⚖️ Handling failure and success",
    "text": "⚖️ Handling failure and success\nOne way to think about what’s required to get a model in production is to think of the answers to the following two questions:\n\nwhat could go wrong with my deployed online model?\nwhat are the possible consequences if everything went really well?\n\nIn terms of the redaction model, there are lots of potential complications:\n\nour model could be slow and therefore users wouldn’t want to hang around for inference to take place\nthe data on which our model was originally trained could start to show its age and so maybe our model wouldn’t perform as well on newer documents being released (perhaps with a different style or method of redaction)\nsome software bug or managed infrastructure outage could bring our hosting down and we’d have to redeploy everything\nthere could be a legal or ethical challenge to our deployed model, and we’d perhaps be required to show the exact process used that resulted in a particular model\nmaybe the way we choose to make our model available is expensive and/or unreliable\nand so on…\n\nIf things went really well, we have a different set of problems: perhaps a million people would be interested in using the service at the same time. This scalability problem could bring down the model inference service completely. Or perhaps the model would be adopted for use in a legal setting and people would start to trust its predictions blindly without taking into account the fact that its performance was starting to decline the further away we got from when it was originally trained. Maybe some legal institution would start using it to make decisions making assumptions about the accuracy of the model’s predictions, or maybe even the various government departments responsible for creating and applying the redactions in the first place would use it as a way of more efficiently or voluminously adding redactions, an unintended consequence that could potentially be harmful.\nAll the above scenarios and more are some of the reasons why MLOps exists. We care about having repeatable and robust processes for getting our models out in the world because the use cases are themselves often complex. We also care that our models are actually a net positive when released into the world rather than just some process that happens after model training is completed from which we’re completely disconnected."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#basic-mlops-building-blocks",
    "href": "posts/2022-05-31-redaction-production-introduction.html#basic-mlops-building-blocks",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🧱 Basic MLOps building blocks",
    "text": "🧱 Basic MLOps building blocks\n\nThese are some of the tools available to those trying to put their models into production. This ‘landscape’ showcases both the explosion of options for various parts of the full lifecycle as well as the way that this space hasn’t yet settled on a set of best-in-class tools. For my redaction project, there are a few basics that will seek to have in place in order to meet the needs of the use case(s):\n\n⎂ Code standardisation\nPerhaps not even worth mentioning, but having some standards around how the code looks and having processes to enforce this is important. Using pre-commit alongside tools like black, isort, darglint, pydocstyle and so on will take you a long way in this direction.\nThis would be especially important if I were working as part of a team. These tools would ensure some kinds of standards and baseline uniformity within the codebase. In my case, I’m doing everything on my own so it matters less, but these tools all help me collaborate with my future self, several months from now, perhaps when I need to fix some bug or add a new feature. Having code that reads clearly and cleanly goes a long way to getting started on that work.\n\n\n📸 Code Versioning\nVersioning your code with a tool like git is also another basic requirement of any process involving software. I’d almost say this barely requires mentioning, but I know that the use of regular atomic git commits is by no means standard practice in the world of data science so it bears restating here.\nWe version our code because we want to be able to go back to earlier points in our code’s history. If we wanted to see the difference between the code used to train our model today and the code used last week, we’d require a tool like git to help us with that. (Code versioning tools also help tremendously when collaborating with a larger team or to an open-source community project.)\nThere are various options for what tool to use but the vast majority of people use git for this as of 2022.\n\n\n🧳 Data, Model and Artifact Versioning\nLast week I wrote about DVC and the ways it can be used as a lightweight way to add data versioning to a project. Since data is as important to a model as the code used to train it, we want ways to step backwards and forwards with our data. This will not only enable us to debug and retrain older models but it will help in general with managing our assets such that we don’t just have a sprawling series of folders with names like training_data_FINAL_july_2021 or validation_data_minus_synthetic_FINAL_FINAL_march_2020.\nNot only does this make sense from the perspective of productivity, but for sensitive or popular ML applications there are increasing legal requirements around this kind of flexibility to introspect how you trained your models.\nDVC is commonly used for this kind of use case, but there are other alternatives such as Pachyderm or LakeFS that might be worth considering if you have larger amounts of data.\n\n\n🧪 Testing\nThis testing is primarily around ensuring that the code does what you think it’s doing, but we also care about preventing regressions in one part of the codebase when you change something somewhere else.\nThere isn’t much in the way of rocket science to testing, but it is a whole world unto its own. For my project, there is a decent amount of code that handles somewhat complicated conversions between different kinds of image formats, multiple competing ideas for how bounding boxes or BBoxes should be handled as data structures and so on. Having a way to be sure that my code is actually doing what I intended is a surefire way to letting me sleep better at night if I intend my model to be used by others.\n\n\n👁 Data Validation\nJust like our code needs to have some kinds of checks and balances, so does the lifeblood of our project: our data. I recently finished a three-part series on data validation in the context of this project, and both Great Expectations and Evidently are excellent options worth considering, depending on your exact requirements.\n\n\n📝 Experiment Tracking\n{% twitter https://twitter.com/bernhardsson/status/1526635195243409408 %}\nWhen starting a project from scratch, you want to iterate quickly, trying out lots of training options or combinations of features and/or data. Not only do you want to be able to replicate the precise combination of data and code used to train a particular model, but you also want to be able to compare these various efforts with one another.\nExperiment trackers like Weights & Biases, MLflow, Tensorboard and Neptune enable you to compare the results of your models as well as practically any combination of the hyperparameters used to train them. I’ve used charts from my own (Weights & Biases-powered) experiment tracker to showcase the different results obtained as part of my process. Not only is it useful for outward-facing engagement with users, stakeholders or other third-parties, but it can be useful to step back from your experiments and evaluate where your model is performing well and how you might improve it.\n\n\n📺 Monitoring\nThere’s a lot of potential complexity packed into the simple term ‘monitoring’. (I’d recommend you check out Shreya Shankar’s four-part series on monitoring if you’re curious to learn more.) For our purposes, this will mainly involve making sure our model doesn’t drift or become stale. We’ll need to make sure that the data used to periodically (re)train or fine-tune our model is somewhat within the original parameters of the original training data. We’ll also want to be monitoring the kinds of predictions that our deployed model is making to make sure that they’re more or less within the reasonable ranges of values that we’d expect. If we start to regularly diverge from these kinds of boundary values it probably should prompt an examination of what’s going on and why we’re overstepping. This is an essential part of what it means to robustly deploy a model in production; you need to know when things are going wrong.\n\n\n🏁 Automated Model Deployment\nAutomation is a big part of what people generally consider to be ‘mature’ MLOps practices. It is useful to have an automated way to take your model from when training is complete to having it deployed and user-facing. Perhaps you know you need to retrain or fine-tune your model once a week because the data context is continually changing. Perhaps (or likely!) you have rigorous monitoring processes and in the event of a failure or series of non-sensical / out-of-bounds predictions you want a way to revert the model used in production to something more stable.\nThis is where the process of putting a model in production resembles the processes and values of DevOps most closely. For my redaction model, I’ll want to have a way to handle the two cases mentioned above as a starting point along with more complex versions of those cases. I’ll also want to automate the process of converting my IceVision VFNet model into something that can be used in the offline ‘on-prem’ use case I described at the beginning of this post.\n\n\n🃏 DAG Cards, Model Cards, Data Cards, All the Cards\nThe basic idea is that you write some notes on the context surrounding your data, or your model or the pipelines you’re using as part of your overall workflow. Your processes and artifacts will likely change with the project, and I know from bitter experience that it’s easy to forget the reasoning behind why you chose to do one thing or another. So you write notes to describe what you were thinking when you created or modified this or that asset. You describe the decisions you made and what tradeoffs and downstream effects this might have. Not only is this a good practice that benefits FutureYou™️ and your project, but anything developed in the open will maybe have users or contributors and they’ll also benefit from these notes.\nThis is the only part of my ‘requirements’ that is (at least currently) a bit more of a ‘fad’ and I wouldn’t say was commonly found. Even five years from now, I imagine that we’ll have more sophisticated or standardised ways of achieving what cards bring, but for now they resonate strongly with some processes I used when working as a historian, archivist and researcher in my previous life. Some tools like the Huggingface Model Hub offer this as a built-in standard."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#mlops-maturity-models",
    "href": "posts/2022-05-31-redaction-production-introduction.html#mlops-maturity-models",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "👴 MLOps Maturity Models",
    "text": "👴 MLOps Maturity Models\nThe pieces I described above relate to my particular use case. Different project will require different levels of automation, or even potentially other additional stages or toolkits. There is a vast spectrum of options and decisions to be made and now is probably a good time to mention that various players have tried to define what it means to do the whole ‘putting a model into production and keeping it healthy’ thing in a good way. These “MLOps Maturity Models” are not completely without value, but remember that what works for Google may not be (or is probably not) applicable to you as an individual working on a small side-project. I wrote an overview of the two most commonly cited maturity models (from Microsoft and Google) over on the ZenML blog and I encourage you to give that a read if you want to learn more.\nBut what does this all mean for my project? What specifically will it all look like and how am I implementing it? I’ll get into some of the details in the coming weeks, but for now let me just outline my two main workflows."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-1-annotation",
    "href": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-1-annotation",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "✍️ Redaction Project Workflow #1: Annotation",
    "text": "✍️ Redaction Project Workflow #1: Annotation\n\nThere are two main pieces to this pipeline that happens before we train our model. My model is still thirsty for annotations and data, so from the very beginning I want to integrate the annotation process in as part of how I set up the workflows. This way, I make it as easy as possible to annotate data and use that data for subsequent training or fine-tuning.\n\nIngestion / Import\n\nHere I will check a series of pre-defined URLs to see if there are any new PDF files available for download. If new files are available (and we’ve confirmed that we haven’t already downloaded them, I can download those files. Those PDFs then get converted into image files and the metadata for each image gets saved centrally so we have that to hand when annotating the files. This point is a good one to save a checkpoint version of our data using DVC.\n\nAnnotation\n\nWe only want to annotate images that haven’t been annotated, so that check is the first to be performed before spinning up Prodigy to randomly select pages from the PDFs (now in the format of image files) to be annotated. 10% of the images that are annotated get saved in a separate ‘test data’ location. This test data is never used in training and is simply held out for a more realistic final validation of the project. We version the annotations file whenever we are done annotating for the day."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-2-continuous-training-continuous-deployment",
    "href": "posts/2022-05-31-redaction-production-introduction.html#redaction-project-workflow-2-continuous-training-continuous-deployment",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "🐙 Redaction Project Workflow #2: Continuous Training, Continuous Deployment",
    "text": "🐙 Redaction Project Workflow #2: Continuous Training, Continuous Deployment\n\nThis longer pipeline contains the core value and most compute-intensive processes like training. We take the data from the raw state as annotations and go all the way to deployment.\n\nAnnotation Checker\n\nWe first check to see if there are any new annotations available since we last ran the pipeline. We will probably need some kind of threshold number of annotations which will make it worth our while to trigger the retraining process.\n\nData Validation\n\nWe’ll use Great Expectations to validate the incoming new annotation data.\n\nSynthetic Data Generation\n\nIf / as we hit certain thresholds, we might want to generate more synthetic data and add it to the dataset.\n\nTraining\n\nWe train or fine-tune the model for a certain number of epochs. We log our experiment metadata with Weights & Biases.\n\nDeployment Trigger / Decision\n\nAt this point we need to decide whether to deploy the model or not, based on some evaluation criteria. Our decision will determine the path of the rest of the workflow.\n\nDeployment\n\nWe take the trained model and make it available for online inference. We save a version of the model using DVC, and we also package it up for use in on-prem / offline settings.\n\nInference & Monitoring\n\nThis step is crucial. We monitor the performance of our deployed model along with the predictions it is making. We want to be able to catch any cases where we notice the predictions to start to drift, or be aware of sluggish response times from our server and so on."
  },
  {
    "objectID": "posts/2022-05-31-redaction-production-introduction.html#final-thoughts",
    "href": "posts/2022-05-31-redaction-production-introduction.html#final-thoughts",
    "title": "It takes a tribe: how I’m thinking about putting my object detection model into production",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nNice work on making it all the way to the end! We took a long tour through the various considerations you need to bear in mind when deploying a model, and finished off with a preview of the kinds of things I’ll be building over the coming weeks to actually put my own object detection model in production.\nIf you have parts of this that you’d like me to cover in more detail, or questions based on what you read here today, please leave a comment below!"
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html",
    "href": "posts/2022-01-08-robust-python-4.html",
    "title": "Different ways to constrain types in Python",
    "section": "",
    "text": "The fourth chapter of ‘Robust Python’ continues on from where we left off last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal.\nNote that you can assign all of these type assignments to variables (‘type aliases’), which might just make your code that much more readable."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#optional-to-catch-none-references",
    "href": "posts/2022-01-08-robust-python-4.html#optional-to-catch-none-references",
    "title": "Different ways to constrain types in Python",
    "section": "Optional to catch None references",
    "text": "Optional to catch None references\nOptional as a type annotation is where you want to allow a specific type or None to be passed in to a particular function:\nfrom typing import Optional\n\ndef some_function(value: Optional[int]) -> int:\n    # your code goes here\nNote that you’ll probably want (and mypy will remind you if you forget) to handle what happens in both those cases inside your function. (You may need to specifically pass in the —strict-optional flag to catch this when using mypy.)"
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#union-to-group-types-together",
    "href": "posts/2022-01-08-robust-python-4.html#union-to-group-types-together",
    "title": "Different ways to constrain types in Python",
    "section": "Union to group types together",
    "text": "Union to group types together\nThis is used when multiple different types can be used for the same variable:\nfrom typing import Union\n\ndef returns_the_input(input: Union[str, int]) -> Union[str, int]:\n    return input\nThis function doesn’t really do anything, but you get the idea. Note, too, that Optional[int] is really a version of Union[int, None]. (The book gets into exactly why we might care about reducing the number of possible options by way of a little detour into set theory.)"
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#literal-to-include-only-specific-values",
    "href": "posts/2022-01-08-robust-python-4.html#literal-to-include-only-specific-values",
    "title": "Different ways to constrain types in Python",
    "section": "Literal to include only specific values",
    "text": "Literal to include only specific values\nA little like what I believe enumerations do, we also have the Literal type. It restricts you to whatever specific values are defined:\nfrom typing import Literal\n\ndef some_function(input: Literal[1, 2, 3]) -> int:\n    return input\nHere the function is restricted to inputs that are either 1, 2 or 3. Note that these are a feature that applies to Python 3.8 and above."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#annotated-for-more-complicated-restrictions",
    "href": "posts/2022-01-08-robust-python-4.html#annotated-for-more-complicated-restrictions",
    "title": "Different ways to constrain types in Python",
    "section": "Annotated for more complicated restrictions",
    "text": "Annotated for more complicated restrictions\nThese are available, but not really useful since they only function as a communication method. You can specify specific restrictions such as the following (example is taken from the book, p. 56:\nfrom typing import Annotated\n\nx: Annotated[int, ValueRange(3,5)]\ny: Annotated[str, MatchesRegex('[abc]{2}')\nRead more about it here. The book doesn’t spend much time on it and it seems like it’s probably best left alone for the moment."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#newtype-to-cover-different-contexts-applied-to-the-same-type",
    "href": "posts/2022-01-08-robust-python-4.html#newtype-to-cover-different-contexts-applied-to-the-same-type",
    "title": "Different ways to constrain types in Python",
    "section": "NewType to cover different contexts applied to the same type",
    "text": "NewType to cover different contexts applied to the same type\nNewType, on the other hand, is quite useful. You can create new types which are identical to some other type, and those new values made with the new type will have access to all the methods and properties as the original type.\nfrom typing import NewType\n\nclass Book:\n    # you implement the class here\n    \nNewBook = NewType(\"NewBook\", Book)\n\ndef process_new_book(book: NewBook):\n    # here you handle what happens to the new book\nYou can achieve something like the same thing with classes and inheritance, I believe, but this is a lightweight version which might be useful to achieve the same end goal."
  },
  {
    "objectID": "posts/2022-01-08-robust-python-4.html#final-to-prevent-reassignment-rebinding",
    "href": "posts/2022-01-08-robust-python-4.html#final-to-prevent-reassignment-rebinding",
    "title": "Different ways to constrain types in Python",
    "section": "Final to prevent reassignment / rebinding",
    "text": "Final to prevent reassignment / rebinding\nYou can specify that a particular variable should have a single value and that value only. (Note that mutations of an object etc are all still possible, but reassignment to a new memory address is not possible.\nfrom typing import Final\n\nNAME: Final = \"Alex\"\nIf you tried to subsequently change this to a different name, mypy would catch that you’d tried to do this. This can be valuable across very large codebases, where the potential for someone to reassign a variable might be not insignificant.\nSo there you have it: a bunch of different ways to handle combinations of types and/or more complicated annotation scenarios. The next chapter will cover what happens when we throw collections into the mix, and what type annotation challenges are raised."
  },
  {
    "objectID": "posts/2022-05-14-sgd-fashion-mnist.html",
    "href": "posts/2022-05-14-sgd-fashion-mnist.html",
    "title": "Using the seven-step SGD process for Fashion MNIST",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\n\nIn the previous post I used the seven-step process to fit to an unknown function. The process as a whole is fairly simple to get your head around, but there are a good few details to keep track of along the way. This will continue to be the case as we get into this walkthrough of how to do the same for the Fashion MNIST pullover vs dress data.\n\nGetting our data into the right format\nThe first thing we need to handle is making sure our data is in the right format, shape and so on. We begin by downloading our data and splitting the data into training and test sets.\n\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor()\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor()\n)\n\ntraining_dresses = [item[0][0] for item in training_data if item[1] == 3]\ntraining_pullovers = [item[0][0] for item in training_data if item[1] == 2]\ntest_dresses = [item[0][0] for item in test_data if item[1] == 3]\ntest_pullovers = [item[0][0] for item in test_data if item[1] == 2]\n\ntraining_dresses_tensor = torch.stack(training_dresses)\ntraining_pullovers_tensor = torch.stack(training_pullovers)\ntest_dresses_tensor = torch.stack(test_dresses)\ntest_pullovers_tensor = torch.stack(test_pullovers)\n\ntraining_dresses_tensor.shape, test_dresses_tensor.shape\n\n(torch.Size([6000, 28, 28]), torch.Size([1000, 28, 28]))\n\n\n\ntrain_x = torch.cat([training_dresses_tensor, training_pullovers_tensor]).view(-1, 28*28)\ntrain_y = torch.cat([torch.ones(len(training_dresses)), torch.zeros(len(training_pullovers))]).unsqueeze(1)\n\nvalid_x = torch.cat([test_dresses_tensor, test_pullovers_tensor]).view(-1, 28*28)\nvalid_y = torch.cat([torch.ones(len(test_dresses)), torch.zeros(len(test_pullovers))]).unsqueeze(1)\ntrain_x.shape, train_y.shape\n\n(torch.Size([12000, 784]), torch.Size([12000, 1]))\n\n\nWe transform our images tensors from matrices into vectors with all the values one after another. We create a train_y vector with our labels which we can use to check how well we did with our predictions.\nWe create datasets out of our tensors. This means that we can feed our data into our training functions in the way that is most convenient (i.e. an image is paired with the correct label).\n\ntrain_dset = list(zip(train_x, train_y))\nvalid_dset = list(zip(valid_x, valid_y))\n\n\n\nInitialising our weights and bias\nAs in the previous times where we’ve done this, we initialise our parameters or weights with random values. This means that for every pixel represented in the images, we’ll start off with purely random values. We initialise our bias to a random number as well.\n\ndef initialise_params(size, std=1.0):\n    return (torch.randn(size) * std).requires_grad_()\n\nweights = initialise_params((28*28, 1))\nbias = initialise_params(1)\n\n\n# calculating a prediction for our first image\n(train_x[0]*weights.T).sum() + bias\n\ntensor([2.8681], grad_fn=<AddBackward0>)\n\n\n\n\nMatrix multiplication to calculate our predictions\nWe’ll need to make many calculations like the one we just made, and luckily the technique of matrix multiplication helps us with exactly the scenario we have: we want to multiply the values of our image (laid out in a single vector) with the weights and to add the bias.\nIn Python, matrix multiplication is carried out with a simple @ operator, so we can bring all of this together as a function:\n\ndef linear1(x_batch):\n    return x_batch@weights + bias\n\npreds = linear1(train_x)\npreds\n\ntensor([[  2.8681],\n        [ -7.6810],\n        [-17.5719],\n        ...,\n        [ -3.8665],\n        [  2.0646],\n        [ -2.5148]], grad_fn=<AddBackward0>)\n\n\nWe can check our accuracy for these predictions:\n\ncorrects = (preds > 0.0).float() == train_y\ncorrects.float().mean().item()\n\n0.35324999690055847\n\n\nOur accuracy is pretty poor! A lot worse than even 50/50 luck which is what you’d expect to get on average from a random set of initial weights. Apparently we had a bad draw of luck this time round!\n\n\nA loss function to evaluate model performance\nWe now need a loss function which will tell us how well we are doing in our predictions, and that can be used as part of the gradient calculations to let us know (as we iterate) how to update our weights.\nThe problem, especially in the data set we’re working with, is that we have a binary probability: either it’s a dress or a pullover. Zero or one. Unlike in a regression problem, or something similar, we don’t have any smooth selection of contiguous values that get predicted. We have zero or one.\nAt this point we learn about the sigmoid function which is a way to reframe this problem in a way that we can use to our advantage. The sigmoid function when plotted looks like this:\n\ndef sigmoid(x):\n    return 1/(1+torch.exp(-x))\n\nplot_function(torch.sigmoid, title=\"Sigmoid\", min=-5, max=5)\n\n\n\n\nThis function, as you can see, takes any input value and squashes it down such that the output value is between 0 and 1. It also has a smooth curve, all headed in the same direction, between those values. This is ideal for our situation. The first thing we must do as part of our loss function, therefore, is to apply the sigmoid function to the inputs.\n\ndef fashion_mnist_loss(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1 - predictions, predictions).mean()\n\nThis torch.where(...) function is a handy way of iterating through all our data, checking whether our target is 1 or not, then outputting the distance from the correct prediction and calculating the mean of these predictions across the entire dataset.\n\n\nDataLoaders and Datasets\nWe’ve already created datasets for our training and validation data. The process of iterating through our data, however, requires some thought as to how we’ll do it. Our options:\n\nwe could iterate through the entire dataset, making the relevant loss and gradient calculations and adjusting the weights but this might make the process quite long, even though we’d benefit from the increased accuracy this would bring since we’d be seeing the entire dataset each iteration.\nwe could do our calculations after just seeing a single image, but then our model would be over-influenced and perturbed by the fluctuations from image to image. This also wouldn’t be what we want.\n\nIn practice, we’ll need to choose something in between. This is where mini-batches or just ‘batches’ come in. These will be need to be large enough (and randomly populated!) that our model can meaningfully learn from them, but not so large that our process takes too long.\nLuckily, we have the abstraction of the DataLoader which will create all our randomly assigned batches for us.\n\ntrain_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\nvalid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True)\n\n\n\nTraining our model\nNow we can bring the whole process together and train our model:\n\nweights = initialise_params((28*28, 1))\nbias = initialise_params(1)\n\ndef calculate_gradient(x_batch, y_batch, model):\n    preds = model(x_batch)\n    loss = fashion_mnist_loss(preds, y_batch)\n    loss.backward()\n\ndef train_epoch(model, learning_rate, params):\n    # iterate over the training data, batch by batch\n    for x_batch, y_batch in train_dl:\n        # calculate the gradients\n        calculate_gradient(x_batch, y_batch, model)\n        \n        for param in params:\n            param.data -= param.grad * learning_rate\n            # set the gradients to zero\n            param.grad.zero_()\n\ndef batch_accuracy(x_batch, y_batch):\n    preds = x_batch.sigmoid()\n    correct = (preds > 0.5) == y_batch\n    return correct.float().mean()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(x_batch), y_batch) for x_batch, y_batch in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nvalidate_epoch(linear1)\n\n0.2173\n\n\nWe start there, but now we can train and watch our accuracy improving:\n\nlearning_rate = 1.\nparams = weights, bias\n\nfor _ in range(30):\n    train_epoch(linear1, learning_rate, params)\n    print(validate_epoch(linear1), end=\" \")\n\n0.5001 0.5016 0.7994 0.9357 0.9481 0.9523 0.9537 0.9555 0.9572 0.9582 0.9578 0.9604 0.9608 0.9609 0.962 0.9611 0.9622 0.9626 0.9631 0.9625 0.963 0.963 0.9633 0.9638 0.964 0.9631 0.9638 0.9638 0.9643 0.9645 \n\n\nWe had 91% accuracy on our validation dataset last time we tried this with pixel similarity.\nAfter 30 epochs of training with our new process we’ve achieved 96%, but we could still do better! We’ll tackle that in the next post.\n\n\nOptimising with an Optimiser\nEverything that we’ve been doing so far is so common that there is pre-built functionality to handle all of the pieces.\n\nour linear1 function (which calculated predictions based on our weights and biases) can be replaced with PyTorch’s nn.Linear module. Actually, nn.Linear does the same thing as our initialise_params and our linear1 function combined.\n\n\n# initialises our weights and bias, and is our model / function\nlinear_model = nn.Linear(28*28, 1)\n\nOur PyTorch module carries an internal representation of our weights and our biases:\n\nweights, bias = linear_model.parameters()\nweights.shape, bias.shape\n\n(torch.Size([1, 784]), torch.Size([1]))\n\n\n\nan optimiser bundles the step functionality and the zero_grad_ functionality. In the book we see how to create our own very basic optimiser, but fastai provides the basic SGD class which we can use that handles these same behaviours.\n\nWe’ll need to amend our training function a little to take this into account:\n\nlinear_model = nn.Linear(28*28, 1)\nopt = SGD(linear_model.parameters(), learning_rate)\n\ndef train_epoch(model):\n    for x_batch, y_batch in train_dl:\n        calculate_gradient(x_batch, y_batch, model)\n        opt.step()\n        opt.zero_grad()\n\ndef train_model(model, epochs):\n    for _ in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=\" \")\n\ntrain_model(linear_model, 30)\n\n0.96 0.9642 0.966 0.9659 0.9663 0.9672 0.9677 0.9668 0.9678 0.9684 0.9681 0.9674 0.9681 0.9671 0.9678 0.9677 0.9684 0.968 0.9687 0.9677 0.9681 0.968 0.9693 0.9684 0.968 0.9686 0.9688 0.9693 0.9698 0.9697 \n\n\n\n\nSome extra fastai abstractions\nfastai handles so much of this for us all, such that the Learner is actually the thing we can use to get all of the above logic built in.\nThe Learner takes all of the pieces that we’ve spent the last few blogs creating:\n\nthe DataLoaders (iterators providing the data in batches, in the right format with paired x and y values)\nthe model itself (our function that we’re trying to optimise)\nthe optimisation function (which receives our weights and bias parameters as well as the learning rate)\nthe loss function\nany optional metrics we want printed\n\n\ndls = DataLoaders(train_dl, valid_dl)\nlearn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=fashion_mnist_loss, metrics=batch_accuracy)\n\nlearn.fit(15, lr = learning_rate)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      batch_accuracy\n      time\n    \n  \n  \n    \n      0\n      0.063289\n      0.045487\n      0.963500\n      00:00\n    \n    \n      1\n      0.044268\n      0.042236\n      0.964500\n      00:00\n    \n    \n      2\n      0.037112\n      0.040228\n      0.965500\n      00:00\n    \n    \n      3\n      0.034010\n      0.038743\n      0.967000\n      00:00\n    \n    \n      4\n      0.032013\n      0.038781\n      0.966000\n      00:00\n    \n    \n      5\n      0.030633\n      0.037635\n      0.966500\n      00:00\n    \n    \n      6\n      0.030458\n      0.037530\n      0.967500\n      00:00\n    \n    \n      7\n      0.029747\n      0.036593\n      0.967000\n      00:00\n    \n    \n      8\n      0.029511\n      0.036479\n      0.967500\n      00:00\n    \n    \n      9\n      0.029305\n      0.035645\n      0.967500\n      00:00\n    \n    \n      10\n      0.028643\n      0.035400\n      0.966500\n      00:00\n    \n    \n      11\n      0.028562\n      0.035477\n      0.966500\n      00:00\n    \n    \n      12\n      0.028788\n      0.035191\n      0.968000\n      00:00\n    \n    \n      13\n      0.028261\n      0.034843\n      0.968000\n      00:00\n    \n    \n      14\n      0.028172\n      0.034883\n      0.968500\n      00:00\n    \n  \n\n\n\nSo there we have it. We learned how to create a linear learner. Obviously 96.8% accuracy is pretty good, but it could be better. Next time we’re going to add the final touches to this process by creating a neural network, adding layers of nonlinearity to ensure our function can fit the complex patterns in our data."
  },
  {
    "objectID": "posts/2024-03-24-publishing-afghanistan-dataset-huggingface.html",
    "href": "posts/2024-03-24-publishing-afghanistan-dataset-huggingface.html",
    "title": "Publishing the ISAF Press Releases dataset",
    "section": "",
    "text": "Yesterday I published two datasets to the Hugging Face Hub and I wanted to briefly add some context to them and what they might be useful for.\n\nTL;DR:\n\nI wrote a paper in 2011 that used international military forces’ press releases about Afghanistan military operations to gain an understanding of what was going on on the ground.\nThe paper was covered in the Guardian and on the BBC and elsewhere and subsequently lead to a significant scaling back of press release coverage.\nThe data is hard to come by now since ISAF and NATO took down the original website and files. I’ve had researchers contact me asking for the data over the years so now I’m making it available as a public dataset on the Hugging Face Hub.\nThe core dataset consists of expert-labelled data pairs (an article with the metadata extracted from the article about how many people were killed and captured and so on) so I’m thinking that it might unintentionally be a useful evaluation task for LLMs. The Hugging Face dataset cards provide more information about what tasks might be appropriate.\n\n\nThe Datasets\nThe ISAF Press Releases dataset contains data used as the basis for the research paper “A Knock on the Door: 22 Months of ISAF Press Releases”, originally published in October 2011. The dataset provides a comprehensive collection of press releases issued by the International Security Assistance Force (ISAF) in Afghanistan from December 1, 2009, to February 21, 2013. The press releases were collected, processed, and annotated to extract information about kill-capture missions carried out by ISAF during this period. The dataset offers valuable insights into the nature and extent of these operations, providing a historical record of ISAF’s activities in Afghanistan. It consists of 4822 press release reports, each labelled with information about the event, including the date, location, target group, and the number of people killed or captured (as represented in the data). The Guardian’s datablog team also published a series of accompanying visualisations at the time.\nThe second dataset is a variation on the same data. It contains the raw HTML files of press releases issued by the International Security Assistance Force (ISAF) in Afghanistan, covering a broader period than the original dataset (spanning 2009-2016). In addition to the raw HTML files containing the news reports, the dataset provides a Parquet file that contains all the data parsed from the HTML files and API requests. This Parquet file serves as the primary resource for researchers and organizations interested in using the dataset. The dataset offers a comprehensive collection of press releases. The HTML files are organized by year and month for archival purposes, while the Parquet file provides a structured and easily accessible format for data analysis.\n\n\nMotivation for the core dataset\nThe dataset was created to provide a comprehensive and publicly accessible record of ISAF’s kill-capture missions in Afghanistan, as reported in their press releases. The motivation was to enable scholars, legal teams, and others to analyse and understand the nature and extent of these operations, as the original ISAF website no longer exists in the same form. The dataset serves as an important historical artifact for Afghan history and provides a means to reverse-engineer the minimum numbers of people killed and captured by ISAF during the specified period.\nThe initial data collection involved manually copying the text of press releases from the now defunct ISAF website at http://www.isaf.nato.int/article/isaf-releases/ into Tinderbox software. The press releases were collected from December 1, 2009, to February 21, 2013, covering a period of over 21 months. All available press releases during this period were included in the dataset.\nThe collected press releases were then processed to split them into individual incident reports. If a press release mentioned multiple incidents, they were separated into distinct entries. The text of the press releases was not modified and remains in its original form.\nThe annotation process involved reading each press release and evaluating it against a set of variables. The annotations were performed using Tinderbox software. The variables included:\n\nBasic data: Incident name, reference number, date of the incident.\nLocation data: Province, district, village name (if provided).\nTarget data: Target group, claimed capture of a “leader” or someone in a leadership position, specific position of the target.\nNumerics: Indication of someone being killed or detained, minimum number killed or detained, exact terms used to refer to those detained or killed, numbers of “leaders” and “facilitators” claimed to be killed or captured, classification as a “capture-or-kill” raid, involvement of an airstrike. The annotator used a fixed list of interpretations for certain terms when estimating the minimum numbers of people killed or detained. Detailed explanations of the annotation process and variable definitions are provided in the associated research paper.\n\n\n\nThe report’s conclusions\nI’ll just quote from the report’s press release for this so as not to get any details wrong:\n\n“ISAF officials have long presented the recently stepped‐up capture‐or‐kill operations as one of the most effective parts of the military mission in Afghanistan. They regularly release large figures describing the number of ‘leaders’, ‘facilitators’ and ‘insurgents’ that were killed or captured, to illustrate the success of the campaign. A closer examination of the information that is publicly available, however, reveals some important inconsistencies, particularly surrounding the classification of who is considered an insurgent ‘leader’.”AAN’s latest report, by Alex Strick van Linschoten and Felix Kuehn, is based on an analysis of all ISAF press releases over the last 22 months. The report provides important baseline data, as well as an insight into how ISAF sees the success of their operations. Alex Strick van Linschoten: “Because there is no solid data and no transparency, the debate [with regard to the capture‐or‐kill strategy] tends to be either emotional or anecdotal. If anything goes wrong, ISAF often dismisses the incident as an exception to a successful strategy. Without proper data, you can’t really have this discussion.” “The research covers the period from 1 December 2009 to 30 September 2011 and included 3,771 ISAF press releases, which reported a total of 3,157 incidents (including 2,365 capture‐or‐kill raids). During this period (at least) 3,873 individuals were reported killed and 7,146 detained. The two peaks of ISAF activity were in September 2010 and June 2011. The numbers show a steady general increase in reported kills and captures each month until June 2011, with a slight decrease over the winter (2010—11). The number of ‘leaders’ and ‘facilitators’ that were reported killed amounted to approximately 5 per cent of the total number of deaths, while the number of ‘leaders’ and ‘facilitators’ detained consists of approximately 13 per cent of the total number of reported detentions.”\n\n\n\nFurther research using this data\nI originally undertook this project and research work because I was sick and unable to do much onerous fieldwork at the time on the ground in Afghanistan. I was surprised at how much value could be extracted from a seemingly benign set of data as a series of press releases. The report I originally wrote (together with Felix Kuehn) barely scratches the surface in terms of possible analyses, and I hope that, by publishing the raw data, others might follow in my footsteps and produce further analysis. Some possible ideas I had for projects that would be interesting:\n\ncorrelate the press releases with the data in the Wikileaks Afghanistan war logs to see the extent to which press releases were reporting the reality of what was going on.\ncorrelate the press releases to local news stories\ncompare the public claims about the kill capture raids with the data shared in press releases. I did a bit of this in the original report, but there’s a lot more to be done here.\n\nSince I’m also sharing this data with the view that it’s also an interesting machine learning dataset, some further ideas include:\n\nuse it for structured data extraction (fine-tune or train a language model that can reproduce the expert labels that I originally made)\nfine-tune a smaller model that can label the full dataset, given the labelled subset (i.e. the original dataset)\nfine-tune a model to write the press releases, given some summary of the days’ activities\nstatistical analysis of the full dataset, esp following LLM-driven labelling\napply the same methodology to the press releases of the Afghan Ministry of Defence, who took over the mantle of providing these daily summaries of kill-capture missions around the country. (I hope to present a separate dataset containing these press releases soon as well, which is important since these have also been taken offline.)\n\n\n\nSome takeaways from the project\nFirstly, data labelling is hard! There are 4822 events/items in the core dataset and I read and labelled every single one of them. This took me weeks to carry out and it’s worth reading the report’s section on data labelling to get a sense of why labelling this particular data was hard.\nSecondly, data loss is a real thing! Even though we have amazing resources like archive.org which preserve old sites, it’s still striking that a military operation that took years and that cost so much money and lives didn’t see it as important to preserve the archival record of its actions. Publishing these press releases as an open dataset is part of other work I’ve done to try to preserve these kinds of documents. I encourage others to do the same.\nFinally, there was an unexpected amount of value in data that at first sight was fairly anodyne and mundane. It’s a good reminder that small projects with focused datasets can provide a lot of value."
  },
  {
    "objectID": "posts/2022-05-07-redaction-mvp-huggingface.html",
    "href": "posts/2022-05-07-redaction-mvp-huggingface.html",
    "title": "A painless way to create an MVP demo using computer vision models",
    "section": "",
    "text": "After the second class of the fastai course, we’re encouraged to create mini-projects that result in models we can deploy online. Deployment is a huge field with its own complexities, of course, but having an option to get something out in the world that’s visible and usable is extremely useful.\nIn this post, I will walk you through how I built a super quick MVP of my redacted document detector project. I used:\n\nfastai to classify and extract redacted pages extracted from PDFs\nicevision (@ai_fast_track) to detect the redacted areas\nHuggingFace Spaces (with Gradio and Streamlit) to deploy my MVP\n\nThe post shows how I went about thinking through the task, showcasing some examples of small prototypes I built along the way, including the final stage where I built: - an app including everything that would be needed by a final ‘deployed’ use case of my model - two models working in tandem in the same app (one classification, one object detection) - optional PDF generation of items detected by the model (!)\nI also explore why you might want to have a minimal deployed version of your application in the first place!"
  },
  {
    "objectID": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-gradio",
    "href": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-gradio",
    "title": "A painless way to create an MVP demo using computer vision models",
    "section": "📐 When to use Gradio",
    "text": "📐 When to use Gradio\n\nif you have a simple use case that you want to highlight\nif your inputs and outputs are clearly defined\nif you have a single model to showcase\nif you want to get something quickly deployed"
  },
  {
    "objectID": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-streamlit",
    "href": "posts/2022-05-07-redaction-mvp-huggingface.html#when-to-use-streamlit",
    "title": "A painless way to create an MVP demo using computer vision models",
    "section": "🌊 When to use Streamlit",
    "text": "🌊 When to use Streamlit\n\nif your use case is more interactive or less simple than just basic input-then-output\nif you want more control on how your demo application is displayed\nif you enjoy a more imperative style of programming\n\nGiven how much inference is going on behind the scenes, I’m surprised that these applications run as fast as it does. For a document with 4 or 5 redacted pages, it takes around 10 seconds to do all the steps described above. 10 seconds is still far too long for a scenario where you wanted to run inference over millions of pages, but in that scenario you wouldn’t be manually uploading them on a web app either.\nIt’s extremely gratifying to have these kinds of tools available to use for free, and really exciting that you get to build out prototypes of this kind after just two weeks of study on the fastai course."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html",
    "href": "posts/2022-01-18-robust-python-5.html",
    "title": "Using type annotation with collections in Python",
    "section": "",
    "text": "The fifth chapter of ‘Robust Python’ continues on from where we left off last time. We saw how to apply type annotations when simple things like strings, integers and floats were involved. This chapter deals with the different ways you annotate your types when collections get involved.\nWe start with the context for why this is even something that requires a separate chapter to deal with. This involves the difference between homogenous and heterogeneous types. For a Python list, we could say it had homogenous types if all the items were of the same type (strings, e.g.). If this list contains multiple different types (a mix of strings and integers, e.g.) then we’d have to say it contained heterogenous types. This is of importance given that the presence of multiple types in a single list is going to require you to handle the types differently. Even in the most trivial of examples (as with strings and integers being together), the interfaces for both are different. Try adding a string to an integer in Python and see what happens.\nSo it’s actually not quite true to say that a collection of homogenous types have to all be exactly the same type, but they must share common interfaces and ideally be handled using the same logic. If you think about it, in the real world heterogenous types are pretty common occurrences. There are often situations where, for example, you have to handle the output of API calls or data that doesn’t derive from code that’s in yous control and then you’ll perhaps be dealing with a dictionary that contains all sorts of types.\nIn Python we do have the typing.Any annotation, but it’s pretty clear — and the book emphasises this — that isn’t really useful in the vast majority of cases. You might as well not bother with type annotations if you’re going to liberally be using Any."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#the-first-of-our-collection-type-helpers-typeddict",
    "href": "posts/2022-01-18-robust-python-5.html#the-first-of-our-collection-type-helpers-typeddict",
    "title": "Using type annotation with collections in Python",
    "section": "The first of our collection type helpers: TypedDict",
    "text": "The first of our collection type helpers: TypedDict\nTypedDict was introduced in Python 3.8 and allows you to communicate intent when it comes to the types that are being passed through your code. Note that, as with a lot of what we’re talking about here, this is all information that’s useful for a type checker and isn’t something that is dynamically checked.\nYou can use TypedDict to define structures that specify the types of fields of your dictionary in a way that is easier to parse as a human reader than just using dict. See this example, adapted from one in the book:\nfrom typing import TypedDict\n\nclass Range(TypedDict):\n    min: float\n    max: float\n\nclass Stats(TypedDict):\n    value: int\n    unit: str\n    confidenceRange: Range\n\nour_stats = Stats(value=3, unit=\"some_name\", confidenceRange=Range(min=1.3, max=5.5))\nprint(our_stats) # returns {'value': 3, 'unit': 'some_name', 'confidenceRange': {'min': 1.3, 'max': 5.5}}\nIf TypedDict doesn’t do everything you need it to, we have some other options."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#custom-collections-with-typevar",
    "href": "posts/2022-01-18-robust-python-5.html#custom-collections-with-typevar",
    "title": "Using type annotation with collections in Python",
    "section": "Custom Collections with TypeVar",
    "text": "Custom Collections with TypeVar\nTypeVar in Python is how you can implement generics. Generics, as I learned while reading, are ways of representing things that are the same, like when you don’t care what specific type is being used. Take this example from the book, where you want to reverse items in a list, but only if the items are all of the same type. You could write the following:\nfrom typing import TypeVar\nT = TypeVar('T')\ndef reverse(coll: list[T]) -> list[T]:\n    return coll[::-1]\nYou can use generics in other ways to create new kinds of collections or groupings. For example, again this one is adapted from the book, if you were writing a series of methods that returned either something useful or a particular error message:\ndef get_weather_data(location: str) -> Union[WeatherData, APIError]:\n    # …\n\ndef get_financial_data(transaction: str) -> Union[FinancialData, APIError]:\n    # …\n…and so on, you could use generics as a way of simplifying how this gets presented:\nT = TypeVar('T')\nAPIResponse = Union[T, APIError]\n\ndef get_weather_data(location: str) -> APIResponse[WeatherData]:\n    # …\n\ndef get_financial_data(transaction: str) -> APIResponse[FinancialData]:\n    # …\nThat looks and feels so much cleaner!"
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#tweaking-existing-functionality-with-collections",
    "href": "posts/2022-01-18-robust-python-5.html#tweaking-existing-functionality-with-collections",
    "title": "Using type annotation with collections in Python",
    "section": "Tweaking existing functionality with collections",
    "text": "Tweaking existing functionality with collections\nIf you’re just making slight changes to the behaviour of collections, instead of subclassing dictionaries or lists or whatever, it’s better to override the methods of collections.UserDict, collections.UserString and/or collections.UserList.\nYou’ll run into fewer problems when you actually implement this. Of course, there is a slight performance cost to importing these collections, so it’s worth making sure this cost isn’t too high.\nYou’ll maybe have noticed that there isn’t a collections.UserSet in the list above. For sets we’ll have to use abstract base classes which are found in collections.abc. The big difference between the User* pattern of classes, there is no built-in storage for the abc classes. You have to provide your own storage if you need it. So for sets, we’d use collections.abc.Set and then implement whatever group of methods are required for that particular class.\nIn the set example, we have to implement __contains__, __iter__ and __len__, and then the other set operations will automatically work. There are currently (as of Python 3.10.2) 25 different ABCs available to use. I definitely will be exploring those as they seem really useful.\nEven though this chapter got into the weeds of collections a little, I learned a lot and I’m already finding places in the ZenML codebase where all of this is being used."
  },
  {
    "objectID": "posts/2022-01-18-robust-python-5.html#typeguard",
    "href": "posts/2022-01-18-robust-python-5.html#typeguard",
    "title": "Using type annotation with collections in Python",
    "section": "Typeguard",
    "text": "Typeguard\nBefore I leave, since we’re still thinking about types, I wanted to share this little package I discovered the other day: typeguard. You can use it in a bunch of different ways, but a useful short video from calmcode.io showed how a simple decorator can simplify code and catch type errors.\nConsider the following example code:\ndef calculate_risk(risk_factor: float) -> str:\n    \"\"\"Calculates how much risk you took\"\"\"\n    return risk_factor * 3 # arbitrary return value :)\nWhat if someone passes in a wrong type into this function? It’ll fail. So maybe we want to handle that particular situation:\ndef calculate_risk(risk_factor: float) -> str:\n    \"\"\"Calculates how much risk you took\"\"\"\n    if not isinstance(risk_factor, float):\n        raise ValueError(\"Wrong type for risk_factor\")\n    return risk_factor * 3\nIf you have lots of parameters in your function and you have to handle them all, this could get messy quite quickly. Instead, we can pip install typeguard and do the following:\nfrom type guard import typechecked\n\n@typechecked\ndef calculate_risk(risk_factor: float) -> str:\n    \"\"\"Calculates how much risk you took\"\"\"\n    return risk_factor * 3\nNow that’s a handy little decorator! It’ll handle all the raising of appropriate errors above based on whether you passed in the right type or not. It works for classes as well. You’re welcome, and thanks Vincent for making the introductory video!"
  },
  {
    "objectID": "posts/2023-03-25-tricks-are-the-thing.html",
    "href": "posts/2023-03-25-tricks-are-the-thing.html",
    "title": "The Trick Is The Thing, Part II",
    "section": "",
    "text": "I’m approaching the end of the MU123 ‘Discovering Mathematics’ module and have really been enjoying some of the recent areas we’ve studied. In particular, quadratic equations really captured my attention and imagination. I’m starting to work on the trigonometry unit and it looks set to be equally amazing. What’s even better is that I think I’m starting to see (the beginnings of) some of the connections between concepts and units that I previous wrote about which makes everything that much more compelling.\nTo broaden those connections even further, in the FastAI course we often are reminded of how lots of parts of deep learning are (just) clever tricks. Those tricks might take the form of how you process the data, or how one thing is combined with another, but together these things combine together to form a more meaningful whole. So it is, I am discovering, with mathematics.\nIn a degree format like the one I’m working my way through, you often don’t encounter the insights and ‘tricks’ in a similar context to when and how they were first discovered. Instead of encountering problems that require solutions, you first cover the solutions and then apply those to some problems. I was struck today, though, at the cumulative weight of these incremental improvements.\nTo take one example, we have trigonometry which is — as I currently understand it, one day of study into the unit (please don’t email me) — the study of how angles and lengths relate to one another. We learn how we can use sin and cos and tan to calculate lengths and angles when we don’t have all the information about a particular geometric shape. When we start off, we’re only talking about right-angled triangles, but then later on we start thinking about all kinds of triangles and the trick is to drop a perpendicular so that we are still actually talking about right-angled triangles. So we’ve made this mental leap which allows us to solve more interesting problems.\nThere are many other examples of this from how we work with quadratic equations, to how something like the VAE (variational autoencoder) helps us train faster in the world of deep learning and Stable Diffusion.\nAs someone watching the breathtaking pace of developments in generative AI, I’m struck by how we can observe the same thing there, but at an accelerated pace compared to the world where many of these mathematical techniques were discovered. The internet combined with a number of smart people thinking about this problem space are proving fertile ground.\nFrom trigonometry to quadratic equations to deep learning, the insights and tricks developed along the way build upon each other to form a more meaningful whole. This idea of clever tricks is not unique to mathematics or deep learning, but it is a universal concept that applies to many areas of human knowledge and understanding. These tricks may take the form of new methods or approaches that allow us to solve problems more efficiently or effectively."
  },
  {
    "objectID": "posts/2025-01-13-assembling-the-prompt:-notes-on-prompt-engineering-for-llms-ch-6.html",
    "href": "posts/2025-01-13-assembling-the-prompt:-notes-on-prompt-engineering-for-llms-ch-6.html",
    "title": "Assembling the Prompt: Notes on ‘Prompt Engineering for LLMs’ ch 6",
    "section": "",
    "text": "Chapter 6 of “Prompt Engineering for LLMs” is devoted to how to structure the prompt and compose its various elements. We first learn about the different kinds of ‘documents’ that we can mimic with our prompts, then think about how to pick which pieces of context to include, and then think through how we might compose all of this together.\n\nThere’s a great figure to give you an idea of ‘the anatomy of a well-constructed prompt’ early on. The introduction is where you introduce the task, then you have the ‘valley of meh’ (which the LLM can struggle to recall or obey) and finally you have the refocusing and restatement of the task.\nThere are two key tips at this point:\n\nthe closer a piece of information is to the end of the prompt, the more impact it has on the model\nthe model often struggles with the information stuffed in the middle of the prompt\n\nSo craft your prompts accordingly!\nA prompt plus the resulting completion is defined as a ‘document’ in this book, and there are various templates that you can follow: an ‘advice conversation’, an ‘analytic report’ (often formatted with Markdown headers), and a ‘structured document’.\nWe learn that analytic report-type documents seem to offer a lighter ‘cognitive load’ for an LLM since it doesn’t have to handle the intricacies of social interaction that it would in the case of an advice conversation. 🤔\nTwo other tips or possible things to include in the analytic report-style document:\n\na table of contents at the beginning to set the scene\na scratchpad or notebook section for the model to ‘think’ in\n\nI haven’t had much use of either of these myself but I can see why they’d be powerful.\nStructured documents can be really powerful, especially when the model has been trained to expect certain kinds of structure (be it JSON or XML or YAML etc). Also TIL that apparently OpenAI’s models are very strong when dealing with JSON as inputs.\nThe context to be inserted into the prompt (usually dynamically depending on use case or needs) can be large or small depending on what is available in terms of context window or latency requirements. There are different strategies to how to select what goes in.\nI was curious about the idea of what they call ‘elastic snippets’, i.e. dynamic decisions that get taken as to what makes it way into the prompt depending on how much space is available etc.\nAnd even then you have to decide about the:\n\nposition (which order do all the elements appear in the prompt)\nimportance (how much will dropping this element from the prompt effect the response)\ndependency (if you include one element, can you drop another and vice versa…)\n\nIn the end, you have a kind of optimisation problem: given a theoretical unlimited potential prompt length, how to combine all the elements together to get the most value given the space limitations that the LLM dictates.\n\nAnd then what strategy do you use to get rid of elements that your prompt budget cannot afford; we learn about the ‘additive greedy approach’ and the ‘subtractive greedy approach’, all the while bearing in mind that these are all just basic prototypes to play around with.\n\nThe next chapter is all about the completion and how to make sure we receive meaningful and accurate responses from our LLM!"
  },
  {
    "objectID": "posts/2023-05-22-balochi-language-model-harms.html",
    "href": "posts/2023-05-22-balochi-language-model-harms.html",
    "title": "The Risks of Language Models in Minority Languages",
    "section": "",
    "text": "In thinking about my work to put together a language model or some utilities relating to the Balochi language, I thought a fair bit about whether I should even start. At a very high level, we can look at general risks that comes from language models, as highlighted in the 2022 Deepmind paper entitled “Taxonomy of Risks posed by Language Models” which covers\n\n“six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms” (p.214)\n\nI’ll leave you to check out the paper if you wish. I can think of a number of specific risks and harms that could be connected to developing a language model for the Balochi language, most of which relate to state actors and their desire for more power and better surveillance capabilities.\nThe communities speaking Balochi have historically and currently been subject to more monitoring than many, either from central governments in Iran and Pakistan or from European and American intelligence agencies, for a variety of reasons. In this context, language models can potentially fit into a system which is geared towards maximising power and influence among the powerful, enhancing state control and surveillance. In the longer term — and I haven’t seen to much by the way of research on this, but I’m going to take a leap — I can imagine that language models could well have the effect of creating a kind of linguistic monoculture. (Just think about the kind(s) of language that you read in default responses from ChatGPT and extrapolate from there.)\nMy assumption is that large, well-funded intelligence agencies already have strong capabilities for Balochi. Indeed, Balochi is one of the language specified as qualifying for the CIA’s ‘Foreign Language Incentive Program’ that offers cash bonuses for new and current employees with foreign language skills in Balochi. There is almost certainly a team working on — among other things — language models that allows for the better monitoring of communications in the Balochi language. (To get more of a sense of what such models and capabilities might be used for, check out this article.)\nWhat, then, are the positive uses of such technology? In no particular order, some things I thought of in the context of Balochi:\n\ndisaster monitoring and outreach following natural disasters (NGOs and aid organisations (or even governments) are sometimes blocked by their inability to effectively communicate with those they are trying to help)\ntranslation and accessibility (heavily caveated, but there is the potential for positive action here as long as technology is responsibly deployed)\nequal access to technologies\naggregation and summarisation tools\nlanguage models as cultural artifacts (in a context of an incremental cultural erasure)\nenhancing the state of the art when it comes to training language models for low resource languages has the potential to support efforts being undertaken for many other languages\nwork done to enhance datasets and the raw materials for language models might potentially be incorporated in bigger efforts by larger organisations (who generally would never think or make the effort to cover a language like Balochi)\nlocal and/or community ownership of the models (vs that of corporations)\na way to reverse linguistic monoculture\nprobably better results and performance for their specialised tasks than generalised mega-large language models\n(and finally, a cause close to my heart) language models can facilitate language learning by those learning it as a second language.\n\nI don’t believe that there’s a simple calculation than can be made, putting potential harm on one side and potential benefits on the other. Given that these models surely do already exist or are being developed by state actors, I also don’t think it’s a matter of staying away from the area entirely. That said, I do think it’s important to have these questions present when doing this kind of work as well as to involve and work within the context of pre-existing community efforts. I’ll turn to other resources and previous work in my next post to give a sense of what low-hanging fruit remains for the Balochi language."
  },
  {
    "objectID": "posts/2021-11-30-vfnet-basics.html",
    "href": "posts/2021-11-30-vfnet-basics.html",
    "title": "What is VFNet?",
    "section": "",
    "text": "VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been other improvements.\nThe original paper is here. The implementation of this model is here.\nThe problem it solves is that when we’re training a model, we have a large number of possible options for objects detected in an image. What we need to do is rank these options in order of likelihood of being a correct bounding of a box.\nIt is based on and draws on the MMDetection model/toolbox. MMDetection is a Pytorch library for object detection. It is modular, allowing for greater customisability."
  },
  {
    "objectID": "posts/2021-11-30-vfnet-basics.html#other-resources",
    "href": "posts/2021-11-30-vfnet-basics.html#other-resources",
    "title": "What is VFNet?",
    "section": "Other resources",
    "text": "Other resources\nAirctic Presentation on VFNet"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html",
    "href": "posts/2022-05-24-data-versioning-dvc.html",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nIf you’ve been following along as I train an object detection model to detect redactions, you’ll know that there have been a few iterations in how I go about doing this. For the most part, though, the dataset has remained relatively static. I downloaded a huge tranche of publicly-released government documents right at the beginning and aside from my experiments in synthetic data creation I haven’t really been adding to this data.\nWhen it comes to turning this model into something that can work in production, it won’t be enough to have a big bucket of image files that I train on. I’ll need to have a bit more control and fine-grained segmentation of the ways this data is being used. In short, if I want to be able to reproduce my workflows then I need some kind of data versioning."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#tldr-data-versioning-for-computer-vision",
    "href": "posts/2022-05-24-data-versioning-dvc.html#tldr-data-versioning-for-computer-vision",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🚦 TL;DR: Data Versioning for Computer Vision",
    "text": "🚦 TL;DR: Data Versioning for Computer Vision\n\n⛽️ We version our data because it is the fuel for our model development and experimentation process.\n💻 Data versioning tools like DVC allow you to apply the same mental model you have for git to your data.\n⋔ The ability to ‘branch’ off your data gives you the flexibility to experiment just as the same is true for branching off your code to try out some new behaviour.\nDVC is probably the leading tool that allows you to version your data and flexibly access all the previous ‘commits’ and checkpoints you make along the way."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#why-do-we-need-data-versioning-isnt-git-enough",
    "href": "posts/2022-05-24-data-versioning-dvc.html#why-do-we-need-data-versioning-isnt-git-enough",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🤔 Why do we need data versioning? Isn’t git enough?",
    "text": "🤔 Why do we need data versioning? Isn’t git enough?\nIf the lifeblood of traditional software engineering is code then the equivalent for machine learning is data. We solve the problem of checkpointing what our code looked like at a particular moment with git and online hubs like Github. Until recently there weren’t many equivalent options for data. We’re trying to solve the problem that often occurs if you’re asked to reproduce the data that was used to train a particular iteration of a model from some point in the past. Without some kind of data version control this is more or less impossible, particularly if your data is constantly changing.\nEven in my case for this redaction project, I wasn’t ingesting new data all the time but I was removing bad annotations or updating those annotations as I conducted error analysis or used tools like FiftyOne to understand why my model wasn’t performing as well as I’d have liked.\nLuckily there’s a pretty great tool in this space that seems to be more or less unchallenged for what it does in the data versioning domain: Data Version Control or DVC."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#dvc-use-cases",
    "href": "posts/2022-05-24-data-versioning-dvc.html#dvc-use-cases",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "👩‍💻 DVC Use Cases",
    "text": "👩‍💻 DVC Use Cases\nDVC does many things, but for our purposes at this moment its core value is that it helps us version our data. It also handles the case where we have large files or a dataset that changes a lot and where we might end up having problems with storing all the versions of this data.\nThe core behaviour we want to use with a data versioning tool is to access our data at one particular moment. Just like you incrementally annotate your code updates using git, with sometimes atomic progressions as you do your work, so it is with DVC that you can checkpoint your data as you make changes.\nAt the beginning this was a slight mental adjustment for me. When working on a project it is now second nature to regularly make git commits along the way, but I wasn’t in the habit of making regular data commits as a second step. In the long-run, this requires a bit of a mental shift but this is exactly what will enable the benefits that using a tool like DVC brings.\nIn particular, being able to experiment with data in a way that you can always roll-back from feels pretty liberating once you’ve covered your back with DVC. Just as you can use create git branches for your code, so you can create branches for your versioned data. Checking out the precise data used for some zany experiment you did is pretty painless. If you realise that the experiment is a dead-end and it’s not helping you move forward, just rewind and reset your data back to a useable state from before you had that crazy idea to create a million synthetic images :)\nOne other thing: DVC is built on top of git and it follows many of the mental models you might have about how versioning works. In this way, DVC luckily is smart about how it allows you to make incremental changes to your data. When it calculates the diff of your dataset before and after, it really is able to do some atomic updates and logging of what changed rather than just storing all the files multiple times over. This helps prevent you building up a really huge data cache and it helps the whole process be efficient.\n{% include info.html text=“I’ve mentioned this for other tools like Evidently before so I should also note that the DVC online community (https://dvc.org/community) is a pretty friendly and helpful place to hang out and learn about data versioning or to troubleshoot your problems. Nobody will tell you to RTFM here and their community events are generally beginner-friendly in my experience. This makes a big difference so they should be commended for the efforts they take to foster this kind community atmosphere. ❤️” %}"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#how-to-get-started-with-dvc",
    "href": "posts/2022-05-24-data-versioning-dvc.html#how-to-get-started-with-dvc",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🚀 How to get started with DVC",
    "text": "🚀 How to get started with DVC\nThe basics are mostly similar to how you’d use a tool like git:\n\nYou init your repository. This add some DVC superpowers on top of what you already have with git.\nYou specify which files you want to have DVC manage and track. It would make a lot of sense, for example, to have DVC handle tracking your models, your image files and your data annotations (if those exist as separate files).\nYou can optionally also specify a remote location where you want these files to be stored. (DVC supports several types of remote storage: local file system, SSH, Amazon S3, Google Cloud Storage, HTTP, HDFS, among others.)\n\n(To get a taste of the full workflow when using DVC for data tracking I’d recommend something like the basic tutorial they have here. They also recently added a three-part tutorial specific to computer vision that you might want to check out.)\nIf you want to use DVC programmatically using their Python API, you can get some information on this in their docs here. Unfortunately, these docs are incomplete and you’ll have to experiment a bit if you want to do anything beyond the simple functionality they themselves list. I’m told it behaves very similarly to how a tool like GitPython works, where you can just use the equivalent add() or checkout() function call that corresponds to a DVC CLI command, but given the lack of documentation it’s a bit harder to get a full sense of what is possible.\n{% include alert.html text=“DVC includes a lot of extra functionality around experiment tracking and pipelining of your code. You can safely ignore all that and just use DVC for data versioning. No shame in that :)” %}"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#when-to-use-dvc",
    "href": "posts/2022-05-24-data-versioning-dvc.html#when-to-use-dvc",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🛠 When to use DVC",
    "text": "🛠 When to use DVC\nIt probably is a good practice to use something like DVC from the start of most projects. If you know you’re never going to need to update the data you use, or if you will only ever generate one model, then maybe you have no need for data versioning. But realistically, when are you going to do that? Generally speaking you’ll be iterating a lot and you’ll be trying things out, so perhaps just start using DVC at the start of any new project: a git init can just as easily be followed by a dvc init…\nDVC will thrive in long-lived projects where you go down certain rabbit-holes, trying out different approaches and techniques. If you have a decent amount of data — and you probably do if you’re bringing deep learning to the table — then you can leverage how DVC makes it easy to store your data in the remote infrastructure of your choice with dvc remote."
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#how-im-using-dvc-in-my-redaction-project",
    "href": "posts/2022-05-24-data-versioning-dvc.html#how-im-using-dvc-in-my-redaction-project",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "📄 How I’m using DVC in my redaction project",
    "text": "📄 How I’m using DVC in my redaction project\nFor my purposes, the things I’m tracking with DVC include:\n\nthe models I train\nthe PDF documents I downloaded from public sources that form the basis of the data in this project\nthe images that I extracted from the PDF documents\nthe annotations I make on the images using the standard COCO Dataset format.\n\nThis covers the core data that I expect to be working with for this project. I keep all this data synced to a remote Amazon S3 bucket which allows me to easily get set up on a new remote machine if needed.\nI’ll next be writing about how to move towards a ‘production-ready’ system in the coming weeks, but one thing I’ll hope to be adding to the current way I do things is to add some kind of ‘data cards’. I think a combination of manual comments and annotations alongside some auto-generated data profiles would be a useful thing to get a sense of for every checkpoint we make, particularly as the data grows and is augmented.\nLet me know if you’re using DVC to version your data for computer vision projects! I’m curious if there are any tricks I’m missing out…"
  },
  {
    "objectID": "posts/2022-05-24-data-versioning-dvc.html#appendix-how-to-switch-from-git-lfs-to-dvc",
    "href": "posts/2022-05-24-data-versioning-dvc.html#appendix-how-to-switch-from-git-lfs-to-dvc",
    "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
    "section": "🏃 Appendix: How to switch from git-lfs to DVC",
    "text": "🏃 Appendix: How to switch from git-lfs to DVC\nWhen I first started this project, git-lfs or Git Large File Storage seemed the best option that didn’t constrain my choices. It allowed me to store any large files I had inside my repository and allowed for some sort of versioning. Over time this ended up being less robust, especially in the context of an ML workflow, so I decided to switch to using DVC backed by an Amazon S3 bucket.\nI didn’t find any useful information on the DVC website or forums on how to make this switch so I’m including my notes on how I switched over myself.\n{% include alert.html text=“Lots of caution is advised when doing this for your own project or work. I hit some major roadblocks along the way while doing this owing to some quirks of how I’d set ‘git-lfs’ up in the beginning. Please take all necessary backups and snapshots of your data in case something goes wrong along the way!” %}\nSome resources I consulted to understand how to do this:\n\nThis Stackoverflow thread\nA Github issue on the same topic\nA super useful Gist I reached via the previous Github issue that ended up guiding me most of the way\n\nThis is what I did, step by step:\n\nCommit and push everything to Git / Github / git-lfs\nCreate a branch, something like fix/remove-lfs\nRemove the hooks using git las uninstall\nGo into the .gitattributes file and delete whatever tracking you don’t want git-lfs to handle from now on. For me, this involved removing lines referring to .pth (model) files and .jpg (image) files.\nGet a list of all the files that git-lfs currently is storing using the following command: git lfs ls-files > files.txt\nModify the file to remove the beginnings of each line that we don’t need. At the end we want our files.txt to contain just a series of paths to our files. I did it with a simple Python script:\n\nwith open(\"filenames.txt\", \"w\") as f:\n    f.write(\"\")\n\nwith open(\"files.txt\", \"r\") as f2:\n    for line in f2:\n        with open(\"filenames.txt\", \"a\") as f:\n            f.write(line.split(\" \")[-1])\n\nRun run git rm --cached for each file that git-lfs is storing. I did this with a simple bash command that uses the file I’d created in the previous step:\n\nwhile read line; do git rm --cached \"$line\"; done < files.txt\n\nInitialise the DVC repository with dvc init\nAdd whatever data sources you want tracked (dvc add FOLDERNAME)\nAllow for autostaging with DVC with the dvc config core.autostage true command\nCommit everything\nCheck that no git-lfs files are left with the git lfs ls-files command. Whatever you uncached in previous steps should not show up any more.\nRemove any lfs with rm -rf .git/lfs\nMerge your branch into main\n\n(if you’re using git-lfs as a team, now is probably the time when other collaborators can uninstall git-lfs as specified above)\n\nIf needed, add your DVC remote storage with dvc remote add … (consult the docs for exactly how to set this up for your specific needs)\ndvc push to get your files synced with your remote storage\n\nAt this point you should be fully transitioned over. As I mentioned above, there are a ton of weird edge cases and quirks to this process and you probably shouldn’t follow this list blindly. I’m mainly writing this up for my own records as much as anything else, so perhaps it’s helpful for someone else seeking to transition but maybe it should be taken less as a direct list of instructions than an inspiration or general template. (I wish DVC would provide some official-ish guidance on this process through their documentation. I imagine that it’s a fairly common path for someone to outgrow git-lfs and want to get going with DVC but currently there are no instructions for how to think this through.)\nUPDATE: I originally made reference to ‘continuous training’ in the title of this blogpost but I didn’t actually get into this specific use case in what I covered, so I took that out of the title and we’ll save the specifics for a subsequent post!"
  },
  {
    "objectID": "posts/2022-01-16-midway-report-redaction-project.html",
    "href": "posts/2022-01-16-midway-report-redaction-project.html",
    "title": "A Midway Report on my Computer Vision Project",
    "section": "",
    "text": "(This post is adapted from a twitter thread, so is a bit more terse than usual.)\nI recently switched what I spend the majority of my professional life doing (history -> software engineering). I’m currently working as an ML Engineer at ZenML and really enjoying this new world of MLOps, filled as it is with challenges and opportunities.\nI wanted to get some context for the wider work of a data scientist to help me appreciate the problem we are trying to address at ZenML, so looked around for a juicy machine learning problem to work on as a longer project.\nI was also encouraged by Jeremy Howard’s advice to “build one project and make it great”. This approach seems like it has really paid off for those who’ve studied the fastai course and I wanted to really go deep on something myself.\nFollowing some previous success working with other mentors from SharpestMinds on a previous project, I settled on Computer Vision and was lucky to find Farid AKA @ai_fast_track to mentor me through the work.\nIn the last 6 weeks, I’ve made what feels like good progress on the problem. This image offers an overview of the pieces I’ve been working on, to the point where the ‘solution’ to my original problem feels on the verge of being practically within reach.\n\nAfter just a few lessons of the FastAI course, I trained a classification model to ~95% accuracy to help me sort redacted images from unredacted images.\nI used Explosion’s Prodigy to annotate an initial round of data to pass into the next step, enjoying how the labelling process brought me into greater contact with the dataset along the way.\nI switched to using IceVision to help me with the more complicated object detection problem, using MMDetection and VFNet to get pretty good results early on.\nI’m currently in the process of creating my own synthetic images to boost the annotations I’ve manually made. (I’ll be writing about this process soon as well, as I’m learning a lot about why this is so important for these kinds of computer vision problems.)\nI’ve also been amazed at the effectiveness of self-training (i.e. using my initial model in my annotation loop to generate an initial set of annotations which I can easily amend as appropriate, then feeding those annotations in to create a better model and so on). More to follow on that step, too.\nI started using Evidently to do some drift detection, inspired by some work I was doing for ZenML on adding Evidently as an integration to our own tool. This helped me think about how new data was affecting the model and the training cycle. I feel like there’s a lot of depth here to understand, and am looking forward to diving in.\nI made a tiny little demo on HuggingFace Spaces to show off the current inference capabilities and to see the model in a setting that feels close to reality. This is a simple little Gradio app but I liked how easy this was to put together (a couple of hours, mainly involving some build issues and a dodgy requirements.txt file)\nAlong the way, I found it sometimes quite painful or fiddly to handle the PDF files that are the main data source for the project, so I built my own Python package to handle the hard work. I used fastai’s nbdev to very quickly get the starters of what I’m hoping might be a useful tool for others using PDF data for ML projects.\nThroughout all this, Farid has been patiently helping guide me forward. He saved me from going down some dark rabbit holes, from spending too long studying skills and parts of the problem that needed relatively little mastery in order to get to where I am.\nFarid has been a consistently enthusiastic and kind advocate for my work, moreover, and this has really helped me stay the course for this project that takes a decent chunk of my time (especially seeing as I do it completely aside / separately from my day job).\nI feel like I’m consistently making progress and learning the skills of a data scientist working in computer vision, even though I have so much left to learn! My project still has a ways to go before it’s ‘done’, but I’m confident that I’ll get there with Farid’s support. (Thank you!)"
  },
  {
    "objectID": "posts/2022-04-19-data-validation-great-expectations-part-1.html",
    "href": "posts/2022-04-19-data-validation-great-expectations-part-1.html",
    "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nData validation is a process of verifying that data is accurate and consistent. It plays a crucial role in end-to-end machine learning pipelines.\nThere is a lack of validation tools in Computer Vision (CV) due the complexity of the data used by the domain.\nIn this series of articles, I will show you how to leverage the Great Expectations open-source library to validate object detection data. This will help you to feed your model with data less prone to break your model performance.\nWhen something goes wrong with a newly trained or newly deployed version of your model, where do you look first? Where does your gut tell you the bug or issue is likely to be found? For me, knee deep into my redaction model project, I immediately think of my data. For sure, I could probably have more in the way of testing to be sure my code is working how I expect it to work, but issues with the data are far more likely to be silent killers. Issues with data are unlikely to raise a loud exception and suddenly bring my training to a stop. Instead, my training will continue, but I’ll either get really unsatisfactory results or I’ll get results that are underperforming the real potential of the data I’m feeding into my model. That’s the scary part: I most likely won’t even know that my data is broken or faulty.\n\n\nFor software engineers, testing your code is a tried-and-tested way to find some confidence in what you’re trying to accomplish. (I am exploring some of these best practices in my review series about Patrick Viafore’s excellent Robust Pythonbook, which covers testing along with typing and various other patterns.) For critical systems, testing is one of the things that allows you to sleep soundly. For those living in the world of machine learning or data science, data validation is like writing tests for your data. You can be confident that your data looks and has the shape of what you feel it should when you address the data quality issue head-on.\nIf you think of your model training workflow as a pipeline, there are certain places where it makes sense to do some kind of data validation:\n\nat the very beginning, when you’re seeing your data for the first time: a lot of exploration and basic analysis really helps at this point. It will help you build up intuition for the general patterns and boundaries of the data you’re going to use to train your model.\nany time you do some kind of conversion: perhaps you have to — as I do with my project — convert from one image annotation format into another and you’re juggling x and y coordinates constantly, or maybe you’re using different image formats at different points?\nprior to training your model: ‘garbage in, garbage out’ as the saying goes… You probably want to make sure that you only have high quality data passing through into your model training pipeline step.\nas part of a continuous training loop: perhaps you’ve trained and deployed a model, but now a few months have passed, you have more data and you want to retrain your model. Are you confident that the new data retains the same characteristics and qualities of your original data?\n\nAs you can see, there are many different approaches that you might take. To discuss where you might want to validate your data is to discuss where your processes might be flawed in some way. For most projects of any size or significance, you probably will find that taking the care with your data inputs will pay dividends.\n\nData validation and computer vision\nIt often seems like computer vision exists in a world unto its own, particularly when it comes to the data used to train models. These idiosyncrasies amount to a strong case for some kind of data validation:\n\nimage data isn’t always easily introspectable, especially on the aggregate level (i.e. what is the ‘average’ of a series of images, or how to think of the standard deviation of your images?)\nfor something like object detection, the annotations are stored in a separate location from the images to which they correspond, leaving the door open for a creeping data drift between the original image locations and what is listed in the annotations.\nFor massive data sets, the original data will likely not be stored in the environment where you’re doing some fine-tuning with new data.\nDifferent model architectures require different kinds of pre-processing for your data and sometimes annotations need converting into slightly different formats (perhaps for your evaluation metric)\nThe pure images (or image-adjacent objects like medical scans) contain a lot of sub-surface metadata that isn’t easily accessed and isn’t automatically used as criteria for comparison or error detection.\n\nIn short, there are lots of ways that training a computer vision model can go wrong, and implementing even basic safeguards against this can give you confidence in the data you’re using. Unfortunately, the landscape of tooling for data validation in the computer vision space feels like it’s lagging behind what exists for tabular data, for example, but that’s almost certainly because it’s just a harder problem. The big data validation libraries don’t really cater towards computer vision as a core domain, and (as you’ll see below) you’ll probably have to crowbar your data into the formats they expect.\n\n\nBig picture: what might this look like for my project?\nAs I outlined above, there are lots of different places where you might want to use some kind of data validation strategy. At the level of code, you might want to make your input and output validation a bit more solid by using type annotations and a type checker like mypy. You can add tests to ensure that edge cases are being handled, and that your assumptions about your code behaviour are proven. You also have your tests to ensure that changes in one function or area of your codebase don’t break something somewhere else.\nAt the level of your data, you can of course use simple assert statements within the functional meat of your codebase. For example, at the point where you’re ingesting data pre-training you could assert that each image is of a certain format and size, and perhaps even that annotations associated with that image ‘make sense’ as per the context of whatever problem you’re solving. You can handle some of these assertions and checks with simple conditionals, perhaps, earlier on in the process when you are ingesting or pre-processing your data.\nA significant benefit of having these simple assertions inside your core functions is that you are handling the ways things can go wrong at the same time as you’re writing the functionality itself. A disadvantage is that your code can easily become cluttered with all this non-core behaviour. It feels a little like the validation can become an afterthought in this scenario. For this reason, it seems to make sense to me that you’d want to have one or more dedicated checkpoints where your data undergoes some kind of validation process. In the context of a pipeline, this means you probably will want one or more steps where this happens.\n\n\nTradeoffs\nFor tiny throwaway projects, or for proof-of-concept experimentation, it might not make sense to start off by working up a massive data validation suite. A really rigorous validation process early on might slow you down more than is useful. Instead, simple assert statements coupled with type annotations on your functions might be the way to go for safeguards and will-this-be-readable-in-the-future sanity checks.\nIdeally, you’ll want to create some kind of end-to-end pipeline or workflow at the beginning of your process, since this will allow you to iterate faster in a manner that’s meaningful for whatever you’re trying to solve. With a basic pipeline in place, data validation can be added as a stage of its own without too much disruption once you have an initial working prototype. As with most things in life, investing for the longer term is going to take a bit more upfront effort but that shouldn’t be too much an issue as long as your project has that kind of a horizon to it.\n\n\nWhat kind of validation does Great Expectations offer?\nGreat Expectations is an open-source data validation tool. It is somewhat agnostic as to what specific use case you have, but I don’t think it’d be wrong to say that it isn’t primarily developed for those working on computer vision problems; tabular data seems to be a much cosier fit.\nI stated above that Great Expectations could be used as if you were adding tests for your data. At a very high level, you can think of it as a fancier way of adding assertions about your data. The ‘expectations’ in the title are like those assertion statements, only in this case there are dozens of different pre-made ‘expectations’ you can choose from. For example, you could assert that you expect that the values of a particular column of a Pandas DataFrame be between 0 and 100, and that if they exceeded those boundaries then it would be only a very small proportion that did so.\nYour expectations make up a ‘suite’, and you run your suite of expectations against a batch or data asset. There are another 10 or 20 concepts or terms that I’d need to define and connect together in a mental map before we covered everything about how Great Expectations works. Unfortunately, this is one of the things I found most confusing about getting to know the library through its documentation. From the outside, it appears that they had one set of terminology, but now it’s partially changed to a different set of terms or abstractions. Presumably for reasons of backwards compatibility, some of the old abstractions remain in the documentation and explanations, which makes it not always clear to understand how the various pieces fit together.\n\nYou can read the glossary over at their documentation site if you want to learn more, but for now everything I explained above should suffice.\nThere seem to be two main ways of setting up and using Great Expectations. One is heavily interactive and driven by executing cells in a series of notebooks. The other is as you’d expect — code-based using a Python library, backed by some external configuration files and templates. I didn’t find the notebook-based configuration and setup very compelling, but it is the one emphasised in the documentation and in online materials, so I will give it due attention in the next part of this blog series. For now, it might suffice to show a very simple version of how the code-based use works:\n\n\nA simple example of using Great Expectations for data validation\nThe first thing I did was to convert my annotations data into a Pandas DataFrame. You can use Pandas, SQL and Apache Spark as sources for your data to be validated through Great Expectations, and luckily my COCO annotations file was just a JSON file so it was easily converted. While doing the conversion, I made sure to add some extra metadata along the way: a column noting whether an image or a redaction was horizontal or vertical in its orientation, for example, or splitting the bbox array into its four constituent parts.\nimport great_expectations as ge\n\nannotations_df = ge.from_pandas(pd.DataFrame(annotations))\n\nfeature_columns = ['area', 'iscrowd', 'image_id', 'category_id', 'id', 'synthetically_generated', 'category_name']\nfor col in feature_columns:\n    annotations_df.expect_column_to_exist(col)\n    \nannotations_df.expect_column_values_to_be_in_set(\n    \"category_name\",\n    [\"content\", \"redaction\"]\n)\nGreat Expectations wraps the Pandas library, so importing the data was easy. Then adding the expectations (methods beginning with expect…) was trivial. Below you can see the result from the second of the expectations. All of the column values were in that set, so the test passed.\n{\n  \"success\": true,\n  \"result\": {\n    \"element_count\": 6984,\n    \"missing_count\": 0,\n    \"missing_percent\": 0.0,\n    \"unexpected_count\": 0,\n    \"unexpected_percent\": 0.0,\n    \"unexpected_percent_total\": 0.0,\n    \"unexpected_percent_nonmissing\": 0.0,\n    \"partial_unexpected_list\": []\n  },\n  \"meta\": {},\n  \"exception_info\": {\n    \"raised_exception\": false,\n    \"exception_traceback\": null,\n    \"exception_message\": null\n  }\n}\nIn the second part of this series, I’ll explore how the interactive way of using Great Expectations works, and I’ll also show the web results interface for your expectations suite. It’s much fancier than the dictionary / object that was output above, and what’s even better is that you can have Great Expectations make some of its own guesses about what the right expectations for your particular dataset might be.\nI hope for now that I’ve made the case for why data validation is probably worth doing, and started you thinking about how that might apply to a computer vision use case."
  },
  {
    "objectID": "posts/2021-09-09-auto-reload-external-libraries.html",
    "href": "posts/2021-09-09-auto-reload-external-libraries.html",
    "title": "How to set a Jupyter notebook to auto-reload external libraries",
    "section": "",
    "text": "The code to insert somewhere into your Jupyter notebook is pretty simple:\n%load_ext autoreload\n%autoreload 2\nWhen you’re working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "",
    "text": "I previously experimented with one-click LLM finetuning providers and now is a good time to return to the core of the matter: evaluating how well all these fine-tuned models and experiments are faring. I have a gut feeling that my fine-tuned models did pretty well, but we’re not in the business of gut feeling so I’m hoping to be able to put some real numbers down to either prove or disprove this hypothesis.\nAs a quick reminder if you didn’t read any of the previous posts in the series, I’m building a model that can take a press release text like this:\n…and then turn it into structured data (i.e. a JSON object) like this:\nI’ve now fine-tuned several models and I want to get a sense of how good these are. I showcased some initial baseline evaluations using OpenAI’s gpt-4-turbo but I want to pit model against model now.\nI’m also interested in teasing out some of the edge cases where I know I struggled as a human annotator. (I released the dataset for this project publicly on the Hugging Face Hub and also was responsible for annotating every single item so I know the data intimately.) I can even consider using the hard examples to generate some synthetic data to boost performance on those edge cases, but that’s a task for much later on.\nThis blogpost is a prose overview of some of the evaluations I’m adding to my suite of tests (and why I’m adding them). I learned a lot from Hamel Husain’s “Your AI Product Needs Evals” blogpost and if you’re interested in this I’d recommend reading it and then actually implementing his suggestions."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#core-evaluations-for-accuracy",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#core-evaluations-for-accuracy",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Core evaluations for accuracy",
    "text": "Core evaluations for accuracy\nThe most important measurement to start with is just a pure “did the LLM make a correct prediction or not?” If I was doing all these evaluations manually myself, I’d take a look at the example above, for example, and ask myself “was the start date of the event mentioned in the blogpost really ‘2011-11-07’ as predicted by the model?” and “did the event take place in Badakhshan province?” and “were the Haqqanis the group targeted?”\nIt’s fairly straightforward to make these determinations when comparing each property one by one. I can then repeat this over every example in my test slice of my dataset and take an average if I want a single aggregate figure, or I can get individual figures for dates, provinces, target groups and so on (to know if maybe there’s one part of the prediction it struggles with most)."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#out-of-domain-data",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#out-of-domain-data",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Out of domain data",
    "text": "Out of domain data\nThe ISAF mission has come to an end, so I don’t have to worry too much about new data and having to adapt to a continuously changing world, but it is possible that some smaller groups weren’t well represented in the training data (for predicting the target group, for example) so I want to know how well my model does with data it hasn’t seen.\nMy prompt passes in the schema for the data and I encourage it to follow the schema in its response, but if there’s a new group will it add the new group to the schema? I can write an evaluation to test this.\nAnother edge case is the possibility that a press release doesn’t follow the standard format. Having read them all, I know that the vast majority are pretty formulaic, but sometimes there is a special event or incident which caused the author of the press release to depart from the standard formula. I want to know that my model will:\n\nnot just make something up so as to have some kind of JSON response even if the press release is about someone’s birthday party\neven better, produce some sort of error code or blank response when this happens.\n\nI can use examples of this out of domain data to see what happens, and put a value to how often my model will just hallucinate something out of nothing. This will be important since the name of the game for this model is accuracy."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#gradations-of-some-a-few-many",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#gradations-of-some-a-few-many",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Gradations of ‘some’, ‘a few’, ‘many’",
    "text": "Gradations of ‘some’, ‘a few’, ‘many’\nThe press releases try to give some information without actually giving too much. Indeed, when I published my report on the press releases back in 2011, ISAF even issued a press release (!) in which they stated that:\n\n“Release of information in insurgent warfare is not always made public, so studies based on the use of press releases can be both incomplete and problematic. […] Authoritative research cannot be conducted through mere analysis of press releases, since the release of information through such releases is, by design, incomplete.”\n\nSo reading the press releases is very much an exercise in reading between the lines. In the press release cited earlier, all the numbers are specific (“a facilitator”, “a male”, “two insurgents”) so it’s easy to put numbers to how many were killed or captured. In many of the press releases, particularly during times where raids were being conducted at a very high tempo, you have to just take assumptions about what their words mean and assign minimum values to those words.\nSo ‘a couple’ meant at least two, but ‘a few’ meant 3 or more. Similarly ‘dozens’ means multiple dozens so that meant a minimum value of at least 24. From the original report:\n\n“If a press release said that ‘insurgents’ were detained, without further details, we assigned that incident as having a minimum number of 2 detained (since we could not be sure of more). ‘A couple’ we took to mean 2. ‘Several’ we took to mean at least 3, even though on other occasions ‘several’ was used to refer to 7 or 8. Other terms we classified as denoting at least 3 included: ‘a few’, ‘some’, ‘a group’, ‘a small group’ and ‘multiple’; these terms sometimes were used to refer to far larger numbers but we chose the smaller number (if no other information was available in the press release) in order to come up with a minimally acceptable figure. ‘Numerous’ and ‘a handful’ we took to mean at least 4, and ‘a large number’ at least 5.”\n\nThe reports mostly referred to events that had taken place that day or the day before, but occasionally they’d refer to events that took place “last Thursday” or “last week” and so then you’d have to know what day the press release was issued and then make calculations accordingly. For this backwards-referring time assignations, I’m particularly interested (read: concerned!) to know how well my LLMs did. Whatever score we get, it’s probably fixable with a bit of manual parsing and logic, but we need to know if there’s a problem or not first.\nGenerally speaking there were province names assigned to incidents (all but 23, to be precise) but when they weren’t, then the LLM has to work back from a village name, potentially, or just specify that an incident took place in southern Afghanistan or Afghanistan as a whole. On a few occasions, the press release actually made an error, stating that village X or Y was in a particular province, when this was incorrect and it was in a different province. So for this, would we expect the LLM to assign the event to the correct province for that village, or just retain the error in the press release?\nSometimes a press release might refer to an event having taken place “in the provincial capital of X province” but without mentioning that city by name. So the LLM will have to have some knowledge of these things and I want to test how well it performs with this.\nThese might seem like tiny errors to get wrong, but for a project like this (where my report was making some strong accusations and drawing certain conclusions based on the data), it wouldn’t do to get things factually wrong. For an internal-facing LLM-powered chatbot, the price of mistakes is minimal, but for a project like this, there are potentially far more serious consequences which is why I’m putting together such detailed evaluations."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#spelling-variation",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#spelling-variation",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Spelling variation",
    "text": "Spelling variation\nAnother issue with some of the press releases is that they use a variety of spellings for the same locations or names of individuals. For some things — province names, for example — it makes sense to standardise on a fixed naming convention but for others it’s not always clear what to do. So our evaluation should ensure that common variations of certain provinces or designations or names are captured correctly by the LLM output."
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#complex-stories",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#complex-stories",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Complex stories",
    "text": "Complex stories\nSome stories are very complicated and there may be no correct way to assign numbers, for example, to the text that was published. In those cases when annotating I often just left the minimum numbers at zero even though we know that something happened. Would the LLM also make the same call? What is the threshold for deciding not to take a chance on making a guess?"
  },
  {
    "objectID": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#next-step",
    "href": "posts/2024-06-25-evaluation-finetuning-manual-dataset.html#next-step",
    "title": "How to think about creating a dataset for LLM finetuning evaluation",
    "section": "Next step",
    "text": "Next step\nThe obvious next step is to actually code up these evaluation criteria and run those across our fine-tuned LLMs as well as the API-driven proprietary ones. I’ll be working on that over the coming days. Luckily, I did most of the work to identify all of the above when I first wrote the report so there isn’t much ground that needs to be broken so much as just sitting down and getting it done."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html",
    "href": "posts/2024-06-15-isafpr-first-finetune.html",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "",
    "text": "We previously looked into how well the top LLMs could do when given press releases and asked to extract structured data from them. I was glad that this clearly wasn’t a task they struggled with, but it was by no means a simple task for them and some basic evaluations that I performed showed that there was room for improvement.\nSince writing that post I also heard from readers to say that perhaps I wasn’t using the OpenAI API in a way that would get the best results. In particular, function calling would give a better accuracy over the raw prompting that I was using. I’ll probably return to that in a separate post when I compare how well we’re doing with finetuning.\nAs a quick reminder, we’re hoping to create something that will allow us to go from an unstructured text (a press release, in our case) to a structured output that accurately extracts certain pieces of metadata from the text. Please give the first post in the series a read if you want more of the context of what we’re doing.\nThis blogpost will be about my first finetune(s) of some models and I’ll showcase how I got the data ready and then some observations about the finetuning process in general."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#loading-the-datasets",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#loading-the-datasets",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Loading the datasets",
    "text": "Loading the datasets\n\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\n\n# Loadthe dataset\ntrain_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"train\")\ntest_dataset = load_dataset(\"strickvl/isafpressreleases\", split=\"test\")\n\n# Convert the dataset to a pandas DataFrame\ntrain_df = pd.DataFrame(train_dataset)\ntest_df = pd.DataFrame(test_dataset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrain_dataset\n\nDataset({\n    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],\n    num_rows: 4098\n})\n\n\n\ntest_dataset\n\nDataset({\n    features: ['name', 'eventrefnumber', 'text', 'StartDate', 'eventtype', 'province', 'citydistrict', 'village', 'targetgroup', 'commander', 'position', 'minkilled', 'mincaptured', 'capturedcharacterisation', 'killedcharacterisation', 'killq', 'captureq', 'killcaptureraid', 'airstrike', 'noshotsfired', 'dataprocessed', 'flagged', 'glossarymeta', 'minleaderskilled', 'minfacilitatorskilled', 'minleaderscaptured', 'minfacilitatorscaptured', 'leaderq'],\n    num_rows: 724\n})\n\n\nWe have 4098 training examples and 724 test examples. This seems a good split to me. I experimented a bit with the exact split and found that 15% seemed like a good compromise. We want enough data to get a good evaluation, but we also want to give our model enough examples to learn. In the course people were frequently talking about somewhere in the order of mid hundreds to low thousands as being the sweet spot, so I hope I’m firmly in that range.\nIt’s also worth reflecting that I’m lucky that I have such a large clean dataset to work with. In a later project I’d like to try working with much less and slowly building up something more complex since that’s a skill in and of itself."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#setting-up-our-pydantic-models-with-validation",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#setting-up-our-pydantic-models-with-validation",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Setting up our Pydantic models with validation",
    "text": "Setting up our Pydantic models with validation\nThere’s a decent amount of code in the next cell, and definitely read the previous posts to understand what all the pieces are about, but in a nutshell we’re setting ourselves up to extract structured data from the text. This Pydantic model is what will hold the data we’re interested in.\n\nfrom enum import Enum\nfrom typing import Set, Annotated, Optional\nfrom pydantic import BaseModel, Field, validator, ValidationInfo\nfrom datetime import date\n\n\nclass EventType(str, Enum):\n    airstrike = \"airstrike\"\n    detention = \"detention\"\n    captureandkill = \"captureandkill\"\n    insurgentskilled = \"insurgentskilled\"\n    exchangeoffire = \"exchangeoffire\"\n    civiliancasualty = \"civiliancasualty\"\n\n\nclass Province(str, Enum):\n    badakhshan = \"badakhshan\"\n    badghis = \"badghis\"\n    baghlan = \"baghlan\"\n    balkh = \"balkh\"\n    bamyan = \"bamyan\"\n    day_kundi = \"day_kundi\"\n    farah = \"farah\"\n    faryab = \"faryab\"\n    ghazni = \"ghazni\"\n    ghor = \"ghor\"\n    helmand = \"helmand\"\n    herat = \"herat\"\n    jowzjan = \"jowzjan\"\n    kabul = \"kabul\"\n    kandahar = \"kandahar\"\n    kapisa = \"kapisa\"\n    khost = \"khost\"\n    kunar = \"kunar\"\n    kunduz = \"kunduz\"\n    laghman = \"laghman\"\n    logar = \"logar\"\n    nangarhar = \"nangarhar\"\n    nimroz = \"nimroz\"\n    nuristan = \"nuristan\"\n    paktya = \"paktya\"\n    paktika = \"paktika\"\n    panjshir = \"panjshir\"\n    parwan = \"parwan\"\n    samangan = \"samangan\"\n    sar_e_pul = \"sar_e_pul\"\n    takhar = \"takhar\"\n    uruzgan = \"uruzgan\"\n    wardak = \"wardak\"\n    zabul = \"zabul\"\n\n\nclass TargetGroup(str, Enum):\n    taliban = \"taliban\"\n    haqqani = \"haqqani\"\n    criminals = \"criminals\"\n    aq = \"aq\"\n    hig = \"hig\"\n    let = \"let\"\n    imu = \"imu\"\n    judq = \"judq\"\n    iju = \"iju\"\n    hik = \"hik\"\n    ttp = \"ttp\"\n    other = \"other\"\n\n\ndef validate_event_type(value: str):\n    valid_values = [\n        \"airstrike\",\n        \"detention\",\n        \"captureandkill\",\n        \"insurgentskilled\",\n        \"exchangeoffire\",\n        \"civiliancasualty\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_province(value: str):\n    valid_values = [\n        \"badakhshan\",\n        \"badghis\",\n        \"baghlan\",\n        \"balkh\",\n        \"bamyan\",\n        \"day_kundi\",\n        \"farah\",\n        \"faryab\",\n        \"ghazni\",\n        \"ghor\",\n        \"helmand\",\n        \"herat\",\n        \"jowzjan\",\n        \"kabul\",\n        \"kandahar\",\n        \"kapisa\",\n        \"khost\",\n        \"kunar\",\n        \"kunduz\",\n        \"laghman\",\n        \"logar\",\n        \"nangarhar\",\n        \"nimroz\",\n        \"nuristan\",\n        \"paktya\",\n        \"paktika\",\n        \"panjshir\",\n        \"parwan\",\n        \"samangan\",\n        \"sar_e_pul\",\n        \"takhar\",\n        \"uruzgan\",\n        \"wardak\",\n        \"zabul\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\ndef validate_target_group(value: str):\n    valid_values = [\n        \"taliban\",\n        \"haqqani\",\n        \"criminals\",\n        \"aq\",\n        \"hig\",\n        \"let\",\n        \"imu\",\n        \"judq\",\n        \"iju\",\n        \"hik\",\n        \"ttp\",\n        \"other\",\n    ]\n    if value.lower() not in valid_values:\n        return \"other\"\n    return value.lower()\n\n\nclass IsafEvent(BaseModel):\n    name: str = Field(\n        description=\"A title or name for the event which summarises the event as a headline\"\n    )\n    text: Optional[str] = Field(description=\"The full text of the press release\")\n    start_date: date = Field(\n        description=\"The start date of the event in YYYY-MM-DD format\"\n    )\n    event_type: Set[Annotated[str, Field(validator=validate_event_type)]] = Field(\n        description=\"The event type. Can be multiple types.\"\n    )\n    province: Set[Annotated[str, Field(validator=validate_province)]] = Field(\n        description=\"The province in which the event occurred. Can be multiple provinces.\"\n    )\n    target_group: Set[Annotated[str, Field(validator=validate_target_group)]] = Field(\n        description=\"The group that was targetted during the event. Can be multiple groups.\"\n    )\n    min_killed: int = Field(\n        description=\"The minimum number of people killed during the event\"\n    )\n    min_captured: int = Field(\n        description=\"The minimum number of people captured during the event\"\n    )\n    killq: bool = Field(\n        description=\"Whether someone was killed or not during the event\"\n    )\n    captureq: bool = Field(\n        description=\"Whether someone was captured or not during the event\"\n    )\n    killcaptureraid: bool = Field(\n        description=\"Whether the event was a so-called 'kill-capture raid'.\"\n    )\n    airstrike: bool = Field(\n        description=\"Whether an airstrike was used during the event\"\n    )\n    noshotsfired: bool = Field(\n        description=\"Whether no shots were fired during the event\"\n    )\n    min_leaders_killed: int = Field(\n        description=\"The minimum number of leaders killed during the event\"\n    )\n    min_leaders_captured: int = Field(\n        description=\"The minimum number of leaders captured during the event\"\n    )\n\n    class Config:\n        arbitrary_types_allowed = True\n\nHere’s what a couple of examples of our training data looks like as Pydantic models when we pass them in:\n\nfrom typing import List\n\nevents: List[IsafEvent] = []\n\nfor i, row in list(train_df.iterrows()):\n    event_types = set(\n        eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n    )\n    provinces = set(province.strip().lower() for province in row[\"province\"].split(\",\"))\n    target_groups = set(\n        target_group.strip().lower() for target_group in row[\"targetgroup\"].split(\",\")\n    )\n\n    events.append(\n        IsafEvent(\n            name=row[\"name\"],\n            text=row[\"text\"],\n            start_date=row[\"StartDate\"].to_pydatetime().date(),\n            event_type=event_types,\n            province=provinces,\n            target_group=target_groups,\n            min_killed=int(row[\"minkilled\"]),\n            min_captured=int(row[\"mincaptured\"]),\n            killq=row[\"killq\"] == \"true\",\n            captureq=row[\"captureq\"] == \"true\",\n            killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n            airstrike=row[\"airstrike\"] == \"true\",\n            noshotsfired=row[\"noshotsfired\"] == \"true\",\n            min_leaders_killed=int(row[\"minleaderskilled\"]),\n            min_leaders_captured=int(row[\"minleaderscaptured\"]),\n        )\n    )\n\nprint(events[:2])\n\n[\n    IsafEvent(\n        name='Several insurgents killed in Helmand',\n        text='ISAF Joint Command Evening Operational Update Feb. 19, 2011\\nISAF Joint Command - \nAfghanistan\\u20282011-02-S-143\\u2028For Immediate Release \\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF \nservice members at a compound in Sangin district, Helmand province observed numerous insurgents north and south of \ntheir position talking on radios today. After gaining positive identification of the insurgent positions, the \ncoalition troops engaged, killing several insurgents. Later, the ISAF troops observed more insurgents positioning \nin the area with weapons. After positive identification, coalition forces continued firing on the various insurgent\npositions, resulting in several more insurgents being killed.',\n        start_date=datetime.date(2011, 2, 18),\n        event_type={'insurgentskilled'},\n        province={'helmand'},\n        target_group={''},\n        min_killed=6,\n        min_captured=0,\n        killq=True,\n        captureq=False,\n        killcaptureraid=False,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=0,\n        min_leaders_captured=0\n    ),\n    IsafEvent(\n        name='Force Continues Targeting Haqqani Leadership',\n        text='Force Continues Targeting Haqqani Leadership\\nISAF Joint Command - Afghanistan\\u20282010-09-CA-211 \nFor Immediate Release\\u2028Download PDF \\u2028\\u2028\\u2028\\xa0KABUL, Afghanistan (Sept. 20) - An Afghan and \ncoalition security force detained two insurgents, including a Haqqani Network sub-commander operating in Khost \nprovince, Sunday. \\u2028\\u2028The commander coordinated and conducted attacks on coalition forces operating in the \nprovince and was formerly active in Kabul. \\u2028\\u2028Intelligence reports led the security force to a compound \nnorthwest of Khost City to search for the commander. Afghan forces called for all occupants to exit the buildings \npeacefully and then the combined force cleared and secured the compound. During the clearance, an armed individual \ncame out of an adjacent building toward the security force. The forced engaged the individual and killed him. \n\\u2028\\u2028After the area was secure, the security force questioned the residents at the scene and detained the \ncommander and one of his associates. The security force also found multiple automatic weapons, magazines and \ngrenades at the scene. \\u2028\\u2028The assault force protected the women and children throughout the search.',\n        start_date=datetime.date(2010, 9, 19),\n        event_type={'captureandkill'},\n        province={'khost'},\n        target_group={'haqqani'},\n        min_killed=1,\n        min_captured=2,\n        killq=True,\n        captureq=True,\n        killcaptureraid=True,\n        airstrike=False,\n        noshotsfired=False,\n        min_leaders_killed=0,\n        min_leaders_captured=0\n    )\n]\n\n\n\nSo this is data that we’ve already labelled. You can see the text that we’ll provide as input to our model, and then you can see the various fields that we’re hoping our model can learn to extract. As a JSON string, the prediction that we’re hoping our model will output would look like this:\n\njson_str = events[0].model_dump_json(exclude={\"text\"})\nprint(json_str)\n\n{\"name\":\"Several insurgents killed in \nHelmand\",\"start_date\":\"2011-02-18\",\"event_type\":[\"insurgentskilled\"],\"province\":[\"helmand\"],\"target_group\":[\"\"],\"mi\nn_killed\":6,\"min_captured\":0,\"killq\":true,\"captureq\":false,\"killcaptureraid\":false,\"airstrike\":false,\"noshotsfired\"\n:false,\"min_leaders_killed\":0,\"min_leaders_captured\":0}\n\n\n\nIf you wish to view more examples of the data, I created an interim dataset which I uploaded to the Hugging Face Hub, but it’s not completely in the required form for finetuning so I’ll just link to it here and you can explore it to see the pairings of input and output if you’re interested."
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#writing-alpaca-sharegpt-format-jsonl",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#writing-alpaca-sharegpt-format-jsonl",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Writing Alpaca / ShareGPT format JSONL",
    "text": "Writing Alpaca / ShareGPT format JSONL\nIn the course, Hamel shared an example from work he’d done with Honeycomb to output in a particular format. He shared his notebooks and I found this useful as an initial example to move forward with. He uses the ShareGPT template to format his data, and I followed that format for my data. This looks something like this:\n{\n    \"conversations\": [\n        {\n            \"from\": \"system\",\n            \"value\": \"Honeycomb is an observability platform that allows you to write queries to inspect trace data. You are an assistant that takes a natural language query (NLQ) and a list of valid columns and produce a Honeycomb query.\"\n        },\n        {\n            \"from\": \"human\",\n            \"value\": \"\\n\\nNLQ: \\\"group by HTTP method\\\"\\n\\nColumns: ['query_string_num_tokens', 'query_string_length', 'data_queries', 'http.target', 'task.id', 'trace_root.http.target', 'topic', 'http.host', 'total_hits', 'db.user', 'domain_types', 'db.name', 'graphql.document', 'history', 'http.scheme', 'http.method', 'frontend.version', 'disposition_for_dBVVysC8x4Ymwg9rtjMckgw9', 'db.system', 'event_name', 'organization', 'auth.logout', 'organizations', 'name', 'net.transport', 'db.operation', 'disposition_for_UvsPPBVUn9FDuzDjsjYCqopq', 'disposition_for_1RUGSd7GdnP5tuKdgqBRZUm2', 'process.pid', 'disposition_for_6uyAoBc3PuvEcTTPFgPM3Rtk', 'exception.stacktrace', 'data_ingestion_individuals_count', 'disposition_for_qrnUBUz8YBfNX7Liekq6nKi3', 'task_type.type', 'disposition_for_JQDNbuUdaQcEbEwQNxUbV5EF', 'disposition_for_rAcWoXfbHw4eWoJFH4ZcY8ue', 'disposition_for_eShqQoC9jUi9VQBidpp2oXHP', 'parent_name', 'template', 'graphql.operation.name', 'span.num_links', 'disposition_for_kNSPtvsCWkDoEyFP2QE6VPmQ', 'disposition_for_UUqf9L1qkFxDNEvcgsVMA2yy', 'disposition_for_vwbbN76HZ7uitLubvkUjPFQE', 'disposition_for_aAto1pGrdF5RunpSX8sY5hvn', 'disposition_for_UbKCMdnkPQ6TuHrfdBo5juZu', 'disposition_for_QfrvmoHxSgLPJXPKZCrZfGo8', 'disposition_for_NoKSSruBRCX6UG28PzmkybUd', 'disposition_for_UZAqvZ5XVBZjKKWuMeRkRayS', 'organization_token', 'duration_ms', 'trace.parent_id', 'db.statement', 'exception.message', 'error', 'service.name', 'http.status_code', 'http.route']\"\n        },\n        {\n            \"from\": \"gpt\",\n            \"value\": \"\\n{\\\"breakdowns\\\": [\\\"http.method\\\"], \\\"calculations\\\": [{\\\"op\\\": \\\"COUNT\\\"}], \\\"time_range\\\": 7200}\"\n        }\n    ]\n}\nYou can see that a single entry is part of a conversations key, and then you have a series of messages with system, human and gpt roles. I followed this closely for my data, but with a base model I’m pretty sure this isn’t strictly necessary. (The second option is what I’ll show a bit later).\nWriting the data is a case of wrangling the data so it fits the format above, and doing it separately for our train and test datasets. You’ll note that in the system call I stuff in the event types, provinces and target groups as a way to provide some context to the model. This was something that Hamel did in his example.\n\nimport os\nfrom datasets import load_dataset\nimport pandas as pd\nfrom rich import print\nfrom typing import List\nimport json\n\n# Load the dataset\ndataset = load_dataset(\"strickvl/isafpressreleases\")\ntrain_target_file_path = \"../data/sharegpt_isaf_press_releases_ft_train.jsonl\"\ntest_target_file_path = \"../data/sharegpt_isaf_press_releases_ft_test.jsonl\"\n\n\ndef write_data_to_jsonl(df: pd.DataFrame, target_file_path: str) -> None:\n    events: List[IsafEvent] = []\n\n    for i, row in list(df.iterrows()):\n        event_types = set(\n            eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n        )\n        provinces = set(\n            province.strip().lower() for province in row[\"province\"].split(\",\")\n        )\n        target_groups = set(\n            target_group.strip().lower()\n            for target_group in row[\"targetgroup\"].split(\",\")\n        )\n\n        events.append(\n            IsafEvent(\n                name=row[\"name\"],\n                text=row[\"text\"],\n                start_date=row[\"StartDate\"].to_pydatetime().date(),\n                event_type=event_types,\n                province=provinces,\n                target_group=target_groups,\n                min_killed=int(row[\"minkilled\"]),\n                min_captured=int(row[\"mincaptured\"]),\n                killq=row[\"killq\"] == \"true\",\n                captureq=row[\"captureq\"] == \"true\",\n                killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n                airstrike=row[\"airstrike\"] == \"true\",\n                noshotsfired=row[\"noshotsfired\"] == \"true\",\n                min_leaders_killed=int(row[\"minleaderskilled\"]),\n                min_leaders_captured=int(row[\"minleaderscaptured\"]),\n            )\n        )\n\n    processed_data = [\n        {\n            \"conversations\": [\n                {\n                    \"from\": \"system\",\n                    \"value\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\",\n                },\n                {\"from\": \"human\", \"value\": f\"PRESS RELEASE TEXT: {event.text}\"},\n                {\"from\": \"gpt\", \"value\": f\"{event.model_dump_json(exclude={'text'})}\"},\n            ]\n        }\n        for event in events\n    ]\n\n    # Write the processed data to a JSONL file\n    os.makedirs(os.path.dirname(target_file_path), exist_ok=True)\n    with open(target_file_path, \"w\") as f:\n        for item in processed_data:\n            f.write(json.dumps(item) + \"\\n\")\n\n\n# Write the data to disk\ntrain_df = pd.DataFrame(dataset[\"train\"])\ntest_df = pd.DataFrame(dataset[\"test\"])\n\nwrite_data_to_jsonl(train_df, train_target_file_path)\nwrite_data_to_jsonl(test_df, test_target_file_path)\n\nThe first line of the training file looks like this now:\n\nwith open(train_target_file_path, \"r\") as f:\n    print(f.readline())\n\n{\"conversations\": [{\"from\": \"system\", \"value\": \"You are an expert at identifying events in a press release. You are\nprecise and always make sure you are correct, drawing inference from the text of the press release. event_types = \n['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces =\n['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', \n'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', \n'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', \n'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', \n'judq', 'iju', 'hik', 'ttp', 'other']\"}, {\"from\": \"human\", \"value\": \"PRESS RELEASE TEXT: ISAF Joint Command Evening\nOperational Update Feb. 19, 2011\\nISAF Joint Command - Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \n\\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF service members at a compound in Sangin district, Helmand \nprovince observed numerous insurgents north and south of their position talking on radios today. After gaining \npositive identification of the insurgent positions, the coalition troops engaged, killing several insurgents. \nLater, the ISAF troops observed more insurgents positioning in the area with weapons. After positive \nidentification, coalition forces continued firing on the various insurgent positions, resulting in several more \ninsurgents being killed.\"}, {\"from\": \"gpt\", \"value\": \"{\\\"name\\\":\\\"Several insurgents killed in \nHelmand\\\",\\\"start_date\\\":\\\"2011-02-18\\\",\\\"event_type\\\":[\\\"insurgentskilled\\\"],\\\"province\\\":[\\\"helmand\\\"],\\\"target_g\nroup\\\":[\\\"\\\"],\\\"min_killed\\\":6,\\\"min_captured\\\":0,\\\"killq\\\":true,\\\"captureq\\\":false,\\\"killcaptureraid\\\":false,\\\"air\nstrike\\\":false,\\\"noshotsfired\\\":false,\\\"min_leaders_killed\\\":0,\\\"min_leaders_captured\\\":0}\"}]}"
  },
  {
    "objectID": "posts/2024-06-15-isafpr-first-finetune.html#writing-template-free-json",
    "href": "posts/2024-06-15-isafpr-first-finetune.html#writing-template-free-json",
    "title": "Finetuning my first LLM(s) for structured data extraction with axolotl",
    "section": "Writing Template-Free JSON",
    "text": "Writing Template-Free JSON\nAnother option available to us, especially if we are finetuning a base LLM (as opposed to one that has been instruction-tuned), is to write our data in a different format. Hamel’s written a guide for this on his blog and that has also been absorbed into the official axolotl documentation, so read the blog if you want more information.\nThe basic idea is that instead of following a format like the one above, we can essentially create our own that’s custom to our own needs. You want this kind of freedom because to follow one of the standard templates is sometimes to shoot yourself in the food with artifacts of those templates that you don’t need in your output.\nThe key is to specify train_on_inputs as false in our axolotl config which will allow us to mask certain segments of our input data. This means that our model won’t learn the inputs but only the outputs (which we’ll specify).\nAll that we have to do is set up the JSONL output in a way that makes sense for our use case:\n\ntemplate_free_train_target_file_path = (\n    \"../data/templatefree_isaf_press_releases_ft_train.jsonl\"\n)\ntemplate_free_test_target_file_path = (\n    \"../data/templatefree_isaf_press_releases_ft_test.jsonl\"\n)\n\n\ndef write_data_to_jsonl(df: pd.DataFrame, target_file_path: str) -> None:\n    events: List[IsafEvent] = []\n\n    for i, row in list(df.iterrows()):\n        event_types = set(\n            eventtype.strip().lower() for eventtype in row[\"eventtype\"].split(\",\")\n        )\n        provinces = set(\n            province.strip().lower() for province in row[\"province\"].split(\",\")\n        )\n        target_groups = set(\n            target_group.strip().lower()\n            for target_group in row[\"targetgroup\"].split(\",\")\n        )\n\n        events.append(\n            IsafEvent(\n                name=row[\"name\"],\n                text=row[\"text\"],\n                start_date=row[\"StartDate\"].to_pydatetime().date(),\n                event_type=event_types,\n                province=provinces,\n                target_group=target_groups,\n                min_killed=int(row[\"minkilled\"]),\n                min_captured=int(row[\"mincaptured\"]),\n                killq=row[\"killq\"] == \"true\",\n                captureq=row[\"captureq\"] == \"true\",\n                killcaptureraid=row[\"killcaptureraid\"] == \"true\",\n                airstrike=row[\"airstrike\"] == \"true\",\n                noshotsfired=row[\"noshotsfired\"] == \"true\",\n                min_leaders_killed=int(row[\"minleaderskilled\"]),\n                min_leaders_captured=int(row[\"minleaderscaptured\"]),\n            )\n        )\n\n    processed_data = [\n        {\n            \"segments\": [\n                {\n                    \"label\": False,\n                    \"text\": \"<s>You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\",\n                },\n                {\"label\": False, \"text\": f\"PRESS RELEASE TEXT: {event.text}\"},\n                {\n                    \"label\": True,\n                    \"text\": f\"{event.model_dump_json(exclude={'text'})}</s>\",\n                },\n            ]\n        }\n        for event in events\n    ]\n\n    # Write the processed data to a JSONL file\n    os.makedirs(os.path.dirname(target_file_path), exist_ok=True)\n    with open(target_file_path, \"w\") as f:\n        for item in processed_data:\n            f.write(json.dumps(item) + \"\\n\")\n\n\nwrite_data_to_jsonl(train_df, template_free_train_target_file_path)\nwrite_data_to_jsonl(test_df, template_free_test_target_file_path)\n\nAnd you can now see the difference in the format of the JSONL dataset we’ve constructured:\n\nwith open(template_free_train_target_file_path, \"r\") as f:\n    print(f.readline())\n\n{\"segments\": [{\"label\": false, \"text\": \"<s>You are an expert at identifying events in a press release. You are \nprecise and always make sure you are correct, drawing inference from the text of the press release. event_types = \n['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces =\n['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', \n'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', \n'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', \n'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', \n'judq', 'iju', 'hik', 'ttp', 'other']\"}, {\"label\": false, \"text\": \"PRESS RELEASE TEXT: ISAF Joint Command Evening \nOperational Update Feb. 19, 2011\\nISAF Joint Command - Afghanistan\\u20282011-02-S-143\\u2028For Immediate Release \n\\u2028\\u2028KABUL, Afghanistan (Feb. 19)\\u2028\\u2028ISAF service members at a compound in Sangin district, Helmand \nprovince observed numerous insurgents north and south of their position talking on radios today. After gaining \npositive identification of the insurgent positions, the coalition troops engaged, killing several insurgents. \nLater, the ISAF troops observed more insurgents positioning in the area with weapons. After positive \nidentification, coalition forces continued firing on the various insurgent positions, resulting in several more \ninsurgents being killed.\"}, {\"label\": true, \"text\": \"{\\\"name\\\":\\\"Several insurgents killed in \nHelmand\\\",\\\"start_date\\\":\\\"2011-02-18\\\",\\\"event_type\\\":[\\\"insurgentskilled\\\"],\\\"province\\\":[\\\"helmand\\\"],\\\"target_g\nroup\\\":[\\\"\\\"],\\\"min_killed\\\":6,\\\"min_captured\\\":0,\\\"killq\\\":true,\\\"captureq\\\":false,\\\"killcaptureraid\\\":false,\\\"air\nstrike\\\":false,\\\"noshotsfired\\\":false,\\\"min_leaders_killed\\\":0,\\\"min_leaders_captured\\\":0}</s>\"}]}"
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "",
    "text": "I’ve taken on some different responsibilities at work and part of this involves me diving into a repository filled with Terraform HCL code. I have some idea of how Terraform works, how the declarative paradigm is different and how things connect to one another, but getting up to speed and effective in a somewhat complex codebase is the goal here so I’ll need more than just high-level talking points.\nIn this post, I share my journey of understanding and working with an existing Terraform codebase, focusing on initial exploration, key concepts, and useful commands. I start with an introduction to basic Terraform commands like init, plan, apply, and destroy. Then, I dive into the structure of a Terraform codebase, discussing root modules, entry points, providers, variables, outputs, and data blocks. I also cover validation and formatting commands, as well as tools to help visualize and document your infrastructure. This guide is ideal for those who, like me, are new to Terraform or tasked with diving into a complex codebase, providing a roadmap for learning and making changes effectively."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#start-at-the-root-modules",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#start-at-the-root-modules",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🎬 Start at the root & Modules",
    "text": "🎬 Start at the root & Modules\nMost Terraform code is structured into modules, which are simply groups of multiple .tf and/or .tf.json files in the same directory. The root module is where you’ll want to start, as specified in the docs:\n\n“Every Terraform configuration has at least one module, known as its root module, which consists of the resources defined in the .tf files in the main working directory.”\n\nYou can have child modules within that root module (i.e. local files that define other separate modules) or you can load modules from a registry (like the Terraform Registry)."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-entrypoint",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-entrypoint",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🚪 Finding our entrypoint",
    "text": "🚪 Finding our entrypoint\nMost root modules will have a main.tf which we can think of as the main entrypoint for the logic of the code. (Child modules can and will likely have their own set of main.tf as well as variable definitions (see below).)\nI found it hard to reason about declarative code of the kind that you find in terraform codebases, because it’s not simply a question of procedurally running through a series of steps one by one. The work that terraform is actually doing is resolving all the interconnections, dependencies and diffs between the entirety of the code all at once, so it’s counterproductive to try to think of it as happening sequentially."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#providers",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#providers",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🏥 Providers",
    "text": "🏥 Providers\nYou’ll likely see some providers being defined in your main.tf file. These are the infrastructure providers that you’re interfacing with when you run apply for your terraform code. The specific logic and definitions for those providers are what gets loaded when you initialise your repository with terraform init. These will look something like this:\nprovider \"kubernetes\" {\n        ...details go here...\n}"
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#variables",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#variables",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "✏️ Variables",
    "text": "✏️ Variables\nInput variables are usually defined in a variables.tf file, which can specify their type, any tags needed to be applied as well as optional defaults. If you define them without any defaults, then running terraform apply will see you being queried about what the values for those variables should be.\nYou can pass them in via the command line, for example:\nterraform apply -var \"server_port=8080\"\nWith more than a few variables it’s unwise to have that be the way you pass those values in, so you can also optionally set those variables using a terraform.tfvars file. This file is where you’d set specific assignments and you can then pass that in via the command line explicitly or (if it’s named terraform.tfvars) these will just get automatically loaded and applied.\nterraform apply -var-file=\"testing.tfvars\"\nTo use these variables elsewhere in the code, simply reference the variable name after var.. For example, if your variable is called server_port as mentioned earlier, you would reference var.server_port when you wanted to access the value for that variable.\nYou can also specify and declare local variables (accessed through the local. prefix) by declaring a locals block within a specific file. For example:\nlocals {\n  service_name = \"forum\"\n  owner        = \"Community Team\"\n}"
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-outputs",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#finding-our-outputs",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📤 Finding our outputs",
    "text": "📤 Finding our outputs\nOutputs are values that will be printed in the console output at the end of whatever happens as part of terraform apply. They will also be accessible and available to you later on to query. For example, let’s say you spin up an instance of a Linux machine somewhere. You’ll want to have the IP address of that machine if you want to SSH into it, so you could set that as one of the outputs. Most likely your terraform codebase will have some outputs, usually stored in an outputs.tf file that is pretty straightforward to read."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#data-blocks",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#data-blocks",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "🧱 Data blocks",
    "text": "🧱 Data blocks\nData blocks are special kinds of resource that allow you to access (read-only) information from some source. It is a way to query a provider’s API for some data. Most providers expose various pieces of data, and these can range from simple things to IP address ranges or AMI IDs and so on. Even the current user’s identity can be accessed through data blocks.\nThese are defined inside a data block. For example, if you’re using the aws provider you could get the data for your default VPC with the following:\ndata \"aws_vpc\" \"default\" {\n    default = true\n}\nThen to use that data elsewhere, you can just reference it using the data. prefix."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#listing-state",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#listing-state",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📦 Listing State",
    "text": "📦 Listing State\nThe terraform state list command shows the resource addresses for every resource Terraform knows about in a configuration, optionally filtered by partial resource address.\nThese values are managed and stored in a terraform.tfstate file which is either stored locally or in a shared location (perhaps inside an S3 bucket that your team all have configured to connect to). I won’t get into all the details that surround how to handle state (shared or otherwise); for that, see chapter 3 of Brikman’s excellent Terraform Up & Running.\nThe CLI command is your way to access information about resources without needing to wade through the raw state file itself, and might be interesting to inspect at some point once you have something deployed."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#graphing-your-infrastructure",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#graphing-your-infrastructure",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📈 Graphing your infrastructure",
    "text": "📈 Graphing your infrastructure\nTo view a diagram of the infrastructure defined in your code, run the following:\nterraform graph -draw-cycles | dot -Tpng > graph.png\nYou’ll probably also need Graphviz to be installed for this to work.\nThis will create a PNG file which allows you to visualise the infrastructure defined in all your terraform code. For simple deployments this can be useful, but in my case this was less useful, as you can see from the following section of the image:\n\n\n\nPart of a terraform graph output\n\n\nAs with all these things, try it out and see if it works for you. You can also just export the data in other formats and manipulate them using your preferred graph data structure visualisation tools.\nYou can also use something like the terraform graph beautifier to make it more comprehensible."
  },
  {
    "objectID": "posts/2023-04-29-exploring-legacy-terraform-repository.html#auto-generated-deployment-documentation",
    "href": "posts/2023-04-29-exploring-legacy-terraform-repository.html#auto-generated-deployment-documentation",
    "title": "Terraform for the Uninitiated: Demystifying Your First Codebase",
    "section": "📄 Auto-generated deployment documentation",
    "text": "📄 Auto-generated deployment documentation\nIf you’re lucky and whoever wrote the terraform code has taken care to write descriptions for variables and the various modules being used, the next tool might prove quite useful. terraform-docs is a way to generate module documentation via a CLI command. Once you’ve installed it, running the following will give you a markdown file that you can preview to learn more about your codebase:\nterraform-docs markdown . --output-file docs.md\n\n\n\nAn example of output from terraform-docs\n\n\nThe file that gets output links to the code and underlying files, so it’s a handy way to sort through exactly what is defined from your root module. I’ve personally found this to be really useful and it’s even more so if the description parameters are defined."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html",
    "href": "posts/2023-06-22-input-variables-terraform.html",
    "title": "Terraform Input Variables",
    "section": "",
    "text": "When working with Terraform code, there are ways to take in user input at the time when you are applying whatever you’ve defined. To take a perhaps needlessly simple example, you might write a definition that allows you to deploy a new S3 bucket but you probably wouldn’t want to hardcode the name of the new bucket; instead, you’d rather take that name at the point of deployment. So if we think of the terraform process as a big function call, our input variables are the inputs we pass into this function application.\nThere are three main ways that you’ll use and encounter input variables in HCL code.\nThe first two are fairly commonly used, especially during development / testing, but are not really a great idea if your aim is a production-grade setup. Let’s walk through them one by one, but first let’s look at the variable block itself in HCL."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variable-blocks",
    "href": "posts/2023-06-22-input-variables-terraform.html#variable-blocks",
    "title": "Terraform Input Variables",
    "section": "Variable blocks",
    "text": "Variable blocks\nA variable block can look like this:\nvariable \"some_variable\" {\n  description = \"This is where you describe the variable\"\n  type        = string\n  default     = \"ginger_cat\"\n  nullable    = false\n  sensitive   = false\n}\nMost of this is self-explanatory. You can also specify a validation value in which you can determine the appropriate or accepted values for the variable. If you set sensitive to true, then terraform will (mostly) keep the value out of any output printed in the terminal. The name for the variable should be unique within the module and the value of this variable must be a literal value; it cannot be the result of an expression. If you don’t specify a default value then on running terraform apply or plan you will be asked what value you want to use.\n(There are also a limited handful of key words that are forbidden for use as names for the variable; you can look them up in the docs.)"
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-at-the-command-line",
    "href": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-at-the-command-line",
    "title": "Terraform Input Variables",
    "section": "Variables passed in at the command line",
    "text": "Variables passed in at the command line\nInstead of just waiting for the terraform CLI to ask you what values you want to assign to variables, you can pass them in at the command line. Your variables might live in a variables.tf file (or be located across your .tf files) but when you run terraform apply you can do so in the following way:\nterraform apply -var=\"some_variable=black_cat\"\nIf you have multiple variables (as you are likely to have) then you can use the -var option multiple times."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-with-files",
    "href": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-with-files",
    "title": "Terraform Input Variables",
    "section": "Variables passed in with files",
    "text": "Variables passed in with files\nAs you might imagine, this rapidly gets unwieldy so there is also a way to populate a whole file with the variable:value pairs. You can either explicitly specify a filename to use for those variables, or there are certain filenames that will be automatically be loaded in and parsed.\n# passing a file name explicitly\nterraform apply -var-file=\"my_vals.tfvars\"\nA file called terraform.tfvars, or terraform.tfvars.json, or any file ending in .auto.tfvars or auto.tfvars.json will all be loaded automatically without the need to explicitly pass them in at the command line."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-using-environment-variables",
    "href": "posts/2023-06-22-input-variables-terraform.html#variables-passed-in-using-environment-variables",
    "title": "Terraform Input Variables",
    "section": "Variables passed in using environment variables",
    "text": "Variables passed in using environment variables\nSome situations require different approaches than populating a file or passing them in via the command line so terraform also checks any environment variable with the TF_VAR_ prefix and imports those automatically. For example, we could run:\nexport TF_VAR_some_variable=white_cat\nAnd the some_variable variable would be populated with the white_cat value."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#using-variables",
    "href": "posts/2023-06-22-input-variables-terraform.html#using-variables",
    "title": "Terraform Input Variables",
    "section": "Using variables",
    "text": "Using variables\nYou can use variables (to get their values) by using the var. prefix. For example, if I was defining an AWS S3 bucket and I wanted to use the value of the some_variable value for that bucket name, I could do something like this in my HCL definition:\nresource \"aws_s3_bucket\" \"my_first_bucket\" {\n  bucket        = var.some_variable\n}"
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#local-variables",
    "href": "posts/2023-06-22-input-variables-terraform.html#local-variables",
    "title": "Terraform Input Variables",
    "section": "Local variables",
    "text": "Local variables\nSometimes you want to define local variables that don’t need exposing as configurable parameters (as the input variables described above are). We have local variables for this, and you define them in their own block:\nlocals {\n  my_cat = \"aria\"\n}\nThe useful thing about local variables is that you can assign expressions as the values for these variables, or you can combine or otherwise transform things.\nThese local variables are accessible elsewhere with the local. prefix (e.g. in the above example, as local.my_cat)\nYou might want to use local variables where you have some particular expression that’s used multiple times in your file so you want to keep it DRY so you use a local variable. Similarly, if something is likely to be changed, you might want to make the change only in one place instead of multiple places. These are only available for use within the module where it is declared and can’t be overridden from the outside."
  },
  {
    "objectID": "posts/2023-06-22-input-variables-terraform.html#variable-precedence",
    "href": "posts/2023-06-22-input-variables-terraform.html#variable-precedence",
    "title": "Terraform Input Variables",
    "section": "Variable Precedence",
    "text": "Variable Precedence\nWith all these different ways to pass in values for variables, how do you know which one gets chosen? There are precedence rules that determine which value is used. Variables are loaded in this order, and the last one seen is the one that is used:\n\nenvironment variables\nthe terraform.tfvars file\nthe terraform.tfvars.json file\nany ….auto.tfvars (or the .json equivalent) files\nany -var and/or -var-file values set via the command line\n\nNothing about using these variables is particularly difficult to understand, but it’s useful to have a sense of those precedence rules nonetheless, especially if you have the same variable defined in multiple places."
  },
  {
    "objectID": "posts/2023-05-21-balochi-language-modelling.html",
    "href": "posts/2023-05-21-balochi-language-modelling.html",
    "title": "Low-resource language models: making a start with Balochi",
    "section": "",
    "text": "Large Language Models are all the rage, but what do you do when the language you want to model is essentially unrepresented in the public datasets used for training? I have a few months before the start of my next maths module and I’d like to use the time in part to dive into the ins and outs of training your own language models from scratch.\nThe language I’m working with is Balochi, in particular the dialect or subset of Balochi that is spoken in southeastern Iran. The parent subgroup of ‘Balochi’ is spoken by 8-10 million people, but those break down into a few varieties which is in turn driven to a large extent by geography. The kind of Balochi used in Pakistan is subject to different linguistic influences than the one I’m interested in, for example.\nDespite the existence of millions of people speaking this language (family), it is more or less unrepresented in benchmarks and so-called breakthroughs in language modelling. Even the raw data to represent the language is absent. Common Crawl, one popular source of data for training language models, doesn’t even include Balochi as one of the languages represented in its corpus. Moreover, there’s no Balochi Wikipedia or anything really like it, so anyone hoping to work on language models in Balochi is first going to have to put together a corpus of data.\nThere’s nothing particularly novel about this problem. Languages with very few resources are known as low-resource languages and there’s a whole field of research (and some practice) busy trying to find ways to better serve these smaller language communities. I view the work as valuable, not only in that it seeks to preserve what might otherwise be lost, but also in terms of the disproportionately large (potential) impact it can have.\nI have personally experienced this issue at a distance, for the most part, having studied and worked with languages for which there are few study materials. The two languages I was passed down by my parents — English and, to a lesser extent, Dutch — are well-represented in the work and time spent by researchers and practitioners thus far. I have learned languages from neighbouring families, both geographically and linguistically, and always wanted to study Balochi myself. My hope is that it will be a gateway for me into the language and its community of speakers.\nI’m quite conscious of wanting to go about this project in a way that is ethical. Work of this kind is too often predicated on a principle of ‘take now, ask later’ so I’ll be writing more about this as I go as well as (hopefully) working with Balochi speakers and researchers to augment the work that is already being done. My initial survey of what has been done so far leads me to think that there is much remaining in the way of low-hanging fruit. I haven’t yet come across a Balochi tokenizer, for example, or embeddings or many other things that you would take for granted if you were working with the English language.\nMy somewhat distant goal — one for which I’m unsure how unrealistic I’m being — for all of this would be develop models and materials that can aid non-native speakers of Balochi to learn the language through what is known as comprehensible input. All of which is to say: I don’t know much Balochi as I start this work, but I hope to develop my fluency over time."
  },
  {
    "objectID": "posts/2022-03-21-docker-in-a-month.html",
    "href": "posts/2022-03-21-docker-in-a-month.html",
    "title": "Starting Docker In A Month Of Lunches",
    "section": "",
    "text": "As far as software engineers go, I’m still barely a spring chicken, six months into my job with ZenML. Working at a startup is fairly fast-paced and the ability to get going quickly with any number of libraries and tools is a requirement of the work. The number of things I could study, learn or practice is vastly larger than the amount of time I have. Accordingly, it helps to try to pick things that will be long-lasting, teach a cross-cutting skill or that are somehow fundamental.\nTwo closely-connected technologies that I’ve realised I can no longer avoid are Docker and Kubernetes. I have some high-level knowledge of both, having worked with Docker images on Ekko and having encountered Kubernetes in recent months, but it’s become clear in the last few weeks that they aren’t going away. More than that, it seems that not having a better (practical) grasp of some of the ins and outs of both is holding me back from grasping more complex decisions that are being made at work.\n[Side-note: I’m very curious about Podman as a Docker-adjacent alternative, but I need to understand Docker better first before I can make comparisons. I’d also note that I’m pretty sure that there are lots of cases where Kubernetes is overkill, and where it doesn’t make much sense to add all that complexity, particularly for smaller teams and projects. It’s nevertheless a feature of life in the MLOps space, it seems, so I must understand it.]\nI’ve had my eye on two Manning books by Elton Stoneman for a while, and now seems the perfect time to dive in. Learn Docker in a Month of Lunches and Learn Kubernetes in a Month of Lunches are very practical introductions to their subjects, come with good reviews and feedback and were published relatively recently. I’m especially happy that both books are extremely hands-on and even though I won’t in any way be an expert in either technology by the end, I’ll at least have some experience of having encountered the core use cases of both and maybe have a strong idea of what I do and don’t know.\nI’m not sure whether I’ll complete each one in exactly a month, but I’ll try to fast-track my reading. The chapters are written in such a way as to be digestible (including exercises) in around an hour. Stoneman says in the introduction to the Kubernetes book that it’s best to start with the Docker one, which I suppose makes sense given that one builds on the other.\nJust like my posts as I read through Robust Python (which I haven’t stopped doing), I’ll write up various things that I learn along the way, mainly as notes for myself but perhaps it will have value beyond this limited goal. So far I’ve read through the first three chapters of the Docker book, so what follows are some notes on the key points from that."
  },
  {
    "objectID": "posts/2022-03-21-docker-in-a-month.html#dockerfile-layout",
    "href": "posts/2022-03-21-docker-in-a-month.html#dockerfile-layout",
    "title": "Starting Docker In A Month Of Lunches",
    "section": "Dockerfile layout",
    "text": "Dockerfile layout\nDockerfiles seem to have some commonalities in terms of the structure:\n\nyou start with a base image on which you’re building. These seem usually or often to be a base image containing a runtime for whatever language your application uses\nThen there’s often environment variables afterwards\nThen you can specify a WORKDIR which is the working directory for the application\nThen you can COPY files from your local filesystem into that working directory\nThen at the end you specify which CMD needs to be run in order to execute the application.\n\nOnce you’re done with writing your Dockerfile, use the following command to build your image:\ndocker image build --tag SOMENAME .\nNote that final . trailing that command. The . states that the current directory is the ‘context’ and thus is used for when you’re copying in files using the COPY command.\nYou can view the layers of your Docker image with the docker image history IMAGENAME command (which will output them to the terminal).\nTo see how much disk space your containers and images are taking up, type docker system df.\nWhen you rebuild an image, you can specify this with a :v2 after the name, as in this command:\ndocker image build -t web-app:v2 .\nWhen it comes to optimising your Dockerfile, bear the following in mind:\n\n“Any Dockerfile you write should be optimised so that the instructions are ordered by how frequently they change — with instructions that are unlikely to change at the start of the Dockerfile, and instructions most likely to change at the end. The goal is for most builds to only need to execute the last instruction, using the cache for everything else. That saves time, disk space, and network bandwidth when you start sharing your images.” (pp. 42-43)\n\nSome command tips and tricks:\n\ncombine multiple commands onto the same line\nput the CMD instruction early on as it’s unlikely to change"
  },
  {
    "objectID": "posts/2022-01-22-robust-python-6.html",
    "href": "posts/2022-01-22-robust-python-6.html",
    "title": "Using mypy for Python type checking",
    "section": "",
    "text": "The final two chapters of part one of Patrick Viafore’s ‘Robust Python’ cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase.\nmypy is the most commonly used option for type checking in Python and it does most of what you probably need it for. You can run it via the command line, inline as part of your IDE, or as part of a CI/CD pipeline. At work we do all three.\nYou can configure mypy to your heart’s desire either with inline comments in your code, or via a configuration file. A configuration file is probably the way to go, particularly if you’re versioning your code and sharing these kinds of settings across a team.\nChapter 6 goes into detail about some of the specific options or settings you can tweak to make mypy more or less sensitive to certain kinds of errors. For example, in a previous post we mentioned how you can implicitly accept None as a type with the Optional type annotation wrapper. But maybe you don’t want to allow this behaviour because it’s generally not a great idea: if so, you can use the —strict-optional flag to get notified whenever you’re using that particular construction.\nmypy also allows for the export of its results to html and xml, and you can run it in the background as a daemon which (particularly for large code bases) might speed it up.\nWe also learn about some alternatives to mypy, namely Pyre and Pyright. Pyre runs as a daemon in the background and allows you to run queries relating to type usage in your codebase. It also includes a static code analyser called Pysa that runs a kind of security analysis on your code called ‘taint analysis’. A quick summary of this would be to say that you can specify specific kinds of security flaws that you want to address and/or prevent being part of your codebase.\nPyright is interesting since it has a useful VS Code integration (via the Pylance extension). You get all sorts of autocompletion and tooltip goodness by using Pyright/Pylance.\nFinally, chapter 7 thinks through how you might want to approach actually using type checking and type hints in a larger codebase, perhaps one that already exists. It’s useful this was included as I imagine these sorts of practicalities are much more of a blocker to adoption than any technical issues. After a brief discussion of tradeoffs, we learn about some different options for where you might want to start with introducing types to a legacy codebase.\n\nFocusing on the pain points — i.e. where the lack of type hints has already seen bugs emerge in the past\nor perhaps adding them to new code only\nor perhaps type annotating the pieces of the codebase that actually drive the product or business’ profits\nor maybe whatever is complex to understand\n\nAll of these are options and it will definitely depend on your particular situation.\nWe also learn about two tools that might help get you started with type annotation: MonkeyType and Pytype. Both auto-generate type hints for your codebase. MonkeyType does so dynamically, so it only generates type hints for parts of your code that it accesses while running the code. Pytype does so by static analysis. Both deliver some kind of output that you can then use (perhaps) as the basis of some annotations of your codebase. My instinct is that these two tools feel like they might lead to some faulty assumptions or errors if you rely on them too much and that in fact it would be better to just methodically go through your code and incrementally add type hints as suggested above.\nThis concludes the type hints part of the book. I feel like I really got a solid overview of why type hints are used in large or complex Python codebases as well as how to implement this practically. I will be writing separately about how we use mypy and type hinting at ZenML as I think it offers an interesting case study on some of the benefits and tradeoffs that we’ve observed on a day-to-day basis.\nNext up in Robust Python: defining your own types with Enums, data classes, classes and how this fits into libraries like Pydantic."
  },
  {
    "objectID": "posts/2021-11-29-prodigy-object-detection-training.html",
    "href": "posts/2021-11-29-prodigy-object-detection-training.html",
    "title": "How to annotate image data for object detection with Prodigy",
    "section": "",
    "text": "I’m back to working on the redaction model, though this time with a slightly more focused objective: object detection.\nObject detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify — for an arbitrary image — which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted.\nFor this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example.\nI showed in an earlier post how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn’t enough to offer a binary ‘yes’ or ‘no’ for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction.\nIn terms of the final output of the annotations, there are two main ways that this could go. I could either:\n\nget x and y coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point\nget the four coordinates for each of the corners of the bounding box.\n\nThe COCO dataset format will eventually want datasets in the second format, but Prodigy has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a custom recipe which will save the data in exactly the format that I want. For now, it’s good enough.\nInstalling Prodigy into your development environment is a breeze now that you can do it with pip:\npip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy # where the XXXs are your license code\nGetting going with the image training was as easy as the following CLI command:\nprodigy image.manual redaction-object-detection /path/to/image/data --label CONTENT,REDACTION --remove-base64\nNote that the --remove-base64 is to ensure that Prodigy doesn’t store the raw binary image data inside the database alongside the annotations. Prodigy (and their sister tool Spacy) is a little more focused on textual data, where storing the original data alongside the annotation doesn’t pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database.\nYou get a local URL to go visit and you see an interface where you can make the necessary annotations:\n\nYou can see that I am distinguishing between two different classes: redactions and content. Redactions are what we’ve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I’d want a final amount closer to 100% for that image rather than the 50% I’d get if I just went with the total percentage of redacted pixels on the whole image file.\nDoing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this:\n\nThe whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction?\nAnd for the following image, what is the right way to think about how to make the annotation?\n\nThis redaction encompasses multiple lines, so to some extent it doesn’t make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)?\n{% include info.html text=“As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important.” %}\nOnce we’re done with our annotations, we can easily export our data to a jsonl file with the following CLI command:\nprodigy db-out redaction-object-detection > ./redaction-object-detection-annotations.jsonl\nThis gives us a file containing all our annotations. A sample for one image gives the idea:\n{\n  \"image\": \"sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg\",\n  \"text\": \"04-F-0269_Global_Screening_Guidance-03\",\n  \"meta\": { \"file\": \"04-F-0269_Global_Screening_Guidance-03.jpg\" },\n  \"path\": \"sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg\",\n  \"_is_binary\": false,\n  \"_input_hash\": 1413334570,\n  \"_task_hash\": 1588323116,\n  \"_view_id\": \"image_manual\",\n  \"width\": 800,\n  \"height\": 1035,\n  \"spans\": [\n    {\n      \"id\": \"0ef6ccd0-4a79-471d-9aa1-9c903c83801e\",\n      \"label\": \"CONTENT\",\n      \"color\": \"yellow\",\n      \"x\": 76.5,\n      \"y\": 112.5,\n      \"height\": 786.1,\n      \"width\": 587.6,\n      \"center\": [370.3, 505.55],\n      \"type\": \"rect\",\n      \"points\": [\n        [76.5, 112.5],\n        [76.5, 898.6],\n        [664.1, 898.6],\n        [664.1, 112.5]\n      ]\n    },\n    {\n      \"id\": \"cd05d521-8efb-416b-87df-4624f16ca7f3\",\n      \"label\": \"REDACTION\",\n      \"color\": \"cyan\",\n      \"x\": 80.3,\n      \"y\": 786.2,\n      \"height\": 20.2,\n      \"width\": 428.4,\n      \"center\": [294.5, 796.3],\n      \"type\": \"rect\",\n      \"points\": [\n        [80.3, 786.2],\n        [80.3, 806.4],\n        [508.7, 806.4],\n        [508.7, 786.2]\n      ]\n    },\n    {\n      \"id\": \"3e268e33-4eba-457d-8d17-8271a79ee589\",\n      \"label\": \"REDACTION\",\n      \"color\": \"magenta\",\n      \"x\": 108.1,\n      \"y\": 772.3,\n      \"height\": 15.1,\n      \"width\": 400.6,\n      \"center\": [308.4, 779.85],\n      \"type\": \"rect\",\n      \"points\": [\n        [108.1, 772.3],\n        [108.1, 787.4],\n        [508.7, 787.4],\n        [508.7, 772.3]\n      ]\n    }\n  ],\n  \"answer\": \"accept\",\n  \"_timestamp\": 1638214078\n}\nEverything we’re interested in is inside the spans attribute, and it actually contains both kinds of the annotation that I mentioned above.\nAs you can see, annotating images in this way is fairly painless, and it brings you in closer contact with your raw data which is an added bonus."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "",
    "text": "What follows are my notes on chapter 9 of Chip Huyen’s ‘AI Engineering’ book. This chapter was on optimising your inference and I learned a lot while reading it! There are interesting techniques like prompt caching and architectural considerations that I was vaguely aware of but hadn’t fully appreciated how they might work in real inference systems."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#chapter-9-overview",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#chapter-9-overview",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Chapter 9: Overview",
    "text": "Chapter 9: Overview\nMachine learning inference optimization operates across three fundamental domains: model optimization, hardware optimization, and service optimization. While hardware optimization often requires significant investment and may offer limited individual leverage, model and service optimizations provide substantial opportunities for AI engineers to improve performance.\n\nCritical Cost Insight: A 2023 survey revealed that inference can account for up to 90% of machine learning costs in deployed AI systems, often exceeding training costs. This emphasizes why inference optimization isn’t just an engineering challenge - it’s a critical business necessity."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#core-concepts-and-bottlenecks",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#core-concepts-and-bottlenecks",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Core Concepts and Bottlenecks",
    "text": "Core Concepts and Bottlenecks\nUnderstanding inference bottlenecks is essential for effective optimization. Two primary types of computational bottlenecks impact inference performance:\n\nCompute-Bound Bottlenecks: Tasks that are limited by raw computational capacity, typically involving complex mathematical operations that take significant time to complete. These bottlenecks are particularly evident in computationally intensive operations within neural networks.\nMemory Bandwidth-Bound Bottlenecks: Limitations arising from data transfer requirements between system components, particularly between memory and processors. This becomes especially relevant in Large Language Models where significant amounts of data need to be moved between different memory hierarchies.\n\nIn Large Language Models (LLMs), different operations exhibit varying profiles of these bottlenecks. This understanding has led to architectural decisions such as decoupling the prefilling step from the decode step in production environments - a practice that has become increasingly common as organizations optimize their inference pipelines."
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-apis-and-service-patterns",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-apis-and-service-patterns",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Inference APIs and Service Patterns",
    "text": "Inference APIs and Service Patterns\nTwo fundamental approaches to inference deployment exist:\n\nOnline Inference APIs\n\nOptimized for minimal latency\nDesigned for real-time responses\nTypically more expensive per inference\nCritical for interactive applications\n\nBatch Inference APIs\n\nOptimized for cost efficiency\nCan tolerate longer processing times (potentially hours)\nAllows providers to optimize resource utilization\nIdeal for bulk processing tasks"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-performance-metrics",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#inference-performance-metrics",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Inference Performance Metrics",
    "text": "Inference Performance Metrics\nSeveral key metrics help quantify inference performance:\n\nLatency Components\n\nTime to First Token\n\nMeasures duration between query submission and initial response\nCritical for user experience in interactive applications\nOften a key optimization target for real-time systems\n\nTime per Output Token\n\nGeneration speed after the first token\nImpacts overall completion time\nCan vary based on model architecture and optimization\n\nInter-token Latency\n\nTime intervals between consecutive tokens\nAffects perceived smoothness of generation\nImportant for streaming applications\n\n\nTotal latency can be expressed as: time_to_first_token + (time_per_token × number_of_tokens)\n\n\nThroughput and Goodput Metrics\n\nThroughput: The number of output tokens per second an inference service can generate across all users and requests. This raw metric provides insight into system capacity.\n\n\nGoodput: The number of requests per second that successfully meet the Service Level Objective (SLO). This metric offers a more realistic view of useful system capacity.\n\n\n\nResource Utilization Metrics\n\nModel FLOPS Utilization (MFU)\n\nRatio of actual to theoretical FLOPS\nIndicates computational efficiency\nKey metric for hardware optimization\n\nModel Bandwidth Utilization (MBU)\n\nPercentage of achievable memory bandwidth utilized\nCritical for memory-intensive operations\nHelps identify memory bottlenecks"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#hardware-considerations-and-ai-accelerators",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#hardware-considerations-and-ai-accelerators",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Hardware Considerations and AI Accelerators",
    "text": "Hardware Considerations and AI Accelerators\nWhile NVIDIA GPUs dominate the market, various specialized chips exist for inference:\n\nPopular AI Accelerators\n\nNVIDIA GPUs (market leader)\nAMD accelerators\nGoogle TPUs\nVarious emerging specialized chips\n\n\nInference vs Training Hardware: Inference-optimized chips prioritize lower precision and faster memory access over large memory capacity, contrasting with training-focused hardware that requires substantial memory capacity.\n\nKey hardware optimization considerations include:\n\nMemory size and bandwidth requirements\nChip architecture specifics\nPower consumption profiles\nPhysical chip architecture variations\nCost-performance ratios"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#model-optimization-techniques",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#model-optimization-techniques",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Model Optimization Techniques",
    "text": "Model Optimization Techniques\n\n\nCore Approaches\n\nQuantization\n\nReduces numerical precision (e.g., 32-bit to 16-bit)\nDecreases memory footprint\nWeight-only quantization is particularly common\nCan halve model size with minimal performance impact\n\nPruning\n\nRemoves non-essential parameters\nPreserves core model behavior\nMultiple techniques available\nRequires careful validation\n\nDistillation\n\nCreates smaller, more efficient models\nMaintains key capabilities\nCovered extensively in Chapter 8\n\n\n\n\nAdvanced Decoding Strategies\n\n\nSpeculative Decoding\nThis approach combines a large model with a smaller, faster model:\n\nSmall model generates rapid initial outputs\nLarge model verifies and corrects as needed\nProvides faster token generation\nEasy to implement\nIntegrated into frameworks like VLLM and LamaCPU\n\n\n\nInference with Reference\n\n\nPerforms mini-RAG operations during decoding\nRetrieves relevant context from input query\nRequires additional memory overhead\nUseful for maintaining context accuracy\n\n\n\nParallel Decoding\nRather than strictly sequential token generation, this method:\n\nGenerates multiple tokens simultaneously\nUses resolution mechanisms to maintain coherence\nImplements look-ahead techniques\nAlgorithmically complex but offers significant speed benefits\nDemonstrated success with look-ahead decoding method\n\n\n\nAttention Optimization\nSeveral strategies exist for optimizing attention mechanisms:\n\nKey-Value Cache Optimization\n\nCritical for large context windows\nRequires substantial memory\nVarious techniques for size reduction\n\nSpecialized Attention Kernels\n\nFlash Attention as leading example\nHardware-specific implementations\nFlash Attention 3 for H100 GPUs"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#service-level-optimization",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#service-level-optimization",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Service-Level Optimization",
    "text": "Service-Level Optimization\n\nBatching Strategies\n\nStatic Batching\n\nProcesses fixed-size batches\nWaits for complete batch (e.g., 100 requests)\nSimple but potentially inefficient\n\nDynamic Batching\n\nUses time windows for batch formation\nProcesses incomplete batches after timeout\nBalances latency and throughput\n\nContinuous Batching\n\nReturns completed responses immediately\nDynamically manages resource utilization\nSimilar to a bus route that continuously picks up new passengers\nOptimizes occupation rate\nBased on Orca paper’s findings\n\n\n\n\nPrefill-Decode Decoupling\n\nSeparates prefill and decode operations\nEssential for large-scale inference providers\nAllows optimal resource allocation\nImproves overall system efficiency\n\n\n\nPrompt Caching\n\n\nStores computations for overlapping text segments\nOffered by providers like Gemini and Anthropic\nMay incur storage costs\nRequires careful cost-benefit analysis\nMust be explicitly enabled\n\n\n\nParallelism Strategies\n\nReplica Parallelism\n\nCreates multiple copies of the model\nDistributes requests across replicas\nSimplest form of parallelism\n\nTensor Parallelism\n\nSplits individual tensors across devices\nEnables processing of larger models\nRequires careful coordination\n\nPipeline Parallelism\n\nDivides model computation into stages\nAssigns stages to different devices\nOptimizes resource utilization\nReduces memory requirements\n\nContext Parallelism\n\nProcesses different parts of input context in parallel\nParticularly useful for long sequences\nCan significantly reduce latency\n\nSequence Parallelism\n\nProcesses multiple sequences simultaneously\nLeverages hardware-specific features\nRequires careful implementation"
  },
  {
    "objectID": "posts/2025-02-07-ai-engineering-chapter-9.html#implementation-considerations",
    "href": "posts/2025-02-07-ai-engineering-chapter-9.html#implementation-considerations",
    "title": "Notes on ‘AI Engineering’ chapter 9: Inference Optimisation",
    "section": "Implementation Considerations",
    "text": "Implementation Considerations\nWhen implementing inference optimizations:\n\nMultiple optimization techniques are typically combined in production\nHardware-specific optimizations require careful testing\nService-level optimizations often provide significant gains with minimal model modifications\nOptimization choices depend heavily on specific use cases and requirements"
  },
  {
    "objectID": "posts/2022-01-01-counter.html",
    "href": "posts/2022-01-01-counter.html",
    "title": "Counter: a shortcut to counting iterables in Python",
    "section": "",
    "text": "I came across this special dictionary type while reading an earlier chapter of ‘Robust Python’ the other day. It’s perhaps best illustrated with an example:\nfrom collections import Counter\nCounter([1,1,2,3])\n# returns Counter({1: 2, 2: 1, 3: 1})\n\nCounter('The Netherlands'.lower())\n# returns Counter({'e': 3, 't': 2, 'h': 2, 'n': 2, ' ': 1, 'r': 1, 'l': 1, 'a': 1, 'd': 1, 's': 1})\nI had no idea this existed, and of course usually default to some kind of a cookie-cutter loop when trying get counts of elements and put those counts into a dict.\nTo get the inividual elements, just call the elements method on the Counter object. To get the most common n elements, call the most_common(n) method. To get the total number of counts inside the dictionary, use the total method. To reset all the counts, use the clear method.\nJust a nice little set of functionality, hiding in plain sight inside the Python standard library.\nPhoto by Ibrahim Rifath on Unsplash"
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "",
    "text": "Code\n!pip install -Uqq fastbook nbdev torch\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.vision.all import *\nfrom fastbook import *\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nIn the previous post I showed a naive approach to calculating the similarity or difference between images, and how that could be used to create a function that did pretty well at estimating whether any particular image was a pullover or a dress.\nChapter 4 of the fastbook then takes us on a journey showing a smarter approach where the computer can make even better estimations and predictions. The broad strokes of this approach are simple to grasp, but of course the individual details are where the nuances of machine learning are to be found."
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html#setup-add-.requires_grad_-to-a-tensor",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html#setup-add-.requires_grad_-to-a-tensor",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "1. Setup: add .requires_grad_() to a tensor",
    "text": "1. Setup: add .requires_grad_() to a tensor\nFor any Tensor where we know we’re going to want to calculate the gradients of values, we call .require_grad() on that Tensor.\n\n# we define a simple function\ndef f(x):\n    return x**2\n\nx_tensor = torch.tensor(3.).requires_grad_()\n\ny_tensor = f(x_tensor)\n\ny_tensor\n\ntensor(9., grad_fn=<PowBackward0>)\n\n\nHere we can see that 3 squared is indeed 9, and we can see the grad_fn as part of the Tensor."
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html#use-.backward-to-calculate-the-gradient",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html#use-.backward-to-calculate-the-gradient",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "2. Use .backward() to calculate the gradient",
    "text": "2. Use .backward() to calculate the gradient\nThis actually refers to backpropagation, something which is explained much later in the book. This step is also known as the ‘backward pass’. Note, that this is again another piece of jargon that we just have to learn. In reality this method might as well have been called .calculate_gradients().\n\ny_tensor.backward()"
  },
  {
    "objectID": "posts/2022-05-12-seven-steps-gradient-calculations.html#access-the-gradient-via-the-.grad-attribute",
    "href": "posts/2022-05-12-seven-steps-gradient-calculations.html#access-the-gradient-via-the-.grad-attribute",
    "title": "Some foundations for machine learning with PyTorch",
    "section": "3. Access the gradient via the .grad attribute",
    "text": "3. Access the gradient via the .grad attribute\nWe view the gradient by checking this .grad attribute.\n\nx_tensor.grad\n\ntensor(6.)\n\n\nI can’t explain why this is the case, since I’ve never learned how to calculate gradients or derivatives (or anything about calculus, for that matter!) but in any case it’s not really important.\nNote that we can do this whole process over Tensors that are more complex than illustrated in the above simple example:\n\ncomplex_x = tensor([3., 5., 12.]).requires_grad_()\n\ndef f(x):\n    return (x**2).sum()\n\ncomplex_y = f(complex_x)\ncomplex_y.backward()\ncomplex_x.grad\n\ntensor([ 6., 10., 24.])\n\n\nSomething else I discovered while doing this was that gradients can only be calculated on floating point values, so this is why when we create x_tensor and complex_x we create them with floating point values (3. etc) instead of just integers. In reality, I think there will be some kind of normalisation of our values as part of the process, so they would probably already be floats, but it’s worth noting."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html",
    "href": "posts/2021-09-11-tfx-paper.html",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "",
    "text": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you’d need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It’s worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale)."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#a-tensorflow-based-general-purpose-machine-learning-platform",
    "href": "posts/2021-09-11-tfx-paper.html#a-tensorflow-based-general-purpose-machine-learning-platform",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "‘A TensorFlow-based general-purpose machine learning platform’",
    "text": "‘A TensorFlow-based general-purpose machine learning platform’\nThe engineers wanted a general-purpose tool, one that could serve many different use cases. I haven’t yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed.\nThe problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn’t anticipated, specifically sequence-to-sequence language models used in machine translation)."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#an-end-to-end-platform",
    "href": "posts/2021-09-11-tfx-paper.html#an-end-to-end-platform",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "An end-to-end platform",
    "text": "An end-to-end platform\nThe ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I’d say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‘best practices’, but certainly there hasn’t been uniform acceptance of these.\nI imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it’s easier to be fast and iterate and do all the things we expect of a more agile process."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#continuous-training-and-serving",
    "href": "posts/2021-09-11-tfx-paper.html#continuous-training-and-serving",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Continuous training and serving",
    "text": "Continuous training and serving\nTFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams.\nIn this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it’s a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#reliable-serving-models-at-scale",
    "href": "posts/2021-09-11-tfx-paper.html#reliable-serving-models-at-scale",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Reliable serving models at scale",
    "text": "Reliable serving models at scale\nIf you’re Google, you need to make sure that you aren’t serving garbage models to your users, or that inconsistencies in the input data aren’t polluting your retraining processes. At scale, even small mistakes compound really easily.\nIn the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn’t entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models — all of which had to happen in a reliable manner — was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#fast-retraining-with-warm-starting",
    "href": "posts/2021-09-11-tfx-paper.html#fast-retraining-with-warm-starting",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Fast retraining with ‘warm-starting’",
    "text": "Fast retraining with ‘warm-starting’\nFor the specific challenge of retraining models with streaming data, engineers were finding that they couldn’t retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‘warm-starting’) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me."
  },
  {
    "objectID": "posts/2021-09-11-tfx-paper.html#missing-pieces",
    "href": "posts/2021-09-11-tfx-paper.html#missing-pieces",
    "title": "Six problems TFX was trying to solve in 2017",
    "section": "Missing pieces",
    "text": "Missing pieces\nThere are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area.\nSimilarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you’d want to care about.\nAs a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you’re operating at serious scale. If you’re a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes.\nA couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I’d be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries.\nAgain on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually.\nI wasn’t active in the field in 2017, so it’s hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn’t seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google."
  },
  {
    "objectID": "posts/2021-12-29-j-language.html",
    "href": "posts/2021-12-29-j-language.html",
    "title": "Exploring J, an array programming language",
    "section": "",
    "text": "I’ve long wanted to explore the J programming language. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He’s since spoken about it in other places.\nIt is part of the family of programming languages that includes APL, K and Q. These can broadly be categorised as array-programming languages, where arrays are generally the core data structure and mental model to keep in mind. They used to be extremely popular in the 1970s and 1980s, particularly among institutions or businesses with a requirement for performant calculation / computation. One of these, Q, continues to live on (as a closed-source language) in the world of finance and trading. (Q is popular alongside the proprietary database kdb+).\nYou’re probably wondering why someone would want to use this fairly specialised and niche language. When you look at examples of J code — like the ones here, for example — it’s easy to simply dismiss it as an unreadable (‘write-only’) language. Indeed, many do dismiss it for this reason. Code is often compact, with single letters or symbols doing all the work. Defenders of J hold this up as a feature, not a problem. The compactness of the language means that you can fit the entirety of the solution (space) of a complex problem on a single screen, whereas in many (most?) other languages you would have to be scrolling up and down through dozens or even hundreds of lines of code.\nThe array languages seem to come at solving problems from a particular perspective. The symbols and letters that transform the arrays in J function as a pattern language. For a simple example, think of what you have to do when you want to find the count of a particular element from within an array/list. The array language paradigm argues that you don’t want to waste your time and screen space writing out boilerplate code to carry out this calculation, when it’s a common pattern that you can just use from the language itself. When problem-solving, therefore, spend your time thinking about the problem and not messing around with syntax or repeating yourself.\nJ and its cousins are extremely efficient. It is written in C, and I recently heard someone quote one of the early J pioneers as having said that “it is not theoretically possible to write J code that is more performant than C, but it often ends up being so”. For some math- or statistics-heavy domains (think the world of finance), it is extremely helpful to have this highly abstracted language that works performantly on large datasets. Moreover, it seems to be even more helpful when you have a hard problem to work on that isn’t fully understood.\nKenneth Iverson’s wrote a paper (“Notation as a Tool of Thought”) that is a classic in computer science and gets into some of the above arguments. (It is written using APL, but it also applies to J). I will probably return to that at a future date, because it often comes up and is recommended as a particularly rich document worth taking time to explore in depth.\nVery much as a project to indulge my curiosity, I will be exploring J over the coming months. I have been listening to the back catalogue of The Array Cast podcast, and I will be slowly working my way through some of the resources listed on the official J site. Let me know if you have experience working with J!"
  },
  {
    "objectID": "posts/2023-06-04-token-odds-and-sods.html",
    "href": "posts/2023-06-04-token-odds-and-sods.html",
    "title": "Tokenizer Links",
    "section": "",
    "text": "This is just a collection of various links and observations that I came across while learning about tokenisation during the past week that would otherwise have no other home."
  },
  {
    "objectID": "posts/2023-06-04-token-odds-and-sods.html#more-questions",
    "href": "posts/2023-06-04-token-odds-and-sods.html#more-questions",
    "title": "Tokenizer Links",
    "section": "More Questions",
    "text": "More Questions\nAnd some other questions (beyond my larger questions around how to evaluate tokenisers):\n\nHow useful (or not) is data augmentation when it comes to training a tokenizer?\nIs a list of dictionary words useful for training a tokenizer?"
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "",
    "text": "I’ve previously written about my use of fastai’s vision_learner to create a classification model that was pretty good (> 95% accuracy) at detecting whether an image contained redactions or not.\nThis week in the course we switched domains and got to know HuggingFace’s transformers library as a pathway into NLP (natural language processing) which is all about text inputs. I struggled quite a bit trying to think of interesting yet self-contained / small uses of NLP that I could try out this week. A lot of the common uses for simple NLP modelling seem to be in the area of things like ‘sentiment analysis’ where I couldn’t really see something I could build. Also there are a lot of NLP uses cases which feel unethical or creepy (perhaps more so than in the computer vision, it felt to me).\nI emerged at the end of this thought process with the idea to try to pit image classification and text classification against one another: could I train an NLP model that would outperform my image classifier in detecting whether a specific document or page contains a redaction or not?\nOf course, the first thing I had to do was to OCR all the pages in my image dataset and convert this all into a text dataset. When it comes to OCR tools, there are a number of different options available and I’d luckily experimented around with them. (A pretty useful overview of three leading options can be found in this blogpost by Francesco Pochetti.) I went with Tesseract as I knew had pretty good performance and accuracy for English-language documents.\nMy process for converting the documents wasn’t particularly inspired. Essentially I just loop over the image files one by one, run the OCR engine over them to extract the text and then create a new .txt file with the extracted text. At the end, I had two folders with my data, one containing texts whose corresponding images I knew had contained redactions, and one where there were no redactions.\nI had two hunches that I hoped would help my NLP model.\nWhat follows is my attempt to follow steps initially outlined in Jeremy Howard’s Kaggle notebook that the course reviewed this week in the live lesson. My code doesn’t depart from the original notebook much.\nI save my .txt files on the machine and I get a list of all the paths of those files.\nI iterate through all the paths, making of list of all the redacted texts as strings."
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html#converting-text-files-into-a-pandas-dataframe",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html#converting-text-files-into-a-pandas-dataframe",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "Converting text files into a Pandas DataFrame",
    "text": "Converting text files into a Pandas DataFrame\nI needed a way of obtaining the labels for my dataset. These labels were the parent label for each path name. The training process below needed the labels to be floats.\n\ndef is_redacted(path):\n    \"Extracts the label for a specific filepath\"\n    if str(path.parent).split(\"/\")[-1] == \"redacted\":\n        return float(1)\n    else:\n        return float(0)\n\nis_redacted(files[1])\n\n0.0\n\n\nConverting a Python dict into a Pandas DataFrame is pretty simple as long as you provide the data in the right formats. I had to play around with this a little when I was getting this to work.\n\ndata = {\n    \"input\": texts,\n    \"labels\": [is_redacted(path) for path in files],\n}\n\n\ndf = pd.DataFrame(columns=[\"input\", \"labels\"], data=data)\n# df\n\n\ndf.describe(include='object')\n\n\n\n\n\n  \n    \n      \n      input\n    \n  \n  \n    \n      count\n      3886\n    \n    \n      unique\n      3830\n    \n    \n      top\n      \n    \n    \n      freq\n      35\n    \n  \n\n\n\n\nWe now have a DataFrame containing 3886 rows of data. You can see here that 35 rows have no visible text. Potentially something went wrong with the OCR extraction, or the redaction covered the entire image. I didn’t really know or want to fiddle around with that too much, so I left those rows in."
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html#moving-into-hf-transformers-land",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html#moving-into-hf-transformers-land",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "Moving into HF Transformers Land",
    "text": "Moving into HF Transformers Land\nWe create a Dataset object from our DataFrame. It requires that our targets have the column name labels.\n\nfrom datasets import Dataset, DatasetDict\n\nds = Dataset.from_pandas(df)\n\n\nds\n\nDataset({\n    features: ['input', 'labels'],\n    num_rows: 3886\n})\n\n\nWe’re finetuning a pre-trained model here, so I start with the small version of Deberta which will allow me (I hope!) to iterate quickly and come up with an initial baseline and sense of whether this is even a viable approach to solving the problem.\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\nBefore we finetune our model, we have to do two things to our text data in order that it works within our gradient descent powered training process:\n\nwe have to tokenise our text data\nwe have to turn those tokens into numbers so they can be crunched within our GPU as numbers.\n\nTokenisation is the process of splitting our words into shorter stubs of text – there are varying schools of thought and use cases on the extent to which you break the words down. We have to use the same tokenisation process that was used by our pretrained model, so we let transformers grab the original tokenisers that was used with deberta-v3-small.\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n\n\n\ndef tok_func(x): return tokz(x[\"input\"])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\nWe split our data into training and validation subsets as per usual so that we know how our model is doing while training.\n\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 2914\n    })\n    test: Dataset({\n        features: ['input', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 972\n    })\n})\n\n\nWe define our metric as Pearson’s r AKA the Pearson correlation coefficient, a metric I don’t feel an immense instinctual understanding for, but suffice it for this blogpost to know that a higher value (up to a maximum of 1) is better.\n\ndef corr(x, y):\n    return np.corrcoef(x, y)[0][1]\n\n\ndef corr_d(eval_pred):\n    return {\"pearson\": corr(*eval_pred)}\n\n\nfrom transformers import TrainingArguments,Trainer\n\nHere we define our batch size, the number of epochs we want to train for as well as the learning rate. The defaults in Jeremy’s NLP notebook were far higher than what you see here. His batch size was 128. When I ran the cells that follow, I hit the infamous “CUDA out of memory” error more or less immediately. I was running on a machine with a 16GB RAM GPU, but this apparently wasn’t enough and the batch size was far too large. I had to reduce it down to 4, as you can see, in order to even be able to train the model. There are tradeoffs to this in terms of how well the model learns, but without spending lots of money on fancy machines this was the compromise I had to make.\n\nbs = 4\nepochs = 5\nlr = 1e-4\n\n\nargs = TrainingArguments(\n    \"outputs\",\n    learning_rate=lr,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    fp16=True,\n    evaluation_strategy=\"epoch\",\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=bs * 2,\n    num_train_epochs=epochs,\n    weight_decay=0.01,\n    report_to=\"none\",\n)\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_nm, num_labels=1\n)\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=dds[\"train\"],\n    eval_dataset=dds[\"test\"],\n    tokenizer=tokz,\n    compute_metrics=corr_d,\n)\n\nSome weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing amp half precision backend\n\n\n\ntrainer.train();\n\nThe following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n***** Running training *****\n  Num examples = 2914\n  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization steps = 3645\n\n\n\n\n    \n      \n      \n      [3645/3645 09:15, Epoch 5/5]\n    \n    \n  \n \n      Epoch\n      Training Loss\n      Validation Loss\n      Pearson\n    \n  \n  \n    \n      1\n      0.250100\n      0.168366\n      0.705429\n    \n    \n      2\n      0.171600\n      0.134761\n      0.748499\n    \n    \n      3\n      0.118200\n      0.114869\n      0.784274\n    \n    \n      4\n      0.089600\n      0.093946\n      0.818484\n    \n    \n      5\n      0.063100\n      0.091717\n      0.822977\n    \n  \n\n\n\nSaving model checkpoint to outputs/checkpoint-500\nConfiguration saved in outputs/checkpoint-500/config.json\nModel weights saved in outputs/checkpoint-500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-1000\nConfiguration saved in outputs/checkpoint-1000/config.json\nModel weights saved in outputs/checkpoint-1000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1000/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-1500\nConfiguration saved in outputs/checkpoint-1500/config.json\nModel weights saved in outputs/checkpoint-1500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-1500/special_tokens_map.json\nSaving model checkpoint to outputs/checkpoint-2000\nConfiguration saved in outputs/checkpoint-2000/config.json\nModel weights saved in outputs/checkpoint-2000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2000/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-2500\nConfiguration saved in outputs/checkpoint-2500/config.json\nModel weights saved in outputs/checkpoint-2500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-2500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\nSaving model checkpoint to outputs/checkpoint-3000\nConfiguration saved in outputs/checkpoint-3000/config.json\nModel weights saved in outputs/checkpoint-3000/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-3000/special_tokens_map.json\nSaving model checkpoint to outputs/checkpoint-3500\nConfiguration saved in outputs/checkpoint-3500/config.json\nModel weights saved in outputs/checkpoint-3500/pytorch_model.bin\ntokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json\nSpecial tokens file saved in outputs/checkpoint-3500/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 972\n  Batch size = 8\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nAt the end of all this, we have a Pearson’s score of 0.82 on our validation set which doesn’t seem to be as good as our image classifier. I’m not sure how I would go about comparing these two different metrics. I imagine I’d want to ensure that both my metrics were identical to make a like-for-like comparison.\nMy model is available on the Huggingface Hub here."
  },
  {
    "objectID": "posts/2022-05-21-nlp-redaction-classifier.html#what-did-i-learn",
    "href": "posts/2022-05-21-nlp-redaction-classifier.html#what-did-i-learn",
    "title": "Redaction Image Classifier: NLP Edition",
    "section": "What did I learn?",
    "text": "What did I learn?\n\nTraining NLP models feels like a bit of a different world from that of computer vision. There are different constraints in the process that I wasn’t previously aware of and working with the transformers library exposed me to a bunch of new errors and hoops I had to jump through.\nIt seems that the RAM needed on the GPU is directly correlated with the length of the text documents. Mine were on the long-ish end of the scale (particularly when compared with tweets which was what Jeremy was training on in his notebook). I wonder how people solve this problem, since mine by were by no means incredibly long.\nNLP models take longer to train than computer vision models; at least, the transformer-based models that I was working with.\nIt’s hard to compare two models together that don’t share the same metric or loss function.\nThere are MANY fiddly knobs to twist with NLP, particularly around the pre-processing of text samples, tokenisation strategies and so on. I wonder how much of those will be abstracted away from the high-level fastai abstraction when the library integrates with transformers in the coming months.\nThe end-to-end process is broadly the same, however, and it was good to have the foundation that we’ve been building up over the previous weeks in the course.\n\nThe next model I train hopefully will not be relating to redactions, I promise!\nUPDATE: I read a bit in the new O’Reilly book by the transformers team, Natural Language Processing with Transformers, which seems to address the issue of the same text size:\n\n“Transformer models have a maximum input sequence length that is referred to as the maximum context size. For applications using DistilBERT, the maximum context size is 512 tokens, which amounts to a few paragraphs of text. […] Texts that are longer than a model’s context size need to be truncated, which can lead to a loss in performance if the truncated text contains crucial information.” (pages 28-29 of the paperback edition)\n\nThe book suggests plotting out the number of tokens to get a sense of the distribution of the data by size:\n\nimport matplotlib.pyplot as plt\n\ndf[\"Tokens per document\"] = df[\"input\"].apply(lambda x: len(x.split()))\ndf.boxplot(\n    \"Tokens per document\",\n    by=\"labels\",\n    grid=False,\n    showfliers=False,\n)\nplt.suptitle(\"\")\nplt.xlabel(\"\")\nplt.show()\n\n\n\n\nHere we can see that we have a fairly wide distribution, with quite a few texts going all the way up to 800 tokens in length, so that is probably responsible for the large amounts of RAM needed, but perhaps the truncation of texts is also harming our performance.\nWhen I visit the deberta-v3-small model card on Huggingface, I also see reference to a maximum sequence length of 256 which would indeed harm my model and its ability to learn, I reckon."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nThe previous two posts in this series have made the case for why you might want to consider adding a Great Expectations step or stage to your computer vision project, particularly once it becomes something you’re going to want to iterate on a few times.\nThis post begins by showcasing how you can use Evidently’s open-source library to calculate and visualise comparisons between your data. I list some of the lighter alternatives to Great Expectations and Evidently, concluding with some thoughts on when you might use it as part of your computer vision pipeline."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#tldr-alternatives-for-data-validation-using-python",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#tldr-alternatives-for-data-validation-using-python",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "TL;DR: Alternatives for data validation using Python",
    "text": "TL;DR: Alternatives for data validation using Python\n\n🛠 Data validation tools come in many flavours, from full-featured libraries like Great Expectations down to the humble assert statement in Python.\n⚠️ The tool you choose should be appropriate to your particular use case and situation. You might not need or want to add a large dependency or take on extra code / project complexity, in which case there are alternative options available to you.\n⏰ You’ll also want to think about when you’re doing your validation. Two key moments stand out for machine learning projects: when you’re ingesting data prior to training or fine-tuning a model, and at the moment where you’re doing inference on a trained model.\n📃 For my project, I’m using a variety of tools as part of my process because I’ve found it gives me confidence in the predictions my model is making and it gives me freedom to experiment and iterate, without needing to also worry that I’m silently breaking something with downstream effects on my model performance."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-using-evidently-for-drift-detection",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-using-evidently-for-drift-detection",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "Alternatives: Using Evidently for drift detection",
    "text": "Alternatives: Using Evidently for drift detection\nI’ve previously written about why Evidently is a great tool to use for drift detection and data monitoring over on the ZenML blog. At its core, Evidently takes two chunks of data and compares them. The statistical comparisons going on under the hood are quite sophisticated, but as an interface to be used it is extremely trivial to get going.\nIn the case of my redaction project data, I did the work of transforming my annotation and image metadata into Pandas DataFrames for Great Expectations already, so using it with Evidently at this point is trivial:\nfrom evidently.dashboard import Dashboard\nfrom evidently.dashboard.tabs import DataDriftTab\nfrom evidently.pipeline.column_mapping import ColumnMapping\n\nreal_annotations = main_annotations_df[['area', 'category_name', 'top_left_x', 'top_left_y', 'width', 'height', 'orientation']]\neasy_synth_annotations = easy_synth_annotations_df[['area', 'category_name', 'top_left_x', 'top_left_y', 'width', 'height', 'orientation']]\nhard_synth_annotations = hard_synth_annotations_df[['area', 'category_name', 'top_left_x', 'top_left_y', 'width', 'height', 'orientation']]\n\ncolumn_mapping = ColumnMapping(\n    numerical_features=[\"area\", \"width\", \"height\", 'top_left_x', 'top_left_y'],\n    categorical_features=[\"category_name\", 'orientation'],\n)\n\ndrift_report = Dashboard(tabs=[DataDriftTab()])\ndrift_report.calculate(real_annotations, hard_synth_annotations, column_mapping=column_mapping)\ndrift_report.save(\"reports/my_report.html\")\nIn this code, I’m comparing between the real (i.e. manually annotated) annotations and the ‘hard’ synthetic annotations that I created (and blogged about recently). I choose the columns I care about, tell Evidently which columns are numerical vs categorical features and save the report. (I can also display the report directly within a Jupyter notebook.) When I open the report, I see this:\n\nYou can unfold the graphs to dive into the details for specific features, as in the following example where I take a look at the orientation of my annotations and see the difference between my manual annotations and the synthetically generated ‘hard’ batch:\n\nIt doesn’t surprise me too much that we have this disparity, since the only annotations that are portrait in the synthetically-generated set are those for the content box around the whole page. All the rest are landscape, and that’s by design. (Note: you can make the comparisons using different statistical tests depending on your use case. I’m told that the next Evidently release will increase the number of available options for this.)\nI can repeat the same test for the image DataFrame. I’ve included some metadata for each image such as how many annotations are associated with the image, or how many redaction vs content annotations are associated and so on. The code is basically the same, except now taking into account the different columns and their types:\n# comparing between real images and hard_synth images\n\ncolumn_mapping = ColumnMapping(\n    numerical_features=[\"area\", \"width\", \"height\", 'annotation_count', 'content_annotation_count', 'redaction_annotation_count', 'area', 'file_size_bytes'],\n    categorical_features=['orientation', 'format', 'mode'],\n)\n\ndrift_report = Dashboard(tabs=[DataDriftTab()])\ndrift_report.calculate(main_images, hard_synth_images, column_mapping=column_mapping)\ndrift_report.save(\"reports/my_report-real-vs-hard-images.html\")\nAnd we get this report:\n\nYou can immediately see how certain things like the number of annotations and the number of redactions in an image was a bit different when comparing the two. We also seem to have a far more even distribution of file sizes in the synthetically generated images and that makes sense since that was essentially randomly determined.\nNote that all the data that goes into making these reports can be accessed programatically as a Python object or JSON through Evidently’s Profile feature, which is probably what you’re going to want when assessing for drift as part of a continuous training / continuous deployment cycle.\nIf you change just a few things once more, you get a really useful data quality report showing distributions, correlations, and various other features of your data at a single glance:\n# profiling data quality\n\nfrom evidently.dashboard.tabs import DataQualityTab\n\nquality_dashboard = Dashboard(tabs=[DataQualityTab()])\nquality_dashboard.calculate(main_images, hard_synth_images, column_mapping=images_column_mapping)\nquality_dashboard.save(\"reports/quality-report.html\")\nYou can get an idea of the report that it produces in the following screen recording from my browser:\n\nAs a place to get started with understanding a dataset, this is a pretty nice visualisation and report to have in your toolkit, but even after immersion in your data it can be useful to take a step back with something like this data quality overview. For instance, it reveals quite clearly how the average number of annotations in my manually annotated dataset is quite a bit lower than that of my synthetically generated examples. Of course, that was by intention, but it is nice to see that confirmed in the data.\nOnce you have your model ready, there are other reports that Evidently offers which perhaps I’ll return to in a subsequent blogpost but for now I hope this has given you a flavour of the tool and how easy it is to get going with it.\n(As a side-note, Evidently’s community is friendly, welcoming and filled with interesting people thinking about these issues. I find it a welcome breath of fresh air when compared with some other tools’ forums or chat platforms, so it also has that going for it!)"
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-some-other-options",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#alternatives-some-other-options",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "Alternatives: some other options",
    "text": "Alternatives: some other options\nWith Evidently, we drifted a little into the ‘visualise your data’ territory which wasn’t really the point of this post, but you can see how they combined clear visualisation with the statistical validation working underneath to calculate whether data was drifting. The following are some other tools I’ve come across that might help you in validating data in a computer vision context. I haven’t found a use for them in my project, but it’s possible that they might gel with what you’re doing:\n\nTensorFlow Data Validation (TFDV) — This is a part of TensorFlow and tfx which uses schemas to validate your data. If you’re using TensorFlow, you might have heard of this and might even be using it already, but I don’t get the sense that this is often much recommended. I include it as it is a prominent option available to you.\nDeepchecks — Deepchecks is adjacent to what Great Expectations offers, albeit with an emphasis on the kinds of tests you might want to do for ML model training code. It has some features and documented use cases for computer vision (object detection and classification) but I haven’t used it myself. Feels like a tool worth keeping your eye on, however. (Works on Pandas dataframes and numpy arrays.)\npandera — This is a statistical tool for validating data inside dataframes, and it overlaps quite a bit in its functionality with Great Expectations, particularly with the hypothesis testing functionality. Worth checking out.\nCerberus — Offers a lightweight schema-based validation functionality for Python objects.\njsonschema — similar in approach to Cerberus, above, this is a lightweight way to test your JSON files based on how they conform to a defined schema. Useful in the case of annotations files, perhaps, if you really want something minimal.\nschema — More of the same: a Pythonic way to validate JSON or YAML files based on schema.\nassert — We shouldn’t forget the humble assert statement, which I have sprinkled in various places within my code where it makes sense to make sure that data flowing through conforms to whatever implicit or explicit contracts exist.\n\nI mention these various options not to suggest that you should use them all, but rather to state that you have options ranging the whole spectrum of complexity and dependency."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#when-to-do-data-validation-in-your-project",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#when-to-do-data-validation-in-your-project",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "When to do data validation in your project",
    "text": "When to do data validation in your project\nRegularly! I’ve written previously about how you can think about data validation as testing for your data. Just like many (most?) engineering teams run their tests every time you add a new commit to the codebase, it’s worth thinking of these kinds of tests as something that get run at any point where the underlying data gets updated.\nThere are three points where it might make sense to do some data validation:\n\nat the point of data ingestion\nat the point just prior to training a model, i.e. after your data has been split into training and validation sets\nat the point of inference (i.e. using the data being passed into the trained model)\n\n\nThe first (at data ingestion) is essential, especially if you have any kind of continuous training or continuous deployment loop going on. You don’t want to be training on data that clearly is unsuitable for training, or where the distribution has shifted so much that it’s going to cause hidden problems down the line.\nThe second (at training-validation split time) may or may not be important depending on your use case. For my redaction project I don’t think there is a great deal of benefit from this and so haven’t incorporated it as such.\nThe third (at inference time) is quite important to have, even though the behaviour when an anomaly is detected might be different from if you were to detect issues earlier on in the process. You might choose to just log the result of your validation check internally, or you could potentially also feed the result back to a user in the terms of some sort of warning (i.e. if the image that they were uploading was a very different kind of image from the data that had been used to train the model)."
  },
  {
    "objectID": "posts/2022-04-28-data-validation-great-expectations-part-3.html#what-im-using-for-my-redaction-project",
    "href": "posts/2022-04-28-data-validation-great-expectations-part-3.html#what-im-using-for-my-redaction-project",
    "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
    "section": "What I’m using for my redaction project",
    "text": "What I’m using for my redaction project\nI don’t have any general advice as to which tool you should use as part of your computer vision model training pipeline. It’s likely to be heavily context-dependent and will differ based on the particular use case or problem you’re trying to solve. For my own project, however, I can be more specific.\nI’m using plain assert statements liberally scattered through my code, in part leftover from when writing the code but also as a failsafe should strange or anomalous data make its way into my functions. I’m not sure if this is a best practice or not — I could imagine someone telling me that it’s not advised — but for now it’s helpful, especially as I continue to change things in the innards of various parts of the process.\nI’m using Great Expectations as a general-purpose validation tool to lay out my ‘expectations’ of my data in a assertive and declarative way, and even though it took a little longer to wrap my head round how it worked, I’m glad I made the effort as it seems really helpful.\nI’m using Evidently to do similar things as Great Expectations, but I find they have different strengths and benefits even as they serve the same purpose. Evidently is a bit more of a lighter piece in the process, I feel, and as such it’s a bit more flexible and you can iterate faster with it. I am not quite at the point where I’m serving my model to accept inference requests from outside, but Evidently will be part of that process when I do, for sure.\nFinally, FiftyOne is also somehow part of the validation process. (I’ve written about that previously.) Having visual tools that allow you to quickly test out a hypothesis or debug something unexpected in your training results is an essential part of the work of developing computer vision models.\nThis brings my short series on data validation for computer vision to a close. I’m fully conscious that I might have missed some obvious opportunities, tricks or workflows that may be widely used in this field, so I welcome any comments and feedback that you might have."
  },
  {
    "objectID": "posts/2022-02-27-python-parsers.html",
    "href": "posts/2022-02-27-python-parsers.html",
    "title": "Three Python Helpers for Parsing Inputs",
    "section": "",
    "text": "I continue to slowly work my way through the calmcode back catalogue. This week I learned about three tiny utility packages that make certain parsing tasks less painful.\nparse (introduced here) is a way of turning simple text patterns into restructured data. Take the following example as an illustration:\nfrom parse import parse\n\nurl = \"https://github.com/strickvl/some-repo/\"\n\nparse(\"https://github.com/{owner}/{repo}/\", url).named\n\n# returns {'owner': 'strickvl', 'repo': 'some-repo'}\nAs Vincent explains, it’s sort of the inverse or opposite operation to what happens with an f-string.\nFor URLs of various kinds that you want to decompose easily, yarl (introduced here) is a great way to approach that in Python.\nFor dates stored in some kind of a string format, you might want to try datefinder (introduced here), an elegant if not always perfect way for converting date strings into datetime.datetime objects."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "",
    "text": "I am excited to announce the release of a new dataset on the Hugging Face Hub: the Afghanwire Dataset. This dataset is a comprehensive collection of translated Afghan media articles from the period of May 2006 to September 2009, created by the Afghanwire media agency, which I co-founded together with Felix Kuehn.\nDuring the years that Afghanwire was active, our team of Afghan translators worked diligently to translate articles from Dari and Pashto media sources into English. The dataset includes translated newspaper and magazine articles, as well as summaries of radio and television content. As most of the original media from this period is no longer available online, and certainly not in English, this dataset represents the largest publicly available trove of translated Afghan media for the 2006-2009 period.\nThe primary motivation for releasing this dataset is to serve as a historical artefact, preserving the voices and perspectives of Afghan civil society during a critical period in the country’s history. By making these translated articles accessible to researchers, historians, and the general public, we aim to shift the focus from foreign powers and military forces to the diverse opinions and discussions within Afghan society."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#dataset-overview",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#dataset-overview",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nThe Afghanwire Dataset consists of 7,990 translated articles, covering a wide range of topics and sourced from media outlets across Afghanistan. The dataset is provided as a single large collection, with no predefined splits. Each article is accompanied by metadata such as the publication date, author (if available), translator, topic, and language.\nOne of the strengths of this dataset is its geographical diversity. Although the Afghanwire office was based in Kabul, efforts were made to obtain newspapers and magazines from the provinces to ensure a representative collection. This inclusivity is particularly valuable for regions like Dai Kundi province, whose media coverage might have otherwise been lost and that we sought to represent."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#potential-applications",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#potential-applications",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Potential Applications",
    "text": "Potential Applications\nWhile the primary purpose of the Afghanwire Dataset is to serve as a historical resource, it also presents opportunities for various Natural Language Processing (NLP) tasks. These include:\n\nNamed Entity Recognition (NER) for entities that may be underrepresented in standard or smaller models.\nSentiment analysis to gauge public opinion on various issues during the covered period.\nTopic modelling to identify the main themes and concerns in Afghan media discourse.\nComparative analysis with other datasets or media sources to identify unique perspectives or biases."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#accessing-the-dataset",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#accessing-the-dataset",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Accessing the Dataset",
    "text": "Accessing the Dataset\nThe Afghanwire Dataset is now available on the Hugging Face Hub under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. Researchers and interested parties can access the dataset here."
  },
  {
    "objectID": "posts/2024-04-01-publishing-afghanwire-dataset.html#next-steps",
    "href": "posts/2024-04-01-publishing-afghanwire-dataset.html#next-steps",
    "title": "Introducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009",
    "section": "Next Steps",
    "text": "Next Steps\nI’ve had these files sitting on my hard drive for over a decade now. The website is barely accessible via archive.org snapshots (see here, for example) but since the newsletter and most articles were behind a login screen (though no paywall 🤟) the only source of the data was a database dump we made before we switched off the servers. (As a side note, this was a raw MySQL dump compatible with v4.1.21 which is actually so old that you can’t find Docker images to support it. I was saved by this amazing repo from @andrebossi which gave me everything I needed to rescue the data and port it out into Parquet files.)\nI hope that researchers, historians and other interested parties find this a useful collection of the voices of Afghan civil society during a critical period in the country’s history. By making these translated articles accessible to a wider audience, as always I hope to foster a deeper understanding of the complex issues and diverse perspectives that shaped Afghanistan during the 2006-2009 period.\nI would like to express my gratitude to the dedicated team of translators at Afghanwire, particularly Hamid Stanikzai, Atif Mohammadzai, Abdul Hassib Rahimi, and Hamid Safi, for their tireless efforts in selecting and translating these articles. Their work has made this dataset possible and ensures that the voices of Afghan civil society will not be forgotten."
  },
  {
    "objectID": "posts/2022-03-12-fiftyone-computervision.html",
    "href": "posts/2022-03-12-fiftyone-computervision.html",
    "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nSo you’ve trained a computer vision model, but you think it could do better. What do you do next? This is a common scenario, especially for computer vision problems where fine-tuning someone else’s pre-trained model is a pretty normal initial step that gets taken. You emerge with a decent score on whatever metric you care about, but it also isn’t great.\nOne part of the solution is certainly ‘more data’. This approach was recently highlighted by Boris Dayma on Twitter:\n{% twitter https://twitter.com/borisdayma/status/1502317249423679495 %}\nIn my case, I currently have a little over 1200 images that have been annotated, but of those some 600 of them don’t contain any redactions at all (i.e. they just have content boxes). I did mention that I was using a similar approach early on, where I’d use the model to help pre-annotate images, but I haven’t been using that recently.\nI’m realising that more important than pure volume of data is to annotate types of images that are the hardest for the model to learn. So what I really want to know at this point is where I should place my focus when it comes to supplementing the training data. My images aren’t currently divided into separate classes, but I have a proxy (the filename) which will be really helpful once I’ve identified which types I need to supplement.\nWhen seeking to improve computer vision models with error analysis, some kind of visual inspection is essential. fastai had a number of utility methods that helped in the interpretation of where a model was underperforming, but for object detection I think you do need something that was built to purpose, where you can really dive into the specific ways each object was or wasn’t detected.\nEnter FiftyOne.\nFiftyOne is an open-source tool built specifically to support the curation and creation of datasets for computer vision models. It is almost two years old in its open-source incarnation, and (or but?) it feels very solid and robust in its implementation. Voxel51, the company behind it, has taken great pains to write excellent documentation and guides, and they have a supportive community behind the scenes, too."
  },
  {
    "objectID": "posts/2022-03-12-fiftyone-computervision.html#viewing-only-high-confidence-predictions",
    "href": "posts/2022-03-12-fiftyone-computervision.html#viewing-only-high-confidence-predictions",
    "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of",
    "section": "Viewing only high-confidence predictions",
    "text": "Viewing only high-confidence predictions\nNot all predictions are created equal, too, so it would be useful to view only those predictions where the confidence was higher than 75%. FiftyOne makes this kind of conditional view easy. You can do it in code, as in the following snippet, or you can do it via the GUI inside the app.\nfrom fiftyone import ViewField as F\n\n# Only contains detections with confidence >= 0.75\n# `dataset` is the FiftyOne core object that was created before\nhigh_conf_view = dataset.filter_labels(\"prediction\", F(\"confidence\") > 0.75)"
  },
  {
    "objectID": "posts/2022-03-12-fiftyone-computervision.html#patches-detailed-views-for-detected-objects",
    "href": "posts/2022-03-12-fiftyone-computervision.html#patches-detailed-views-for-detected-objects",
    "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of",
    "section": "‘Patches’: detailed views for detected objects",
    "text": "‘Patches’: detailed views for detected objects\nFor a more fine-grained understanding on the ways our model is predicting redactions, we can create what are called ‘patches’ to view and scroll through prediction-by-prediction.\n\nThis is an excellent way to view things through the eyes of your model. These are all the objects it considers to be redactions. We’ll get to finding the ones where it doesn’t do as well in a bit, but this view allows us to immerse ourselves in the reality of how our model is predicting redaction boxes. We can see that certain types of boxes are well-represented in our dataset: coloured or shaded rectangles in particular."
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "",
    "text": "Here are the final notes from ‘Prompt Engineering for LLMs’, a book I’ve been reading over the past few days (and enjoying!)."
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-10-evaluating-llm-applications",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-10-evaluating-llm-applications",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "Chapter 10: Evaluating LLM Applications",
    "text": "Chapter 10: Evaluating LLM Applications\nThe chapter begins with an interesting anecdote about GitHub Copilot - the first code written in their repository was the evaluation harness, highlighting the importance of testing in LLM applications. The authors, who worked on the project from its inception, emphasise this as a best practice.\n\nEvaluation Framework\nWhen evaluating LLM applications, three main aspects can be assessed:\n\nThe model itself - its capabilities and limitations\nIndividual interactions with the model (prompts and responses)\nThe integration of multiple interactions within the broader application\n\nAs a general rule of thumb, you should always track and record:\n\nLatency\nToken consumption statistics\nOverall system approach metrics\n\n\n\nOffline Evaluation\n\nExample Suites\nThe foundation of offline evaluation is creating example suites - collections of 10-20 (minimum) input-output pairs that serve as test cases. These should be accompanied by scripts that apply your application’s logic to each example and compare the results.\nExample sources come from three main areas:\n\nExisting examples from your project\nReal-time user data collection\nSynthetic creation\n\nWhen using synthetic data, it’s crucial to use different LLMs for creation versus application/judging to avoid potential biases.\n\n\nEvaluation Approaches\n\nGold Standard Matching\n\n\nCan be exact or partial matching\nParticularly effective for binary decisions or multi-label classification\nCan leverage “logical frogs” tricks from Chapter 7 to assess model confidence\nFree-form text requires more creative evaluation approaches\nTool-use scenarios may be easier to evaluate, especially in agent-driven applications\n\n\nFunctional Testing\n\n\nA step up from unit tests but not full end-to-end testing\nFocuses on testing specific system components\n\n\nLLM as Judge\n\n\nCurrently trendy but requires careful implementation\nShould include human verification loop, preferably multiple humans\nKey insight: Always frame the evaluation as if the LLM is grading someone else’s work, never its own\nRecommendations for quantitative measures:\n\nUse gradient and multi-aspect coverage (MA)\nImplement 1-5 scales with specific criteria\nPlace all instructions and criteria before the content to be evaluated\nBreak down “Goldilocks” questions (was it just right?) into separate questions about whether it was enough and whether it was too much\n\n\n\n\n\nOnline Evaluation\nThe chapter transitions to discussing why we need online testing despite having offline evaluation capabilities. While offline testing is safer and more scalable, real human interactions are unpredictable and require live testing.\nKey points about online evaluation:\n\nAB testing is the standard approach\nExisting solutions include Optimizely, VWO Consulting, and AB Tasty\nApplications need to support running in two modes (A and B)\nConsider rollout timing and users on older versions\n\nFive main metrics for online evaluation (from most to least straightforward):\n\nDirect feedback (user responses to suggestions)\nFunctional correctness\nUser acceptance (following suggestions)\nAchieved impact (user benefit)\nIncidental metrics (surrounding measurements)\n\nDirect feedback data is particularly valuable as it can later be used for model fine-tuning. It’s recommended to track more incidental metrics rather than fewer, both for quality indicators and investigating unexpected changes."
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-11-looking-ahead",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#chapter-11-looking-ahead",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "Chapter 11: Looking Ahead",
    "text": "Chapter 11: Looking Ahead\nThe final chapter covers several forward-looking topics:\n\nMultimodality in LLMs\nUser experience and interface considerations\nPublished artifacts from Anthropic\nRisks and rewards of custom interfaces\nTrends in model intelligence, cost, and speed\n\n\nBook-Level Conclusions\nTwo main lessons emerge from the book:\n\nLLMs as Text Completion Engines\n\nThey fundamentally mimic training data\nSuccess comes from aligning prompts with training data patterns\nParticularly relevant for completion models\n\nEmpathy with LLMs\n\n\nThink of them as mechanical friends with internet knowledge\nFive key insights:\n\nLLMs are easily distracted; keep prompts focused\nIf humans can’t understand the prompt, LLMs will struggle\nProvide clear instructions and examples\nInclude all necessary information (LLMs aren’t psychic)\nGive space for “thinking out loud” (chain of thought)"
  },
  {
    "objectID": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#personal-reflections",
    "href": "posts/2025-01-17-final-notes-on-prompt-engineering-for-llms.html#personal-reflections",
    "title": "Final notes on ‘Prompt Engineering for LLMs’",
    "section": "Personal Reflections",
    "text": "Personal Reflections\nThe book, while not revolutionary, provides valuable insights and is a recommended read at 250 pages. It can be completed in about 10-11 days. The heavy focus on completion models versus chat models is interesting, likely due to the authors’ experience with GitHub Copilot. While some points were novel, none were completely mind-blowing. The book’s emphasis on completion models versus chat models is both intriguing and occasionally confusing, though this perspective is understandable given the authors’ background with GitHub Copilot."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html",
    "href": "posts/2024-06-17-one-click-finetuning.html",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "",
    "text": "The last post in this series showed that finetuning an LLM needn’t be particularly difficult. I used axolotl to produce finetuned versions of Llama3, Mistral and TinyLlama models. During the course we were given a bunch of credits by various companies in the LLM and finetuning space. Among those were credits from some finetuning-as-a-service companies and I thought now might be a good time to try out these services now that I’ve done the process manually a few times.\nI picked three to try out: Predibase, OpenPipe and OpenAI. All were surprisingly similar in the approach they took. I’ll give a few details on the experience for each and how they compare to each other. With all the services, the process was roughly the same as when I did it manually:\nThe step I had the most trouble with was the custom data upload, since each provider wanted the data in a different format. Converting the data from the Pydantic models I had previously created was not a huge deal, but I wasn’t sure about the tradeoffs that I was making (or that were being made for me) by converting my data into these formats."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#predibase",
    "href": "posts/2024-06-17-one-click-finetuning.html#predibase",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "Predibase",
    "text": "Predibase\nI started with Predibase since I had enjoyed the talk Travis Addair had given during the course. Predibase is famous for their work on LORA adapters, particularly their demonstration of Lora Land where they gave some examples of how finetuned LORA models / adapters could outperform GPT-4.\nPredibase requires that the data you upload has certain column names depending on the task you select for the finetuning. At the moment they have instruction tuning and text completion as their two tasks, but it wasn’t clear to me which to select. (They also have a Colab notebook to help with constructing splits from your data.)\nOnce your data is ready and validated, you can select the model you want to finetune along with a few other hyperparameters. This is the full extent of what you can set from the web UI:\n\n\n\nScreenshot of Predibase website and the hyperparameters you can set\n\n\nThere’s also a helpful dataset preview pane to give a final sanity check for your data, to make sure that the inputs and outputs look what you’d expect:\n\n\n\nScreenshot of Predibase website and the dataset preview pane\n\n\nAs you’ll read in a little bit, this feature helps catch potentially costly errors before you start the finetuning process.\nOnce you click the button to start the training, there isn’t a great deal of information available to you beyond (eventually) a loss curve that you can see. I chose to finetune Qwen2 in Predibase and this took about 53 minutes using an A-100 GPU accelerator.\nOnce your model is ready, you can prompt the model in the UI, or using their REST API / Python SDK. They give code snippets prefilled with some dummy text that you can easily try out locally. Let’s show that here, but before you can run your inference query you have to first deploy the model. I hadn’t expected this extra step, and it takes a while to spin up since it’s deploying the adaptor along with the base model it was finetuned alongside. My Qwen2 model has a context window of 131072 tokens and supposedly would cost $3.90 per hour that it was up (as a dedicated deployment).\nLet’s show the results we got:\n\npr1 = \"\"\"2011-11-S-011 ISAF Joint Command - Afghanistan For Immediate Release\n      KABUL, Afghanistan (Nov. 7, 2011) — A combined Afghan and coalition\n      security force conducted an operation in search of a Haqqani facilitator\n      in Argo district, Badakshan province. The facilitator coordinates suicide\n      attacks with other insurgent leaders in the area. During the operation, a\n      local national male failed to comply with repeated verbal warnings and\n      displayed hostile intent toward the security force. The security force\n      engaged the individual, resulting in his death. The security force\n      confiscated a shotgun and intelligence linking the local national to the\n      Haqqani network. The security force also detained two suspected insurgents during the operation.\"\"\"\n\nprompt = f\"\"\"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other']\n\n### Instruction:\n\nPRESS RELEASE TEXT: '{pr1}'\n\n### Response:\n\"\"\"\n\n\nimport os\nfrom predibase import Predibase\n\npb = Predibase(api_token=os.getenv(\"PREDIBASE_API_KEY\"))\n# pb = Predibase(api_token=\"\")\n\nlorax_client = pb.deployments.client(\"isafpr\")\nprint(lorax_client.generate(prompt, max_new_tokens=100).generated_text)\n\nUnfortunately my Predibase model deployment was still ‘initializing’ after a couple of hours of spinning up. I didn’t want to leave that dedicated deployment up and running overnight, so I just deleted the deployment and I’ll try to get this going at a later date. So no inference sample to show you for this one. I’m very curious to see how Qwen2 did, though!"
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#openai",
    "href": "posts/2024-06-17-one-click-finetuning.html#openai",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "OpenAI",
    "text": "OpenAI\nI was actually surprised that this is even a thing that people do or that is offered by OpenAI. Currently you’re able to finetune three versions of GPT3.5 as well as babbage-002 and davinci-002. In the OpenAI presentation during the course they mentioned that they were working to make it possible to finetune GPT4 as well, but no timeline was given on this.\nSo why would someone want to finetune GPT3.5? I think there are some problems that are sufficiently complex or of a specific nature where the OpenAI GPT family shines where you might want to squeeze out a final last bit of performance and where the open LLMs just aren’t there yet.\nThe OpenAI models are sort of the antithesis of an ‘open’ model and nothing about the finetuning process lent itself to disabusing you of that idea. This was the UI to fill in in order to finetune a model and as you can see there aren’t really too many options available to you.\n\n\n\nOpenAI Finetuning UI\n\n\nSupposedly the data you upload (options for train as well as a separate test set here) will never be used by OpenAI to train their models but you have to just trust them on that front.\n \nAs with Predibase, during finetuning you don’t have access to any logs or even too much feedback during training. You get a loss curve and a few scraps of metadata and that’s it. The training took around 90 minutes to run and then you’re able to prompt the model to see how it works, using the standard OpenAI interface and methods you’re used to:\n\nfrom openai import OpenAI\nfrom rich import print\nimport json\nimport os\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\nresponse = client.chat.completions.create(\n    model=\"ft:gpt-3.5-turbo-SOME_EXTRA_STUFF_HERE_FOR_MY_MODEL\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": pr1\n        }\n    ],\n    temperature=0\n)\n\nprint(json.loads(response.choices[0].message.content))\n\n{\n    'name': '1',\n    'start_date': '2011-11-07',\n    'event_type': ['captureandkill'],\n    'province': ['badakhshan'],\n    'target_group': ['haqqani'],\n    'min_killed': 1,\n    'min_captured': 2,\n    'killq': True,\n    'captureq': True,\n    'killcaptureraid': True,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 0,\n    'min_leaders_captured': 0\n}\n\n\n\nThey also give you an interface to see the response of the base model side-by-side against the finetuned model:\n\n\n\nSide-by-side UI of base model and finetuned model inference\n\n\nAs you can see, it’s done pretty well! It stuck to the JSON structure, and the extracted metadata looks good. Of course, since this is a GPT3.5 model, there’s no way to now download this model and run it locally. You’re hostage to OpenAI, to being online, etc etc. Not a scenario I’d like to be in, so I don’t think I’ll pursue this much further and rather use my OpenAI credits for other purposes.\nAll that said, I do think there might be some scenarios where only the OpenAI models are reliable enough to use (be that in terms of accuracy or with sufficient guardrails) and there were people in the course who were in this boat."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#openpipe",
    "href": "posts/2024-06-17-one-click-finetuning.html#openpipe",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "OpenPipe",
    "text": "OpenPipe\nThis was the last one-click provider I tried. As with the others, you upload your data first. When I tried this, I got a fairly opaque error message but I guess the format I’d used was incompatible. OpenPipe uses the same format as OpenAI does, it turns out, but it handles the train/test split itself so you just have to set your data up in a single file (unlike with OpenAI where they can take two separate files).\nThe interface for finetuning your model was somehow the most threadbare of all:\n\n\n\nOpenPipe finetuning UI\n\n\nMoreover, the selection of base models on which to finetune were also pretty slim: Llama3, Mistral, Mixtral and two OpenAI GPT3.5 models. I was surprised by the estimate of how much it’d cost to finetune the model (around $30 USD) but by limiting the number of options available to the user the path forward really was pretty easy.\nYou get no single morsel of information during the finetuning process and for me it took a while for the job to even start working, but after an hour or two (I can’t be sure as I left my desk) you get a model out the other end. At this point you can export the weights or just try out the model with a Python call.\nHelpfully, the web UI gives you code snippets you can use for Python, Javascript and cURL, and the snippets even have your prompt pre-filled with an example from your dataset. This was a nice touch.\n\n# pip install openpipe\n\nfrom openpipe import OpenAI\nfrom rich import print\nimport json\nimport os\n\nclient = OpenAI(\n  openpipe={\"api_key\": os.getenv(\"OPENPIPE_API_KEY\")}\n)\n\ncompletion = client.chat.completions.create(\n    model=\"openpipe:MY_MODEL_ID_WAS_HERE\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert at identifying events in a press release. You are precise and always make sure you are correct, drawing inference from the text of the press release. event_types = ['airstrike', 'detention', 'captureandkill', 'insurgentskilled', 'exchangeoffire', 'civiliancasualty'], provinces = ['badakhshan', 'badghis', 'baghlan', 'balkh', 'bamyan', 'day_kundi', 'farah', 'faryab', 'ghazni', 'ghor', 'helmand', 'herat', 'jowzjan', 'kabul', 'kandahar', 'kapisa', 'khost', 'kunar', 'kunduz', 'laghman', 'logar', 'nangarhar', 'nimroz', 'nuristan', 'paktya', 'paktika', 'panjshir', 'parwan', 'samangan', 'sar_e_pul', 'takhar', 'uruzgan', 'wardak', 'zabul'], target_groups = ['taliban', 'haqqani', 'criminals', 'aq', 'hig', 'let', 'imu', 'judq', 'iju', 'hik', 'ttp', 'other'].\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": pr1\n        }\n    ],\n    temperature=0,\n    openpipe={\n        \"tags\": {\n            \"prompt_id\": \"counting\",\n            \"any_key\": \"any_value\"\n        }\n    },\n)\n\nprint(json.loads(completion.choices[0].message.content))\n\n{\n    'name': '3 killed and 2 captured in Badakhshan',\n    'start_date': '2011-11-07',\n    'event_type': ['captureandkill'],\n    'province': ['badakhshan'],\n    'target_group': ['haqqani'],\n    'min_killed': 3,\n    'min_captured': 2,\n    'killq': True,\n    'captureq': True,\n    'killcaptureraid': True,\n    'airstrike': False,\n    'noshotsfired': False,\n    'min_leaders_killed': 0,\n    'min_leaders_captured': 0\n}\n\n\n\nAgain you can see we have a really nice result here: JSON is good and the content is solid too. This was a finetune of Llama3 so clearly the problem I noted in the previous blog post was a problem with how I’d set up my local finetune and not with Llama3 itself.\nI liked how OpenPipe automatically deployed my model for me once the finetune was complete. Moreover, there was no extra cost associated with this. (Since their base models are limited, I assume this means that they have lost of customers’ LORA adapters all connected to these base models and that’s how they’re able to keep all these deployments up and cost-effective.)\nThere was one final trick that OpenPipe had up its sleeve: an ‘evals’ interface. The interface is pretty simple again, but the gist is that you get to select OpenAI models to compare your finetune against a test dataset and get a comparison. You can select multiple models to run at the same time and the cost is pretty reasonable.\n\n\n\nOpenpipe eval UI\n\n\nThe evaluation is parallelised and you get a nice table with the aggregate results:\n\n\n\nOpenpipe eval results\n\n\nYou also (in the datasets tab) get a table with the individual responses for the test data:\n\n\n\nOpenpipe eval datasets\n\n\nLooking at the results you quickly become aware that this specific evaluation didn’t really make much sense. Comparing the same prompt between the finetuned model and GPT4 could never have been fair since my prompt never asks for the result back in a certain format, or that it should be JSON and so on.\nMoreover, you can see that the evaluation prompt itself doesn’t do a good job of picking up that the finetuned model really did a great job on the whole and so the aggregate comparison scores don’t really make much sense here.\nThat said, I found this feature a useful ‘nice-to-have’ and I can see how someone might find this helpful if they either wanted to run a quick experiment or weren’t particularly technically savvy."
  },
  {
    "objectID": "posts/2024-06-17-one-click-finetuning.html#final-thoughts",
    "href": "posts/2024-06-17-one-click-finetuning.html#final-thoughts",
    "title": "One-click LLM finetuning with Predibase, OpenPipe and OpenAI",
    "section": "Final thoughts",
    "text": "Final thoughts\nOverall I found this an interesting experience to do these finetunes in parallel. I suspect that I am not the core audience / market for these services. I was surprised how little customisation they offered, and I actually wonder who is using them. They were easy to use, however, and they do potentially open up the possibility for someone less technical to do something somewhat advanced with LLMs that they wouldn’t otherwise be able to do.\nThe moment you want to do something slightly custom, with your prompt template or with the architecture or try something new and cutting-edge, then immediately these services aren’t for you. Similarly, even though I think all of the services offer a Python SDK to replicate what I did in the web UI, I think you essentially have the same limited options available to you if you wanted to trigger these jobs programatically as part of a larger pipeline.\nFor the most part you never had the feeling that you were part of a wider ecosystem of these open models, with new techniques coming out all the time and new models as well. These are some of the things I missed from the experience, but as I mentioned before, I’m not the core audience here.\nI do appreciate the opportunity to try these out a few times and the companies for providing credits to do some meaningful attempts at doing something useful. I’ll try these a bit further down the road again and report back if my impression changes or if/when new features are added."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html",
    "title": "Starting the Hugging Face Agents course",
    "section": "",
    "text": "I finished the first unit of the Hugging Face Agents course, at least the reading part. I still want to play around with the code a bit more, since I imagine we’ll be doing that more going forward. In the meanwhile I wanted to write up some reflections on the course materials from unit one, in no particular order…"
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#code-agents-prominence",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#code-agents-prominence",
    "title": "Starting the Hugging Face Agents course",
    "section": "Code agents’ prominence",
    "text": "Code agents’ prominence\nThe course materials and smolagents in general places special emphasis on code agents, citing multiple research papers and they seem to make some solid arguments for it but it also seems pretty risk at the same time. Having code agents instead of pre-defined tool use is good because:\n\nComposability: could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?\nObject management: how do you store the output of an action like generate_image in JSON?\nGenerality: code is built to express simply anything you can have a computer do.\nRepresentation in LLM training data: plenty of quality code actions is already included in LLMs’ training data which means they’re already trained for this!\n\nThe thing that gives me pause is that it seems like we moved through the spectrum from highly structured and known workflows (a chain, perhaps, or even something like a DAG) to tool use in a loop (which had some arbitrary or dynamic parts but ultimately was at least a little defined), and all the way out then to code agents where basically anything is possible.\nIf I think about this as an engineer tasked with building a robust, dependable and reliable system, then the last thing I think I want to add into the system is an agent that can basically do any thing under the sun (i.e. code agents). Perhaps I’m misrepresenting the position here of code agents, so I’m looking forward to reading the papers cited above as well as understanding it more from the course authors’ perspective."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#evals-testing",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#evals-testing",
    "title": "Starting the Hugging Face Agents course",
    "section": "Evals & testing",
    "text": "Evals & testing\nFollowing on to my confusion around code agents, I’m very curious how the course will recommend one tests and evaluates these arbitrary code agents. Things I could imagine:\n\ntesting out the specific scenarios that your application or use case requires (i.e. end to end)\ntesting out each component of the system, such as you can break it down into smaller sub-components\nincluding things like linting / unit tests maybe once code is generated by the agent (?) i.e. real-time evaluation of the robustness of the system?\nprobably LLM as a judge somewhere in the mix, though that opens up its own can of worms…\n\nI do hope they talk about that in the later units of the course."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#general-patterns",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#general-patterns",
    "title": "Starting the Hugging Face Agents course",
    "section": "General patterns",
    "text": "General patterns\nThe core loop that came up in unit 1 was:\n\nplan -> act -> feedback/reflection\n\nAnd all of that gets packaged up in a loop and repeated in various forms depending on exactly how you’re using it. And this pattern is related to the ReACT loop which lots of people cite but seems to be a specific version of the general idea mentioned above.\nAnd the fact that all of this works is somehow all powered by the very useful enablement of tool use, which is itself powered by the fact that the model providers finetuned this ability into the models. Crazy, brittle, impressive and many other words for the fact that this ‘hack’ has such power."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#chat-templates",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#chat-templates",
    "title": "Starting the Hugging Face Agents course",
    "section": "Chat templates",
    "text": "Chat templates\nI liked how the unit really impresses on you the impact and importance of chat templates as the real way that LLMs are implemented. You may pass in your requests through a handy Python SDK, passing your tools as a list of function definitions, but in the end this is all being parsed down and out into very precise syntax with many tokens not intended for human consumption."
  },
  {
    "objectID": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#points-of-leverage",
    "href": "posts/2025-02-11-starting-the-hugging-face-agents-course.html#points-of-leverage",
    "title": "Starting the Hugging Face Agents course",
    "section": "Points of leverage",
    "text": "Points of leverage\nAt the end of the unit, I was thinking about all the places where an engineer has leverage over agents. What I could initially think of was:\n\nthe variety and usefulness of tools that you provide to your agent (or perhaps the extent to which you allow your code agent to ‘write’ things out into the world)\nthe discrimination in the volume or choice of a combination of tools or APIs\nhow you chain everything together\n(how robustly you handle failure)\n\nBeyond that there are quite a few things that are somewhat out of your hands unless you decide to custom finetune your own models for a specific use case.\nOverall it was a good start to the course: made me think and also got my hands dirty working on a very simple agent with tools using smolagent and a Gradio demo app in the Hugging Face Hub. I’ll write more after unit two next week."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html",
    "href": "posts/2022-04-06-synthetic-data-results.html",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "",
    "text": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)\nA clean and focused dataset is probably at the top of the list of things that would be nice to have when starting to tackle a machine learning problem. For object detection, there are some useful starting points, but for many use cases you’re probably going to have to start from scratch. This is what I’ve been doing for the past few months: working to bootstrap my way into a dataset that allows me to get decent performance training a model that can recognise redactions made on documents.\nAs part of that journey so far, some of the big things that I’ve taken time to do include:\nAt the end of my synthetic data creation blogpost, I mentioned that the next step would be to test the effect of adding in the new synthetic examples. Well… the results are in!"
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#a-failed-attempt-to-train-with-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#a-failed-attempt-to-train-with-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "A failed attempt to train with synthetic data",
    "text": "A failed attempt to train with synthetic data\nI wasn’t sure exactly how much synthetic data would be appropriate or performant to use, so created a loose experiment where I started with 20% of the total images and increasing up until I reached 50%. (I figured that more than 50% synthetic data probably wasn’t a great idea and would probably not help my model perform out in the real world.)\n\nAs you can see above: my initial experiment did not show great results. In fact, in several places, if I added synthetic data my model actually performed worse. This was a strong repudiation of my intuition of what would happen. After all, the whole point of adding the synthetic data was to get the model more of a chance to learn / train and thus improve its ability to recognise redaction object in documents.\nI dug into the data that I’d generated and the data I’d been using to train, and discovered a nasty bug which was tanking the performance. A week of debugging mislabelled bounding boxes in evenings after work and I was back with results that finally made sense."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#performance-boosts-after-adding-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#performance-boosts-after-adding-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "Performance boosts after adding synthetic data",
    "text": "Performance boosts after adding synthetic data\n\nIn this chart, at the bottom you can see how training the model without the synthetic data (no-synthetic-batch16) performed. Ok, not great. Then the next best performing (combined-75real-25synthetic-randomsplit)was when 25% of the total number of images was synthetic, and the rest were real manually annotated images. At the top, with around an 81% COCO score, was the model where I used 50% synthetic and 50% real images. This seemed to fit what my intuition said would happen.\nMore synthetic data helped. I guessed that if I had millions of labelled images then the synthetic data would perhaps have been less useful, but starting from scratch it was really supporting the process.\nI was curious what would happen when I returned to FiftyOne to carry out some error analysis on the new model’s performance. Even before I had reached those results, I had a hunch that the synthetic images I’d created were perhaps too generic. I think they probably were helping boost some baseline performance of my model, but I knew they weren’t helping with the hard parts of detecting redactions."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#hard-examples-creating-targeted-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#hard-examples-creating-targeted-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "‘Hard examples’: creating targeted synthetic data",
    "text": "‘Hard examples’: creating targeted synthetic data\nAs a reminder, this is the kind of image that is ‘hard’ for my model (or even a human) to be able to identify all the redactions:\n\nThe FiftyOne visualisations of what was and wasn’t working validated my hunch: yes, synthetic data helped somewhat, but the model’s low performance seemed much more vulnerable to misrecognition of the hard examples. Even with a 50/50 split between synthetic data and real manually annotated data, the hard examples were still hard! (And the converse was also true: the model was already pretty good at identifying ‘easy’ redactions (e.g. of the black box type).\nIf we look back at the example of a ‘hard’ redaction above, two things stood out:\n\nThey’re hard, even for a human! This was borne out in the way I needed to take special care not to forget or mislabel when I was adding manual annotations.\nThere are lots of redactions on a single page/image.\n\nThe second point was probably important, not only in the sense that there were more chances of getting something wrong on a single page, but also in the sense that the redactions were (relatively) small. The detection of small objects is almost its own field in the world of computer vision and I don’t know too much about it, but I do know it’s somewhat an unsolved problem. That said, finding a way to boost the performance of the models on these ‘hard’ examples (there were a few other types of hard image) seemed like it might tackle a significant shortcoming of my model.\nI decided to try creating a separate batch of synthetic image data, this time fully tailored to tackling some of the hardness mentioned above: it would have many small redactions on a single page, they would all be white boxes and there might also be things like tables with white box-like shapes coexisting next to redactions.\nLuckily, the work I’d done previously on creating synthetic data helped me get started quickly. I returned to borb, an open-source tool for quickly creating PDF documents that allows for a pretty flexible prototyping of layouts with all sorts of bells and whistles added. These were some of the documents I generated:\n\nThe hard images were hard, and I had created some synthetic chimeras that (I believed) approximated some of the features of the original hard images. I did not want to overbalance my training data, however, and took care not to create too many of this type of image.\nMy script — as with the previous synthetic data — also required me to create the annotation files at the same time as creating the document. With borb it was relatively trivial to get the bounding box data for objects created, and there was even in-built functionality to create and apply redactions onto a document. (I’m moving fairly quickly over the mechanics of how this all worked, but it’s not too far distant from how I described it in my previous post so I’d refer you there for more details).\nOnce the images were created and added to my datasets, it was time to retrain the model and see what benefit it brought.\n\nAs you can see, the model jumped up from around 80.5 to 84% when I aded the hard synthetic examples in. That’s a pretty nice jump as far as I’m concerned, especially given that I only added in 300 images to the training data. I still had a little over a thousand of the original basic synthetic images that I was using, but this result showed me that tackling the badly performing parts of the model head-on seemed to have a positive outcome.\nAt this point, I did some more experiments around the edges, applying other things I knew would probably boost the performance even more, notably first checking what would happen if I increased the image size from 512 to 640. I got up to an 86% COCO score with that improvement alone.\nIn a final twist, I second-guessed myself and wondered whether the original synthetic data was even helping at all… I removed the thousand or so ‘basic’ synthetic images from the data and retrained the model. To my surprise, I achieved more or less the same COCO score as I had with the basic synthetic images. I’m taking this as a strong suggestion that my basic synthetic images aren’t actually helping as much as I’d thought, and that probably a smaller number of them as a % of the total would be beneficial."
  },
  {
    "objectID": "posts/2022-04-06-synthetic-data-results.html#reflections-on-experimenting-with-synthetic-data",
    "href": "posts/2022-04-06-synthetic-data-results.html#reflections-on-experimenting-with-synthetic-data",
    "title": "‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data",
    "section": "Reflections on experimenting with synthetic data",
    "text": "Reflections on experimenting with synthetic data\nSo, what can I conclude from this whole excursion into the world of synthetic image creation as a way of boosting model performance?\n\nadding synthetic data really can help!\nthe world of synthetic data creation is a huge rabbit hole and potentially you can get lost trying to create the perfect synthetic versions of your original data. (I mean this both in the sense of ‘there’s lots to learn’ as well as ‘you can spend or lose a ton of time here’.)\nTargeted synthetic data designed to clear up issues where the model has been identified as underperforming is probably best. (Conversely, and I’ll be careful how much I generalise here, middle-of-the-road synthetic data that doesn’t resemble the original dataset may not be worth your time.)\nKnowing your original data and domain really well helps. A lot. My intuition about what things the model would stumble on was fuelled by this knowledge of the documents and the domain, as well as by the experience of having done manual annotations for many hours.\n\nThere are probably many (many) more things I can do to continually tinker away at this model to improve it:\n\ncontinue down the path of more error analysis, which would fuel more targeted addition of annotations, and so on.\ncreate better versions of synthetic data with more variation to encompass the various kinds of documents out in the real world.\nmore self-training with the model in the loop to fuel my manual annotation process.\nfurther increases to the image size (perhaps in conjunction with progressive resizing).\nincreasing the backbone from resnet50 to resnet101.\n\nIn general, improving the quality of the data used to train my model seems to have been (by far) the best way to improve my model performance. Hyper-parameter tuning of the sort that is often referenced in courses or in blog posts does not seem to have had much of a benefit.\nIt is probably (mostly) good enough for my use case and for where I want to be heading with this project. There are other things that need addressing around the edges, notably parts of the project that could be made more robust and ‘production-ready’. More about that in due course, but for now please do comment below if you have suggestions for things that I haven’t thought of that might improve my model performance!"
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "",
    "text": "Chapter 10 of Chip Huyen’s “AI Engineering,” focuses on two fundamental aspects: architectural patterns in AI engineering and methods for gathering and using user feedback. The chapter presents a progressive architectural framework that evolves from simple API calls to complex agent-based systems, while also diving deep into the crucial aspect of user feedback collection and analysis."
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html#progressive-architecture-patterns",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html#progressive-architecture-patterns",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "1. Progressive Architecture Patterns",
    "text": "1. Progressive Architecture Patterns\nThe evolution of AI engineering architecture typically follows a pattern of increasing complexity and capability. Each stage builds upon the previous one, adding new functionality while managing increased complexity.\n\nBase Layer: Direct Model Integration\n\nThe simplest architectural pattern begins with direct queries to model APIs. While straightforward, this approach lacks the sophistication needed for most production applications.\n\n\nEnhancement Layer: Context Augmentation\n\nThe first major enhancement comes through Retrieval-Augmented Generation (RAG). This layer enriches model responses by incorporating custom data and sources into LLM queries, significantly improving response quality and relevance.\n\n\nProtection Layer: Guardrails Implementation\n\n\nGuardrails: Protective mechanisms that filter both inputs and outputs to ensure system safety and reliability.\n\nThe protection layer implements two types of guardrails:\n\nInput Guardrails: Filter sensitive information before it reaches the LLM, such as:\n\nPersonal customer information\nAPI keys\nOther confidential data\n\nOutput Guardrails: Monitor and manage model outputs for:\n\nFormat compliance (e.g., valid JSON)\nFactual consistency\nHallucination detection\nToxic content filtering\nPrivacy protection\n\n\n\n\nRouting Layer: Gateway and Model Selection\n\nThis layer introduces two key components:\n\nAI Gateway: A centralized access point for LLM interactions that manages costs, usage tracking, and API key abstraction.\n\n\nModel Router: An intent classifier that directs queries to appropriate models based on complexity and requirements.\n\nThe routing layer enables cost optimization by directing simpler queries (like FAQ responses) to less expensive models while routing complex tasks to more sophisticated systems.\n\n\nPerformance Layer: Caching Strategies\n\nThe architecture implements two distinct caching approaches:\n\nExact Caching:\n\nStores identical queries and their responses\nParticularly valuable for multi-step operations\nRequires careful consideration of cache eviction policies:\n\nLeast Recently Used (LRU)\nLeast Frequently Used (LFU)\nFirst In, First Out (FIFO)\n\n\nSemantic Caching:\n\nUses embedding-based search to identify similar queries\nDepends on high-quality embeddings and reliable similarity metrics\nMore prone to failure due to component complexity\n\n\n\nSecurity Note: Cache implementations must carefully consider potential data leaks between users accessing similar queries.\n\n\n\nAgent Layer: Advanced Functionality\n\nThe final architectural layer introduces agent patterns, enabling:\n\nRetry loops for reliability\nTool usage capabilities\nAction execution (email sending, file operations)\nComplex workflow orchestration"
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html#monitoring-and-observability",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html#monitoring-and-observability",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "Monitoring and Observability",
    "text": "Monitoring and Observability\nThe complete architecture requires robust monitoring systems tracking key metrics:\n\nMean Time to Detection (MTTD): Time to identify issues\nMean Time to Response (MTTR): Time to resolve detected issues\nChange Failure Rate (CFR): Percentage of deployments requiring fixes\n\nThe monitoring system should track:\n\nFactual consistency\nGeneration relevancy\nSafety metrics (toxicity, PII detection)\nModel quality through conversational signals\nComponent-specific metrics (RAG, generation, vector database performance)\n\n\nAI Pipeline Orchestration\na discussion of AI pipeline orchestration, addressing the trade-offs between using existing frameworks (Langchain, Haystack, Llama Index) versus custom implementations. This decision should be based on specific project requirements, team expertise, and maintenance considerations."
  },
  {
    "objectID": "posts/2025-02-09-ai-eg-chapter-10.html#user-feedback-systems",
    "href": "posts/2025-02-09-ai-eg-chapter-10.html#user-feedback-systems",
    "title": "AI Engineering Architecture and User Feedback",
    "section": "2. User Feedback Systems",
    "text": "2. User Feedback Systems\nThe second major focus of the chapter explores comprehensive user feedback collection and utilization strategies.\n\nFeedback Collection Methods\n\nDirect Feedback:\n\nExplicit mechanisms (thumbs up/down)\nRating systems\nFree-form comments\n\nImplicit Feedback:\n\nEarly termination patterns\nError corrections\nSentiment analysis\nResponse regeneration requests\nDialogue diversity metrics\n\n\n\n\nFeedback Collection Timing\nFeedback can be gathered at various stages:\n\nInitial user preference specification\nDuring negative experiences\nWhen model confidence is low\nThrough comparative choice interfaces (e.g., ChatGPT’s response preference selection)\n\n\n\nFeedback Limitations\n\nFeedback Bias: User feedback systems inherently contain various biases that must be considered when making system improvements.\n\nKey limitations include:\n\nNegative experience bias (users more likely to report negative experiences)\nSelf-selection bias in respondent demographics\nPreference and position biases\nPotential feedback loops affecting system evolution\n\n\n\nImplementation Considerations\nThe implementation of feedback systems requires careful attention to:\n\nUI/UX design for feedback collection\nBalance between different user needs\nMonitoring feedback impact on system performance\nRegular inspection of production data\nDetection of system drift (prompts, user behavior, model changes)"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html",
    "href": "posts/2023-03-05-stable-eights-adversarial.html",
    "title": "Tricking my digits classifier with diffusion",
    "section": "",
    "text": "In the lesson 9 video of the FastAI course, Jeremy explains how diffusion models work at a very high level. (These initial videos were released to the public early on, though the rest of the course is still ongoing and thus hasn’t yet been released.) Early on we’re introduced to a basic algorithm which would work for generating images. In doing so, we are given a model for how to think about the diffusion process. This mental model itself reminds of how we train models in the standard machine learning workflow.\nThe process goes as follows:\nOn some level, the rest of the complexity around the actual Stable Diffusion models relate to things like being able to combine a text prompt with the image generation, or making the process of training the model more efficient and so on. But for now, I’m choosing to focus on the core process as described above.\nMy choice to do this centered around the digit ‘8’ is arbitrary. We could as well have chosen something like ‘images of a shoe’ or whatever, but that potentially adds more complexity that could distract from the core process."
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#training-an-8-digit-classifier",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#training-an-8-digit-classifier",
    "title": "Tricking my digits classifier with diffusion",
    "section": "1: Training an ‘8’ digit classifier",
    "text": "1: Training an ‘8’ digit classifier\nTraining a model that can output the probability that a digit is a number eight is fairly trivial and is something that the FastAI part 1 course prepares you well to tackle, so I’ll handle that first since it’s a pre-requisite for everything that happens subsequently.\nNeedless to say, I’ll use the MNIST dataset as my source of training data. Potentially I can use Fashion MNIST as a stretch goal later on, but I’ll start with the digit ‘8’ and see where we get.\n\n!pip install -Uqq pip\n!pip install timm fastai torch datasets rich -Uqq\n\n\nfrom fastai.vision.all import *\nimport timm\n\ntorch.set_printoptions(precision=6, sci_mode=False)\n\n\n# dataset patched together from the original MNIST dataset\npath = Path(\"./mnist_8_or_not/training\")\npath.ls()\n\n(#4) [Path('mnist_8_or_not/training/not_8'),Path('mnist_8_or_not/training/8'),Path('mnist_8_or_not/training/.ipynb_checkpoints'),Path('mnist_8_or_not/training/eight_classifier.pkl')]\n\n\n\nfnames = get_image_files(path)\ndef label_func(x): return x.parent.name\n\ndls = ImageDataLoaders.from_path_func(path, fnames, label_func)\ndls.show_batch()\n\n\n\n\nAs you can see, we now have a dataloader with images of handwritten eights and images of handwritten digits that are not eights. This data was taken from the MNIST dataset and reassembled for the purposes of making this model. (Heavy data class imbalance in favour of the not-eights, but putting that issue to one side for now).\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n# only train the model if we have no model already\nmodel_path = Path(\"./eight_classifier.pkl\")\nif not model_path.exists():\n    learn.fine_tune(5)\nlearn = load_learner(\"./eight_classifier.pkl\")\n\n\nan_eight = Path(path / \"8\").ls()[0]\nnot_an_eight = Path(path / \"not_8\").ls()[0]\n\n\nlearn.predict(an_eight), learn.predict(not_an_eight)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(('8', TensorBase(0), TensorBase([1.0000e+00, 5.5913e-07])),\n ('not_8', TensorBase(1), TensorBase([2.2082e-06, 1.0000e+00])))\n\n\nOur model is pretty good at detecting whether digits are eights or not. I only trained it for five epochs, but it was already getting an error rate of almost zero. Good enough for the purposes of this proof-of-concept exploration.\n\nfrom typing import Union\n\ndef get_eight_probability(image_pth: Union[Path, torch.Tensor], learner: Learner) -> torch.Tensor:\n    _, _, probs = learner.predict(image_pth)\n    return probs[0]\n\n\nget_eight_probability(an_eight, learn)\n\n\n\n\n\n\n\n\nTensorBase(0.995113)\n\n\n\nget_eight_probability(not_an_eight, learn)\n\n\n\n\n\n\n\n\nTensorBase(0.000268)\n\n\n\n# export our model so we don't have to retrain it every time from now on\nif not model_path.exists():\n    learn.export(\"eight_classifier.pkl\")\n\n\nloaded_learn = load_learner(\"./eight_classifier.pkl\")\nget_eight_probability(an_eight, loaded_learn)\n\n\n\n\n\n\n\n\nTensorBase(0.999999)"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#create-a-random-noise-image",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#create-a-random-noise-image",
    "title": "Tricking my digits classifier with diffusion",
    "section": "2: Create a random noise image",
    "text": "2: Create a random noise image\nSo we have a way to get the probability that an image is an eight. Now we need to shift to the generation process, and we start by creating an image that is filled with random noise. We’ll shift to raw PyTorch code here probably so that it’s more explicit what’s going on, and so our ‘images’ will be Tensors. Let’s maybe make our images explicitly the same size as our training data, just to eliminate any confusion.\n\nfrom PIL import Image\n\nimg = Image.open(an_eight)\nwidth, height = img.size\n\nprint(f\"Image dimensions: {width} x {height}\")\n\nImage dimensions: 28 x 28\n\n\nNow we can generate a 28x28 image with random noise scattered throughout it.\n\nimport torch\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n\n# Generate a 28x28 tensor filled with random noise\nnoise_tensor = torch.randn(1, 1, 28, 28)\n\n# Convert the tensor to an image\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Grayscale(num_output_channels=1)\n])\nnoise_image = transform(noise_tensor.squeeze())\n\n# Display the image\nplt.imshow(noise_image, cmap='gray')\nplt.axis('off')\nplt.show()\n\n\n\n\nLet’s turn that into a function so we can use it later on…\n\n# we don't end up using this first function much as we're working with tensors\ndef get_noisy_starter_image() -> Image:\n    # Generate a 28x28 tensor filled with random noise\n    noise_tensor = torch.randn(1, 1, 28, 28)\n\n    # Convert the tensor to an image\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Grayscale(num_output_channels=1)\n    ])\n    return transform(noise_tensor.squeeze())\n\n# we generate a 3x28x28 tensor with random values assigned\n# we ensure that we can use PyTorch's autograd on the values\ndef get_noisy_starter_tensor() -> torch.Tensor:\n    noise_tensor = torch.randn(1, 3, 28, 28, requires_grad = True)\n    return noise_tensor\n\n\nget_noisy_starter_image()\n\n\n\n\n\ntype(get_noisy_starter_tensor())\n\ntorch.Tensor"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#get-the-derivatives-for-the-probability-that-its-an-eight",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#get-the-derivatives-for-the-probability-that-its-an-eight",
    "title": "Tricking my digits classifier with diffusion",
    "section": "3: Get the derivatives for the probability that it’s an eight",
    "text": "3: Get the derivatives for the probability that it’s an eight\nOur task now is go through the pixels of this 28x28 image and get the derivatives of each pixel with respect to the probability that the image is a digit ‘8’. We don’t have to do this pixel by pixel (i.e. the ‘finite differencing method’) since we have PyTorch, so we can use ‘analytic derivatives’ to calculate the whole lot in a single sweep.\n(Sidebar: I’m using those terms to reference two ways of calculating gradients from the universe of calculus, but I don’t know anything really about what they signify or how they work. Just wanted to signal that I’m using them just because it came up in lesson 9 and it’s often nice to have a name to put to a technique, even if you don’t necessarily know how it works. If you don’t know how it works too, welcome to the club and know that you don’t always need to know everything at every moment :) )\n\nrandom_sample = get_noisy_starter_tensor()\n\n\nrandom_sample.shape\n\ntorch.Size([1, 3, 28, 28])\n\n\n\nAdding and updating our loss function\nI experimented a bit with getting a loss function to do what I needed and it took several forms. I’m leaving the code here for posterity, but it doesn’t get used anywhere else in this blog. The two helper functions are for displaying a tensor in the notebook, and for turning a tensor into an image that can be displayed.\n\n\nCode\ndef new_eights_loss(preds: torch.Tensor, learner: Learner) -> torch.Tensor:\n    targets = torch.Tensor([[1.0, 0]])\n    return torch.nn.functional.mse_loss(preds, targets)\n\ndef display_tensor(tns: torch.Tensor) -> None:\n    plt.imshow(tns, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\ndef tns_to_img(tns: torch.Tensor) -> Image:\n    transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Grayscale(num_output_channels=1)\n        ])\n    return display_tensor(transform(tns.squeeze()))"
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#automating-the-iterations",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#automating-the-iterations",
    "title": "Tricking my digits classifier with diffusion",
    "section": "Automating the iterations",
    "text": "Automating the iterations\nAt this point we should just give it a try. In what follows, we get an image of something that isn’t an eight, then iterate a number of times through the iterate_image function which:\n\nensures that we’re tracking and calculating the gradients on the image\ngets the predictions for our image (i.e. whether it’s an eight or not) and passing them through a softmax\nwe pass our predictions and our targets into a l1_loss function (i.e. the mean absolute error) to get the loss for these predictions.\nwe call backward() on the loss so that the gradients are calculated\nwe then update our image data by some constant times our gradients (just like we do in the normal ML training loop)\nwe zero out the gradients prior to the next iteration\n\n(Every 15 iterations we output the loss as well as the image at that particular moment.)\n\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n\nimage = transform(Image.open(not_an_eight).convert(\"RGB\"))\n_ = image.requires_grad_()\n\n\nimport numpy as np\n\ndef iterate_image(image, iota: int, update_rate: float = 0.1):\n    image.requires_grad_()\n    preds = torch.softmax(loaded_learn.model(image[None]), dim=1)\n    targets = torch.Tensor([[1.0, 0]])\n    # loss = torch.nn.functional.mse_loss(preds, targets)\n    loss = torch.nn.functional.l1_loss(preds, targets)\n    loss.backward()\n    if i % 15 == 0:\n        print(f\"grad_sum: {image.grad.data.sum()}, loss: {loss}\")\n    \n    image.data -= (update_rate * image.grad.data)\n    image.grad.zero_()\n    \n    if i % 15 == 0:\n        # N.B. Use v1 to get a sense of what's changing, v2 for the current values\n        \n        # VERSION 1\n        # plt.imshow(np.log1p(image[0].detach().numpy()))\n        # plt.show(plt.gcf())\n        \n        # VERSION 2\n        plt.imshow(image[0].detach().numpy())\n        plt.show()\n\nWe iterate 50 times, and we our constant for the update_rate is fairly high (i.e. 1). Normally for a learning_rate (the equivalent for this value) we would choose something between 0.01 and 0.1 so as not to update too far in any one direction.\nAlso, just to confirm, these are our predictions for the not_an_eight image we’re using as the basis for this iterative process:\n\nlearn = load_learner(\"./eight_classifier.pkl\")\nlearn.predict(not_an_eight)\n\n\n\n\n\n\n\n\n('not_8', TensorBase(1), TensorBase([    0.000268,     0.999732]))\n\n\n\nfor i in range(50):\n    iterate_image(image, i, update_rate = 1)\n\ngrad_sum: -0.0006039840518496931, loss: 0.9997323155403137\n\n\n\n\n\ngrad_sum: -0.003538962686434388, loss: 0.0007090563885867596\n\n\n\n\n\ngrad_sum: -0.0011799032799899578, loss: 0.00022288746549747884\n\n\n\n\n\ngrad_sum: -0.0007250444614328444, loss: 0.00012537218572106212\n\n\n\n\n\nAs you can see, the loss reduces throughout (and would continue to do so were I to let it continue onwards). The image still looks like a two, however, and doesn’t seem to resemble an eight. So is it doing what we want? A quick check is to run that image we’ve updated / ‘generated’ into the model and get the predictions:\n\ntorch.sigmoid(loaded_learn.model(image[None]))\n\nTensorBase([[0.989906, 0.010533]], grad_fn=<AliasBackward0>)\n\n\n\n# double-checking using a freshly imported learner to make sure\nlearn = load_learner(\"./eight_classifier.pkl\")\ntorch.sigmoid(learn.model(image[None]))\n\nTensorBase([[0.989906, 0.010533]], grad_fn=<AliasBackward0>)\n\n\nSo even though the image still looks like a two, our model has been bamboozled into thinking it’s an eight. The iteration process has updated the weights just enough that we’ve minimised our loss, and our model now is almost completely certain that this is an image of an eight.\nI’ve tried the same process on the random noise and you don’t get quite as impressive updating as quickly as with our two, but maybe with more iterations it would gradually get there.\nNeedless to say, this was a surprising result. I expected to have a neat progression through which the image would gradually look more and more like a number eight. This updating where we adjust enough to completely switch the predictions of the model from certainty that it isn’t an eight, to certainty that it is an eight, that was unexpected to say the least.\nI’m really appreciative of the various people who helped me during the process of working on this little experiment: Yogya, Kevin, and all those with whom I have the pleasure to meet each week in the Delft FastAI Study Group."
  },
  {
    "objectID": "posts/2023-03-05-stable-eights-adversarial.html#appendix-some-pytorch-things",
    "href": "posts/2023-03-05-stable-eights-adversarial.html#appendix-some-pytorch-things",
    "title": "Tricking my digits classifier with diffusion",
    "section": "Appendix: Some PyTorch Things",
    "text": "Appendix: Some PyTorch Things\nI learned or encountered a lot of small PyTorch tricks while working on this, so making a note of them as a record for future reference:\n\nautograd - this is PyTorch’s way of automatically calculating the gradients. I’ve never done this process manually myself, nor do I know how to do it, but I reckon this probably saves us a lot of time and complication.\nsqueeze() and unsqueeze() - these remove and add a dimension to a tensor. It can help in getting things into the right shape.\nview() - maybe even more helpful when it comes to getting tensors in the right shape. I ran into lots of places where I was passing something of slightly the wrong shape or dimensions in somewhere and that was causing problems.\nzero_() - a way of zeroing out the gradients in-place\ndetach() - this is a way of getting a copy of the current tensor, but one that isn’t attached to the graph of gradient calculations that are going on.\nrequires_grad_() - again, another way to ensure that a tensor has the gradient calculations enabled, and it happens in-place.\ntorch.equals(t1, t2) - a way of comparing two tensors to see if they’re identical or not.\n\nI discovered that FastAI’s learn.predict() function does some things to whatever you pass in, such that if you use it in the process above you’ll hit an error where it says that there are no longer any gradients to update / accumulate. This is why we’re passing our image values or our tensor directly into the model to get our predictions since this bypasses whatever FastAI is doing. I wasn’t able to fully grok / dig into exactly what’s happening there, but perhaps this is something to look into in the future.\n\nDebugging tips\nOn Saturday, I had basically a whole day of hitting lots of walls, somewhat-opaque PyTorch error messages galore. The thing that really seemed to help the most (and that my study group colleagues did with me on Sunday morning) was just to break out each individual step separately, inspecting the return values and the .shape and so on.\nDoing this systematically, interrogating what you expect to happen vs what you see being returned or happening definetely seems like the way to go. At this level of (non-)abstraction, however, there were quite a few PyTorch messages that were hard to wrap my head around. I suppose when you do this often enough you get an intuition for the kinds of ways these things fail, and how to massage your data into the formats required."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "",
    "text": "This chapter represents a crucial bridge between academic research and production engineering practice in AI system evaluation. What sets it apart is the Chip’s very balanced perspective - neither succumbing to the prevalent hype in the field nor becoming overly academic. Instead, she melds together practical insights with theoretical foundations, creating a useful framework for evaluation that acknowledges both technical and ethical considerations."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#introduction-and-context",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#introduction-and-context",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Introduction and Context",
    "text": "Introduction and Context\n\nKey Insight: The author’s approach demonstrates that effective AI system evaluation requires a synthesis of academic rigour and practical engineering concerns, much like how traditional software engineering evolved to balance theoretical computer science with practical development methodologies.\n\nThe chapter is structured in three main parts, each building upon the previous to create a complete picture of AI system evaluation:\n\nEvaluation criteria fundamentals\nModel selection and benchmark navigation\nPractical pipeline implementation"
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-1-evaluation-criteria---a-deep-dive",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-1-evaluation-criteria---a-deep-dive",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Part 1: Evaluation Criteria - A Deep Dive",
    "text": "Part 1: Evaluation Criteria - A Deep Dive\n\nThe Evolution of Evaluation-Driven Development\nThe author introduces evaluation-driven development (EDD), a methodological evolution that adapts the principles of test-driven development to the unique challenges of AI systems.\n\nEvaluation-Driven Development: A methodology where AI application development begins with explicit evaluation criteria, similar to how test-driven development starts with test cases. However, EDD encompasses a broader range of metrics and considerations specific to AI systems.\n\nThe fundamental principle here is that AI applications require a more nuanced and multifaceted approach to evaluation than traditional software. Where traditional software might have binary pass/fail criteria, AI systems often operate in a spectrum of performance across multiple dimensions.\n\n\nThe Four Pillars of Evaluation\n\n1. Domain-Specific Capability\nThe author presents domain-specific capability evaluation as the foundational layer of AI system assessment. This approach is particularly innovative in its use of multiple choice evaluation techniques - a method that bridges the gap between human-interpretable results and machine performance metrics.\nFor example, when evaluating code generation capabilities, presenting a model with multiple implementations where only one is functionally correct serves as both a test and a teaching tool. This mimics how human experts often evaluate junior developers’ understanding of coding patterns and best practices.\n\n\n2. Generation Capability\nThe section on generation capability draws parallels with the historical development of Natural Language Generation (NLG) in computational linguistics. This historical context provides valuable insights into how we can approach modern language model evaluation.\nThe author breaks down factual consistency into two crucial dimensions:\n\nLocal Factual Consistency: The internal coherence of generated content and its alignment with the immediate context of the prompt. This is analogous to maintaining logical consistency within a single conversation or document.\nGlobal Factual Consistency: The accuracy of generated content when compared against established knowledge and facts. This represents the model’s ability to maintain truthfulness in a broader context.\n\nThe discussion of hallucination detection is particularly noteworthy, presenting three complementary approaches:\n\nBasic Prompting: Direct detection through carefully crafted prompts\nSelf-Verification: A novel approach using internal consistency checks across multiple generations\nKnowledge-Augmented Verification: Advanced techniques like Google DeepMind’s SAFE paper (search augmented factuality evaluator)\n\nThe knowledge-augmented verification system represents a fascinating approach to fact-checking that mirrors how human experts verify information:\n\nIt breaks down complex statements into atomic claims\nEach claim is independently verified through search\nThe results are synthesised into a final accuracy assessment\n\nSeems pricey, though :)\n\n\n3. Instruction Following Capability\nThe author makes a crucial observation about the bidirectional nature of instruction following evaluation. Poor performance might indicate either model limitations or instruction ambiguity - a distinction that’s often overlooked in practice.\n\nInstruction-Performance Paradox: The quality of instruction following cannot be evaluated in isolation from the quality of the instructions themselves, creating a circular dependency that must be carefully managed in evaluation design.\n\nThe solution proposed is the development of custom benchmarks that specifically target your application’s requirements. This approach ensures that your evaluation criteria align perfectly with your practical needs rather than relying solely on generic benchmarks.\n\n\n4. Cost and Latency Considerations\nThe author introduces the concept of Pareto optimization in the context of AI system evaluation, demonstrating how different performance metrics often involve trade-offs that must be carefully balanced.\n\nPareto Optimization: A multi-objective optimization approach where improvements in one metric cannot be achieved without degrading another, leading to a set of optimal trade-off solutions rather than a single optimal point."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-2-model-selection---a-strategic-approach",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-2-model-selection---a-strategic-approach",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Part 2: Model Selection - A Strategic Approach",
    "text": "Part 2: Model Selection - A Strategic Approach\n\nThe Four-Step Evaluation Workflow\nThe author presents a sophisticated workflow that combines both quantitative and qualitative factors in model selection. This approach is particularly valuable because it acknowledges the complexity of real-world deployment while providing a structured path forward.\n\nInitial Filtering The first step involves filtering based on hard constraints, which might include:\n\nDeployment requirements (on-premise vs. cloud)\nSecurity and privacy considerations\nLicensing restrictions\nResource constraints\n\nPublic Information Assessment This stage involves a systematic review of:\n\nBenchmark performances across relevant tasks\nLeaderboard rankings with context\nPublished latency and cost metrics\n\nThe author emphasises the importance of looking beyond raw numbers to understand the context and limitations of public benchmarks.\nExperimental Evaluation This phase involves hands-on testing with your specific use case, considering:\n\nCustom evaluation metrics\nIntegration requirements\nReal-world performance characteristics\n\nContinuous Monitoring The final step acknowledges that evaluation is an ongoing process, not a one-time event. This involves:\n\nRegular performance monitoring\nFailure detection and analysis\nFeedback collection and incorporation\nContinuous improvement cycles\n\n\n\n\nThe Build vs. Buy Decision Matrix\nThe author provides an analysis of the build vs. buy decision, going beyond simple cost comparisons to consider factors like:\n\nTotal Cost of Ownership (TCO): The complete cost picture including: - Direct costs (API fees, computing resources) - Indirect costs (engineering time, maintenance) - Opportunity costs (time to market, feature development) - Risk costs (security, reliability, vendor lock-in)\n\nThis section particularly shines in its discussion of the often-overlooked aspects of model deployment, such as the hidden costs of maintaining self-hosted models and the true value of vendor-provided updates and improvements."
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-3-building-evaluation-pipelines---practical-implementation",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#part-3-building-evaluation-pipelines---practical-implementation",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Part 3: Building Evaluation Pipelines - Practical Implementation",
    "text": "Part 3: Building Evaluation Pipelines - Practical Implementation\n\nSystem Component Evaluation\nThe author advocates for a dual-track evaluation approach:\n\nEnd-to-end system evaluation\nComponent-level assessment\n\nThis approach allows organisations to:\n\nIdentify bottlenecks and failure points\nUnderstand component interactions\nMake targeted improvements\nMaintain system reliability during updates\n\n\n\nCreating Effective Evaluation Guidelines\nThe author emphasises the importance of creating clear, actionable evaluation guidelines that bridge technical and business metrics. This section introduces the concept of metric alignment - ensuring that technical evaluation metrics directly correspond to business value.\n\nMetric Alignment: The process of mapping technical performance metrics to business outcomes, creating a clear connection between model improvements and business value.\n\n\n\nData Management and Sampling\nChip provides valuable insights into data management for evaluation, including:\n\nData Slicing: The strategic separation of evaluation data into meaningful subsets to: - Identify performance variations across different use cases - Detect potential biases - Enable targeted improvement efforts - Avoid Simpson’s paradox in performance analysis\n\nThe discussion of sample size is particularly practical, providing concrete guidelines based on statistical confidence levels and desired detection thresholds. The author cites OpenAI’s research suggesting that sample sizes between 100 and 1,000 are typically sufficient for most evaluation needs, depending on the required confidence level.\n\n\n\nMeta-Evaluation: Evaluating Your Evaluation\nThe chapter concludes with a crucial discussion of meta-evaluation - the process of assessing and improving your evaluation pipeline itself. This includes considerations of:\n\nSignal quality and reliability\nMetric correlation and redundancy\nResource utilisation and efficiency\nIntegration with development workflows"
  },
  {
    "objectID": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#conclusion",
    "href": "posts/2025-01-22-notes-on-ai-engineering-chip-huyen-chapter-4.html#conclusion",
    "title": "Notes on ‘AI Engineering’ (Chip Huyen) chapter 4",
    "section": "Conclusion",
    "text": "Conclusion\nThe author concludes around the inherent limitations of AI system evaluation: no single metric or method can fully capture the complexity of these systems. However, this acknowledgment leads to a constructive approach: combining multiple evaluation methods, maintaining awareness of their limitations, and continuously iterating based on real-world feedback.\nThis chapter ultimately provides a solid framework for AI system evaluation that is both theoretically sound and practically applicable. It serves as a valuable resource for organisations working to implement effective evaluation strategies for their AI systems, while maintaining a clear-eyed view of both the possibilities and limitations of current evaluation methods."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html",
    "href": "posts/2025-02-21-beeminder-mcp.html",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "",
    "text": "I spent the morning building an MCP server for Beeminder, bridging the gap between AI assistants and my personal goal tracking data. This project emerged from a practical need — ok, desire :) — to interact more effectively with my Beeminder data through AI interfaces like Claude Desktop and Cursor."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#understanding-beeminder",
    "href": "posts/2025-02-21-beeminder-mcp.html#understanding-beeminder",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Understanding Beeminder",
    "text": "Understanding Beeminder\nFor those unfamiliar with Beeminder, it’s a tool that combines self-tracking with commitment devices to help users achieve their goals. The platform draws what they call a “Bright Red Line” – a visual commitment path that shows exactly where you need to be to stay on track. What makes Beeminder unique is its approach to accountability: users pledge real money to stay on their path, and there’s a seven-day “akrasia horizon” that prevents immediate goal changes, helping to overcome moments of impulsivity.\nI’ve written a lot about Beeminder over on my personal blog in the past so do go check that out if you’re interested to learn more about how I use it. I can attest that if it clicks with you, you’ll find it incredibly valuable. I have used in the past to write books, learn languages, finish my PhD and many many other things."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#the-role-of-mcp",
    "href": "posts/2025-02-21-beeminder-mcp.html#the-role-of-mcp",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "The Role of MCP",
    "text": "The Role of MCP\nThe Model Context Protocol (MCP) serves as a standardised way for AI assistants to interact with various data sources and tools. Think of it as a universal adapter that allows AI systems to directly access and manipulate data in your applications. Instead of copying and pasting information between your AI assistant and Beeminder, MCP creates a secure, direct connection.\nThis standardisation is particularly valuable because it means you can build one interface that works across multiple AI platforms. Whether you’re using Claude Desktop, Cursor, or other MCP-compatible tools, the same server provides consistent access to your Beeminder data."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#building-the-server",
    "href": "posts/2025-02-21-beeminder-mcp.html#building-the-server",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Building the Server",
    "text": "Building the Server\nThe development process was surprisingly straightforward, largely due to two factors: the well-documented MCP specification from Anthropic and an existing Python client for Beeminder’s API by @ianm118. Most of the implementation work involved mapping Beeminder’s API endpoints to MCP’s expected interfaces and ensuring proper error handling.\nAnd obviously, much of the code was actually written by Claude itself. After providing the initial structure, writing a couple of tools the way I wanted them and providing documentation, I found that Claude could generate the remainder of the code, requiring only minor adjustments and debugging from me."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#using-the-beeminder-mcp-server",
    "href": "posts/2025-02-21-beeminder-mcp.html#using-the-beeminder-mcp-server",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Using the Beeminder MCP Server",
    "text": "Using the Beeminder MCP Server\nHaving an MCP server for Beeminder opens up several practical possibilities. You can have natural conversations with AI assistants about your goals, analyse patterns in your data, and even update your tracking information – all while the AI has direct access to your actual Beeminder account. This direct connection means the AI can provide more contextual and accurate assistance, whether you’re adjusting goal parameters or analysing your progress trends.\nI’ve found that sometimes Claude needs a bit of coaxing to display the information it’s getting back from the Beeminder API in appropriate formats, which is to say, in table format. I will probably update my Claude settings so that it knows it should use tables (either Markdown or React components) to display Beeminder results that would benefit from such a presentation."
  },
  {
    "objectID": "posts/2025-02-21-beeminder-mcp.html#looking-forward",
    "href": "posts/2025-02-21-beeminder-mcp.html#looking-forward",
    "title": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data",
    "section": "Looking Forward",
    "text": "Looking Forward\nNow that I have my Beeminder MCP server, I also want one for Omnifocus, my task management app of choice. That’ll probably have to wait since it doesn’t appear that they offer a REST API, but it’ll be great when I can mash up the results of those two tool queries as that’s what I currently do manually as part of my process.\nThe ease of building this MCP server suggests an interesting future where more of our tools and services become directly accessible to AI assistants. The real value isn’t in any single connection, but in the potential for creating a network of interconnected tools that AI can help us manage more effectively.\nIf you’re interested in trying this out yourself, you can find the code and setup instructions in the GitHub repository. While this implementation focuses on Beeminder, the same principles could be applied to create MCP servers for other services and tools."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned (TIL)",
    "section": "",
    "text": "What is the Rust prelude?\n\n\n\nrust\n\n\nlearning\n\n\nTIL\n\n\n\nA quick post on what the ‘prelude’ is and why it exists.\n\n\n\nAlex Strick van Linschoten\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLOps.systems Blog",
    "section": "",
    "text": "Building an MCP Server for Beeminder: Connecting AI Assistants to Personal Data\n\n\n\n\n\n\n\ntools\n\n\nanthropic\n\n\nclaude\n\n\nminiproject\n\n\n\n\nI built a Model Context Protocol (MCP) server for Beeminder to connect AI assistants with my personal goal tracking data. Here’s how I implemented this integration using Claude Desktop, what I learned about MCP development, and my vision for AI-powered personal productivity workflows.\n\n\n\n\n\n\nFeb 21, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTinbox: an LLM-based document translation tool\n\n\n\n\n\n\n\ntranslation\n\n\nllm\n\n\nllms\n\n\nlanguages\n\n\nresearch\n\n\nminiproject\n\n\npython\n\n\ntools\n\n\n\n\nExplores an open-source tool I built that tackles the challenges of large-scale document translation using LLMs. Born from my experience as both a historian working with Afghan primary sources and a developer, it offers innovative solutions to common translation problems through smart chunking algorithms and local model support, making multilingual content more accessible for researchers and developers alike.\n\n\n\n\n\n\nFeb 16, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nStarting the Hugging Face Agents course\n\n\n\n\n\n\n\nagents\n\n\nhuggingface\n\n\nskillbuilding\n\n\nllmops\n\n\nllms\n\n\n\n\nSome observations on completing unit one of the new course hosted by Hugging Face.\n\n\n\n\n\n\nFeb 11, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nAI Engineering Architecture and User Feedback\n\n\n\n\n\n\n\nbooks-i-read\n\n\nllm\n\n\nllms\n\n\nllmops\n\n\nevaluation\n\n\n\n\nMy notes on chapter 10 of Chip Huyen’s ‘AI Engineering’, an exploration of modern AI system architecture patterns and user feedback mechanisms, covering the evolution from simple API integrations to complex agent-based systems, including practical implementations of RAG, guardrails, caching strategies, and systematic approaches to gathering and utilizing user feedback for continuous improvement.\n\n\n\n\n\n\nFeb 9, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘AI Engineering’ chapter 9: Inference Optimisation\n\n\n\n\n\n\n\nbooks-i-read\n\n\ninference\n\n\nllm\n\n\nllms\n\n\nhardware\n\n\n\n\nChapter 9 is a guide to ML inference optimization covering compute and memory bottlenecks, performance metrics, and practical implementation strategies. This technical summary explores model-level, hardware-level, and service-level optimizations, with detailed explanations of batching strategies, parallelism approaches, and attention mechanisms - essential knowledge for ML engineers working to reduce inference costs and improve system performance.\n\n\n\n\n\n\nFeb 7, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nDataset Engineering: The Art and Science of Data Preparation\n\n\n\n\n\n\n\nbooks-i-read\n\n\ndatasets\n\n\ndatalabelling\n\n\nllm\n\n\nllms\n\n\nfinetuning\n\n\n\n\nExplores Chapter 8 of Chip Huyen’s ‘AI Engineering,’ examining the intricate landscape of dataset engineering through the lenses of curation, augmentation, and processing.\n\n\n\n\n\n\nFeb 5, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 7: Finetuning\n\n\n\n\n\n\n\nbooks-i-read\n\n\nfinetuning\n\n\nllm\n\n\nllms\n\n\n\n\nExplores when and how to implement finetuning effectively, looking at key technical aspects like memory considerations and PEFT, while emphasising fine-tuning as a last-resort approach after simpler solutions like prompt engineering and RAG have been exhausted.\n\n\n\n\n\n\nJan 26, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 6\n\n\n\n\n\n\n\nbooks-i-read\n\n\nllm\n\n\nllms\n\n\nagents\n\n\nrag\n\n\nevaluation\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 4\n\n\n\n\n\n\n\nbooks-i-read\n\n\nllm\n\n\nllms\n\n\nevaluation\n\n\n\n\nA comprehensive guide to AI system evaluation, synthesising Chapter 4 of Chip Huyen’s ‘AI Engineering.’ These notes detail practical frameworks for assessing AI models, covering evaluation criteria, model selection strategies, and pipeline implementation, while maintaining a balanced perspective between academic rigour and real-world application needs.\n\n\n\n\n\n\nJan 22, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 3\n\n\n\n\n\n\n\nbooks-i-read\n\n\nllm\n\n\nllms\n\n\nevaluation\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nNotes on ‘AI Engineering’ (Chip Huyen) chapter 1\n\n\n\n\n\n\n\nbooks-i-read\n\n\nllm\n\n\nllms\n\n\nfinetuning\n\n\nprompt-engineering\n\n\n\n\nA detailed analysis of Chapter 1 from Chip Huyen’s ‘AI Engineering’ book, covering the transition from ML Engineering to AI Engineering, the three-layer AI stack, and modern development paradigms. Includes insights from a study group discussion on enterprise adoption challenges and emerging evaluation techniques.\n\n\n\n\n\n\nJan 19, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nFinal notes on ‘Prompt Engineering for LLMs’\n\n\n\n\n\n\n\nllm\n\n\nprompt-engineering\n\n\nbooks-i-read\n\n\nevaluation\n\n\n\n\nDetailed notes covering Chapters 10 and 11 of ‘Prompt Engineering for LLMs’ by Berryman and Ziegler, focusing on LLM application evaluation and future trends. Chapter 10 explores comprehensive testing frameworks including offline example suites and online AB testing, while Chapter 11 discusses multimodality, user interfaces, and core principles for effective prompt engineering. Includes personal insights on the book’s emphasis on completion models versus chat models.\n\n\n\n\n\n\nJan 17, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nAssembling the Prompt: Notes on ‘Prompt Engineering for LLMs’ ch 6\n\n\n\n\n\n\n\nllm\n\n\nprompt-engineering\n\n\nbooks-i-read\n\n\n\n\nA detailed breakdown of Chapter 6 from ‘Prompt Engineering for LLMs,’ examining prompt structure, document types, and optimization strategies for effective prompt engineering, with practical tips on information positioning and context selection within prompts.\n\n\n\n\n\n\nJan 13, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nPrompt Content: Notes on ‘Prompt Engineering for LLMs’ ch 5\n\n\n\n\n\n\n\nllm\n\n\nprompt-engineering\n\n\nbooks-i-read\n\n\nRAG\n\n\n\n\nChapter 5 of ‘Prompt Engineering for LLMs’ explores static content (fixed instructions and few-shot examples) versus dynamic content (runtime-assembled context like RAG) in prompts, offering tactical guidance on implementation choices, tradeoffs, and potential pitfalls while emphasising practical examples throughout.\n\n\n\n\n\n\nJan 12, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nStarting to read Prompt Engineering for LLMs\n\n\n\n\n\n\n\nllm\n\n\nprompt-engineering\n\n\nbooks-i-read\n\n\ntokenisation\n\n\n\n\nSummary notes from the first two chapters of ‘Prompt Engineering for LLMs’.\n\n\n\n\n\n\nJan 9, 2025\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nAll the things I learned while trending on Hacker News\n\n\n\n\n\n\n\nllms\n\n\nminiproject\n\n\nfinetuning\n\n\nisafpr\n\n\nevaluation\n\n\nnlp\n\n\n\n\nI was on the front page of Hacker News for my two last blog posts and I learned various things forom the discussion and scrutiny of my approach to evaluating my finetuned LLMs.\n\n\n\n\n\n\nJul 7, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nMy finetuned models beat OpenAI’s GPT-4\n\n\n\n\n\n\n\nnlp\n\n\nafghanistan\n\n\nllms\n\n\nminiproject\n\n\nfinetuning\n\n\nisafpr\n\n\nevaluation\n\n\n\n\nFinetunes of Mistral, Llama3 and Solar LLMs are more accurate for my test data than OpenAI’s models.\n\n\n\n\n\n\nJul 1, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow to think about creating a dataset for LLM finetuning evaluation\n\n\n\n\n\n\n\nllms\n\n\nfinetuning\n\n\nisafpr\n\n\nafghanistan\n\n\ndatasets\n\n\nevaluation\n\n\nminiproject\n\n\n\n\nI summarise the kinds of evaluations that are needed for a structured data generation task.\n\n\n\n\n\n\nJun 25, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nOne-click LLM finetuning with Predibase, OpenPipe and OpenAI\n\n\n\n\n\n\n\nnlp\n\n\nllms\n\n\nminiproject\n\n\nfinetuning\n\n\nisafpr\n\n\n\n\nI tried out some services that promise to simplify the process of finetuning open models. I describe my experiences with Predibase, OpenPipe and OpenAI.\n\n\n\n\n\n\nJun 17, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nFinetuning my first LLM(s) for structured data extraction with axolotl\n\n\n\n\n\n\n\nnlp\n\n\nafghanistan\n\n\nllms\n\n\nminiproject\n\n\nfinetuning\n\n\nisafpr\n\n\n\n\nI finetuned my first LLM(s) for the task of extracting structured data from ISAF press releases. Initial tests suggest that it worked pretty well out of the box.\n\n\n\n\n\n\nJun 15, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction\n\n\n\n\n\n\n\nnlp\n\n\nafghanistan\n\n\ndatalabelling\n\n\nllms\n\n\nisafpr\n\n\nminiproject\n\n\nevaluation\n\n\n\n\nI evaluated the baseline performance of OpenAI’s GPT-4-Turbo on the ISAF Press Release dataset.\n\n\n\n\n\n\nJun 3, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nStructured Data Extraction for ISAF Press Releases with Instructor\n\n\n\n\n\n\n\nnlp\n\n\nafghanistan\n\n\ndatalabelling\n\n\nisafpr\n\n\nllms\n\n\nminiproject\n\n\n\n\nI used Instructor to understand how well LLMs are at extracting data from the ISAF Press Releases dataset. They did pretty well, but not across the board.\n\n\n\n\n\n\nJun 2, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing the Afghanwire Dataset: A Unique Collection of Translated Afghan Media Articles from 2006-2009\n\n\n\n\n\n\n\nminiproject\n\n\nafghanistan\n\n\ndatalabelling\n\n\ndatasets\n\n\nnlp\n\n\nllms\n\n\nisafpr\n\n\n\n\nI’m publishing a unique new dataset of Afghan newspaper and magazine articles from the 2006-2009 period. This collection of over 7990 articles were originally translated from Dari and Pashto and published by Afghanwire, a media monitoring organisation that I co-founded and ran in Kabul at the time.\n\n\n\n\n\n\nApr 1, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nWriting a custom Terraform provider to deploy Huggingface Spaces\n\n\n\n\n\n\n\ndevops\n\n\nminiproject\n\n\nterraform\n\n\ngo\n\n\nskillbuilding\n\n\n\n\nI worked on this short project to allow people to create/deploy Huggingface Spaces using Terraform (instead of via the API or using the website)\n\n\n\n\n\n\nMar 31, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nPublishing the ISAF Press Releases dataset\n\n\n\n\n\n\n\nminiproject\n\n\nafghanistan\n\n\ndatalabelling\n\n\ndatasets\n\n\nnlp\n\n\nllms\n\n\n\n\nI published a dataset from my previous work as a researcher in Afghanistan. It consists of press releases about military operations as well as full annotations showcasing information extracted from those press releases. It has value as a historical artifact but potentially could be used as an LLM evaluation task as well.\n\n\n\n\n\n\nMar 24, 2024\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nAutomating database backups with Tarsnap\n\n\n\n\n\n\n\ndatabases\n\n\nskillbuilding\n\n\nsoftwareengineering\n\n\ntools\n\n\nminiproject\n\n\n\n\nI added a cronjob to automate database backups for my MathsPrompt questions.\n\n\n\n\n\n\nJul 24, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nBuilding MathsPrompt: a tool to help me review and practice problems for my degree\n\n\n\n\n\n\n\nopenai\n\n\nllms\n\n\nmathematics\n\n\nrust\n\n\nmu123\n\n\nq31\n\n\nskillbuilding\n\n\nsoftwareengineering\n\n\ntools\n\n\nminiproject\n\n\n\n\nI built a tool to help me practice the parts of mathematics that I find hardest. I also have been reading some books about Rust and I also wanted to play around with that so used it for the server / backend.\n\n\n\n\n\n\nJul 23, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTerraform Input Variables\n\n\n\n\n\n\n\nterraform\n\n\ndevops\n\n\nsoftwareengineering\n\n\n\n\nAll the ways you can set input and local variables when using Terraform.\n\n\n\n\n\n\nJun 22, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTokenizer Links\n\n\n\n\n\n\n\nnlp\n\n\nbalochi-language-model\n\n\ntokenisation\n\n\nlinks\n\n\n\n\nSome links and random observations relating to tokenisation as gathered over the past week.\n\n\n\n\n\n\nJun 4, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTokenizing Balochi with HuggingFace’s Tokenizer and FastAI/Spacy\n\n\n\n\n\n\n\nnlp\n\n\nbalochi-language-model\n\n\ntokenisation\n\n\nbalochi\n\n\n\n\nI explore language tokenization using FastAI, Spacy, and Huggingface Tokenizers, with a special focus on the less-represented Balochi language. I share the challenges I faced due to language-specific limitations, my initiative to expand language metadata, and my plans to assess and enhance tokenization efficiency.\n\n\n\n\n\n\nJun 3, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nThe What, Why, and How of Tokenisation in Machine Learning\n\n\n\n\n\n\n\nnlp\n\n\nbalochi-language-model\n\n\ntokenisation\n\n\n\n\nThe basics around the tokenisation process: why we do it, the spectrum of choices when you get to choose how to do it, and the family of algorithms most commonly used at the moment.\n\n\n\n\n\n\nJun 1, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Balochi Language Dataset for NLP Applications\n\n\n\n\n\n\n\nbalochi\n\n\nnlp\n\n\nbalochi-language-model\n\n\nethics\n\n\ndatasets\n\n\n\n\nI share my journey of building language models for Balochi, a language with few digital resources. I discuss assembling a dataset of 2.6 million Balochi words.\n\n\n\n\n\n\nMay 29, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nThe Risks of Language Models in Minority Languages\n\n\n\n\n\n\n\nbalochi\n\n\nnlp\n\n\nbalochi-language-model\n\n\ndeep-learning\n\n\nethics\n\n\n\n\nThe dual-edged nature of developing a language model for the Balochi language, weighing potential benefits like improved communication, accessibility, and language preservation against serious risks such as misuse by state actors for surveillance and power consolidation, and the unintentional promotion of linguistic monoculture.\n\n\n\n\n\n\nMay 22, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nLow-resource language models: making a start with Balochi\n\n\n\n\n\n\n\nbalochi\n\n\nnlp\n\n\nbalochi-language-model\n\n\ndeep-learning\n\n\n\n\nThe Balochi language is underrepresented in NLP. I’m interested in contributing to the field by building a language model for Balochi from scratch and contributing training resources and datasets along the way.\n\n\n\n\n\n\nMay 21, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nFinishing MU123\n\n\n\n\n\n\n\nmathematics\n\n\nmu123\n\n\nq31\n\n\n\n\nI completed the first module from my maths degree with the Open University. Highlights were quadratic equations, trigonometry and exponential functions.\n\n\n\n\n\n\nMay 14, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nExponents and Logarithms: a MU123 review\n\n\n\n\n\n\n\nmathematics\n\n\nmu123\n\n\nq31\n\n\n\n\nI delved into exponents and logarithms in my Open University Maths degree, discovering their practical applications and connections to concepts like Euler’s number. Gaining a deeper understanding, I enjoyed manipulating symbols and working with these fascinating mathematical tools.\n\n\n\n\n\n\nMay 2, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTerraform for the Uninitiated: Demystifying Your First Codebase\n\n\n\n\n\n\n\nterraform\n\n\nsoftwareengineering\n\n\ndevops\n\n\n\n\nLearn the essentials of working with Terraform as a beginner, including basic commands like init, plan, apply, and destroy. Gain insights into code structure, variables, outputs, and providers while exploring a new codebase.\n\n\n\n\n\n\nApr 29, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow to remove a commit (or two) from your git branch\n\n\n\n\n\n\n\ngit\n\n\nsoftwareengineering\n\n\nversioncontrol\n\n\n\n\nInstructions how to remove a commit from your git logs.\n\n\n\n\n\n\nApr 28, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nThe Trick Is The Thing, Part II\n\n\n\n\n\n\n\nmathematics\n\n\nmu123\n\n\nq31\n\n\ndeeplearning\n\n\n\n\nI’ve enjoyed learning about quadratic equations and trigonometry for my Maths degree, and am struck by how many incremental steps along the way contributed to the total edifice of understanding.\n\n\n\n\n\n\nMar 25, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nBuilding Blocks For Better Stable Eights\n\n\n\n\n\n\n\ncomputervision\n\n\nfastai\n\n\nparttwo\n\n\n\n\nAn impromptu continuation of the last blog, where I use perceptual loss to get the updates to my random noise image that I wanted and finally manage to ‘generate’ an image of the digit eight.\n\n\n\n\n\n\nMar 18, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTricking my digits classifier with diffusion\n\n\n\n\n\n\n\ncomputervision\n\n\nfastai\n\n\nparttwo\n\n\n\n\nI accidentally built a way to adversarially generate handwritten images that seem to be of the number eight, but aren’t. This blog showcases an experiment I made around the core process going on in the generative diffusion process.\n\n\n\n\n\n\nMar 5, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nOn mathematical literacy\n\n\n\n\n\n\n\nmathematics\n\n\nmu123\n\n\nq31\n\n\n\n\nThinking aloud about how to tie a collection of mathematical ‘tricks’ and operations together in some sort of logical and rounded whole.\n\n\n\n\n\n\nJan 1, 2023\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nFrom the foundation up: Fashion-MNIST basics from Lesson 10\n\n\n\n\n\n\n\ncomputervision\n\n\nfastai\n\n\nparttwo\n\n\n\n\nNotes and some personal exploration following through the lesson 10 course materials from FastAI part 2. We cover the basics of loading in our data and generating our matrix.\n\n\n\n\n\n\nOct 24, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nDeep learning tricks all the way down, with a bit of mathematics for good measure\n\n\n\n\n\n\n\ncomputervision\n\n\nfastai\n\n\nparttwo\n\n\n\n\nNotes and reflections based on the first lesson (aka ‘lesson 9’) of the FastAI Part II course. This covers the fundamentals of Stable Diffusion, how it works and some core concepts or techniques.\n\n\n\n\n\n\nOct 17, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nAvoiding BIDMAS, or how J does notation\n\n\n\n\n\n\n\nj\n\n\nmathematics\n\n\nmu123\n\n\nq31\n\n\nnotation\n\n\n\n\nI learned about prefix, postfix and infix notation, and how J evaluates mathematical expressions which makes the BIDMAS rules unnecessary.\n\n\n\n\n\n\nOct 16, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nStoring Bytes: what data serialisation is and why you need it for machine learning\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\nmlops\n\n\npython\n\n\ntools\n\n\nzenml\n\n\n\n\nI explain the basics around data serialisation and deserialisation, why it’s a commonly-encountered topic, and showcase where I had to implement some custom logic to serialise custom Python objects used in a computer vision project.\n\n\n\n\n\n\nSep 7, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nIt takes a tribe: how I’m thinking about putting my object detection model into production\n\n\n\n\n\n\n\ntools\n\n\nredactionmodel\n\n\ncomputervision\n\n\nmlops\n\n\n\n\nThere are many pieces involved when deploying a model. This post covers the ones that relate to my object detection model and I explain how I’m going to put together the pipelines that will drive a continuous training loop once it’s all up.\n\n\n\n\n\n\nMay 31, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nMore Data, More Problems: Using DVC to handle data versioning for a computer vision problem\n\n\n\n\n\n\n\ntools\n\n\nredactionmodel\n\n\ncomputervision\n\n\nmlops\n\n\n\n\nI show you why you probably want to be versioning your data alongside your code. I introduce the basic functionality of DVC, the industry-standard tool for data versioning. I also explain specifically how I’m using DVC for my computer vision project.\n\n\n\n\n\n\nMay 24, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nRedaction Image Classifier: NLP Edition\n\n\n\n\n\n\n\nfastai\n\n\nnlp\n\n\npartone\n\n\n\n\nI train an NLP model to see how well it does at predicting whether an OCRed text contains a redaction or not. I run into a bunch of issues when training, leading me to conclude that training NLP models is more complicated than I’d at first suspected.\n\n\n\n\n\n\nMay 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\nA neural network for Fashion MNIST data\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\npartone\n\n\n\n\nThe final step of this series looking at chapter 4 of the fastai book tackles the final step where we construct a very simple 3-layer neural network which learns to distinguish a pullover from a dress.\n\n\n\n\n\n\nMay 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUsing the seven-step SGD process for Fashion MNIST\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\npartone\n\n\n\n\nI apply all the lessons we’ve learned so far on the Fashion MNIST dataset. This requires us learning a few new concepts like optimisers, ReLU, nonlinearity and so on.\n\n\n\n\n\n\nMay 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nStochastic Gradient Descent: a mini-example of the whole game\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\npartone\n\n\n\n\nThis short post shows how you iterate through a simple example of optimising three values as passed into a quadratic equation/function. We use SGD to optimise these.\n\n\n\n\n\n\nMay 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSome foundations for machine learning with PyTorch\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\npartone\n\n\n\n\nI outline the basic process that a computer uses when training a model, greatly simplified and all explained through the lens of PyTorch and how it calculates gradients. These are some pre-requisite foundations that we will later apply to our Fashion MNIST dataset.\n\n\n\n\n\n\nMay 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nA dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\npartone\n\n\n\n\nI read part of chapter four of the fastai course book, learning about a naive approach to image classification (sort of!)\n\n\n\n\n\n\nMay 11, 2022\n\n\n\n\n\n\n  \n\n\n\n\nA painless way to create an MVP demo using computer vision models\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\nredactionmodel\n\n\ntools\n\n\n\n\nI created a few deployed MVP demos showcasing models I’d created while participating in the fastai course, uploading them to the Huggingface Hub and using a Gradio Demo hosted on Huggingface Spaces.\n\n\n\n\n\n\nMay 7, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow my pet cat taught me a lesson about validation data for image classification\n\n\n\n\n\n\n\nfastai\n\n\ncomputervision\n\n\npartone\n\n\n\n\nI learn a valuable lesson about how a model often will ‘cheat’ when training and sometimes the solution is a separate held-out set of ‘test’ data which can give a more accurate assessment of how well the model is performing.\n\n\n\n\n\n\nMay 2, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)\n\n\n\n\n\n\n\ntools\n\n\nredactionmodel\n\n\ncomputervision\n\n\ndatavalidation\n\n\n\n\nIn this third and final post on data validation for the computer vision context, I cover some alternative tools that you might want to consider, from Evidently to the humble ‘assert’ statement. I conclude by setting out some guidelines for when you might want to be doing data validation and which tools might be more or less appropriate for your specific problem.\n\n\n\n\n\n\nApr 28, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)\n\n\n\n\n\n\n\ntools\n\n\nredactionmodel\n\n\ncomputervision\n\n\ndatavalidation\n\n\n\n\nIn this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of the Great Expectations library to get you started with increasing your confidence in your object detection annotations.\n\n\n\n\n\n\nApr 26, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)\n\n\n\n\n\n\n\ntools\n\n\nredactionmodel\n\n\ncomputervision\n\n\ndatavalidation\n\n\n\n\nAn overview of the problem that data validation seeks to solve, explored through the lens of an object detection problem and some of the tradeoffs that such an approach might bring. I introduce and simplify the high-level concepts you need to use the Great Expectations library.\n\n\n\n\n\n\nApr 19, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\n‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data\n\n\n\n\n\n\n\ntools\n\n\nredactionmodel\n\n\ncomputervision\n\n\n\n\nI show how adding synthetic data has improved my redaction model’s performance. Once I trained with the synthetic images added, I realised a more targeted approach would do even better.\n\n\n\n\n\n\nApr 6, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nSome characteristics of best-in-class ML portfolio projects\n\n\n\n\n\n\n\ncomputervision\n\n\nskillbuilding\n\n\n\n\nI wrote about some of the things that go into creating a really great portfolio project for machine learning. For this post I’m less interested in the technical achievements than I am in how it is presented.\n\n\n\n\n\n\nApr 4, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nBuilding my own image to use IceVision with Paperspace\n\n\n\n\n\n\n\ntools\n\n\ndocker\n\n\ncomputervision\n\n\n\n\nI setup a new Paperspace project that uses a custom Docker image to provision its environment, saving me a bunch of initial installation time and dependency bug pain. A huge productivity win!\n\n\n\n\n\n\nMar 25, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nStarting Docker In A Month Of Lunches\n\n\n\n\n\n\n\ntools\n\n\ndockerinamonthoflunches\n\n\nbooks-i-read\n\n\n\n\nI’m reading Elton Stoneman’s ‘Learn Docker in a Month of Lunches’ and blogging as I learn along the way. In chapters 1-3 we learn about the context for Docker as well as some basic commands for running and building containers.\n\n\n\n\n\n\nMar 21, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nFiguring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\ntools\n\n\ndebugging\n\n\njupyter\n\n\n\n\nI used the under-appreciated tool FiftyOne to analyse the ways that my object detection model is underperforming. For computer vision problems, it’s really useful to have visual debugging aids and FiftyOne is a well-documented and solid tool to help with that.\n\n\n\n\n\n\nMar 12, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nIncremental Improvements to my Redaction Detection Model\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\ntools\n\n\n\n\nI used a series of techniques to improve the performance of my model while creating a pathway to (hopefully) bigger gains going forward.\n\n\n\n\n\n\nMar 3, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nThree Python Helpers for Parsing Inputs\n\n\n\n\n\n\n\npython\n\n\ntools\n\n\n\n\nThe parse, yarl and datefinder packages are all ways in Python to help parse input data of different formats and types. Nothing essential here, but useful nonetheless.\n\n\n\n\n\n\nFeb 27, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nIt’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\npython\n\n\ntools\n\n\n\n\nI iterated through several prototypes to get to a script that could autogenerate synthetic training data for my computer vision model. I hoped to bootstrap my training to get a bit jump in model performance.\n\n\n\n\n\n\nFeb 10, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nWhat are invariants and how can they help make your Python classes more robust?\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nChapter 10 covers the last of the user-defined types explored in ‘Robust Python’: classes. We learn what an ‘invariant’ is and how to decide whether to use a data class or a class when rolling your own types.\n\n\n\n\n\n\nFeb 8, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nUpgrade your Python dicts with data classes\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nChapter 9 of ‘Robust Python’ dives into the uses of data classes, a user-defined datatype in which you can store heterogenous data together. They help formalise implicit concepts within your code and as a result also improve code readability.\n\n\n\n\n\n\nFeb 5, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow and where to use enums in Python\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nThe eight chapter of Patrick Viafore’s book, ‘Robust Python’, gets into enums which you can use when you have a grouping of some constants that belong together.\n\n\n\n\n\n\nJan 30, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nUsing mypy for Python type checking\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nReflections on the sixth and seventh chapters of Patrick Viafore’s book, ‘Robust Python’. We slowly wind down our discussion of type hints in Python code and think through using mypy and how to introduce type hints to a legacy codebase.\n\n\n\n\n\n\nJan 22, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nUsing type annotation with collections in Python\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nReflections on the fifth chapter of Patrick Viafore’s book, ‘Robust Python’. We learn about how to use type annotations when collections (lists, dictionaries and sets, primarily) are involved.\n\n\n\n\n\n\nJan 18, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nA Midway Report on my Computer Vision Project\n\n\n\n\n\n\n\npython\n\n\nfastai\n\n\ntools\n\n\nredactionmodel\n\n\n\n\nA report midway through my computer vision project to detect the presence of redactions on government documents.\n\n\n\n\n\n\nJan 16, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nDifferent ways to constrain types in Python\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nReflections on the fourth chapter of Patrick Viafore’s recent book, ‘Robust Python’. We learn about the different options for combining types and constraining exactly which sets of types are permitted for a particular function or variable signature.\n\n\n\n\n\n\nJan 8, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nLearning about ‘nbdev’ while building a Python package for PDF machine learning datasets\n\n\n\n\n\n\n\npython\n\n\njupyter\n\n\nfastai\n\n\ntools\n\n\n\n\nSome early thoughts on the benefits and possible drawbacks of using fastai’s ‘nbdev’ literate programming tool which is a suite of tools that allows you to Python software packages from Jupyter notebooks.\n\n\n\n\n\n\nJan 6, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nGetting practical with type annotations and mypy\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nReflections on the third chapter of Patrick Viafore’s recent book, ‘Robust Python’. We get some quick practical examples of how to use type annotation and how to use tools like mypy to analyse how typed values pass through your code.\n\n\n\n\n\n\nJan 3, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nCounter: a shortcut to counting iterables in Python\n\n\n\n\n\n\n\npython\n\n\n\n\nA nice little helper from the Python standard library\n\n\n\n\n\n\nJan 1, 2022\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s special about types in Python?\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nReflections on the second chapter of Patrick Viafore’s recent book, ‘Robust Python’. We learn about types and how they fit into Python.\n\n\n\n\n\n\nDec 30, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nWhat makes code robust?\n\n\n\n\n\n\n\nrobustpython\n\n\npython\n\n\nbooks-i-read\n\n\n\n\nReflections on the first chapter of Patrick Viafore’s recent book, ‘Robust Python’.\n\n\n\n\n\n\nDec 29, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nExploring J, an array programming language\n\n\n\n\n\n\n\nj\n\n\n\n\nWhat I have learned so far about why the J language exists and what problems it tries to solve.\n\n\n\n\n\n\nDec 29, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nA Taxonomy of Redaction\n\n\n\n\n\n\n\nredactionmodel\n\n\n\n\nA brief analysis of some of the types of redactions that are commonly found in FOIA documents. I use these as the dataset used to train an object detection model for redactions.\n\n\n\n\n\n\nDec 15, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\n73% accuracy for redaction object detection\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\nprogressreport\n\n\n\n\nI made some progress on my redaction model.\n\n\n\n\n\n\nDec 11, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nWhat is VFNet?\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\n\n\nSome basics I learned about the object detection model vfnet.\n\n\n\n\n\n\nNov 30, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nHow to annotate image data for object detection with Prodigy\n\n\n\n\n\n\n\nredactionmodel\n\n\ncomputervision\n\n\ndatalabelling\n\n\n\n\nHow I used Prodigy to annotate my data ahead of training an object detection model\n\n\n\n\n\n\nNov 29, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck your security vulnerabilities with safety\n\n\n\n\n\n\n\nsecurity\n\n\ntools\n\n\ncalmcode\n\n\n\n\nThe database is only updated once a month, but it is a useful check nonetheless.\n\n\n\n\n\n\nNov 27, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nLaunching a podcast about MLOps\n\n\n\n\n\n\n\nzenml\n\n\npodcast\n\n\nappearances\n\n\n\n\nI will be co-hosting a new podcast about MLOps called Pipeline Conversations.\n\n\n\n\n\n\nNov 27, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set and get environment variables using Python\n\n\n\n\n\n\n\npython\n\n\n\n\nA short post on setting environment variables using Python.\n\n\n\n\n\n\nNov 26, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nentr: a tool to run commands when files change\n\n\n\n\n\n\n\ndebugging\n\n\ntesting\n\n\ntools\n\n\ncalmcode\n\n\n\n\nentr is a useful tool to rerun things when watched files change. It’s especially useful when testing.\n\n\n\n\n\n\nNov 25, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn failure\n\n\n\n\n\n\n\ndebugging\n\n\nemotions\n\n\n\n\nSome reflections on the idea of what it is that we do as software engineers.\n\n\n\n\n\n\nNov 21, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome things I learned about debugging\n\n\n\n\n\n\n\ndebugging\n\n\n\n\nA few lessons I’ve learned about debugging at work in recent weeks\n\n\n\n\n\n\nOct 25, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nWriting Code\n\n\n\n\n\n\n\npython\n\n\nskillbuilding\n\n\n\n\nA reminder that early on, nothing really beats writing code for growing as a coder\n\n\n\n\n\n\nSep 18, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nReading Python Code\n\n\n\n\n\n\n\npython\n\n\nskillbuilding\n\n\n\n\nSome of the code libraries I plan on reading to improve my Pythonic style\n\n\n\n\n\n\nSep 18, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTensors all the way down\n\n\n\n\n\nDiscovering an optimised alternative to numpy arrays that are used from the ground up when you train deep learning models\n\n\n\n\n\n\nSep 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\nA Baseline Python Development Setup\n\n\n\n\n\n\n\npython\n\n\ntools\n\n\n\n\nGetting a development environment setup for Python and having to choose between pyenv vs virtualenv vs venv\n\n\n\n\n\n\nSep 14, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nSix problems TFX was trying to solve in 2017\n\n\n\n\n\n\n\ntfx\n\n\ntensorflow\n\n\ngoogle\n\n\nmlops\n\n\npapers-i-read\n\n\n\n\nI extracted the core problems that TensorFlow Extended (TFX) was looking to solve from its 2017 public launch paper.\n\n\n\n\n\n\nSep 11, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nRetrieval Practice with fastai chapters 1 and 2\n\n\n\n\n\nReviewing the main building blocks of the deep learning training workflow in the first chapters of the fastai book\n\n\n\n\n\n\nSep 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to set a Jupyter notebook to auto-reload external libraries\n\n\n\n\n\n\n\njupyter\n\n\n\n\nA small bit of Jupyter notebook magic\n\n\n\n\n\n\nSep 9, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nA Baseline Understanding of MLOps\n\n\n\n\n\n\n\nmlops\n\n\n\n\nWhat I understand of the domain, prior to starting to work in this area full-time\n\n\n\n\n\n\nSep 8, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\n  \n\n\n\n\nTraining a classifier to detect redacted documents with fastai\n\n\n\n\n\n\n\nfastai\n\n\nredactionmodel\n\n\ncomputervision\n\n\ndatalabelling\n\n\n\n\nHow I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training\n\n\n\n\n\n\nSep 6, 2021\n\n\nAlex Strick van Linschoten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am an ML Engineer working at ZenML. This blog is a place for me to process and share what I learn along the way.\nI built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. I have multiple years of experience in the Python, Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker.\nI have a PhD in History and authored several books based on my research work in Afghanistan. I have a different long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here."
  },
  {
    "objectID": "til/2024-09-16-what-is-the-rust-prelude.html",
    "href": "til/2024-09-16-what-is-the-rust-prelude.html",
    "title": "What is the Rust prelude?",
    "section": "",
    "text": "I’m studying Rust these days on the side and one thing that I keep hearing and seeing is the idea of the ‘prelude’. I thought I’d write a quick blog to cement exactly what’s going on here.\nAt a very high level, the prelude is a bunch of functions, methods and other things that are automatically available to you when you start working on your project without you having to manually or explicitly import them. As the Rust docs state:\n\n“The prelude is the list of things that Rust automatically imports into every Rust program. It’s kept as small as possible, and is focused on things, particularly traits, which are used in almost every single Rust program.”\n\nI thought maybe a good example of this is the classic ‘Hello, World!’ starter when you create a new project using cargo new ...:\nfn main() {\n    println!(\"Hello, World!\")\n}\nSo here we have println! which is actually a macro, and from what I read this is not part of the prelude, though it is available to us by default.\nA better / actual list of some things that are made available would include some types like Option, Result, String and Vec, as well as some traits like Copy, Clone, Eq and so on. For a full list, refer to the official prelude contents as listed in the docs. Note that there are several versions (2015, 2018, 2021 etc) of the prelude. My understanding is that each successive version only adds new things that are exported by default. If that wasn’t the case, then I’m guessing it would be hard to provide those solid backwards-compatibility guarantees.\nSo basically, there are some symbols or imports that were deemed to be used so often that they decided not to force you to have to import them explicitly every time you want to get started writing code."
  }
]